Title,Link,Abstract,Author Keywords,Index Keywords
"UAV-based structural damage mapping: A review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077571589&doi=10.3390%2fijgi9010014&partnerID=40&md5=708dc44e5a994554c9b93087d0c269bd","Structural disaster damage detection and characterization is one of the oldest remote sensing challenges, and the utility of virtually every type of active and passive sensor deployed on various air- and spaceborne platforms has been assessed. The proliferation and growing sophistication of unmanned aerial vehicles (UAVs) in recent years has opened up many new opportunities for damage mapping, due to the high spatial resolution, the resulting stereo images and derivatives, and the flexibility of the platform. This study provides a comprehensive review of how UAV-based damage mapping has evolved from providing simple descriptive overviews of a disaster science, to more sophisticated texture and segmentation-based approaches, and finally to studies using advanced deep learning approaches, as well as multi-temporal and multi-perspective imagery to provide comprehensive damage descriptions. The paper further reviews studies on the utility of the developed mapping strategies and image processing pipelines for first responders, focusing especially on outcomes of two recent European research projects, RECONASS (Reconstruction and Recovery Planning: Rapid and Continuously Updated Construction Damage, and Related Needs Assessment) and INACHUS (Technological and Methodological Solutions for Integrated Wide Area Situation Awareness and Survivor Localization to Support Search and Rescue Teams). Finally, recent and emerging developments are reviewed, such as recent improvements in machine learning, increasing mapping autonomy, damage mapping in interior, GPS-denied environments, the utility of UAVs for infrastructure mapping and maintenance, as well as the emergence of UAVs with robotic abilities. © 2019 by the authors.","CNN; Computer vision; Drone; First responder; GAN; INACHUS; Machine learning; Point clouds; RECONASS",
"MAPPING POVERTY in the PHILIPPINES USING MACHINE LEARNING, SATELLITE IMAGERY, and CROWD-SOURCED GEOSPATIAL INFORMATION","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081564635&doi=10.5194%2fisprs-archives-XLII-4-W19-425-2019&partnerID=40&md5=61a7b9267d467c27ddbe6bc89699e166","Mapping the distribution of poverty in developing countries is essential for humanitarian organizations and policymakers to formulate targeted programs and aid. However, traditional methods for obtaining socioeconomic data can be time-consuming, expensive, and labor-intensive. Recent studies have demonstrated the effectiveness of combining machine learning and satellite images to estimate wealth in sub-Saharan African countries (Xie et al., 2016, Jean et al., 2016). In this study, we investigate the extent to which this method can be applied in the context of the Philippine archipelago to predict four different socioeconomic indicators: wealth level, years of education, access to electricity, and access to water. We also propose an alternative, cost-effective approach that leverages a combination of volunteered geographic information from OpenStreetMap and nighttime lights satellite imagery for estimating socioeconomic indicators. The best models, which incorporate regional indicators as predictors, explain approximately 63% of the variation in asset-based wealth. Our findings also indicate that models trained on publicly available, volunteer-curated geographic data achieve the same predictive performance as that of models trained using proprietary satellite images. © 2020 Authors.","Deep learning; GIS; Machine learning; OpenStreetMap; Poverty estimation; Remote sensing; Transfer learning; Volunteered geographic information","Cost effectiveness; Crowdsourcing; Deep learning; Developing countries; Geographic information systems; Mapping; Remote sensing; Satellite imagery; Transfer learning; Cost-effective approach; Geo-spatial informations; Night-time lights; OpenStreetMap; Predictive performance; Socio-economic data; Socio-economic indicators; Volunteered geographic information; Learning systems"
"Research on remote sensing image data attack method based on machine deep learning network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081065949&doi=10.1145%2f3377713.3377787&partnerID=40&md5=5dd593639497e0e1e5fb575cad49a5e9","At present, with the breakthrough and application of in-depth learning technology in the field of artificial intelligence, the content of information acquired through interpretation of remote sensing images is more and more abundant, and how to guard information security has become a new technology hotspot. [1] In order to camouflage and conceal our important objects, it is necessary to attack the acquired remote sensing images to remove and confuse the target information and mislead the ""enemy"" to make wrong image analysis, which is a means to protect information security.[2] In this paper, the mainstream algorithm principle is introduced based on machine deep learning network technology. Aiming at remote sensing image data poisoning attack and sample attack in the process of deep learning network training, the purpose of tampering with original image data and hiding characteristic targets is realized. © 2019 ACM.","Data Attack; Deep Learning; Remote Sensing Image","Engineering education; Learning systems; Remote sensing; Security of data; Data attacks; Important object; Learning network; Learning technology; Original images; Poisoning attacks; Remote sensing images; Target information; Deep learning"
"Remote sensing and time series data fused multimodal prediction model based on interaction analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081181890&doi=10.1145%2f3376067.3376100&partnerID=40&md5=f2bd5b26d16f91983d468662d2ccba41","With the rapid development of the times, human's life is becoming more and more modern, helping people experience surrounding environment better. People can see attractive scenery, hear marvelous voice, smell fragrant flavor, touch soft objects and taste delicious food. All these feelings can be generalized by 'Modality'. As there are heterogeneous modalities, the way to learning from multiple such modalities become an emerging research topic. Multimodal machine learning has a wide range of applications while it still has many challenges. Challenges can be included in five categories: Representation, Translation, Alignment, Fusion, Co-learning. In this paper, we focus on the representation and fusion problem of multimodal and solve a practical problem of urban functional area classification. In this paper, we propose a scalable interaction model based on Squeezeand-Excitation block to fuse image modality from remote sensing images and temporal modality from user visit sequence. Crucially, our model produces improvements over typical multimodal methods. © 2019 Association for Computing Machinery.","Deep learning; Multi-classification; Multimodal; Remote sensing image; Time series data","Deep learning; Image processing; Time series; Time series analysis; Video signal processing; Interaction analysis; Multi-classification; Multi-modal; Practical problems; Remote sensing images; Surrounding environment; Temporal modalities; Time-series data; Remote sensing"
"Refining the features transferred from pre-trained inception architecture for aerial scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067875541&doi=10.1080%2f01431161.2019.1629716&partnerID=40&md5=e2f849fe58568f3c1b41bff94389eec7","Feature selection plays a vital role in image classification task. With the advent of deep learning, significant efforts have been made in developing deep architectures with the aim of getting relevant features. Moreover, deep architectures require large number of training data and also the training time is very high. Due to scarcity of high-dimensional remote-sensing images, many researchers adopted pre-trained deep architectures as feature extractors. When transferring these large number of features to train the classification layer, the network still overfits, yielding inaccurate classification results. To overcome this issue to some extent, we proposed two feature ranking methods to further refining the features transferred from pre-trained inception architecture for scene classification task. Another important fact is that down sampling large images to the input size of the pre-trained architecture causes loss of some informative pixels, so in our framework, we fed image patches of size equal to the size of the input layer of the pre-trained inception and then the features extracted from all the patches are combined. Finally, we compared our framework with some existing methods and obtained acceptable results. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",,"Antennas; Deep learning; Image processing; Importance sampling; Network layers; Refining; Remote sensing; Classification results; Deep architectures; Feature extractor; Feature ranking; High-dimensional; Relevant features; Remote sensing images; Scene classification; Network architecture; aerial survey; architecture; conceptual framework; image analysis; image classification; pixel; remote sensing"
"Classification of the functions of urban area based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086070149&doi=10.1145%2f3386415.3387103&partnerID=40&md5=e3c87378b9ece21035de305335cf0b2e","Classification of the Functions of Urban Area is of great significance to urban construction and refined management. High-resolution remote sensing images are widely used in urban area functional classification, and urban area functions are closely related to human activities. Therefore, in the urban area functional classification model, the effects of remote sensing images and user behavior will be significantly improved. Firstly, the remote sensing image acquired by the satellite is random. We need to delete the all black or all white image and then perform the defogging operation to make the feature more obvious; Secondly, the user behavior data is processed as a three-dimensional image according to the divided time; Finally, the densenet, resnet, se-resnext, inception network models are integrated to extract image data and text data for classification. The experimental results show that compared with other methods, our proposed method has a good effect on the classification of the functions of urban area which based on remote sensing image data and user behavior data. © 2019 ACM.","Image Classification; Model Fusion; Remote Sensing Image","Behavioral research; Image enhancement; Remote sensing; Text processing; Functional classification; High resolution remote sensing images; Human activities; Network models; Remote sensing images; Three dimensional images; Urban construction; User behaviors; Deep learning"
"Landslide detection of hyperspectral remote sensing data based on deep learning with constrains","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079317514&doi=10.1109%2fJSTARS.2019.2951725&partnerID=40&md5=1ac76a0cc8e1a99d54bc9846de84f0b0","Detecting and monitoring landslides are hot topics in remote sensing community, particularly with the development of remote sensing technologies and the significant progress of computer vision. To the best of our knowledge, no study focused on deep learning-based methods for landslide detection on hyperspectral images. We proposes a deep learning framework with constraints to detect landslides on hyperspectral image. The framework consists of two steps. First, a deep belief network is employed to extract the spectral-spatial features of a landslide. Second, we insert the high-level features and constraints into a logistic regression classifier for verifying the landslide. Experimental results demonstrated that the framework can achieve higher overall accuracy when compared to traditional hyperspectral image classification methods. The precision of the landslide detection on the whole image, obtained by the proposed method, can reach 97.91%, whereas the precision of the linear support vector machine, spectral information divergence, and spectral angle match are 94.36%, 84.50%, and 86.44%, respectively. Also, this article reveals that the high-level feature extraction system has a significant potential for landslide detection, especially in multi-source remote sensing. © 2008-2012 IEEE.","Deep belief network (DBN); deep learning; feature extraction; hyperspectral data; landslide","Extraction; Feature extraction; Landslides; Logistic regression; Remote sensing; Spectroscopy; Support vector machines; Deep belief network (DBN); High-level feature extractions; Hyperspectral Data; Hyperspectral remote sensing data; Linear Support Vector Machines; Logistic regression classifier; Remote sensing technology; Spectral information divergences; Deep learning; image classification; landslide; machine learning; numerical model; regression analysis; remote sensing; satellite data; satellite imagery"
"Next generation mapping: Combining deep learning, cloud computing, and big remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076550652&doi=10.3390%2frs11232881&partnerID=40&md5=95b7e65d6b45d1317d23fe7e215d2a55","The rapid growth of satellites orbiting the planet is generating massive amounts of data for Earth science applications. Concurrently, state-of-the-art deep-learning-based algorithms and cloud computing infrastructure have become available with a great potential to revolutionize the image processing of satellite remote sensing. Within this context, this study evaluated, based on thousands of PlanetScope images obtained over a 12-month period, the performance of three machine learning approaches (random forest, long short-term memory-LSTM, and U-Net). We applied these approaches to mapped pasturelands in a Central Brazil region. The deep learning algorithms were implemented using TensorFlow, while the random forest utilized the Google Earth Engine platform. The accuracy assessment presented F1 scores for U-Net, LSTM, and random forest of, respectively, 96.94%, 98.83%, and 95.53% in the validation data, and 94.06%, 87.97%, and 82.57% in the test data, indicating a better classification efficiency using the deep learning approaches. Although the use of deep learning algorithms depends on a high investment in calibration samples and the generalization of these methods requires further investigations, our results suggest that the neural network architectures developed in this study can be used to map large geographic regions that consider a wide variety of satellite data (e.g., PlanetScope, Sentinel-2, Landsat-8). © 2019 by the authors.","Deeplearning; LSTM; LULC classification; Machine learning; Planet Scope; Randomforest; U-Net","Cloud computing; Decision trees; Earth (planet); Image processing; Investments; Learning algorithms; Learning systems; Long short-term memory; Machine learning; Network architecture; Orbits; Remote sensing; Satellites; Classification efficiency; Cloud computing infrastructures; Learning-based algorithms; LSTM; Machine learning approaches; Randomforest; Satellite remote sensing; Science applications; Deep learning"
"Remote sensing and texture image classification network based on deep learning integrated with binary coding and sinkhorn distance","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076561725&doi=10.3390%2frs11232870&partnerID=40&md5=09eda4af82fc5f44ad65223656b28b6b","In the past two decades, traditional hand-crafted feature based methods and deep feature based methods have successively played the most important role in image classification. In some cases, hand-crafted features still provide better performance than deep features. This paper proposes an innovative network based on deep learning integrated with binary coding and Sinkhorn distance (DBSNet) for remote sensing and texture image classification. The statistical texture features of the image extracted by uniform local binary pattern (ULBP) are introduced as a supplement for deep features extracted by ResNet-50 to enhance the discriminability of features. After the feature fusion, both diversity and redundancy of the features have increased, thus we propose the Sinkhorn loss where an entropy regularization term plays a key role in removing redundant information and training the model quickly and efficiently. Image classification experiments are performed on two texture datasets and five remote sensing datasets. The results show that the statistical texture features of the image extracted by ULBP complement the deep features, and the new Sinkhorn loss performs better than the commonly used softmax loss. The performance of the proposed algorithm DBSNet ranks in the top three on the remote sensing datasets compared with other state-of-the-art algorithms. © 2019 by the authors.","Deep features; Hand-crafted features; Image classification; Sinkhorn loss","Binary codes; Deep learning; Image classification; Image coding; Image enhancement; Image texture; Network coding; Remote sensing; Textures; Classification networks; Deep features; Feature-based method; Hand-crafted features; Regularization terms; State-of-the-art algorithms; Statistical texture features; Uniform local binary patterns; Classification (of information)"
"Analysis of remote sensing imagery for disaster assessment using deep learning: a case study of flooding event","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062834349&doi=10.1007%2fs00500-019-03878-8&partnerID=40&md5=bcb5be80f292cf83539f03c74b46f8c5","This paper proposes a methodology that integrates deep learning and machine learning for automatically assessing damage with limited human input in hundreds of thousands of aerial images. The goal is to develop a system that can help automatically identifying damaged areas in massive amount of data. The main difficulty consists in damaged infrastructure looking very different from when undamaged, likely resulting in an incorrect classification because of their different appearance, and the fact that deep learning and machine learning training sets normally only include undamaged infrastructures. In the proposed method, a deep learning algorithm is firstly used to automatically extract the presence of critical infrastructure from imagery, such as bridges, roads, or houses. However, because damaged infrastructure looks very different from when undamaged, the set of features identified can contain errors. A small portion of the images are then manually labeled if they include damaged areas, or not. Multiple machine learning algorithms are used to learn attribute–value relationships on the labeled data to capture the characteristic features associated with damaged areas. Finally, the trained classifiers are combined to construct an ensemble max-voting classifier. The selected max-voting model is then applied to the remaining unlabeled data to automatically identify images including damaged infrastructure. Evaluation results (85.6% accuracy and 89.09% F1 score) demonstrated the effectiveness of combining deep learning and an ensemble max-voting classifier of multiple machine learning models to analyze aerial images for damage assessment. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Damage assessment; Deep learning; Image classification; Machine learning; Spatiotemporal data; TensorFlow","Antennas; Bridges; Classification (of information); Damage detection; Deep learning; Image analysis; Image classification; Learning systems; Machine learning; Remote sensing; Damage assessments; Damaged infrastructure; Evaluation results; Multiple machine; Remote sensing imagery; Spatio-temporal data; TensorFlow; Voting classifiers; Learning algorithms"
"Novel Deep-Learning-Based Spatial-Spectral Feature Extraction for Hyperspectral Remote Sensing Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081412468&doi=10.1109%2fBigData47090.2019.9006210&partnerID=40&md5=e07b53a2deed14eb6c16b9c43556229a","Hyperspectral remote sensing presents a unique Big Data research paradigm through its rich data collected as hundreds of spectral bands which embodies vital spatial and spectral information about the underlying terrains. Typical hyperspectral data analysis methods are often based on spectral information. Although there has been prior efforts in literature for incorporation of spatial, spectral, contextual and other forms of information to improve the classification performance of hyperspectral data analysis, this additional information extraction and knowledge discovery process comes at the expense of increased computation and memory requirements. Therefore, the caveats of large scale data analysis such as increased computation, transmission and memory requirements presents a major impediment to efficient automation and classification performance of hyperspectral data analysis methods. In this respect, this paper presents a novel deep learning-based hyperspectral data analysis model, which provides an efficient means for automation and extraction of the spatial and spectral information present in the hyperspectral data compared to conventional spatial-or spectral information-only based methods. In this work, the concept of Gabor filtering is used for spatial feature extraction along with sparse random projections for computationally efficient spectral feature extraction and dimensionality reduction purposes. A convolutional neural network based supervised classification is then performed to validate the performance of the proposed method with respect to conventional spatial-spectral information extraction methods. Experimental results reveal that the proposed hyperspectral data analysis model outperforms the conventional spectral-spatial feature extraction techniques compared. © 2019 IEEE.","Convolutional Neural Network; Dimensionality Reduction; Feature Extraction; Gabor Filtering; Gaussian Filtering; Hyperspectral Remote Sensing; Principal Component Analysis; Sparse Random Projection","Big data; Convolution; Convolutional neural networks; Data handling; Data mining; Deep learning; Dimensionality reduction; Feature extraction; Gabor filters; Information retrieval; Principal component analysis; Remote sensing; Gabor filtering; Gaussian filtering; Hyperspectral data analysis; Hyperspectral remote sensing; Knowledge discovery process; Sparse random projections; Spectral feature extraction; Spectral information extraction; Classification (of information)"
"Aircraft Segmentation Based on Deep Learning framework : from extreme points to remote sensing image segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080862342&doi=10.1109%2fSSCI44817.2019.9002656&partnerID=40&md5=6173453f4f7aa8bb9c1950645680fcce","Remote sensing image segmentation is a very important technology. Although the segmentation method based on convolutional neural networks (CNNs) has achieved promising results in natural image test set, e.g. VOC or COCO, they provide inferior performance when being transferred to remote sensing images. Due to the limits of labeled remote sensing images, finetuning pre-trained CNNs using remote sensing images do not benefit the image segmentation performance. Inspired by the recent works of interactive segmentation methods which exploit several extreme clicks that are fed into CNNs to improve the accuracy of the segmentation, we propose an effective method to improve the segmentation accuracy, which uses four extreme points (the top, bottom, left, and right) as the guide information. In terms of mIoU, our method achieves 84.4% on remote sensing image dataset, which outperforms the previous work by 23.1%. Compared with the previous interactive segmentation methods, the proposed method achieves superior performance. In addition, an improved method with an extra point is proposed based on the inaccurate part of results obtained by four extreme points. It is very feasible to be applied in an interactive segmentation toolbox. © 2019 IEEE.","Deep learning; Interactive segmentation; Remote sensing images; Semantic segmentation","Aircraft; Convolutional neural networks; Deep learning; Remote sensing; Semantics; Interactive segmentation; Interactive segmentation methods; Learning frameworks; Remote sensing images; Segmentation accuracy; Segmentation methods; Segmentation performance; Semantic segmentation; Image segmentation"
"Remote sensing big data classification with high performance distributed deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077880981&doi=10.3390%2frs11243056&partnerID=40&md5=e25e2fc145780cd862b3e889e7dfc7f2","High-Performance Computing (HPC) has recently been attracting more attention in remote sensing applications due to the challenges posed by the increased amount of open data that are produced daily by Earth Observation (EO) programs. The unique parallel computing environments and programming techniques that are integrated in High-Performance Computing (HPC) systems are able to solve large-scale problems such as the training of classification algorithms with large amounts of Remote Sensing (RS) data. This paper shows that the training of state-of-the-art deep Convolutional Neural Networks (CNNs) can be efficiently performed in distributed fashion using parallel implementation techniques on HPC machines containing a large number of Graphics Processing Units (GPUs). The experimental results confirm that distributed training can drastically reduce the amount of time needed to perform full training, resulting in near linear scaling without loss of test accuracy. © 2019 by the authors.","Classification; Convolutional neural network; Distributed deep learning; High performance computing; Residual neural network; Sentinel-2","Application programs; Big data; Classification (of information); Computer graphics; Convolution; Graphics processing unit; Neural networks; Open Data; Program processors; Remote sensing; Convolutional neural network; High performance computing; High performance computing (HPC); High performance computing systems; Parallel implementations; Parallel-computing environment; Remote sensing applications; Sentinel-2; Deep neural networks"
"Urban Land Use and Land Cover Change Prediction via Self-Adaptive Cellular Based Deep Learning with Multisourced Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079344015&doi=10.1109%2fJSTARS.2019.2956318&partnerID=40&md5=a54a8433f1743ebcc0cee21bb5eaa722","The urban sustainable development becomes an essential goal presented in '2030 Agenda for Sustainable development' by the United Nations. Urban land use and land cover (LULC) change prediction, which is a significant indicator to evaluate urban construction strategy, urges to be solved. Remote sensing technique and neural network, are two practical tools widely utilized in urban LULC change prediction. Because of the rapid improvement of remote sensing technique, urban data can be periodically captured in a short time interval. A large amount of data cause the conventional neural networks method having bad efficiency in dealing with it. Moreover, as human activities are in high intensity in the urban area, society related factors require to be taken into consideration in urban land change trend prediction. In this article, a self-adaptive cellular-based deep learning analysis method by utilizing the multisourced data is proposed for urban LULC change prediction. Multisourced data, including weather related data, economy related data, construction related data, and remote sensing data, are normalized and formalized by the proposed self-adaptive cellular-based method. Deep learning long short term memory neural network, which is an advanced model of recurrent neural network and has a strong ability in dealing with sequence data, is utilized to urban LULC change prediction. Remote sensing data captured from 1984 to 2016 are utilized to conduct experiments. Experiment results illustrate that the proposed method can effectively and efficiently make LULC change prediction, and the accuracy is up to 93.1%. © 2008-2012 IEEE.","Deep learning; long short term memory (LSTM) neural network; remote sensing; self-adaptive cellular-based method; urban land use and land cover (LULC) change prediction","Brain; Cellular neural networks; Deep neural networks; Forecasting; Land use; Learning systems; Long short-term memory; Planning; Remote sensing; Sustainable development; Advanced modeling; Change prediction; Remote sensing data; Remote sensing techniques; self-adaptive cellular-based method; Short time intervals; Urban construction; Urban sustainable development; Deep learning; artificial neural network; experimental study; human activity; land cover; land use change; prediction; remote sensing; satellite data; sustainable development; urban area"
"Color calibration of proximal sensing RGB images of oilseed rape canopy via deep learning combined with k-means algorithm","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077878467&doi=10.3390%2frs11243001&partnerID=40&md5=c93ee9b8ecbb08a6ba25ce622c87ff26","Plant color is a key feature for estimating parameters of the plant grown under different conditions using remote sensing images. In this case, the variation in plant color should be only due to the influence of the growing conditions and not due to external confounding factors like a light source. Hence, the impact of the light source in plant color should be alleviated using color calibration algorithms. This study aims to develop an efficient, robust, and cutting-edge approach for automatic color calibration of three-band (red green blue: RGB) images. Specifically, we combined the k-means model and deep learning for accurate color calibration matrix (CCM) estimation. A dataset of 3150 RGB images for oilseed rape was collected by a proximal sensing technique under varying illumination conditions and used to train, validate, and test our proposed framework. Firstly, we manually derived CCMs by mapping RGB color values of each patch of a color chart obtained in an image to standard RGB (sRGB) color values of that chart. Secondly, we grouped the images into clusters according to the CCM assigned to each image using the unsupervised k-means algorithm. Thirdly, the images with the new cluster labels were used to train and validate the deep learning convolutional neural network (CNN) algorithm for an automatic CCM estimation. Finally, the estimated CCM was applied to the input image to obtain an image with a calibrated color. The performance of our model for estimating CCM was evaluated using the Euclidean distance between the standard and the estimated color values of the test dataset. The experimental results showed that our deep learning framework can efficiently extract useful low-level features for discriminating images with inconsistent colors and achieved overall training and validation accuracies of 98.00% and 98.53%, respectively. Further, the final CCM provided an average Euclidean distance of 16.23 DE and outperformed the previously reported methods. This proposed technique can be used in real-time plant phenotyping at multiscale levels. © 2019 by the authors.","Color calibration; Deep learning; k-means algorithm; Multivariate regression; Plant phenotyping","Calibration; Color; K-means clustering; Learning algorithms; Light sources; Neural networks; Oilseeds; Regression analysis; Remote sensing; Statistical tests; Automatic color calibration; Color calibration; Convolutional neural network; Estimating parameters; Illumination conditions; Multivariate regression; Plant phenotyping; Remote sensing images; Deep learning"
"Deep learning classifiers for hyperspectral imaging: A review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075120360&doi=10.1016%2fj.isprsjprs.2019.09.006&partnerID=40&md5=588a5e62e0740464b9f70cf898565ec1","Advances in computing technology have fostered the development of new and powerful deep learning (DL) techniques, which have demonstrated promising results in a wide range of applications. Particularly, DL methods have been successfully used to classify remotely sensed data collected by Earth Observation (EO) instruments. Hyperspectral imaging (HSI) is a hot topic in remote sensing data analysis due to the vast amount of information comprised by this kind of images, which allows for a better characterization and exploitation of the Earth surface by combining rich spectral and spatial information. However, HSI poses major challenges for supervised classification methods due to the high dimensionality of the data and the limited availability of training samples. These issues, together with the high intraclass variability (and interclass similarity) –often present in HSI data– may hamper the effectiveness of classifiers. In order to solve these limitations, several DL-based architectures have been recently developed, exhibiting great potential in HSI data interpretation. This paper provides a comprehensive review of the current-state-of-the-art in DL for HSI classification, analyzing the strengths and weaknesses of the most widely used classifiers in the literature. For each discussed method, we provide quantitative results using several well-known and widely used HSI scenes, thus providing an exhaustive comparison of the discussed techniques. The paper concludes with some remarks and hints about future challenges in the application of DL techniques to HSI classification. The source codes of the methods discussed in this paper are available from: https://github.com/mhaut/hyperspectral_deeplearning_review. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Classification; Deep learning (DL); Earth observation (EO); Hyperspectral imaging (HSI)","Classification (of information); Hyperspectral imaging; Observatories; Remote sensing; Spectroscopy; Amount of information; Computing technology; Data interpretation; Earth observations; Learning classifiers; Remotely sensed data; Spatial informations; Supervised classification; Deep learning; image classification; imaging method; instrumentation; literature review; machine learning; remote sensing; satellite data; spectral analysis; supervised classification"
"Matching of TerraSAR-X derived ground control points to optical image patches using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074359168&doi=10.1016%2fj.isprsjprs.2019.09.010&partnerID=40&md5=c5cb99f3978c7cf17ce3d9bb9dd89bcc","High resolution synthetic aperture radar (SAR) satellites like TerraSAR-X are capable of acquiring images exhibiting an absolute geolocation accuracy within a few centimeters, mainly because of the availability of precise orbit information and by compensating range delay errors due to atmospheric conditions. In contrast, satellite images from optical missions generally exhibit comparably low geolocation accuracies because of the propagation of errors in angular measurements over large distances. However, a variety of remote sensing applications, such as change detection, surface movement monitoring or ice flow measurements, require precisely geo-referenced and co-registered satellite images. By using Ground Control Points (GCPs) derived from TerraSAR-X, the absolute geolocation accuracy of optical satellite images can be improved. For this purpose, the corresponding matching points in the optical images need to be localized. In this paper, a deep learning based approach is investigated for an automated matching of SAR-derived GCPs to optical image elements. Therefore, a convolutional neural network is pretrained with medium resolution Sentinel-1 and Sentinel-2 imagery and fine-tuned on precisely co-registered TerraSAR-X and Pléiades training image pairs to learn a common descriptor representation. By using these descriptors, the similarity of SAR and optical image patches can be calculated. This similarity metric is then used in a sliding window approach to identify the matching points in the optical reference image. Subsequently, the derived points can be utilized for co-registration of the underlying images. The network is evaluated over nine study areas showing airports and their rural surroundings from several different countries around the world. The results show that based on TerraSAR-X-derived GCPs, corresponding points in the optical image can automatically and reliably be identified with a pixel-level localization accuracy. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; GCP matching; Geolocation accuracy improvement; Multi-sensor image matching; Optical satellite images; Synthetic aperture radar","Angle measurement; Backpropagation; Deep learning; Geometrical optics; Neural networks; Orbits; Radar imaging; Remote sensing; Rock mechanics; Satellites; Space-based radar; Synthetic aperture radar; Accuracy Improvement; Convolutional neural network; Descriptor representation; GCP matching; High resolution synthetic aperture radar; Multi sensor images; Optical satellite images; Remote sensing applications; Image enhancement; algorithm; ground control; ice flow; image analysis; measurement method; pixel; remote sensing; satellite imagery; Sentinel; synthetic aperture radar; terrane; TerraSAR-X"
"Single remote sensing image dehazing using a prior-based dense attentive network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077904619&doi=10.3390%2frs11243008&partnerID=40&md5=6f7c2ec983a725168a2c46f5f9ff19e5","Remote sensing image dehazing is an extremely complex issue due to the irregular and non-uniform distribution of haze. In this paper, a prior-based dense attentive dehazing network (DADN) is proposed for single remote sensing image haze removal. The proposed network, which is constructed based on dense blocks and attention blocks, contains an encoder-decoder architecture, which enables it to directly learn the mapping between the input images and the corresponding haze-free image, without being dependent on the traditional atmospheric scattering model (ASM). To better handle non-uniform hazy remote sensing images, we propose to combine a haze density prior with deep learning, where an initial haze density map (HDM) is firstly extracted from the original hazy image, and is subsequently utilized as the input of the network, together with the original hazy image. Meanwhile, a large-scale hazy remote sensing dataset is created for training and testing of the proposed method, which contains both uniform and non-uniform, synthetic and real hazy remote sensing images. Experimental results on the created dataset illustrate that the developed dehazing method obtains significant progresses over the state-of-the-art methods. © 2019 by the authors.","Convolutional neural network; Haze removal; Non-uniform haze; Remote sensing image","Deep learning; Demulsification; Large dataset; Neural networks; Statistical tests; Atmospheric scattering models; Convolutional neural network; Encoder-decoder architecture; Haze removal; Non-uniform; Non-uniform distribution; Remote sensing images; State-of-the-art methods; Remote sensing"
"An Optimized Residual Network with Block-soft Clustering for Road Extraction from Remote Sensing Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081175535&doi=10.1109%2fIAEAC47372.2019.8997695&partnerID=40&md5=5366e78a48697fcd27449a1d2ea578ca","the task of road extraction from remote sensing imagery faces many challenges, traditional methods require complex extracting processes with relatively low precision. Deep learning methods such as convolutional neural network, VggNet, AlexNet, GoogleNet can obtain higher accuracy of road extraction, but requires lots of computing resources, training time and unsatisfactory real-time performance. Based on the reasons mentioned above, this paper proposes an optimized residual network with block-soft clustering (ORNBSC) for road extraction from remote sensing imagery. The block-soft clustering module aims at extracting essential features from satellite images and reducing the dimensionality, therefore accelerating the extraction speed. Meanwhile, the residual neural network module to improve the accuracy of extraction. Groups of experiments using Massachusetts roads dataset demonstrate that the ORNBSC model achieves better performance than traditional methods on precision of road extraction from remote sensing imagery. © 2019 IEEE.","block-soft clustering; optimal residual network; remote sensing; road extraction","Convolutional neural networks; Deep learning; Extraction; Feature extraction; Learning systems; Roads and streets; Computing resource; Essential features; Learning methods; Real time performance; Remote sensing imagery; Road extraction; Satellite images; Soft clustering; Remote sensing"
"Content Based Image Retrieval - Inspired by Computer Vision Deep Learning Techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089222829&doi=10.1109%2fICEECCOT46775.2019.9114610&partnerID=40&md5=b0ed14eaf832c705dfa1393a3e8af7c1","With the growing popularity of digital services and internet technology billions of people are prone to information sharing and uploading photos. For accurate retrieval of images from huge digital image databases, Content Based Image Retrieval (CBIR) method are emerging as an influential next generation tools, with wide range of applications in fields like criminal investigation, shape recognition, medical diagnosis, remote sensing, digital forensic, radar engineering and robotics. The key challenge of CBIR systems lies in building the 'semantic gap' that exists between the differences in the way of perceiving things from basic to complex image features. The intention behind our paper is to provide deeper analysis and impact of continuous advancements in Image Retrieval techniques since its origin. An empirical examination of most popular and upcoming techniques in this survey paved the way in selecting the suitable computer vision and Deep Learning Techniques to improve the performances of retrieval systems. Several Benchmark datasets are considered to validate and study the robustness of the techniques influencing feature extraction and classification. © 2019 IEEE.","Benchmark Datasets; Classification; Computer Vision; Deep learning; Feature Extraction; Image Retrieval; Semantic gap","Classification (of information); Computer vision; Content based retrieval; Diagnosis; Digital forensics; Information services; Learning systems; Medical imaging; Medical robotics; Remote sensing; Search engines; Semantics; Content-Based Image Retrieval; Criminal investigation; Digital image database; Empirical examination; Feature extraction and classification; Image retrieval techniques; Internet technology; Learning techniques; Deep learning"
"Remote sensing scene classification based on rotation-invariant feature learning and joint decision making","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059629589&doi=10.1186%2fs13640-018-0398-z&partnerID=40&md5=339f4cbbd1b45f9d9941a968b9a28f53","With the popular use of high-resolution satellite images, remote sensing scene classification has always been a hot research topic in its related areas. However, limited to the issues of remote sensing datasets including the small scale of scene classes, the lack of rich label information and so on, it is quite challenging for deep learning methods to learn powerful feature representation. To overcome this problem, we propose a rotation-invariant feature learning and joint decision-making method based on Siamese convolutional neural networks with the combination of identification and verification models. Firstly, a novel data augmentation strategy is proposed specially for the Siamese model to learning rotation-invariant features. Secondly, a joint decision mechanism is introduced in our method, which is realized by the identification and verification model to better improve the classification performance. The proposed method can not only suppress problems caused by lack of rich label samples but also improve the robustness of Siamese convolutional neural networks. Experimental results demonstrate that the proposed method is effective and efficient for remote sensing scene classification. © 2019, The Author(s).","Identification and verification models; Joint decision mechanism; Remote sensing scene classification; Rotation-invariant features","Classification (of information); Convolution; Decision making; Deep learning; Neural networks; Rotation; Classification performance; Convolutional neural network; Feature representation; High resolution satellite images; Joint decisions; Rotation invariant features; Scene classification; Verification model; Remote sensing"
"Unsupervised deep slow feature analysis for change detection in multi-temporal remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075656339&doi=10.1109%2fTGRS.2019.2930682&partnerID=40&md5=5b98702f5eb4452d89fd193fa3925787","Change detection has been a hotspot in the remote sensing technology for a long time. With the increasing availability of multi-temporal remote sensing images, numerous change detection algorithms have been proposed. Among these methods, image transformation methods with feature extraction and mapping could effectively highlight the changed information and thus has a better change detection performance. However, the changes of multi-temporal images are usually complex, and the existing methods are not effective enough. In recent years, the deep network has shown its brilliant performance in many fields, including feature extraction and projection. Therefore, in this paper, based on the deep network and slow feature analysis (SFA) theory, we proposed a new change detection algorithm for multi-temporal remotes sensing images called deep SFA (DSFA). In the DSFA model, two symmetric deep networks are utilized for projecting the input data of bi-temporal imagery. Then, the SFA module is deployed to suppress the unchanged components and highlight the changed components of the transformed features. The change vector analysis pre-detection is employed to find unchanged pixels with high confidence as training samples. Finally, the change intensity is calculated with chi-square distance and the changes are determined by threshold algorithms. The experiments are performed on two real-world data sets and a public hyperspectral data set. The visual comparison and the quantitative evaluation have shown that DSFA could outperform the other state-of-the-art algorithms, including other SFA-based and deep learning methods. © 1980-2012 IEEE.","Change detection; deep network; remote sensing images; slow feature analysis (SFA)","Deep learning; Extraction; Image analysis; Remote sensing; Signal detection; Change detection; Change detection algorithms; Multi-temporal remote sensing; Quantitative evaluation; Remote sensing images; Remote sensing technology; Slow Feature Analysis(SFA); State-of-the-art algorithms; Feature extraction; algorithm; data set; detection method; mapping method; remote sensing; satellite data; satellite imagery; unsupervised classification"
"Deep green diagnostics: Urban green space analysis using deep learning and drone images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076113252&doi=10.3390%2fs19235287&partnerID=40&md5=5faae042ae810258eec1bf3f0efba6df","Nowadays, more than half of the world’s population lives in urban areas, and this number continues increasing. Consequently, there are more and more scientific publications that analyze health problems of people associated with living in these highly urbanized locations. In particular, some of the recent work has focused on relating people’s health to the quality and quantity of urban green areas. In this context, and considering the huge amount of land area in large cities that must be supervised, our work seeks to develop a deep learning-based solution capable of determining the level of health of the land and to assess whether it is contaminated. The main purpose is to provide health institutions with software capable of creating updated maps that indicate where these phenomena are presented, as this information could be very useful to guide public health goals in large cities. Our software is released as open source code, and the data used for the experiments presented in this paper are also freely available. © 2019 by the authors.","Biomass analysis; Deep learning (for social good); Remote sensing","Drones; Health; Open source software; Open systems; Remote sensing; Land areas; Large cities; Open-source code; Scientific publications; Urban areas; Urban green area; Urban green spaces; Deep learning; biomass; environmental monitoring; human; procedures; remote sensing; software; Biomass; Deep Learning; Environmental Monitoring; Humans; Remote Sensing Technology; Software"
"Combining Sentinel-1 and Sentinel-2 Satellite Image Time Series for land cover mapping via a multi-source deep learning architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072703005&doi=10.1016%2fj.isprsjprs.2019.09.016&partnerID=40&md5=abb0a9ddc70d80fe0f8fae4808b05b9f","The huge amount of data currently produced by modern Earth Observation (EO) missions has allowed for the design of advanced machine learning techniques able to support complex Land Use/Land Cover (LULC) mapping tasks. The Copernicus programme developed by the European Space Agency provides, with missions such as Sentinel-1 (S1) and Sentinel-2 (S2), radar and optical (multi-spectral) imagery, respectively, at 10 m spatial resolution with revisit time around 5 days. Such high temporal resolution allows to collect Satellite Image Time Series (SITS) that support a plethora of Earth surface monitoring tasks. How to effectively combine the complementary information provided by such sensors remains an open problem in the remote sensing field. In this work, we propose a deep learning architecture to combine information coming from S1 and S2 time series, namely TWINNS (TWIn Neural Networks for Sentinel data), able to discover spatial and temporal dependencies in both types of SITS. The proposed architecture is devised to boost the land cover classification task by leveraging two levels of complementarity, i.e., the interplay between radar and optical SITS as well as the synergy between spatial and temporal dependencies. Experiments carried out on two study sites characterized by different land cover characteristics (i.e., the Koumbia site in Burkina Faso and Reunion Island, a overseas department of France in the Indian Ocean), demonstrate the significance of our proposal. © 2019","Data fusion; Deep learning; Land cover classification; Satellite Image Time Series; Sentinel-1; Sentinel-2","Architecture; Data fusion; Land use; Mapping; Network architecture; Radar imaging; Remote sensing; Satellites; Space applications; Space-based radar; Time series; High temporal resolution; Land cover classification; Learning architectures; Machine learning techniques; Proposed architectures; Satellite images; Sentinel-1; Sentinel-2; Deep learning; data acquisition; land cover; machine learning; mapping method; remote sensing; satellite imagery; Sentinel; spatial resolution; time series analysis; Burkina Faso; Mascarene Islands; Reunion"
"Spatial–Spectral Jointed Stacked Auto-Encoder-Based Deep Learning for Oil Slick Extraction from Hyperspectral Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073962735&doi=10.1007%2fs12524-019-01045-y&partnerID=40&md5=24d2c566895bb9e65ef8495b53e3e240","Hyperspectral remote sensing provides an outstanding tool in oil slick detection and classification, for its advantages in abundant spectral information. Many classification methods have been proposed and tested for oil spill extraction using hyperspectral images. However, the deep learning method was hardly researched to classify oil slicks using hyperspectral images. In this work, we proposed a spatial–spectral jointed stacked auto-encoder (SSAE) to extract and classify oil slicks on the sea surface. The traditional machine learning methods, support vector machine, back-propagation neural network and stacked auto-encoder were also adopted. The experimental results reveal that our proposed SSAE model can remarkably outperform the other models, especially for the thick oil films. The results of this work could provide an alternative method to extract oil slicks on hyperspectral remote sensing images. © 2019, Indian Society of Remote Sensing.","Hyperspectral remote sensing; Oil spill; Stacked auto-encoder","detection method; extraction method; image analysis; machine learning; numerical method; oil spill; sea surface; spatial analysis; spectral analysis"
"Matching RGB and infrared remote sensing images with densely-connected convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076523191&doi=10.3390%2frs11232836&partnerID=40&md5=c143dd4502dfe3dcfacb112b384c0c80","We develop a deep learning-based matching method between an RGB (red, green and blue) and an infrared image that were captured from satellite sensors. The method includes a convolutional neural network (CNN) that compares the RGB and infrared image pair and a template searching strategy that searches the correspondent point within a search window in the target image to a given point in the reference image. A densely-connected CNN is developed to extract common features from different spectral bands. The network consists of a series of densely-connected convolutions to make full use of low-level features and an augmented cross entropy loss to avoid model overfitting. The network takes band-wise concatenated RGB and infrared images as the input and outputs a similarity score of the RGB and infrared image pair. For a given reference point, the similarity scores within the search window are calculated pixel-by-pixel, and the pixel with the highest score becomes the matching candidate. Experiments on a satellite RGB and infrared image dataset demonstrated that our method obtained more than 75% improvement on matching rate (the ratio of the successfully matched points to all the reference points) over conventional methods such as SURF, RIFT, and PSO-SIFT, and more than 10% improvement compared to other most recent CNN-based structures. Our experiments also demonstrated high performance and generalization ability of our method applying to multitemporal remote sensing images and close-range images. © 2019 by the authors.","Convolutional neural network; Image matching; Remote sensing image; Template matching","Convolution; Deep learning; Image matching; Infrared imaging; Neural networks; Pixels; Remote sensing; Template matching; Conventional methods; Convolutional neural network; Generalization ability; Infrared remote sensing; Multi-temporal remote sensing; Red , green and blues; Remote sensing images; Searching strategy; Image enhancement"
"Remote sensing image super-resolution of open-pit mining area based on texture transfer [基于纹理转移的露天矿区遥感图像超分辨率重建]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079863284&doi=10.13225%2fj.cnki.jccs.SH19.1028&partnerID=40&md5=2dad441b26e531bea660275ac29d4db1","Remote sensing image is one of the main sources in open-pit mines production. Its spatial resolution affects the identification of the boundary of each scene in mining area, the interpretation of small ground objects and the location of control points. Remote sensing images play an important role in the open-pit production management and monitoring. Aiming at the problem of high spatial resolution image that meets the restrictions, such as cost and technological constraints, it is proposed to use a super-resolution technology to improve the spatial resolution of remote sensing images in open-pit mining areas. According to the obvious texture features of each scene in the open-pit mining area, the research used a deep learning texture transfer super-resolution method. Through an end-to-end deep learning model, the processes are as follows: input low-resolution images and corresponding reference images, divide them into several image patches, extract the features of image patches using feature extraction network, and compare low-resolution feature image blocks and reference features. The image patches texture similarity adaptively transfers the texture from reference images, and fuses the plurality of ex-changed texture feature maps into the generation network in the feature layers of various scales to construct the reconstructed images with rich texture details. At the same time, the ResNet34 with deeper network depth and smaller computing capacity is used to replace the VGG19, which further improves the feature extraction effect. The research used a self-made open-pit mining area dataset for experiments. Compared with the advanced image super-resolution methods, in terms of the influence of the reference image on the results, the results show that the improved method reconstruction effect will increase with the similarity between reference images and input images. Compared with other methods, the values of peak signal-to-noise ratio and structural similarity are better than the original methods, EDSR and SRGAN, and the visual perception is better. © 2019, Editorial Office of Journal of China Coal Society. All right reserved.","Open-pit mining area; Reference images; Super-resolution; Texture transfer","Deep learning; Extraction; Feature extraction; Image coding; Image enhancement; Image reconstruction; Image resolution; Image texture; Learning systems; Optical resolving power; Remote sensing; Signal to noise ratio; Textures; Transfer learning; High spatial resolution images; Image super resolutions; Peak signal to noise ratio; Reference image; Super resolution; Superresolution methods; Technological constraints; Texture transfer; Open pit mining"
"Land cover change detection from high-resolution remote sensing imagery using multitemporal deep feature collaborative learning and a semi-supervised chan-vese model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076515725&doi=10.3390%2frs11232787&partnerID=40&md5=5d46d41ebfa1f5742aa6239d44bf3c16","This paper presents a novel approach for automatically detecting land cover changes from multitemporal high-resolution remote sensing images in the deep feature space. This is accomplished by using multitemporal deep feature collaborative learning and a semi-supervised Chan-Vese (SCV) model. The multitemporal deep feature collaborative learning model is developed to obtain the multitemporal deep feature representations in the same high-level feature space and to improve the separability between changed and unchanged patterns. The deep difference feature map at the object-level is then extracted through a feature similarity measure. Based on the deep difference feature map, the SCV model is proposed to detect changes in which labeled patterns automatically derived from uncertainty analysis are integrated into the energy functional to efficiently drive the contour towards accurate boundaries of changed objects. The experimental results obtained on the four data sets acquired by different high-resolution sensors corroborate the effectiveness of the proposed approach. © 2019 by the authors.","Chan-Vese model; Change detection; Deep feature learning; High-resolution remote sensing imagery; Semi-supervised learning; Uncertainty analysis","Digital storage; Feature extraction; Machine learning; Remote sensing; Space optics; Supervised learning; Uncertainty analysis; Chan-Vese model; Change detection; Deep feature learning; High resolution remote sensing imagery; Semi- supervised learning; Deep learning"
"Remote Sensing Object Localization with Deep Heterogeneous Superpixel Features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081411600&doi=10.1109%2fBigData47090.2019.9006120&partnerID=40&md5=fcfa2b09fffac812967e2bbbb7823e7b","Object detection and localization within high-resolution remote sensing imagery (HR-RSI) is a challenging task for a variety of reasons, such as the complexity and clutter of the image scene and the compactness of the intermixed object classes. Even the most comprehensive training datasets cannot adequately account for the rich diversity and complexity of anthropogenic objects and their contextual settings in large-scale HR-RSI collections. Recent approaches using deep learning techniques include bounding box approaches (e.g., YOLO), object nomination then recognition (e.g., R-CNN), and post-detection object localization of deep neural network detections. Herein, we propose a novel technique that leverages heterogeneous superpixels and deep neural feature extraction to classify the superpixel segmentation through relational analysis. In this preliminary research, we demonstrate the validity of this approach for object detection and localization, as well as its suitability for identifying the irregular shapes of objects (as opposed to a bounding box). Experiments are performed using a sub-set of the xView benchmark dataset with a goal of spearheading future techniques in cluttered scene object recognition that allows deep feature extractors to have more focus on the target objects instead of the surrounding area or nearby object pixels. © 2019 IEEE.","deep feature extraction; object localization; remote sensing; superpixel","Complex networks; Deep learning; Deep neural networks; Extraction; Feature extraction; Large dataset; Object recognition; Remote sensing; Superpixels; Benchmark datasets; High resolution remote sensing imagery; Learning techniques; Object detection and localizations; Object localization; Relational analysis; Superpixel segmentations; Training data sets; Object detection"
"CAD-net: A context-aware detection network for objects in remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075642514&doi=10.1109%2fTGRS.2019.2930982&partnerID=40&md5=a644f9552cb51e2e9c40dc63987aef48","Accurate and robust detection of multi-class objects in optical remote sensing images is essential to many real-world applications, such as urban planning, traffic control, searching, and rescuing. However, the state-of-the-art object detection techniques designed for images captured using ground-level sensors usually experience a sharp performance drop when directly applied to remote sensing images, largely due to the object appearance differences in remote sensing images in terms of sparse texture, low contrast, arbitrary orientations, and large-scale variations. This paper presents a novel object detection network [(context-aware detection network (CAD-Net)] that exploits attention-modulated features as well as global and local contexts to address the new challenges in detecting objects from remote sensing images. The proposed CAD-Net learns global and local contexts of objects by capturing their correlations with the global scene (at scene level) and the local neighboring objects or features (at object level), respectively. In addition, it designs a spatial-and-scale-aware attention module that guides the network to focus on more informative regions and features as well as more appropriate feature scales. Experiments over two publicly available object detection data sets for remote sensing images demonstrate that the proposed CAD-Net achieves superior detection performance. The implementation codes will be made publicly available for facilitating future works. © 1980-2012 IEEE.","Convolutional neural networks (CNNs); deep learning; object detection; optical remote sensing images","Computer aided design; Deep learning; Deep neural networks; Neural networks; Object detection; Object recognition; Textures; Traffic control; Arbitrary orientation; Convolutional neural network; Detecting objects; Detection networks; Detection performance; Optical remote sensing; Remote sensing imagery; Remote sensing images; Remote sensing; algorithm; data set; detection method; remote sensing; satellite imagery; satellite sensor"
"Multi-scale densenets-based aircraft detection from remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075865204&doi=10.3390%2fs19235270&partnerID=40&md5=69f3ed2a4442a18b860eff1ddbeeb01d","Deep learning-based aircraft detection methods have been increasingly implemented in recent years. However, due to the multi-resolution imaging modes, aircrafts in different images show very wide diversity on size, view and other visual features, which brings great challenges to detection. Although standard deep convolution neural networks (DCNN) can extract rich semantic features, they destroy the bottom-level location information. The features of small targets may also be submerged by redundant top-level features, resulting in poor detection. To address these problems, we proposed a compact multi-scale dense convolutional neural network (MS-DenseNet) for aircraft detection in remote sensing images. Herein, DenseNet was utilized for feature extraction, which enhances the propagation and reuse of the bottom-level high-resolution features. Subsequently, we combined feature pyramid network (FPN) with DenseNet to form a MS-DenseNet for learning multi-scale features, especially features of small objects. Finally, by compressing some of the unnecessary convolution layers of each dense block, we designed three new compact architectures: MS-DenseNet-41, MS-DenseNet-65, and MS-DenseNet-77. Comparative experiments showed that the compact MS-DenseNet-65 obtained a noticeable improvement in detecting small aircrafts and achieved state-of-the-art performance with a recall of 94% and an F1-score of 92.7% and cost less computational time. Furthermore, the experimental results on robustness of UCAS-AOD and RSOD datasets also indicate the good transferability of our method. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Aircraft detection; Compact multi-scale dense convolutional neural network; Multi-scale training; Remote sensing images","Backpropagation; Convolution; Deep learning; Feature extraction; Neural networks; Remote sensing; Semantics; Training aircraft; Compact architecture; Comparative experiments; Computational time; Convolution neural network; Convolutional neural network; Multi-scale features; Remote sensing images; State-of-the-art performance; Aircraft detection; aircraft; article; convolutional neural network; feature extraction; learning; recall; remote sensing"
"Deep learning for conifer/deciduous classification of airborne LiDAR 3D point clouds representing individual trees","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074299216&doi=10.1016%2fj.isprsjprs.2019.10.011&partnerID=40&md5=62c640f4c0cb3733b32489cda0d81fbe","The purpose of this study was to investigate the use of deep learning for coniferous/deciduous classification of individual trees segmented from airborne LiDAR data. To enable processing by a deep convolutional neural network (CNN), we designed two discrete representations using leaf-off and leaf-on LiDAR data: a digital surface model with four channels (DSM × 4) and a set of four 2D views (4 × 2D). A training dataset of tree crowns was generated via segmentation of tree crowns, followed by co-registration with field data. Potential mislabels due to GPS error or tree leaning were corrected using a statistical ensemble filtering procedure. Because the training data was heavily unbalanced (~8% conifers), we trained an ensemble of CNNs on random balanced sub-samples. Benchmarked against multiple traditional shallow learning methods using manually designed features, the CNNs improved accuracies up to 14%. The 4 × 2D representation yielded similar classification accuracies to the DSM × 4 representation (~82% coniferous and ~90% deciduous) while converging faster. Further experimentation showed that early/late fusion of the channels in the representations did not affect the accuracies in a significant way. The data augmentation that was used for the CNN training improved the classification accuracies, but more real training instances (especially coniferous) likely results in much stronger improvements. Leaf-off LiDAR data were the primary source of useful information, which is likely due to the perennial nature of coniferous foliage. LiDAR intensity values also proved to be useful, but normalization yielded no significant improvement. As we observed, large training data may compensate for the lack of a subset of important domain data. Lastly, the classification accuracies of overstory trees (~90%) were more balanced than those of understory trees (~90% deciduous and ~65% coniferous), which is likely due to the incomplete capture of understory tree crowns via airborne LiDAR. In domains like remote sensing and biomedical imaging, where the data contain a large amount of information and are not friendly to human visual system, human-designed features may become suboptimal. As exemplified by this study, automatic, objective derivation of optimal features via deep learning can improve prediction tasks in such domains. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural network; Mislabel correction; Remote sensing; Representation engineering; Unbalanced training data","Convolution; Deep neural networks; Medical imaging; Neural networks; Optical radar; Remote sensing; Airborne lidar data; Classification accuracy; Convolutional neural network; Digital surface models; Filtering procedures; Human Visual System; Statistical ensembles; Training data; Forestry; artificial neural network; coniferous tree; coordinate; data set; deciduous tree; error correction; GPS; learning; lidar; overstory; prediction; remote sensing; segmentation; Coniferophyta"
"A Comparison of Deep Learning Vehicle Group Detection in Satellite Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081319896&doi=10.1109%2fBigData47090.2019.9006415&partnerID=40&md5=b6fec29c5e280f71361ed15dc853eba5","Object detection is a challenging but important task for computer vision, and this is especially true in the remote sensing domain where data collections may bring billions of pixels in a single image. There are many methods for object detection, but in recent years the You Only Look Once (YOLO) algorithm has become a leading technique, gaining popularity due to its ability to perform real-time object detection. While YOLO and its successors have shown excellent results in realtime detection, there are many object detection tasks that require better precision, and do not require real-time detection. In this paper, YOLOv3 is compared to other deep neural networks (DNN) for detecting Vehicle Groups in very high resolution remote sensing imagery (VHR-RSI). A unique centerpoint-based dataset is developed by leveraging a novel data framework and combining quality assured chips with regions of interest in the XView Challenge Dataset. This dataset is then used to train state of the art models including two Neural Architecture Search (NAS) variant DNN for object detection. Additionally, a blind test set is developed to further compare our methods with the YOLOv3 algorithm. The results shows that our method detects vehicle groups with a lower false positive rate (FPR) and better true positive rate (TPR) than state-of-the-art YOLOv3 models for the blind test set; achieving a reduction in error rate of 26.70% over YOLOv3 in F1 Score on the blind test set. © 2019 IEEE.","deep neural networks; object detection; remote sensing","Big data; Deep neural networks; Object detection; Object recognition; Remote sensing; Satellite imagery; Data collection; False positive rates; Neural architectures; Real-time detection; Regions of interest; State of the art; True positive rates; Very high resolution; Deep learning"
"Vehicle Detection from High-Resolution Remote Sensing Imagery Using Convolutional Capsule Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075622518&doi=10.1109%2fLGRS.2019.2912582&partnerID=40&md5=5d7a907b170a8b0fc058a2598bd2ee48","Vehicle detection plays an important role in a variety of traffic-related applications. However, due to the scale and orientation variations and partial occlusions of vehicles, it is still challengeable to accurately detect vehicles from remote sensing images. This letter proposes a convolutional capsule network for detecting vehicles from high-resolution remote sensing images. First, a test image is segmented into superpixels to generate meaningful and nonredundant patches. Then, these patches are input to a convolutional capsule network to label them into vehicles or the background. Finally, nonmaximum suppression is adopted to eliminate repetitive detections. Quantitative evaluations on four test data sets show that average completeness, correctness, quality, and F1-measure of 0.93, 0.97, 0.90, and 0.95, respectively, are obtained. Comparative studies with three existing methods confirm that the proposed method effectively performs in detecting vehicles of various conditions. © 2019 IEEE.","Convolutional capsule network; deep learning; remote sensing imagery; superpixel segmentation; vehicle detection","Convolution; Deep learning; Superpixels; Vehicles; High resolution remote sensing imagery; High resolution remote sensing images; Non-maximum suppression; Quantitative evaluation; Remote sensing imagery; Remote sensing images; Superpixel segmentations; Vehicle detection; Remote sensing; data set; detection method; image analysis; orientation; pixel; quantitative analysis; remote sensing"
"Eyes in the skies: A data-driven fusion approach to identifying drug crops from remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079322056&doi=10.1109%2fJSTARS.2019.2917024&partnerID=40&md5=c451df3907ae5953bfc3c703eb60c6a6","Automatic classification of sensitive content in remote sensing images, such as drug crop sites, is a promising task, as it can aid law-enforcement institutions in fighting illegal drug dealers worldwide, while, at the same time, it can help monitor legalized crops in countries that regulate them. However, existing art on detecting drug crops from remote sensing images is limited in some key factors, not taking full advantage of the available hyperspectral information for analysis. In this paper, departing from these methods, we propose a data-driven ensemble method to detect drug sites from remote sensing images. Our method comprises different convolutional neural network architectures applied to distinct image representations, which are able to represent complementary characterizations of such crops. To validate the proposed approach, we considered in our experiments a dataset containing Cannabis Sativa crops, spotted by police operations in a Brazilian region called the Marijuana Polygon. The results in this dataset show that our ensemble approach outperforms other data-driven and feature-engineering methods in a real-world experimental setup, in which unbalanced samples are present and acquisitions from different places in the same region are used for training and testing the methods, highlighting the promising use of this solution to aid police operations in detecting and collecting evidence of such sensitive content properly. © 2008-2012 IEEE.","Convolutional neural networks (CNNs); deep learning; detection of drug crops; sensitive remote sensing analysis","Convolution; Convolutional neural networks; Crops; Deep learning; Deep neural networks; Hemp; Law enforcement; Network architecture; Statistical tests; Automatic classification; Ensemble approaches; Feature engineerings; Hyperspectral information; Image representations; Remote sensing analysis; Remote sensing images; Training and testing; Remote sensing; crop plant; drug; image analysis; image classification; machine learning; remote sensing; satellite data; satellite imagery; Cannabis sativa"
"Remote sensing image mining area change detection based on improved UNet siamese network [基于改进UNet孪生网络的遥感影像矿区变化检测]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079896179&doi=10.13225%2fj.cnki.jccs.SH19.1026&partnerID=40&md5=c64c17df76a90b48ba45f775080c7110","The exploitation of mineral resources, while promoting regional economic development, has also caused some damage on land surface and has an impact on the ecological environment. The remote sensing technology for timely obtaining the change information on land cover and ecological environment in mining areas can be used in practical applications such as guiding ecological protection and restoration of mining areas. Aiming at the need to extract a large number of artificially designed image features in the traditional change detection method, an improved UNet siamese network is proposed. The convolution layer is used to replace the pooling layer in the UNet structure, and the siamese structure with dual channels, the feature pyramid module and central surround module was added. Firstly, the preprocessed remote sensing images of the two periods are cropped by central surround and the multi-scale information of the image is acquired, and the images of the central area and the surround area are respectively input into the encoder part of the network structure. The different information between the two periods is extracted by the weight shared siamese structure, then, the features on the same feature layer are subtracted, and the different images on different convolutional layers are obtained and the feature fusion is performed. The merged image is sent to the feature pyramid module to obtain image multi-scale context information. Finally, the corresponding features of the encoder and the decoder are fused by skip connection, and the end-to-end prediction is performed to obtain the change binary image of the mining area remote sensing image in the two periods. The results show that the improved change detection network method can automatically extract the low-level features and high-level semantic features of the image compared with the traditional methods, which avoids the cumbersome manual extraction of image features. In addition, from the detection results, the improved change detection method has a significant improvement in overall accuracy and Kappa coefficient compared with the comparison method, and also reduces the commission error and omission error of the detection result. © 2019, Editorial Office of Journal of China Coal Society. All right reserved.","Deep learning; High resolution remote sensing image; Mine area change detection; UNet","Binary images; Convolution; Deep learning; Ecology; Feature extraction; Image acquisition; Mineral resources; Regional planning; Remote sensing; Semantics; Signal encoding; Ecological environments; High resolution remote sensing images; High-level semantic features; Mine areas; Multi-scale informations; Regional economic development; Remote sensing technology; UNet; Image enhancement"
"Brown planthopper damage detection using remote sensing and machine learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080876040&doi=10.1109%2fICMLA.2019.00024&partnerID=40&md5=48d8cf0f7cca463b19552a86edb869b4","Every year paddy cultivators lose a significant amount of crop yield due to diseases and pests. Brown Planthopper (BPH) is one of the most common diseases that affect paddy cultivation. Sri Lankan government is struggling to make appropriate estimations regarding Brown Planthopper prevalence due to the absence of accurate and timely data. To solve this issue, a machine learning approach is proposed based on optical and synthetic aperture radar remote sensing data in this study. However, there is no previous effort for detecting Brown Planthopper attacks using machine learning and satellite remote sensing data under field conditions. This study consists of two phases. A time series classification based on SAR imagery is implemented to identify cultivated paddy fields in the first phase. Ratio and standard difference indices derived from optical satellite images are used in the second phase to identify regions affected by BPH attacks in paddy fields. Convolution neural network that is used in the first phase reports an accuracy of 96.20% for identifying cultivated paddy regions. A Support Vector Machine is used to detect areas damaged by BPH attacks in the second phase. The Combined approach of the first and the second phases shows promising results with an accuracy of 96.31% for detecting Brown Planthopper attacks. © 2019 IEEE.","Agriculture; Brown Planthopper; Deep Neural Networks; Remote Sensing; Rice; Support Vector Machine; Synthetic Aperture Radar","Agricultural implements; Agriculture; Animals; Cultivation; Damage detection; Deep learning; Deep neural networks; Learning systems; Radar imaging; Space-based radar; Support vector machines; Synthetic aperture radar; Brown planthopper; Convolution neural network; Machine learning approaches; Optical satellite images; Paddy cultivation; Rice; Satellite remote sensing data; Time series classifications; Remote sensing"
"A Review on Recent Advances in Remote Sensing Image Retrieval Techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074025159&doi=10.1007%2fs12524-019-01049-8&partnerID=40&md5=31360cdb7a516909fb05927f005cea43","Due to the rapid advancements in the remote sensing (RS) imaging modalities, the scientific fraternity has been challenged to develop sophisticated methods for retrieving similar images from huge image archives. Developing efficient retrieval methods has become more challenging, as the quantity of RS image databases is growing fast in the spatial information domain. Even though numerous techniques have been developed for Remote Sensing Image Retrieval (RSIR) in the last decade, most of them are found to be less effective on a large volume of RS image databases. Several studies have been conducted to analyze the issues related to the challenges involved in the design of efficient and reliable retrieval techniques for an RSIR. A systematic study has been conducted on the existing RSIR methods, especially on the performance of the techniques with large datasets, and the findings are explained in this paper. The discussions and findings presented in this paper will give new insight, into the different RSIR techniques. The recommendations given at the end of the paper will help the new researchers in the RS domain to choose effective methodologies that can improve the performance of the RSIR system in different retrieval schemes. © 2019, Indian Society of Remote Sensing.","Active learning; Deep learning; Feature extraction; Relevance feedback; Similarity measure","database; extraction method; image analysis; performance assessment; remote sensing; spatial analysis"
"Transfer learning approach for patch level classification in remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083036382&doi=10.1109%2fINDICON47234.2019.9030283&partnerID=40&md5=c253245f0f4264c26c8dd12be4cdad2e","Deep learning approach characterizes information by separating significant features automatically. Transfer learning approach for feature extraction from a pre-trained deep model is a well-known strategy in classification or target recognition task. In this paper, VGG-16, Inception-v3, and ResNet- 50 pre-trained models are utilized for extraction of features. Features are extracted from the patches of different locales like urban, forest and water, from the RGB image of Synthetic Aperture Radar data of RISAT-1. Extracted features are further trained on SVM for better classification. All the three networks are also re-trained by fine-tuning the fully connected layers to classify the patches. Among the three pre-trained models, VGG- 16 outperformed the other classifiers in terms of accuracy of classification and feature extraction. © 2019 IEEE.","Convolutional Neural Network; Patch level classification; Pre-trained models; Transfer learning","Deep learning; Extraction; Feature extraction; Remote sensing; Support vector machines; Synthetic aperture radar; Accuracy of classifications; Fine tuning; Learning approach; RGB images; Target recognition; Three networks; Transfer learning"
"Polsarnet: A deep fully convolutional network for polarimetric sar image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079348219&doi=10.1109%2fJSTARS.2019.2956650&partnerID=40&md5=0d6742850d7c5931401e617398359000","Deep learning has successfully improved the classification accuracy of optical remote sensing images. Recent works attempted to transfer the success of these techniques to the microwave domain to classify Polarimetric SAR data. So far, most deep learning networks separate amplitude and phase as separate input images. In this article, we present a deep fully convolutional network that uses real-valued weight kernels to perform pixel-wise classification of complex-valued images. We evaluated the performance of this network by comparing it with support vector machine, Random Forest, complex-valued convolutional neural network (CV-CNN), and a network that uses amplitude and phase information separately as real channels. The evaluation was done on a quad-polarized AIRSAR image and a dual-polarimetric multitemporal Sentinel-1 data acquired over Flevoland, the Netherlands. The proposed method achieved higher accuracy compared to all other networks with the same architecture. © 2008-2012 IEEE.","Convolutional neural network (CNN); deep learning; image classification; machine learning; polarimetric SAR (PolSAR)","Complex networks; Convolution; Decision trees; Deep learning; Deep neural networks; Image classification; Image enhancement; Learning systems; Polarimeters; Radar imaging; Random forests; Remote sensing; Support vector machines; Synthetic aperture radar; Classification accuracy; Convolutional networks; Learning network; Microwave domains; Optical remote sensing; Phase information; Polarimetric SAR; Polarimetric SAR data; Convolutional neural networks; accuracy assessment; amplitude; image classification; machine learning; performance assessment; satellite data; satellite imagery; Sentinel; support vector machine; synthetic aperture radar; Flevoland; Netherlands"
"Hyperspectral Images Classification based on Inception Network and Kernel PCA","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081596226&doi=10.1109%2fTLA.2019.9011544&partnerID=40&md5=a17c2785139d06cb1310c2946858fd2f","Recent advances in remote sensing have shown great potential for different kind of applications like vegetation and crop supervision. Using hyperspectral images (HSI), this process can be performed over large areas of land, allowing for a fast and non-invasive analysis of variables such as water stress, diseases, type of crops, among many others. In this context, representative spatial-spectral features are crucial to develop the data classification process of hyperspectral remote sensing images. In this way, the use of deep learning networks has shown a remarkable classification performance in many applications as the processing of hyperspectral images. In this paper, a data classification model that use kernel PCA (K-PCA) and inception network architecture to generate deep spatial-spectral features is proposed. The features generated are used to label the different kind of classes into the HSI. The labeling process uses a logistic regression (LR) algorithm. This model was validated using six different datasets. The experiments performed shown that the proposed strategy allows improving the data classification performance regarding the results obtained by the traditional stacked layer model of convolutional neural networks (CNN). Quantitatively speaking, the overall accuracy over all experiments of the proposed model for all tested datasets is greater than 92%. © 2003-2012 IEEE.","convolutional neural network; deep learning; dimensionality reduction; hyperspectral images; inception network; principal component analysis; remote sensing","Convolution; Convolutional neural networks; Crops; Deep learning; Deep neural networks; Dimensionality reduction; Hyperspectral imaging; Image analysis; Image classification; Logistic regression; Multilayer neural networks; Network architecture; Principal component analysis; Remote sensing; Spectroscopy; Classification performance; Data classification; Data classification models; Hyperspectral Remote Sensing Image; Images classification; Logistic regression algorithms; Overall accuracies; Spectral feature; Classification (of information)"
"Low Resolution Recognition of Aerial Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079234482&doi=10.1109%2fVCIP47243.2019.8965672&partnerID=40&md5=8dbd5e93372987191fd73384827e7c56","Remote sensing for classification has been widely studied and is useful for a lot of applications like precision agriculture, surveillance, and military applications. Recently, due to tremendous results achieved by deep learning using Convolutional Neural Networks (CNN) for Imagenet dataset, there have been a large number of works which use deep learning for aerial image classification. Most of the works concentrate on original resolution and there are no works on low-resolution recognition of aerial images. This work is critical because aerial images are taken from a very high distance from the ground and the cost of installing high definition cameras is high, so it is hard to get a high resolution of the image. In this paper, we explore how we can do the better classification of aerial images for original spatial resolution and low spatial resolution in deep learning by using texture information. In our framework, we use YUV color space which is generally used for video coding and we also use Laplacian of Gaussian (LOG) information to exploit the texture information. We decouple RGB information into luminance information (Y channel), color information (UV) and texture information (LOG) and we train a separate CNN for each feature and combine them using autoencoder and with our results, we show that we do better than RGB images in original resolution and low resolution. © 2019 IEEE.","Classification; Deep Learning; Low Resolution Recognition; Remote Sensing","Antennas; Classification (of information); Color; Convolutional neural networks; Deep learning; Image classification; Image resolution; Large dataset; Military applications; Remote sensing; Space optics; Textures; Video signal processing; Visual communication; Color information; High definition; High resolution; Laplacian of Gaussian; Low resolution; Spatial resolution; Texture information; YUV color space; Image texture"
"Learning and Recognizing Archeological Features from LiDAR Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081385643&doi=10.1109%2fBigData47090.2019.9005548&partnerID=40&md5=2127e56630041f11db6678a675e23205","We present a remote sensing pipeline that processes LiDAR (Light Detection And Ranging) data through machine deep learning for the application of archeological feature detection on big geo-spatial data platforms such as e.g. IBM PAIRS Geoscope [1], [2].Today, archeologists get overwhelmed by the task of visually surveying huge amounts of (raw) LiDAR data in order to identify areas of interest for inspection on the ground. We showcase a software system pipeline that results in significant savings in terms of expert productivity while missing only a small fraction of the artifacts.Our work employs artificial neural networks in conjunction with an efficient spatial segmentation procedure based on domain knowledge. Data processing is constraint by a limited amount of training labels and noisy LiDAR signals due to vegetation cover and decay of ancient structures. We aim at identifying geo-spatial areas with archeological artifacts in a supervised fashion allowing the domain expert to flexibly tune parameters based on her needs. © 2019 IEEE.","archeology; LiDAR data processing; machine learning; remote sensing applications","Big data; Data handling; Data visualization; Deep learning; Learning systems; Neural networks; Pipelines; Remote sensing; Ancient structures; Archeological artifacts; archeology; Feature detection; LIDAR (light detection and ranging); LIDAR data; Remote sensing applications; Spatial segmentation; Optical radar"
"Spectral-spatial feature extraction using PCA and multi-scale deep convolutional neural network for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082995478&doi=10.1109%2fICCIT48885.2019.9038385&partnerID=40&md5=a8a56e8bd3a80af7ec0ef49f16643ed8","Hyperspectral imaging (HSI) is one of the emerging research fields in remote sensing technology as it contains huge information about a scene which can be further analyzed to detect various kind of objects. HSI contains several narrow and contiguous spectral bands which provide different research challenges for dealing with high dimensional data for better classification. Most of the traditional method takes only the spectral feature into account for classification. Principal Component Analysis (PCA) is widely used for this task. Recently with the introduction of the convolutional neural network, spatial features are being fused with the spectral features for better classification. However, traditional convolutional neural network (CNN) based methods take only a single scale spatial kernel in order to explore the spatial information. In this paper, we have proposed a hybrid approach of a dimensionality reduction technique based on PCA and a novel multi-scale convolutional neural network named PCA-MS-CNN in order to extract spectral and spatial feature for hyperspectral image classification. We have achieved an average accuracy of 99.10% on Indian Pines dataset using this method. This result shows that it outperforms other state-of-the-art deep learning based methods used for HSI classification. © 2019 IEEE.","Hyperspectral image classification; Multi-Scale Convolutional Neural Network; Principal Component Analysis; Spectral- Spatial Features","Classification (of information); Clustering algorithms; Convolution; Deep learning; Deep neural networks; Dimensionality reduction; Hyperspectral imaging; Image classification; Object detection; Principal component analysis; Remote sensing; Spectroscopy; Dimensionality reduction techniques; High dimensional data; Learning-based methods; Remote sensing technology; Research challenges; Spatial features; Spatial informations; Spectral feature; Convolutional neural networks"
"Microwave SAIR imaging approach based on deep convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075661699&doi=10.1109%2fTGRS.2019.2934154&partnerID=40&md5=9cab54a16c36680989a9d8c1b8ef59ad","Microwave synthetic aperture interferometric radiometers (SAIRs) are very powerful instruments for high-resolution remote sensing of the atmosphere and the earth surfaces at microwave frequencies. Microwave SAIR imaging reconstruction from interferometric measurements suffers from hardware non-identities, limited prior information, and noise interference, and consequently often requires expert calibration strategies to reduce imaging error and improve the accuracy of the reconstruction. In this article, we propose a new SAIR imaging approach with a deep convolutional neural network (CNN) learning framework to optimize the reconstruction performance. We interpret interferometric measurements of SAIR as a signal encoding representation and SAIR imaging as the corresponding decoding representation. A deep CNN framework with additional fully connected layers is utilized to autonomously learn the decoding representation from interferometric measurement samples and perform SAIR imaging. The supervised learning forward model with hyperparameters makes that the proposed approach could accurately obtain the SAIR imaging representation involving multiple systematic features for real applications. We demonstrate the performance of the proposed imaging approach through extensive numerical experiments. Compared with conventional handcrafted Fourier transform and sparse regularization reconstruction imaging approaches, the proposed imaging approach based on deep learning is superior in terms of image quality, computing efficiency, and noise suppression. © 1980-2012 IEEE.","Convolutional neural network (CNN); deep learning (DL); imaging approach; microwave synthetic aperture interferometric radiometers (SAIRs)","Convolution; Decoding; Earth atmosphere; Image processing; Microwaves; Neural networks; Radiometers; Remote sensing; Signal encoding; Synthetic apertures; Computing efficiency; Convolutional neural network; High resolution remote sensing; Imaging reconstruction; Interferometric measurement; Interferometric radiometers; Numerical experiments; Sparse regularizations; Deep neural networks; algorithm; artificial neural network; calibration; imaging method; interferometry; microwave radiation; radiometer; reconstruction; remote sensing; supervised learning"
"Hyperspectral image classification based on two-phase relation learning network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075687826&doi=10.1109%2fTGRS.2019.2934218&partnerID=40&md5=08c663a7277757232f03d51d45d0cc31","Deep learning-based classification methods are competent to achieve an excellent performance under one necessary condition, i.e., there are sufficient labeled samples in each class, which is extremely impractical in most of the remote sensing tasks. To improve the performance with small training sets, we resort to other hyperspectral images and design a two-phase relation learning network that can be transferred between different images for general information sharing and fine-trained on a specific hyperspectral image for individual information learning. Specifically, we use a relation learning method to compare samples and deal with the task inconsistency between different data sets, and we adopt an episode-based training strategy to mimic the testing setup and learn the transferable comparison ability. Benefited from these two strategies, the proposed network takes the advantage of extra knowledge for information supplement and learns to compare rather than to classify for information exploration, which guarantees a reasonable performance even with small training sets. Extensive experiments and analysis on three benchmarks demonstrate that the proposed method can provide an effective solution for hyperspectral image classification with small training sets, which makes it possible to work on large-scale applications of earth observation with less effort on field investigation. © 1980-2012 IEEE.","Classification; deep network; hyperspectral image","Ability testing; Benchmarking; Classification (of information); Deep learning; Hyperspectral imaging; Image enhancement; Remote sensing; Spectroscopy; Classification methods; Earth observations; Effective solution; Field investigation; General information; Information exploration; Large-scale applications; Training strategy; Image classification; algorithm; data set; field margin; field method; image analysis; image classification; remote sensing; satellite data"
"Differentially deep subspace representation for unsupervised change detection of SAR images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076518843&doi=10.3390%2frs11232740&partnerID=40&md5=f7bedc40cf993024e4ae412511697e68","Temporal analysis of synthetic aperture radar (SAR) time series is a basic and significant issue in the remote sensing field. Change detection as well as other interpretation tasks of SAR images always involves non-linear/non-convex problems. Complex (non-linear) change criteria or models have thus been proposed for SAR images, instead of direct difference (e.g., change vector analysis) with/without linear transform (e.g., Principal Component Analysis, Slow Feature Analysis) used in optical image change detection. In this paper, inspired by the powerful deep learning techniques, we present a deep autoencoder (AE) based non-linear subspace representation for unsupervised change detectionwithmulti-temporal SAR images. The proposed architecture is built upon an autoencoder-like (AE-like) network, which non-linearly maps the input SAR data into a latent space. Unlike normal AE networks, a self-expressive layer performing like principal component analysis (PCA) is added between the encoder and the decoder, which further transforms the mapped SAR data to mutually orthogonal subspaces. To make the proposed architecture more efficient at change detection tasks, the parameters are trained to minimize the representation difference of unchanged pixels in the deep subspace. Thus, the proposed architecture is namely the Differentially Deep Subspace Representation (DDSR) network for multi-temporal SAR images change detection. Experimental results on real datasets validate the effectiveness and superiority of the proposed architecture. © 2019 by the authors.","AutoEncoder-like (AE-like) network; Differentially deep subspace representation (DDSR); SAR; Unsupervised change detection","Deep learning; Geometrical optics; Mathematical transformations; Network architecture; Principal component analysis; Remote sensing; Synthetic aperture radar; Time series analysis; Vectors; Auto encoders; Change vector analysis; Multi-temporal SAR images; Orthogonal subspaces; Proposed architectures; Slow feature analysis; Subspace representation; Unsupervised change detection; Radar imaging"
"A point-wise LiDAR and image multimodal fusion network (PMNet) for aerial point cloud 3D semantic segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077901612&doi=10.3390%2frs11242961&partnerID=40&md5=73f74581ce2882efef643f5f11580b8f","3D semantic segmentation of point cloud aims at assigning semantic labels to each point by utilizing and respecting the 3D representation of the data. Detailed 3D semantic segmentation of urban areas can assist policymakers, insurance companies, governmental agencies for applications such as urban growth assessment, disaster management, and traffic supervision. The recent proliferation of remote sensing techniques has led to producing high resolution multimodal geospatial data. Nonetheless, currently, only limited technologies are available to fuse the multimodal dataset effectively. Therefore, this paper proposes a novel deep learning-based end-to-end Point-wise LiDAR and Image Multimodal Fusion Network (PMNet) for 3D segmentation of aerial point cloud by fusing aerial image features. PMNet respects basic characteristics of point cloud such as unordered, irregular format and permutation invariance. Notably, multi-view 3D scanned data can also be trained using PMNet since it considers aerial point cloud as a fully 3D representation. The proposed method was applied on two datasets (1) collected from the urban area of Osaka, Japan and (2) from the University of Houston campus, USA and its neighborhood. The quantitative and qualitative evaluation shows that PMNet outperforms other models which use non-fusion and multimodal fusion (observational-level fusion and feature-level fusion) strategies. In addition, the paper demonstrates the improved performance of the proposed model (PMNet) by over-sampling/augmenting the medium and minor classes in order to address the class-imbalance issues. © 2019 by the authors.","3D segmentation; Aerial images; Aerial point cloud; CNN; Deep learning; LiDAR; Multimodal fusion; Multispectral; PMNet; PointNet; Urban utilities","Antennas; Deep learning; Disaster prevention; Disasters; Image segmentation; Insurance; Optical radar; Remote sensing; Semantic Web; Semantics; Urban growth; 3D segmentation; Aerial images; Multi-modal fusion; Multi-spectral; PMNet; Point cloud; PointNet; Urban utility; Image fusion"
"Classification of the global Sentinel-1 SAR vignettes for ocean surface process studies","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074261220&doi=10.1016%2fj.rse.2019.111457&partnerID=40&md5=541ccb6791561d97a1673d5322047ded","Spaceborne synthetic aperture radar (SAR) can provide finely-resolved (meters-scale) images of ocean surface roughness day-or-night in nearly all weather conditions. This makes it a unique asset for many geophysical applications. Initially designed for the measurement of directional ocean wave spectra, Sentinel-1 SAR wave mode (WV) vignettes are small 20 km scenes that have been collected globally since 2014. Recent WV data exploration reveals that many important oceanic and atmospheric phenomena are also well captured, but not yet employed by the scientific community. However, expanding applications of this whole massive dataset beyond ocean waves requires a strategy to automatically identify these geophysical phenomena. In this study, we propose to apply the emerging deep learning approach in ocean SAR scenes classification. The training is performed using a hand-curated dataset that describes ten commonly-occurring atmospheric or oceanic processes. Our model evaluation relies on an independent assessment dataset and shows satisfactory and robust classification results. To further illustrate the model performance, regional patterns of rain and sea ice are qualitatively analyzed and found to be very consistent with independent remote sensing datasets. In addition, these high-resolution WV SAR data can resolve fine, sub-km scale, spatial structure of rain events and sea ice that complement other satellite measurements. Overall, such automated SAR vignettes classification may open paths for broader geophysical application of maritime Sentinel-1 acquisitions. © 2019 Elsevier Inc.","Convolutional neural network (CNN); Deep learning; Image classification; Ocean surface phenomena; Sentinel-1 wave mode; Synthetic aperture radar (SAR)","Classification (of information); Deep learning; Deep neural networks; Geophysics; Image classification; Neural networks; Oceanography; Rain; Remote sensing; Sea ice; Surface roughness; Synthetic aperture radar; Water waves; Convolutional neural network; Geophysical applications; Geophysical phenomena; Independent assessment; Robust classification; Satellite measurements; Spaceborne synthetic aperture radars; Wave modes; Space-based radar; algorithm; artificial neural network; climate conditions; image classification; ocean wave; remote sensing; sea surface; Sentinel; surface roughness; synthetic aperture radar"
"Deep and machine learnings of remotely sensed imagery and its multi-band visual features for detecting oil palm plantation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068207960&doi=10.1007%2fs12145-019-00387-y&partnerID=40&md5=d08993d6941c2f711190f91a48b1d9b5","Characterization of oil palm plantation is a crucial step toward many geographical based management strategies, ranging from determining regional planting and appropriate species to irrigation and logistics planning. Accurate and most updated plantation identification enables well informed and effective measures for such schemes. This paper proposes a computerized method for detecting oil-palm plantation from remotely sensed imagery. Unlike other existing approaches, where imaging features were retrieved from spectral data and then trained with a machine learning box for region of interest extraction, this paper employed 2-stage detection. Firstly, a deep learning network was employed to determine a presence of oil-palm plantation in a generic Google satellite image. With irrelevant samples being disregarded and thus the problem space being so contained, the images with detected oil-palm had their plantation delineated at higher accuracy by using a support vector machine, based on Gabor texture descriptor. The proposed coupled detection-delineation was benchmarked against different feature descriptors and state-of-the-art supervised and unsupervised machine learning techniques. The validation was made by comparing the extraction results with those ground surveyed by an authority. It was shown in the experiments that it could detect and delineate the plantations with an accuracy of 92.29% and precision, recall and Kappa of 91.16%, 84.97%, and 0.81, respectively. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Gabor wavelet; GoogLeNet; Oil palm plantation; Texture analysis","airborne sensing; image analysis; machine learning; plantation; remote sensing; texture; visual analysis; wavelet; Elaeis"
"Extended Semi-Supervised Learning GAN for Hyperspectral Imagery Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081997157&doi=10.1109%2fICSPCS47537.2019.9008719&partnerID=40&md5=d5326ed9afd8e7f21944cc3659d01ff4","Hyperspectral imagery (HSI) cubes are high-dimensional datasets that lend themselves well to deep learning approaches for classification. Deep learning approaches, specifically generative adversarial networks (GANs), have been shown to be very effective in classification and generation of accurate synthetic data in computer vision problems. This work proposes an extension of an existing GAN training scheme, called extended semi-supervised learning (ESSL), metrics for evaluating GAN training performance, and demonstrates the effectiveness of the proposed training scheme to improve classification of HSI. Using ESSL with GAN, we have been able to achieve approximately 0.8% increase in classification accuracy over convolutional neural networks as well as generate extremely accurate synthetic imagery. © 2019 IEEE.",,"Classification (of information); Convolutional neural networks; Deep learning; Image classification; Remote sensing; Spectroscopy; Adversarial networks; Classification accuracy; Computer vision problems; High dimensional datasets; Hyperspectral imagery; Hyperspectral imagery classifications; Learning approach; Synthetic imagery; Semi-supervised learning"
"Learned Patch-Based Regularization for Inverse Problems in Imaging","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082398564&doi=10.1109%2fCAMSAP45676.2019.9022624&partnerID=40&md5=40b78af1f6b5b0b3afddbfe22c38d81d","Many modern approaches to image reconstruction are based on learning a regularizer that implicitly encodes a prior over the space of images. For large-scale images common in imaging domains like remote sensing, medical imaging, astronomy, and others, learning the entire image prior requires an often-impractical amount of training data. This work describes a deep image patch-based regularization approach that can be incorporated into a variety of modern algorithms. Learning a regularizer amounts to learning the a prior for image patches, greatly reducing the dimension of the space to be learned and hence the sample complexity. Demonstrations in a remote sensing application illustrates that learning patch-based regularizers produces high-quality reconstructions and even permits learning from a single ground-truth image. © 2019 IEEE.","deblurring; deep learning; inverse problems; Patch-based methods; remote sensing","Array processing; Deep learning; Differential equations; Image reconstruction; Inverse problems; Remote sensing; Space optics; Deblurring; High quality reconstruction; Image patches; Patch-based methods; Regularization approach; Remote sensing applications; Sample complexity; Training data; Medical imaging"
"Analyzing the effects of temporal resolution and classification confidence for modeling land cover change with long short-term memory networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076565323&doi=10.3390%2frs11232784&partnerID=40&md5=ac3579c07a55975191addfef8305a873","Land cover change (LCC) is typically characterized by infrequent changes over space and time. Data-driven methods such as deep learning (DL) approaches have proven effective in many domains for predictive and classification tasks. When applied to geospatial data, sequential DL methods such as long short-term memory (LSTM) have yielded promising results in remote sensing and GIScience studies. However, the characteristics of geospatial datasets selected for use with these methods have demonstrated important implications on method performance. The number of data layers available, the rate of LCC, and inherent errors resulting from classification procedures are expected to influence model performance. Yet, it is unknown how these can affect compatibility with the LSTM method. As such, the main objective of this study is to explore the capacity of LSTM to forecast patterns that have emerged fromLCC dynamics given varying temporal resolutions, persistent land cover classes, and auxiliary data layers pertaining to classification confidence. Stacked LSTM modeling approaches are applied to 17-year MODIS land cover datasets focused on the province of British Columbia, Canada. This geospatial data is reclassified to four major land cover (LC) classes during pre-processing procedures. The evaluation considers the dataset at variable temporal resolutions to demonstrate the significance of geospatial data characteristics on LSTM method performance in several scenarios. Results indicate that LSTM can be utilized for forecasting LCC patterns when there are few limitations on temporal intervals of the datasets provided. Likewise, this study demonstrates improved performance measures when there are classes that do not change. Furthermore, providing classification confidence data as ancillary input also demonstrated improved results when the number of timesteps or temporal resolution is limited. This study contributes to future applications of DL and LSTM methods for forecasting LCC. © 2019 by the authors.","Deep learning; Geospatial data modeling; Land cover change; Long short-term memory; Recurrent neural networks; Sensitivity analysis; Temporal resolution","Brain; Classification (of information); Deep learning; Deep neural networks; Forecasting; Recurrent neural networks; Remote sensing; Sensitivity analysis; British Columbia , Canada; Classification confidence; Classification procedure; Classification tasks; Geo-spatial data; Land-cover change; Performance measure; Temporal resolution; Long short-term memory"
"Using the U-net convolutional network to map forest types and disturbance in the Atlantic rainforest with very high resolution images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062521857&doi=10.1002%2frse2.111&partnerID=40&md5=062244dcf0b2ddda73e2f23e5fa527f5","Mapping forest types and tree species at regional scales to provide information for ecologists and forest managers is a new challenge for the remote sensing community. Here, we assess the potential of a U-net convolutional network, a recent deep learning algorithm, to identify and segment (1) natural forests and eucalyptus plantations, and (2) an indicator of forest disturbance, the tree species Cecropia hololeuca, in very high resolution images (0.3 m) from the WorldView-3 satellite in the Brazilian Atlantic rainforest region. The networks for forest types and Cecropia trees were trained with 7611 and 1568 red-green-blue (RGB) images, respectively, and their dense labeled masks. Eighty per cent of the images were used for training and 20% for validation. The U-net network segmented forest types with an overall accuracy &gt;95% and an intersection over union (IoU) of 0.96. For C. hololeuca, the overall accuracy was 97% and the IoU was 0.86. The predictions were produced over a 1600 km2 region using WorldView-3 RGB bands pan-sharpened at 0.3 m. Natural and eucalyptus forests compose 79 and 21% of the region's total forest cover (82 250 ha). Cecropia crowns covered 1% of the natural forest canopy. An index to describe the level of disturbance of the natural forest fragments based on the spatial distribution of Cecropia trees was developed. Our work demonstrates how a deep learning algorithm can support applications such as vegetation, tree species distributions and disturbance mapping on a regional scale. © 2019 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London","Deep learning; Image segmentation; Keras; Rstudio; Tensorflow; Tree crown delineation; Tree species detection; Vegetation type detection; WorldView-3 image",
"An enhanced deep convolutional model for spatiotemporal image fusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077888518&doi=10.3390%2frs11242898&partnerID=40&md5=cb2cdcf1e245da6e9279708d24112302","Earth observation data with high spatiotemporal resolution are critical for dynamic monitoring and prediction in geoscience applications, however, due to some technique and budget limitations, it is not easy to acquire satellite images with both high spatial and high temporal resolutions. Spatiotemporal image fusion techniques provide a feasible and economical solution for generating dense-time data with high spatial resolution, pushing the limits of current satellite observation systems. Among existing various fusion algorithms, deep-learning-based models reveal a promising prospect with higher accuracy and robustness. This paper refined and improved the existing deep convolutional spatiotemporal fusion network (DCSTFN) to further boost model prediction accuracy and enhance image quality. The contributions of this paper are twofold. First, the fusion result is improved considerably with brand-new network architecture and a novel compound loss function. Experiments conducted in two different areas demonstrate these improvements by comparing them with existing algorithms. The enhanced DCSTFN model shows superior performance with higher accuracy, vision quality, and robustness. Second, the advantages and disadvantages of existing deep-learning-based spatiotemporal fusion models are comparatively discussed and a network design guide for spatiotemporal fusion is provided as a reference for future research. Those comparisons and guidelines are summarized based on numbers of actual experiments and have promising potentials to be applied for other image sources with customized spatiotemporal fusion networks. © 2019 by the authors.","CNN; Deep learning; EDCSTFN; Image fusion; Landsat; MODIS; Remote sensing; Spatiotemporal","Budget control; Convolution; Deep learning; Image enhancement; Network architecture; Remote sensing; EDCSTFN; High spatial resolution; High temporal resolution; LANDSAT; MODIS; Satellite observation systems; Spatio-temporal resolution; Spatiotemporal; Image fusion"
"Spatial-spectral relation network for hyperspectral image classification with limited training samples","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079347465&doi=10.1109%2fJSTARS.2019.2957047&partnerID=40&md5=78af6f0a15ca41385c37d74cc9d46398","Due to the data-hungry nature of deep neural networks and the high dimensionality of hyperspectral imagery (HSI) data, the scarcity of training samples remains a significant challenge for HSI classification based on deep learning. Recently, the pairing or recombining of samples has proven to be an efficient approach to increase the amount of input training data, and accordingly, the architecture of a network trained by these samples has to be redesigned, since the input is no longer a single sample. Following this strategy, in this article, we propose a spatial-spectral relation network (SS-RN) for HSI classification with limited training samples. The SS-RN takes advantage of the relation network architecture to precisely capture the profound similarity between samples. Additionally, to make SS-RN more suitable for HSI classification, an entire three-dimensional (3-D) neighborhood instead of the isolated pixel is considered to explore both spectral and spatial information thoroughly. Multiple support samples are also selected for each class to make sure that the extracted features are stable enough to avoid the intraclass similarity and interclass dissimilarity problem of HSI. The network is composed on the basis of 3-D convolutional neural network blocks with the aim of extracting spatial-spectral features. Extensive experiments on three widely used HSI datasets demonstrate that the proposed method can achieve better classification accuracy than conventional deep learning methods with limited labeled training samples. © 2008-2012 IEEE.","Few-shot learning; hyperspectral image classification; relation network (RN); small sample size; three-dimensional (3-D) convolutional neural networks","Classification (of information); Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Hyperspectral imaging; Image classification; Learning systems; Sampling; Spectroscopy; Classification accuracy; Few-shot learning; High dimensionality; Hyperspectral imagery; relation network (RN); Small Sample Size; Spatial informations; Threedimensional (3-d); Network architecture; artificial neural network; image classification; pixel; remote sensing"
"Detecting Hardly Visible Roads in Low-Resolution Satellite Time Series Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081339737&doi=10.1109%2fBigData47090.2019.9006251&partnerID=40&md5=0580850d0b67d608f4094f6f467ca899","Massive amounts of satellite data have been gathered over time, holding the potential to unveil a spatiotemporal chronicle of the surface of Earth. These data allow scientists to investigate various important issues, such as land use changes, on a global scale. However, not all land-use phenomena are equally visible on satellite imagery. In particular, the creation of an inventory of the planet's road infrastructure remains a challenge, despite being crucial to analyze urbanization patterns and their impact. Towards this end, this work advances data-driven approaches for the automatic identification of roads based on open satellite data. Given the typical resolutions of these historical satellite data, we observe that there is inherent variation in the visibility of different road types. Based on this observation, we propose two deep learning frameworks that extend state-of-the-art deep learning methods by formalizing road detection as an ordinal classification task. In contrast to related schemes, one of the two models also resorts to satellite time series data that are potentially affected by missing data and cloud occlusion. Taking these time series data into account eliminates the need to manually curate datasets of high-quality image tiles, substantially simplifying the application of such models on a global scale. We evaluate our approaches on a dataset that is based on Sentinel 2 satellite imagery and OpenStreetMap vector data. Our results indicate that the proposed models can successfully identify large and medium-sized roads. We also discuss opportunities and challenges related to the detection of roads and other infrastructure on a global scale. © 2019 IEEE.","Big Data; Deep Learning; Remote Sensing; Road and Infrastructure Detection; Satellite Data; Segmentation","Automatic identification; Big data; Deep learning; Image segmentation; Land use; Learning systems; Remote sensing; Roads and streets; Time series; Data-driven approach; High quality images; Learning frameworks; Ordinal classification; Road infrastructures; Satellite data; State of the art; Time-series data; Satellite imagery"
"Abundance-Guided Superpixels and Recurrent Neural Network for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078704768&doi=10.1109%2fDICTA47822.2019.8946060&partnerID=40&md5=2d698cc946528230390ba6cab70dc495","Mixed spectral responses from different ground materials often create confusions in complex remote sensing scenes and restrict classification performance. In this regard, unmixing approaches are being successfully carried out to decompose mixed pixels into a collection of spectral signatures. In this paper, we propose a method to integrate unmixing into a deep feature learning model in order to classify hyperspectral data. We propose to generate superpixels from the abundance estimations of the underlying materials of the image obtained from an unsupervised endmember extraction algorithm called vertex component analysis (VCA). The mean abundances of the superpixels are then used as features for a deep classifier. Our proposed deep model, formulated as a joint convolutional neural network and recurrent neural network, receives significant spectral-spatial information in the data to produce better and powerful features and achieve improved classification performance than several alternative methods. © 2019 IEEE.","Image Classification; Recurrent Neural Network (RNN); Unmixing; Vertex Component Analysis (VCA)","Deep learning; Image analysis; Image classification; Recurrent neural networks; Remote sensing; Spectroscopy; Superpixels; Abundance estimation; Classification performance; Convolutional neural network; Deep feature learning; Recurrent neural network (RNN); Unmixing; Unsupervised endmember extraction; Vertex component analysis; Classification (of information)"
"Convolutional neural network with dilated anchors for object detection in very high resolution satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084699804&doi=10.1109%2fICCES48960.2019.9068145&partnerID=40&md5=7724e734f4764e7d80da035e84844bba","Nowadays, object detection has acquired a great concentration either in ordinary images or satellite images. For satellite images, object detection is a challenging problem because objects have different scales and sparsity with very complicated background. Recent deep learning approaches have achieved breaking results for object detection than traditional ones. The ability of bounding boxes to catch existing objects with a complete and precise manner is still a challenging problem. We propose a dilated anchor method based on You Only Look Once version 3(YOLOv3) algorithm to make object detection more flexible and precise. The proposed method uses greater size anchor bounding boxes with about 30 % to 40 % larger than the traditional ones. This increase in anchor size increases the ability to catch more class objects with less influence on location detection. The experimental results using public NWPU VHR-10 dataset demonstrate the effectiveness of the proposed method in object detection of most classes and increase the overall accuracy with minimal effect on the precise location. © 2019 IEEE.","Artificial Intelligence; Convolutional Neural Networks; Object Detection; Remote Sensing (RS); Satellite Images; YOLO Algorithm","Convolutional neural networks; Deep learning; Object recognition; Satellites; Class objects; Learning approach; Location detection; Minimal effects; Overall accuracies; Precise locations; Satellite images; Very high resolution satellite images; Object detection"
"Convolutional Neural Networks enable efficient, accurate and fine-grained segmentation of plant species and communities from high-resolution UAV imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075712641&doi=10.1038%2fs41598-019-53797-9&partnerID=40&md5=36d9812dc9d6244a6775590f843f7ab0","Recent technological advances in remote sensing sensors and platforms, such as high-resolution satellite imagers or unmanned aerial vehicles (UAV), facilitate the availability of fine-grained earth observation data. Such data reveal vegetation canopies in high spatial detail. Efficient methods are needed to fully harness this unpreceded source of information for vegetation mapping. Deep learning algorithms such as Convolutional Neural Networks (CNN) are currently paving new avenues in the field of image analysis and computer vision. Using multiple datasets, we test a CNN-based segmentation approach (U-net) in combination with training data directly derived from visual interpretation of UAV-based high-resolution RGB imagery for fine-grained mapping of vegetation species and communities. We demonstrate that this approach indeed accurately segments and maps vegetation species and communities (at least 84% accuracy). The fact that we only used RGB imagery suggests that plant identification at very high spatial resolutions is facilitated through spatial patterns rather than spectral information. Accordingly, the presented approach is compatible with low-cost UAV systems that are easy to operate and thus applicable to a wide range of users. © 2019, The Author(s).",,"article; computer vision; convolutional neural network; deep learning; human; human experiment; image analysis; imagery; plant identification; vegetation"
"Hyperspectral image classification using SWT and CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082868455&doi=10.1109%2fICIT48102.2019.00037&partnerID=40&md5=af617f1aa46324be53e66e6910f91e73","The new paradigm of remote sensing captures huge volume of hyperspectral images of earth concern to various fields in human kind. Processing of such high-volume multidi-mensional image datasets, object detection, feature extraction, prediction and hyperspectral image classification are some of the active issues in current computing arena. So far many methods have been developed to classify hyperspectral images. This paper investigates latest approaches and proposes a model using stationary wavelet transform(SWT), principal component analysis(PCA) and convolutional neural networks(CNN). SWT is used to extract the meaningful features from the hyperspectral data. Later, PCA selects a subset of transformed coefficients for classification. A CNN has been designed to classify the selected transformed coefficients. Experiment analysis of the model is performed with two popular online hyperspectral image datasets, where the model based classification accuracy outperforms to other models being discussed in the paper. © 2019 IEEE.","Convolutional neural network (CNN); Deep learning; Hyperspectral image spatial features; Principal component analysis (PCA); Stationary wavelet transform (SWT)","Classification (of information); Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Feature extraction; Object detection; Principal component analysis; Remote sensing; Spectroscopy; Wavelet transforms; Current computing; Experiment analysis; High volumes; Hyperspectral Data; Image datasets; Model-based classifications; Spatial features; Stationary wavelet transforms; Image classification"
"Tree cover for the year 2010 of the metropolitan region of São Paulo, Brazil","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075464535&doi=10.3390%2fdata4040145&partnerID=40&md5=1440c70457692651c08940253174a754","Mapping urban trees with images at a very high spatial resolution (≤1 m) is a particularly relevant recent challenge due to the need to assess the ecosystem services they provide. However, due to the effort needed to produce these maps from tree censuses or with remote sensing data, few cities in the world have a complete tree cover map. Here, we present the tree cover data at 1-m spatial resolution of the Metropolitan Region of São Paulo, Brazil, the fourth largest urban agglomeration in the world. This dataset, based on 71 orthorectified RGB aerial photographs taken in 2010 at 1-m spatial resolution, was produced using a deep learning method for image segmentation called U-net. The model was trained with 1286 images of size 64 × 64 pixels at 1-m spatial resolution, containing one or more trees or only background, and their labelled masks. The validation was based on 322 images of the same size not used in the training and their labelled masks. The map produced by the U-net algorithm showed an excellent level of accuracy, with an overall accuracy of 96.4% and an F1-score of 0.941 (precision = 0.945 and recall = 0.937). This dataset is a valuable input for the estimation of urban forest ecosystem services, and more broadly for urban studies or urban ecological modelling of the São Paulo Metropolitan Region. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image segmentation; U-net model; Urban ecosystem services; Urban tree cover",
"3-D Deep Feature Construction for Mobile Laser Scanning Point Cloud Registration","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068502745&doi=10.1109%2fLGRS.2019.2910546&partnerID=40&md5=abb1ee5b46f606daef1d6bf190606ce0","Due to errors in sensors and positioning, there exist mismatches between different phases of mobile laser scanning point clouds, which impede the application of point cloud, such as changing detection and deformation monitoring. To rectify such mismatches, we designed a 3-D deep feature construction method for point cloud registration. The proposed method combines two 3-D convolutional neural networks into a uniform deep learning model to extract 3-D deep features. First, the corresponding points and noncorresponding points are set to train the deep learning model to minimize the distance between corresponding points' features and maximize the distance between features of noncorresponding points. Second, in the test phase, the 3-D deep feature for each keypoint was extracted by the trained deep learning model. This could be used to determine the corresponding points by the k -dimensional tree and random sample consensus (RANSAC) algorithm. Finally, a transformation matrix was calculated based on the corresponding points and was then applied to point cloud registration. The experimental results illustrated that the proposed method of using 3-D deep features is more efficient at a corresponding point search than representatives of three existing methods. It also improved registration accuracy. © 2019 IEEE.","3-D deep features; convolutional neural networks (CNNs); mobile laser scanning (MLS) point clouds; registration","Convolution; Deep neural networks; Linear transformations; Neonatal monitoring; Neural networks; Scanning; Surface measurement; Trees (mathematics); 3-D deep features; Convolutional neural network; Laser scanning point clouds; Point cloud; Point cloud registration; Random sample consensus (RANSAC) algorithm; registration; Transformation matrices; Laser applications; algorithm; artificial neural network; construction method; coordinate; deformation mechanism; image processing; remote sensing; three-dimensional modeling"
"A Super-resolution convolutional-neural-network-based approach for subpixel mapping of hyperspectral images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079342911&doi=10.1109%2fJSTARS.2019.2941089&partnerID=40&md5=b0f582be26a8ea54f88c1897f7e11ae0","A new subpixel mapping (SPM) method based on a super-resolution convolutional neural network (SRCNN) is proposed to generate subpixel land cover maps for hyperspectral images. The SRCNN is used to restore the image spatial resolution from a coarse input image, which is equivalent to interpolation. First, an efficient subpixel convolutional neural network, which is a state-of-The-Art SRCNN, is utilized to calculate the subpixel soft class value via a transfer learning strategy. Then, a classifier is used to transform the subpixel soft class values to hard-classified land cover maps with the constraint of fraction images. Experiments on three different hyperspectral images demonstrate that the SPM accuracy of the proposed SRCNN-based method is significantly better than those of three traditional SPM methods. In addition, the SRCNN-based SPM method has a simplified calculation process, does not require training data, and is less time consuming. This article provides a new solution for SPM of hyperspectral images. © 2008-2012 IEEE.","Deep learning; hyperspectral remote sensing image; subpixel mapping (SPM); super-resolution convolutional neural network (SRCNN); transfer learning (TL)","Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Optical resolving power; Pixels; Remote sensing; Spectroscopy; Transfer learning; Fraction images; Hyperspectral Remote Sensing Image; Image spatial resolution; Land cover maps; Simplified calculations; State of the art; Sub-pixel mapping; Super resolution; Mapping; artificial neural network; image resolution; land cover; machine learning; mapping method; multispectral image; pixel; remote sensing; spatial resolution"
"Application of neural network to GNSS-R wind speed retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075645481&doi=10.1109%2fTGRS.2019.2929002&partnerID=40&md5=b273d0bc048467de7e70fabda75cf3ac","This paper applies a machine learning (ML) algorithm based on the multi-hidden layer neural network (MHL-NN) for ocean surface wind speed estimation using global navigation satellite system (GNSS) reflection measurements. Unlike conventional wind speed retrieval methods that often depend on limited scalar delay-Doppler map (DDM) observables, the proposed MHL-NN makes use of information captured by the entire DDM. Both simulated and real data sets are used to train and evaluate the performance of the MHL-NN and compare it to a conventional wind speed retrieval method and other prevailing ML algorithms. The results show that the MHL-NN algorithm outperforms the other methods in terms of the root mean square error (RMSE) and mean absolute percentage error (MAPE) of the wind speed estimation. © 1980-2012 IEEE.","Advanced Scatterometer (ASCAT); cyclone global navigation satellite system (CYGNSS); deep learning; delay-Doppler map (DDM); GNSS-reflectometry (GNSS-R); multi-hidden layer neural network (MHL-NN); spaceborne remote sensing; wind speed retrieval","Communication satellites; Deep learning; Deep neural networks; Global positioning system; Information retrieval; Information use; Machine learning; Mean square error; Meteorological instruments; Multilayer neural networks; Network layers; Remote sensing; Salinity measurement; Speed; Storms; Delay-doppler maps; Global Navigation Satellite Systems; Hidden layers; Reflectometry; Scatterometers; Spaceborne remote sensing; Wind speed; Wind; algorithm; artificial neural network; ASCAT; GNSS; machine learning; reflectometry; remote sensing; sea surface; surface wind; wind velocity"
"Respiration monitoring for premature neonates in NICU","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076685812&doi=10.3390%2fapp9235246&partnerID=40&md5=e638497ada53e03e0381f206f31265c0","In this paper, we investigate an automated pipeline to estimate respiration signals from videos for premature infants in neonatal intensive care units (NICUs). Two flow estimation methods, namely the conventional optical flow- and deep learning-based flow estimation methods, were employed and compared to estimate pixel motion vectors between adjacent video frames. The respiratory signal is further extracted via motion factorization. The proposed methods were evaluated by comparing our automated extracted respiration signals to that extracted from chest impedance on videos of five premature infants. The overall average cross-correlation coefficients are 0.70 for the optical flow-based method and 0.74 for the deep flow-based method. The average root mean-squared errors are 6.10 and 4.55 for the optical flow- and the deep flow-based methods, respectively. The experimental results are promising for further investigation and clinical application of the video-based respiration monitoring method for infants in NICUs. © 2019 by the authors.","Biomedical monitoring; Remote sensing; Respiration; Respiration rate; Video processing",
"Super-resolution of PROBA-V images using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081573720&doi=10.1007%2fs42064-019-0059-8&partnerID=40&md5=d7e98b1dab609f8e07ea71566a986583","European Space Aqency (ESA)’s PROBA-V Earth observation (EO) satellite enables us to monitor our planet at a large scale to study the interaction between vegetation and climate, and provides guidance for important decisions on our common global future. However, the interval at which high-resolution images are recorded spans over several days, in contrast to the availability of lower-resolution images which is often daily. We collect an extensive dataset of both high- and low-resolution images taken by PROBA-V instruments during monthly periods to investigate Multi Image Super-resolution, a technique to merge several low-resolution images into one image of higher quality. We propose a convolutional neural network (CNN) that is able to cope with changes in illumination, cloud coverage, and landscape features which are introduced by the fact that the different images are taken over successive satellite passages at the same region. Given a bicubic upscaling of low resolution images taken under optimal conditions, we find the Peak Signal to Noise Ratio of the reconstructed image of the network to be higher for a large majority of different scenes. This shows that applied machine learning has the potential to enhance large amounts of previously collected EO data during multiple satellite passes. © 2019, Tsinghua University Press.","convolutional neural network (CNN); deep learning; Earth observation (EO); remote sensing; super-resolution imaging",
"PERSIANN-CNN: Precipitation estimation from remotely sensed information using artificial neural networks–convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076544546&doi=10.1175%2fJHM-D-19-0110.1&partnerID=40&md5=6f6f49ee263571a6b9ccc9f01e467f02","Accurate and timely precipitation estimates are critical for monitoring and forecasting natural disasters such as floods. Despite having highŠresolution satellite information, precipitation estimation from remotely sensed data still suffers from methodological limitations. StateŠofŠtheŠart deep learning algorithms, renowned for their skill in learning accurate patterns within large and complex datasets, appear well suited to the task of precipitation estimation, given the ample amount of highŠresolution satellite data. In this study, the effectiveness of applying convolutional neural networks (CNNs) together with the infrared (IR) and water vapor (WV) channels from geostationary satellites for estimating precipitation rate is explored. The proposed model performances are evaluated during summer 2012 and 2013 over central CONUS at the spatial resolution of 0.088 and at an hourly time scale. Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks (PERSIANN)–Cloud Classification System (CCS), which is an operational satelliteŠbased product, and PERSIANN–Stacked Denoising Autoencoder (PERSIANNŠSDAE) are employed as baseline models. Results demonstrate that the proposed model (PERSIANNŠCNN) provides more accurate rainfall estimates compared to the baseline models at various temporal and spatial scales. Specifically, PERSIANNŠCNN outperforms PERSIANNŠCCS (and PERSIANNŠSDAE) by 54% (and 23%) in the critical success index (CSI), demonstrating the detection skills of the model. Furthermore, the rootŠmean-square error (RMSE) of the rainfall estimates with respect to the National Centers for Environmental Prediction (NCEP) Stage IV gauge–radar data, for PERSIANNŠCNN was lower than that of PERSIANNŠCCS (PERSIANNŠSDAE) by 37% (14%), showing the estimation accuracy of the proposed model. © 2019 American Meteorological Society.",,"algorithm; artificial neural network; data set; geostationary satellite; precipitation assessment; rainfall; remote sensing; satellite data; spatial resolution; water vapor"
"Statistical retrieval of atmospheric profiles with deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074405842&doi=10.1016%2fj.isprsjprs.2019.10.002&partnerID=40&md5=e498b77714988a6dec39331f5e3c9c43","Infrared atmospheric sounders, such as IASI, provide an unprecedented source of information for atmosphere monitoring and weather forecasting. Sensors provide rich spectral information that allows retrieval of temperature and moisture profiles. From a statistical point of view, the challenge is immense: on the one hand, “underdetermination” is common place as regression needs to work on high dimensional input and output spaces; on the other hand, redundancy is present in all dimensions (spatial, spectral and temporal). On top of this, several noise sources are encountered in the data. In this paper, we present for the first time the use of convolutional neural networks for the retrieval of atmospheric profiles from IASI sounding data. The first step of the proposed pipeline performs spectral dimensionality reduction taking into account the signal to noise characteristics. The second step encodes spatial and spectral information, and finally prediction of multidimensional profiles is done with deep convolutional networks. We give empirical evidence of the performance in a wide range of situations. Networks were trained on orbits of IASI radiances and tested out of sample with great accuracy over competing approximations, such as linear regression (+32%). We also observed an improvement in accuracy when predicting over clouds, thus increasing the yield by 34% over linear regression. The proposed scheme allows us to predict related variables from an already trained model, performing transfer learning in a very easy manner. We conclude that deep learning is an appropriate learning paradigm for statistical retrieval of atmospheric profiles. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Atmospheric measurements; Information retrieval; Infrared measurements; Neural networks","Convolution; Information retrieval; Neural networks; Regression analysis; Signal to noise ratio; Weather forecasting; Atmosphere monitoring; Atmospheric measurement; Convolutional networks; Convolutional neural network; Dimensionality reduction; Infrared Atmospheric Sounder; Infrared measurements; Retrieval of atmospheric profile; Deep neural networks; artificial neural network; IASI; instrumentation; measurement method; satellite data; weather forecasting"
"Semi-supervised Classification Method for Remote Sensing scene Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077344708&doi=10.1088%2f1742-6596%2f1345%2f3%2f032015&partnerID=40&md5=7ab9011b8f2fe41879bcc5d8592980a7","Aiming at the classification problem of remote sensing scenes, this paper proposes a semi-supervised deep learning method combining the generative generation network with the convolutional neural network. The method improves the loss function of the deep convolutional generation network, and designs the even convolutional network EM-10 and its transposed convolution network as discriminators and generators respectively, which improves its ability to extract remote sensing features. The trained discriminator is used for the classification of remote sensing scenes with an accuracy of over 90%. © 2019 Published under licence by IOP Publishing Ltd.",,"Convolution; Neural networks; Remote sensing; Supervised learning; Convolutional networks; Convolutional neural network; Learning methods; Loss functions; Scene-based; Semi-supervised; Semi-supervised classification method; Deep learning"
"Assessing flood severity from georeferenced photos","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077231078&doi=10.1145%2f3371140.3371145&partnerID=40&md5=da527a6d65d5db5030ba4b3c76779e2d","The use of georeferenced social media data in disaster and crisis management is increasing rapidly. Particularly in connection to flooding events, georeferenced images shared by citizens can provide situational awareness to emergency responders, as well as assistance to financial loss assessment, giving information that would otherwise be very hard to collect through conventional sensors or remote sensing products. Moreover, recent advances in computer vision and deep learning can perhaps support the automated analysis of these data. In this paper, focusing on ground-level images taken by humans during flooding events, we evaluate the use of deep convolutional neural networks for (i) discriminating images showing direct evidence of a flood, and (ii) estimating the severity of the flooding event. Considering distinct datasets (i.e., the European Flood 2013 dataset, and data from different editions of the MediaEval Multimedia Satellite Task), we specifically evaluated models based on the DenseNet and EfficientNet neural network architectures, concluding that these models for image classification can achieve a very high accuracy on both tasks. © 2019 Association for Computing Machinery.","Analysis of Georeferenced Images; Convolutional Neural Networks; Deep Learning for Computer Vision; Flood Detection","Classification (of information); Computer vision; Convolution; Floods; Information retrieval; Losses; Network architecture; Neural networks; Remote sensing; Automated analysis; Conventional sensors; Convolutional neural network; Emergency responders; Flood detections; Geo-referenced images; Multimedia satellites; Situational awareness; Deep neural networks"
"Digital image forgery detection using deep learning approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077330440&doi=10.1088%2f1742-6596%2f1368%2f3%2f032028&partnerID=40&md5=50843ce30991dfa16a9eb4355b6a3d73","This paper presents an algorithm for detecting one of the most commonly used types of digital image forgeries-splicing. The algorithm is based on the use of the VGG-16 convolutional neural network. The proposed network architecture takes image patches as input and obtains classification results for a patch: original or forgery. On the training stage we select patches from original image regions and on the borders of embedded splicing. The obtained results demonstrate high classification accuracy (97.8% accuracy for fine-Tuned model and 96.4% accuracy for the zero-stage trained) for a set of images containing artificial distortions in comparison with existing solutions. Experimental research was conducted using CASIA dataset. © Published under licence by IOP Publishing Ltd.",,"Computer crime; E-learning; Image processing; Nanotechnology; Network architecture; Neural networks; Remote sensing; Classification accuracy; Classification results; Convolutional neural network; Digital image; Experimental research; Image patches; Learning approach; Original images; Deep learning"
"Crop Identification of Drone Remote Sensing Based on Convolutional Neural Network [基于卷积神经网络的无人机遥感农作物分类]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076423931&doi=10.6041%2fj.issn.1000-1298.2019.11.018&partnerID=40&md5=3d63febf09bbcd8cc2384e0466ffe0fb","Crop identification and classification, as a fundamental part of modern agriculture, is an important prerequisite for ensuring regional food security and sustainable agricultural development. With the development of remote sensing technology, high-resolution visible light remote sensing imagery has become a convenient and reliable source of remote sensing data. At present, the use of these remote sensing data to carry out detailed classification research of typical crops has extremely important practical significance. However, these remote sensing images lack enough spectral information and therefore are unable to give high-accuracy recognition of the crops, it is necessary to apply deep learning techniques on the crops classification research to solve these problems. Based on composite wing unmanned aerial vehicle which equipped a remote sensing imaging equipment to obtain remote sensing data, including cotton, corn and squash, covering an area of over 867 hm2. According to the characteristics of the data, the convolutional neural network (CNN) was designed to extract the crop classification information. By adjusting the network parameters and the sample spectral combination, the parameter optimization of the training process was divided into two levels: at the level of network parameters, the adjustment included the learning rate and the batch, the scale of the convolution kernel and the depth of the network; at the spectral feature level of the sample, three types of samples included single-band, dual-band and triple-band were constructed as inputs, and the model was trained in turn. The experimental results showed that the CNN can effectively extract the crop information in the image and realize the fine classification of crops. Most sensing areas had qualified classification result except the edge places planted with sparse and mixed crops. The overall classification accuracy achieved 97.75% by using the optimized CNN. Compared with the support vector machine based on radial basis kernel function and the back propagation neural network, the optimized CNN had the best effect and the highest classification accuracy. It was worthy of noting that the adjustment of network parameters would affect the final training effect. A CNN model with large learning rate (0.1), small convolutional kernel (7×7) and appropriate depth (7) was advised on the basis that the typical crops in the remote sensing images were with high density, small features and rich textures. This promised the small features in the sample would not be missed when the convergence of the training accuracy was increased. Samples with different spectral features also had an impact on the training of the model. The blue band in the visible light was more appropriate than the green and red ones on training the CNN for crops recognition. A combination of the three bands would improve the recognition accuracy and stability, but more training time was required since more input information was given. The experiments demonstrated the effectiveness and reliability of the proposed CNN on crops fine classification. This method can be regarded as a reference for the application of deep learning in agricultural remote sensing. © 2019, Chinese Society of Agricultural Machinery. All right reserved.","Convolutional neural network; Crop classification; Deep learning; Drone remote sensing","Antennas; Backpropagation; Classification (of information); Convolution; Crops; Deep learning; Deep neural networks; Drones; Food supply; Learning algorithms; Light; Radial basis function networks; Support vector machines; Textures; Wings; Agricultural remote sensing; Back propagation neural networks; Classification accuracy; Convolutional neural network; Crop classification; High resolution visible; Remote sensing technology; Sustainable agricultural development; Remote sensing"
"Photonic human identification based on deep learning of back scattered laser speckle patterns","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075714923&doi=10.1364%2fOE.27.036002&partnerID=40&md5=b7928ccd4250ba767c15cbd7ab30a5dd","The analysis of the dynamics of speckle patterns that are generated when laser light is back scattered from a tissue has been recently shown as very applicable for remote sensing of various bio-medical parameters. In this work, we present how the analysis of a static single speckle pattern scattered from the forehead of a subject, together with advanced machine learning techniques based on multilayered neural networks, can offer novel approach to accurate identification within a small predefined number of classes (e.g., a ‘smart home’ setting which restricts its operations for family members only). Processing the static scattering speckle pattern by neural networks enables extraction of unique features with no previous expert knowledge being required. Using the right model allows for a very accurate differentiation between desirable categories, and that model can form a basis for using speckles patterns as a form of identity measure of ‘forehead-print’. © 2019 Optical Society of America under the terms of the OSA Open Access Publishing Agreement.",,"Automation; Remote sensing; Speckle; Dynamics of speckle pattern; Expert knowledge; Human identification; Laser speckle pattern; Machine learning techniques; Multilayered neural networks; Number of class; Speckle patterns; Deep learning; algorithm; human; photon; theoretical model; Algorithms; Deep Learning; Humans; Models, Theoretical; Photons"
"Building segmentation in high-resolution remote sensing image through deep neural network and conditional random fields [深度神经网络条件随机场高分辨率遥感图像建筑物分割]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076546874&doi=10.11834%2fjrs.20198141&partnerID=40&md5=baca1967ce96bbaee034ed484606fd4f","The core of building segmentation in high-resolution remote sensing image is to establish the mapping from an image feature space to a segmentation result with high dimension and strong nonlinearity. In a high-resolution remote sensing image, a building frequently emerges at any location in the entire image, thereby indicating that non-neighborhood pixels may be related to the current semantic segmentation pixel. The segmentation precision and generalization are significantly improved by adopting a Deep Neural Network (DNN) to extract the features and learn the nonlinear mapping in image segmentation. However, the non-neighborhood feature cannot be directly extracted by the DNN. This study presents an encoder-decoder deep learning architecture with ResNet and Conditional Random Field (CRF) for building semantic segmentation in a high-resolution remote sensing image to obtain high segmentation precision and reduce the obstacles from roads, staggered floors, and shadows. In the DNN, ResNet is used to establish the encoder for automatically extracting the building features, in which ResNet avoids the problems of vanishing and exploding gradient and accelerates the convergence of DNN weights. Before each convolution operation, batch normalization is adopted to normalize the sampling data and reduce the training difficulty of the DNN. Then, transposed convolution is applied to establish the decoder for reconstructing the image while segmenting the buildings. At the end of the DNN, the CRF is used to adjust the raw segmentation produced by the decoder. The value of a unary potential function in the CRF is given by the raw result of the decoder, and the pairwise potential function denotes the feature of pixel pairs in the entire image, which constructs a fully connected CRF (FCCRF). Considering that the calculation of FCCRF is considerable, a mean field algorithm is used to approximate the pairwise potential function value. Thus, convolution is used to obtain the pairwise potential function value, and a high-dimensional Gaussian filter is applied to implement the convolution operation. The mean field algorithm is implemented through an RNN mechanism. Thus, FCCRF becomes a part of the DNN, and the parameters of the CRF are trained with the encoder and decoder simultaneously. Experiments are conducted to validate the effectiveness of the proposed methodology. The remote sensing image dataset is Inria Aerial Image Labeling Dataset. A total of 4500 samples with 1000×1000×3 pixels are found in each sample, in which their resolution is 0.3 m. The typical kinds of building, such as building with order, single building with complicated roof, and building without order, are segmented through VGG, ResNet, and the proposed methodology (denoted as ResNetCRF), correspondingly. The results show that ResNetCRF overcomes the interruption of roads in which their color features are similar to the building and effectively reduces the disturbance of staggered floors and shadows. Thus, ResNetCRF obtains the optimal segmentation precision. The multi-resolution experiment demonstrates that ResNetCRF has a strong generalization under a limited range of resolution change. Accurate mapping of building segmentation is established to reduce the disturbance of roads, shadows, and staggered floors by introducing CRFs in the encoder-decoder based on ResNet to segment the building in a high-resolution remote sensing image. In the future work, we will investigate the reduction of FCCRF calculation, overcome the missing segmentation of small buildings, and reduce the segmentation errors of a building whose color feature is similar to the background without a noticeable edge. © 2019, Science Press. All right reserved.","Building segmentation; Conditional random fields; Deep neural network; High resolution remote sensing image","Antennas; Buildings; Convolution; Decoding; Deep neural networks; Floors; Image enhancement; Mapping; Object recognition; Pixels; Random processes; Remote sensing; Semantics; Signal encoding; Space optics; Conditional random field; High resolution remote sensing images; Learning architectures; Optimal segmentation; Remote sensing images; Segmentation precision; Segmentation results; Semantic segmentation; Image segmentation"
"A method of building detection in remote sensing images based on deep learning with multiple lightness detectors","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081607887&doi=10.5194%2fisprs-archives-XLII-4-W20-27-2019&partnerID=40&md5=dc2ab777e39f6b1ab581805c532ab4af","Buildings, where most human activities happen, are one of the most important crucial objects in remote sensing images. Extracting building information is of great significance importance for conducting sustainable development-related researches. The extracted building information is a fundamental data source for further researches, including evaluating the living conditions of people, monitoring building conditions, predicting disaster risks and so on. In recent years, convolutional neural networks have been widely employed in building detection, and have gained significant progresses. However, in these automatic detection procedures, the critical brightness information is often neglected, with all buildings simply classified into the same category. To make the building detection more efficient and precise, we propose a simple yet efficient multitask method employing several lightness detectors, each of which is dedicated to the building detection in a specific brightness interval. Experiment results show that the building detection accuracy could be improved by 8.1% with the assistance of the additional lightness information. © Authors 2019. CC BY 4.0 License.","Building detection; Deep learning; Multiple lightness detectors; Remote sensing","Buildings; Convolutional neural networks; Luminance; Remote sensing; Automatic Detection; Building conditions; Building detection; Extracting buildings; Human activities; In-buildings; Living conditions; Remote sensing images; Deep learning"
"ISPRS and GEO Workshop on Geospatially-enabled SDGs Monitoring for the 2030 Agenda","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081595935&partnerID=40&md5=d78e9108758f407ad03e15e3fb7c6f22","The proceedings contain 17 papers. The topics discussed include: SIMILE, a geospatial enabler of the monitoring of sustainable development goal 6 (ensure availability and sustainability of water for all); estimating aboveground biomass of bamboo and mixed bamboo forest in Thua Thien-Hue Province, Vietnam using PALSAR-2 and LANDSAT OLI data; an approach for evaluating the information content of remote sensing images; a method of building detection in remote sensing images based on deep learning with multiple lightness detectors; fair and standard access to spatial data as the means for achieving sustainable development goals; and comparison and analysis of the accuracy of gee platform pixel-based supervised classification-taking Shandong Province as an example.",,
"A temporal-spatial interpolation and extrapolation method based on geographic Long Short-Term Memory neural network for PM2.5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069828727&doi=10.1016%2fj.jclepro.2019.117729&partnerID=40&md5=5328596ab68f5d22bc3ccd05066254af","Nowadays, real-time air pollution monitoring has been an important approach for supporting pollution control and reduction. However, due to the high construction cost and limited detection range of monitoring stations, not all the air pollutant concentrations in every corner can be monitored, and a whole picture of the spatial distribution of air pollution is usually lacked for comprehensive spatial analysis and air quality control. To address this problem, satellite remote sensing and spatial interpolation/extrapolation technologies have been commonly used in past research. However, the spatial distribution calculated by remote sensing techniques could be less accurate due to the limited amount of recorded data for testing and adjustments. Performance of traditional spatial interpolation/extrapolation techniques, such as Kriging and IDW, was limited by several subjective assumptions and pre-set formulations that are less suitable for non-linear real-world situations. As an alternative, machine learning and neural network-based methods have been proposed recently. However, most of these methods failed to well consider the long short temporal trend and spatial associations of air pollution simultaneously. To overcome these limitations, this paper proposed a newly designed spatial interpolation/extrapolation methodology namely Geo-LSTM to generate the spatial distribution of air pollutant concentrations. The model was developed based on the Long Short-Term Memory (LSTM) neural network to capture the long-term dependencies of air quality. A geo-layer was designed to integrate the spatial-temporal correlation from other monitoring stations. To evaluate the effectiveness of the proposed methodology, a case study in Washington state was conducted. The experimental results show that Geo-LSTM has a RMSE of 0.0437, and is almost 60.13% better than traditional methods like IDW. © 2019 Elsevier Ltd","Air quality; Deep learning; Geographic LSTM; PM2.5; Spatial extrapolation","Air pollution control; Air quality; Brain; Deep learning; Edge detection; Extrapolation; Interpolation; Pollution detection; Quality control; Remote sensing; Spatial distribution; Air pollutant concentrations; Air pollution monitoring; Geographic LSTM; Long-term dependencies; PM2.5; Remote sensing techniques; Satellite remote sensing; Spatial-temporal correlation; Long short-term memory"
"Scaling activity recognition using channel state information through convolutional neural networks and transfer learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076591006&doi=10.1145%2f3363347.3363362&partnerID=40&md5=501dcc95d5042fe1f9ce14fe6e2e5901","Unobtrusive sensing is receiving much attention in recent years, as it is less obtrusive and more privacy-Aware compared to other monitoring technologies. Human activity recognition is one of the fields in which unobtrusive sensing is heavily researched as this is especially important in health care. In this regard, investigating WiFi signals, and more specifically 802.11n channel state information, is one of the more prominent research fields. However, there is a challenge in scaling it up. Transfer learning is rarely applied, and when applied, it is done on filtered/modified data or extracted features. This paper focuses on two aspects. First, convolutional networks are used across multiple participants, days and activities and analysis is done based on these results. Secondly, it looks into the possibility of applying transfer learning based on raw channel state information over multiple participants and activities over multiple days. Results show channel state information is accurate for single participants (F1-score of 0.90), but sensitive to different participants and fluctuating WiFi signals over days (F1-score of 0.25-0.35). Furthermore, results show both clustering and transfer learning can be applied to increase the performance to 0.80 when using minimal resources and retraining. © 2019 Association for Computing Machinery.","Channel state information; Convolutional neural networks; Datasets; Deep learning; Human activity recognition; Remote sensing; Transfer learning","Convolution; Data communication systems; Deep learning; Deep neural networks; Internet of things; Machine learning; Neural networks; Pattern recognition; Remote sensing; Wireless local area networks (WLAN); Activity recognition; Convolutional networks; Convolutional neural network; Datasets; Human activity recognition; Monitoring technologies; Research fields; Transfer learning; Channel state information"
"Keras Spatial Extending deep learning frameworks for preprocessing and on-the-fly augmentation of geospatial data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075569591&doi=10.1145%2f3356471.3365240&partnerID=40&md5=e508b8d543af4f288acba22eaf1e013f","The application of deep learning techniques to remote sensing and geospatial data is a burgeoning area of research. However, most state-of-the-art systems have their roots in computer vision with procedures that do not necessarily lend themselves to remote sensing data. On the contrary, managing geospatial data require handling different projections, spatial resolutions and data formats. We present Keras Spatial, a python package for preprocessing and augmenting geospatial data. Keras Spatial provides three main components (1) a spatial data generator class, which is similar to the Keras image data generator, (2) a GeoDataFrame for storing training samples boundaries and properties, (3) a callback mechanism for on-the-fly reprojection and data augmentation.The novelty of the developed package arises due to the flexibility to combine the components in different ways to solve a variety of data preparation problems. We demonstrate the usage of Keras Spatial package by preparing digital elevation data for a segmentation model, and replacing conventional manual preprocessing steps, such as tiling rasters to samples, masking samples outside of the study area, and adding digital elevation model derivatives. We also discuss advanced data preprocessing features of this package, such as accessing remote data source directly, combining different input rasters data regardless of their native projection and resolution, and decoupling the model configuration, input layer dimensions, from the geographic scale, where the latter feature allows training the same model to recognize geographic objects that exist at hierarchical scales. © 2019 Association for Computing Machinery.","Deep Learning; Geospatial Big Data; Image Preprocessing; Remote Sensing; Scientific Reproducibility","Data handling; Rasterization; Remote sensing; Digital elevation data; Digital elevation model; Geo-spatial; Image preprocessing; Learning techniques; Pre-processing step; Reproducibilities; State-of-the-art system; Deep learning"
"Urban flood mapping with residual patch similarity learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075583171&doi=10.1145%2f3356471.3365235&partnerID=40&md5=8b15074ca5b8fa5ea7865d593df607b3","Urban flood mapping is essential for disaster rescue and relief missions, reconstruction efforts, and financial loss evaluation. Much progress has been made to map the extent of flooding with multisource remote sensing imagery and pattern recognition algorithms. However, urban flood mapping at high spatial resolution remains a major challenge due to three main reasons: (1) the very high resolution (VHR) optical remote sensing imagery often has heterogeneous background involving various ground objects (e.g., vehicles, buildings, roads, and trees), making traditional classification algorithms fail to capture the underlying spatial correlation between neighboring pixels within the flood hazard area; (2) traditional flood mapping methods with handcrafted features as input cannot fully leverage massive available data, which requires robust and scalable algorithms; and (3) due to inconsistent weather conditions at different time of data acquisition, pixels of the same objects in VHR optical imagery could have very different pixel values, leading to the poor generalization capability of classical flood mapping methods. To address this challenge, this paper proposed a residual patch similarity convolutional neural network (ResPSNet) to map urban flood hazard zones using bi-temporal high resolution (3m) pre- and post-flooding multispectral surface reflectance satellite imagery. Besides, remote sensing specific data augmentation was also developed to remove the impact of varying illuminations due to different data acquisition conditions, which in turn further improves the performance of the proposed model. Experiments using the high resolution imagery before and after the 2017 Hurricane Harvey flood in Houston, Texas, showed that the developed ResPSNet model, along with associated remote sensing specific data augmentation method, can robustly produce flood maps over urban areas with high precision (0.9002), recall (0.9302), F1 score (0.9128), and overall accuracy (0.9497). The research sheds light on multi-temporal image fusion for high precision image change detection, which in turn can be used for monitoring natural hazards. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Deep learning; Flood extent estimation; Flood mapping; Patch similarity; Residual learning","Data acquisition; Deep learning; Hazards; Image fusion; Losses; Mapping; Neural networks; Optical correlation; Pattern recognition; Pixels; Remote sensing; Satellite imagery; Trees (mathematics); Classification algorithm; Convolutional neural network; Flood mapping; Generalization capability; Optical remote-sensing imagery; Patch similarity; Pattern recognition algorithms; Residual learning; Floods"
"Mapping Miscanthus using multi-temporal convolutional neural network and google earth engine","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075548067&doi=10.1145%2f3356471.3365242&partnerID=40&md5=0e020f99d8d9ef20a7b8c8f813f6dbad","Grasslands play an essential role in ecology and agriculture. Accurately mapping the grasslands at a large scale is essential for productivity monitoring, policymaking, and environmental assessment. The advancements in remote sensing and machine learning technologies have enabled the generation of high accuracy national level crop layers. Although the national crop layer for the US includes grasslands, it does not differentiate them well at the species level. To fill the gap of mapping grassland at the national scale with high accuracy, we used a Convolutional Long Short-Term Memory (Convolutional-LSTM) neural network model for grass identification using multi-temporal Sentinel-2 images. Miscanthus (Miscanthus x giganteus) is used as a case study for this short paper. The classification of Miscanthus using the Convolutional-LSTM model yielded a 98.8% accuracy, which is significantly higher than the 92% accuracy produced by a benchmark model 3-layer fully connected neural network. Additionally, we demonstrated the efficiency and effectiveness of cloud computing practices by implementing the entire analytical process in a cloud-based environment. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Agriculture; Deep learning; Grassland classification; Remote sensing","Agriculture; Convolution; Crops; Deep learning; Mapping; Remote sensing; Analytical process; Convolutional neural network; Environmental assessment; Fully connected neural network; Grassland classifications; Machine learning technology; Miscanthus x giganteus; Neural network model; Long short-term memory"
"The downscaling of the SMOS global sea surface salinity product based on MODIS data using a deep convolution network approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079039385&doi=10.1145%2f3373419.3373462&partnerID=40&md5=f61374b36695e984e77d71adb87394c8","Downscaling is a very important process to convert a coarse domain satellite product to a finer spatial resolution. In this paper, a deep learning based downscaling method was designed to improve the spatial resolution of the global sea surface salinity (SSS) products of Soil Moisture and Ocean Salinity (SMOS) satellite. The proposed algorithm is able to efficiently and effectively use high spatial-resolution Moderate Resolution Imaging Spectroradiometer (MODIS) satellite data to improve the spatial resolution of SMOS SSS products. © 2019 Association for Computing Machinery.","Deep convolutional network; Deep learning; Downscaling; MODIS; Remote Sensing; SMOS","Convolution; Deep learning; Image resolution; Radiometers; Remote sensing; Satellites; Soil moisture; Surface waters; Convolutional networks; Down-scaling; High spatial resolution; Moderate resolution imaging spectroradiometer satellites; MODIS; Sea surface salinity; SMOS; Soil Moisture and Ocean Salinity (SMOS); Image processing"
"Improved UAV opium poppy detection using an updated YOLOV3 model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074656469&doi=10.3390%2fs19224851&partnerID=40&md5=62d73b85e512315dc22a69cf3aa0445c","Rapid detection of illicit opium poppy plants using UAV (unmanned aerial vehicle) imagery has become an important means to prevent and combat crimes related to drug cultivation. However, current methods rely on time-consuming visual image interpretation. Here, the You Only Look Once version 3 (YOLOv3) network structure was used to assess the influence that different backbone networks have on the average precision and detection speed of an UAV-derived dataset of poppy imagery, with MobileNetv2 (MN) selected as the most suitable backbone network. A Spatial Pyramid Pooling (SPP) unit was introduced and Generalized Intersection over Union (GIoU) was used to calculate the coordinate loss. The resulting SPP-GIoU-YOLOv3-MN model improved the average precision by 1.62% (from 94.75% to 96.37%) without decreasing speed and achieved an average precision of 96.37%, with a detection speed of 29 FPS using an RTX 2080Ti platform. The sliding window method was used for detection in complete UAV images, which took approximately 2.2 sec/image, approximately 10× faster than visual interpretation. The proposed technique significantly improved the efficiency of poppy detection in UAV images while also maintaining a high detection accuracy. The proposed method is thus suitable for the rapid detection of illicit opium poppy cultivation in residential areas and farmland where UAVs with ordinary visible light cameras can be operated at low altitudes (relative height < 200 m). © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","CNN; Deep learning; GIoU; Object detection; Opium poppy; Spatial pyramid pooling; UAV; YOLOv3 model","Antennas; Deep learning; Image enhancement; Military vehicles; Object detection; Titanium compounds; Unmanned aerial vehicles (UAV); Generalized intersection; GIoU; Network structures; Opium poppy; Sliding window methods; Spatial pyramids; UAV (unmanned aerial vehicle); Visual interpretation; Aircraft detection; opiate; aerial plant part; altitude; devices; metabolism; Papaver; physiology; plant; remote sensing; Altitude; Opium; Papaver; Plant Components, Aerial; Plants; Remote Sensing Technology"
"Introduction to social sensing and big data computing for disaster management","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073476156&doi=10.1080%2f17538947.2019.1670951&partnerID=40&md5=73f74f6d5a3d5edd50104c748ab0f6be","Traditional data collection methods such as remote sensing and field surveying often fail to offer timely information during or immediately following disaster events. Social sensing enables all citizens to become part of a large sensor network, which is low cost, more comprehensive, and always broadcasting situational awareness information. However, data collected with social sensing is often massive, heterogeneous, noisy, unreliable from some aspects, comes in continuous streams, and often lacks geospatial reference information. Together, these issues represent a grand challenge toward fully leveraging social sensing for emergency management decision making under extreme duress. Meanwhile, big data computing methods and technologies such as high-performance computing, deep learning, and multi-source data fusion become critical components of using social sensing to understand the impact of and response to the disaster events in a timely fashion. This special issue captures recent advancements in leveraging social sensing and big data computing for supporting disaster management. Specifically analyzed within these papers are some of the promises and pitfalls of social sensing data for disaster relevant information extraction, impact area assessment, population mapping, occurrence patterns, geographical disparities in social media use, and inclusion in larger decision support systems. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","big data; natural hazards; Social media; spatial computing; VGI",
"Remote sensing image scene classification based on SURF feature and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082387832&doi=10.1109%2fAPSIPAASC47483.2019.9023118&partnerID=40&md5=bc3d08be6c59dc355432cfb59851a980","Remote sensing image scene classification is one of the key points in remote sensing image interpretation. The traditional remote sensing image scene classification feature performance is not strong, and the deep learning extraction semantic feature process is complex. This paper proposes a fusion feature remote sensing image scene classification method which is based on artificial features and deep learning semantic features. Firstly, the SURF feature of the remote sensing image is extracted and encoded by the VLAD algorithm. The semantic feature of a remote sensing image is extracted by transfer learning. Then the feature reduction is performed by PCA algorithm and feature fusion is performed. Finally, the scene classifier is trained by using the random forest algorithm. The experimental results show that the classification accuracy and Kappa coefficient of this method are higher and the method is effective. © 2019 IEEE.",,"Classification (of information); Decision trees; Deep learning; Image classification; Random forests; Semantics; Transfer learning; Classification accuracy; Feature reduction; Kappa coefficient; Learning semantics; Random forest algorithm; Remote sensing image interpretations; Remote sensing images; Semantic features; Remote sensing"
"Evaluation of Deep Learning CNN Model for Land Use Land Cover Classification and Crop Identification Using Hyperspectral Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073985155&doi=10.1007%2fs12524-019-01041-2&partnerID=40&md5=fb598bd6a499516d02a959f1157d97fa","Deep learning convolutional neural network (CNN) is popular as being widely used for classification of unstructured data. Land use land cover (LULC) classification using remote sensing data can be used for crop identification also. Present study aims to examine the use of deep learning CNN for LULC classification on Indian Pines dataset and for crop identification on our study area dataset. In the present work, AVIRIS sensor’s Indian Pines standard dataset has been used for LULC classification. Study area from Phulambri, Aurangabad, MH, India, has been used for crop classification. Data have been gathered from EO-1 Hyperion sensor. The accuracy of CNN model depends on optimizer, activation function, filter size, learning rate and batch size. Deep learning CNN is evaluated by changing these parameters. It has been observed that deep learning CNN using optimized combination of parameters has provided 97.58% accuracy for the Indian Pines dataset, while 79.43% accuracy for our study area dataset. The empirical results demonstrate that CNN works well in practice for unstructured data as well as for small size dataset. © 2019, Indian Society of Remote Sensing.","Convolutional neural network (CNN); Deep learning; Hyperspectral data; Land use land cover (LULC); Principal component analysis (PCA); Remote sensing data","artificial neural network; AVIRIS; classification; data set; Hyperion; identification method; land cover; land use change; machine learning; numerical model; principal component analysis; remote sensing; satellite imagery; spectral analysis; Aurangabad; India; Maharashtra"
"Deep learning approaches for the mapping of tree species diversity in a tropical wetland using airborne LiDAR and high-spatial-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079808782&doi=10.3390%2fF10111047&partnerID=40&md5=8e0622b683e52a9ca78fac9b05672cc1","The monitoring of tree species diversity is important for forest or wetland ecosystem service maintenance or resource management. Remote sensing is an efficient alternative to traditional field work to map tree species diversity over large areas. Previous studies have used light detection and ranging (LiDAR) and imaging spectroscopy (hyperspectral or multispectral remote sensing) for species richness prediction. The recent development of very high spatial resolution (VHR) RGB images has enabled detailed characterization of canopies and forest structures. In this study, we developed a three-step workflow for mapping tree species diversity, the aim of which was to increase knowledge of tree species diversity assessment using deep learning in a tropical wetland (HaizhuWetland) in South China based on VHR-RGB images and LiDAR points. Firstly, individual trees were detected based on a canopy height model (CHM, derived from LiDAR points) by the local-maxima-based method in the FUSION software (Version 3.70, Seattle, USA). Then, tree species at the individual tree level were identified via a patch-based image input method, which cropped the RGB images into small patches (the individually detected trees) based on the tree apexes detected. Three different deep learning methods (i.e., AlexNet, VGG16, and ResNet50) were modified to classify the tree species, as they can make good use of the spatial context information. Finally, four diversity indices, namely, the Margalef richness index, the Shannon-Wiener diversity index, the Simpson diversity index, and the Pielou evenness index, were calculated from the fixed subset with a size of 30 x 30 m for assessment. In the classification phase, VGG16 had the best performance, with an overall accuracy of 73.25% for 18 tree species. Based on the classification results, mapping of tree species diversity showed reasonable agreement with field survey data (R2 Margalef = 0.4562, root-mean-square error RMSEMargalef = 0.5629; R2 Shannon-Wiener = 0.7948, RMSEShannon-Wiener = 0.7202; R2 Simpson = 0.7907, RMSESimpson = 0.1038; and R2 Pielou = 0.5875, RMSEPielou = 0.3053). While challenges remain for individual tree detection and species classification, the deep-learning-based solution shows potential for mapping tree species diversity. © 2019 by the authors.","Deep learning; High-resolution remote sensing images; Individual tree level; LiDAR; Tree species diversity; Tropical wetland","Biodiversity; Classification (of information); Deep learning; Ecosystems; Hyperspectral imaging; Image resolution; Learning systems; Mapping; Mean square error; Optical radar; Remote sensing; Tropics; Wetlands; High resolution remote sensing images; Individual tree; Light detection and ranging; Multispectral remote sensing; Shannon-wiener diversity indices; Tree species diversity; Tropical wetland; Very-high spatial resolutions; Forestry; airborne sensing; artificial neural network; forest canopy; lidar; spatial resolution; species diversity; tree; vegetation mapping; wetland; Biodiversity; Ecosystems; Mapping; Remote Sensing; Tropics; Wetlands; China"
"Integration of remote sensing and social sensing data in a deep learning framework for hourly urban PM2.5 mapping","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074139729&doi=10.3390%2fijerph16214102&partnerID=40&md5=20c74cb157943a78d8edc6a0630e2eaf","Fine spatiotemporal mapping of PM2.5 concentration in urban areas is of great significance in epidemiologic research. However, both the diversity and the complex nonlinear relationships of PM2.5 influencing factors pose challenges for accurate mapping. To address these issues, we innovatively combined social sensing data with remote sensing data and other auxiliary variables, which can bring both natural and social factors into the modeling; meanwhile, we used a deep learning method to learn the nonlinear relationships. The geospatial analysis methods were applied to realize effective feature extraction of the social sensing data and a grid matching process was carried out to integrate the spatiotemporal multi-source heterogeneous data. Based on this research strategy, we finally generated hourly PM2.5 concentration data at a spatial resolution of 0.01°. This method was successfully applied to the central urban area of Wuhan in China, which the optimal result of the 10-fold cross-validation R2 was 0.832. Our work indicated that the real-time check-in and traffic index variables can improve both quantitative and mapping results. The mapping results could be potentially applied for urban environmental monitoring, pollution exposure assessment, and health risk research. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Feature extraction; PM2.5; Remote sensing; Social sensing","accuracy assessment; environmental monitoring; health risk; integrated approach; learning; mapping; particulate matter; pollution exposure; remote sensing; satellite data; temporal variation; urban atmosphere; article; China; deep learning; environmental monitoring; feature extraction; health hazard; human; remote sensing; risk assessment; social aspect; urban area; air pollutant; air pollution; city; particulate matter; pollution; procedures; remote sensing; time factor; China; Hubei; Wuhan; Air Pollutants; Air Pollution; China; Cities; Deep Learning; Environmental Monitoring; Environmental Pollution; Humans; Particulate Matter; Remote Sensing Technology; Time Factors"
"Application of deep learning for delineation of visible cadastral boundaries from remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074666341&doi=10.3390%2frs11212505&partnerID=40&md5=f57981998aeb9ccd05530e7e21092436","Cadastral boundaries are often demarcated by objects that are visible in remote sensing imagery. Indirect surveying relies on the delineation of visible parcel boundaries from such images. Despite advances in automated detection and localization of objects from images, indirect surveying is rarely automated and relies on manual on-screen delineation. We have previously introduced a boundary delineation workflow, comprising image segmentation, boundary classification and interactive delineation that we applied on Unmanned Aerial Vehicle (UAV) data to delineate roads. In this study, we improve each of these steps. For image segmentation, we remove the need to reduce the image resolution and we limit over-segmentation by reducing the number of segment lines by 80% through filtering. For boundary classification, we show how Convolutional Neural Networks (CNN) can be used for boundary line classification, thereby eliminating the previous need for Random Forest (RF) feature generation and thus achieving 71% accuracy. For interactive delineation, we develop additional and more intuitive delineation functionalities that cover more application cases. We test our approach on more varied and larger data sets by applying it to UAV and aerial imagery of 0.02-0.25 m resolution from Kenya, Rwanda and Ethiopia. We show that it is more effective in terms of clicks and time compared to manual delineation for parcels surrounded by visible boundaries. Strongest advantages are obtained for rural scenes delineated from aerial imagery, where the delineation effort per parcel requires 38% less time and 80% fewer clicks compared to manual delineation. © 2019 by the authors.","Boundary delineation; Boundary extraction; Cadastral mapping; CNN; Deep learning; Image analysis; Indirect surveying; Machine learning; RF","Aerial photography; Antennas; Decision trees; Image analysis; Image resolution; Image segmentation; Learning systems; Neural networks; Object detection; Remote sensing; Surveys; Unmanned aerial vehicles (UAV); Automated detection; Boundary classification; Boundary delineation; Boundary extraction; Convolutional neural network; Feature generation; Over segmentation; Remote sensing imagery; Deep learning"
"Multi-Scale and Multi-Task Deep Learning Framework for Automatic Road Extraction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074448549&doi=10.1109%2fTGRS.2019.2926397&partnerID=40&md5=b5dbffb0bfd6067710e288ce69de2d3f","Road detection and centerline extraction from very high-resolution (VHR) remote sensing imagery are of great significance in various practical applications. Road detection and centerline extraction operations depend on each other, to a certain extent. The road detection constrains the appearance of the centerline, and the centerline enhances the linear features of the road detection. However, most of the previous works have addressed these two tasks separately and have not considered the symbiotic relationship between them, making it difficult to obtain smooth and complete roads. In this paper, a novel multi-scale and multi-task deep learning framework for automatic road extraction (MSMT-RE) is proposed to build the relationship between them and simultaneously complete the road detection and centerline extraction tasks. U-Net is selected as the basic network for multi-task learning due to its strong ability to preserve spatial details. Multi-scale feature integration is also applied in the framework to increase the robustness of the feature extraction. Meanwhile, an adaptive loss function is introduced to solve the problems of roads taking up a small percentage of the training samples, and the fact that the positive samples of the two tasks are unbalanced. Finally, experiments were conducted on two public road data sets and two large images from Google Earth, and the proposed framework was compared with other state-of-the-art deep learning-based road extraction methods, both quantitatively and qualitatively. The proposed approach outperformed all the compared methods, confirming its advantages in automatic road extraction. © 2019 IEEE.","Adaptive loss function; centerline extraction; deep learning framework; multi-scale and multi-task; remote sensing; road detection","Extraction; Feature extraction; Remote sensing; Roads and streets; Adaptive loss functions; Automatic road extraction; Centerline extraction; Learning frameworks; Remote sensing imagery; Road detection; Road extraction method; Symbiotic relationship; Deep learning; algorithm; data set; detection method; image resolution; remote sensing; satellite data"
"Fine-grained classification of hyperspectral imagery based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075348422&doi=10.3390%2frs11222690&partnerID=40&md5=551e9aa50fb640cff13f474bd314e5fc","Hyperspectral remote sensing obtains abundant spectral and spatial information of the observed object simultaneously. It is an opportunity to classify hyperspectral imagery (HSI) with a fine-grained manner. In this study, the fine-grained classification of HSI, which contains a large number of classes, is investigated. On one hand, traditional classification methods cannot handle fine-grained classification of HSI well; on the other hand, deep learning methods have shown their powerfulness in fine-grained classification. So, in this paper, deep learning is explored for HSI supervised and semi-supervised fine-grained classification. For supervised HSI fine-grained classification, densely connected convolutional neural network (DenseNet) is explored for accurate classification. Moreover, DenseNet is combined with pre-processing technique (i.e., principal component analysis or auto-encoder) or post-processing technique (i.e., conditional random field) to further improve classification performance. For semi-supervised HSI fine-grained classification, a generative adversarial network (GAN), which includes a discriminative CNN and a generative CNN, is carefully designed. The GAN fully uses the labeled and unlabeled samples to improve classification accuracy. The proposed methods were tested on the Indian Pines data set, which contains 33,3951 samples with 52 classes. The experimental results show that the deep learning-based methods provide great improvements compared with other traditional methods, which demonstrate that deep models have huge potential for HSI fine-grained classification. © 2019 by the authors.","Convolutional neural network (CNN); Deep learning; Generative adversarial network (GAN); Hyperspectral imagery classification; Semi-supervised classification","Convolution; Deep learning; Deep neural networks; Neural networks; Principal component analysis; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Classification performance; Convolutional neural network; Hyper-spectral imageries; Hyperspectral imagery classifications; Hyperspectral remote sensing; Post-processing techniques; Semi-supervised classification; Image classification"
"Evaluation of three deep learning models for early crop classification using Sentinel-1A imagery time series-a case study in Zhanjiang, China","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075399373&doi=10.3390%2frs11222673&partnerID=40&md5=3ab8bb3f1090ce2421565b91483eb76f","Timely and accurate estimation of the area and distribution of crops is vital for food security. Optical remote sensing has been a key technique for acquiring crop area and conditions on regional to global scales, but great challenges arise due to frequent cloudy days in southern China. This makes optical remote sensing images usually unavailable. Synthetic aperture radar (SAR) could bridge this gap since it is less affected by clouds. The recent availability of Sentinel-1A (S1A) SAR imagery with a 12-day revisit period at a high spatial resolution of about 10 m makes it possible to fully utilize phenological information to improve early crop classification. In deep learning methods, one-dimensional convolutional neural networks (1D CNNs), long short-term memory recurrent neural networks (LSTM RNNs), and gated recurrent unit RNNs (GRU RNNs) have been shown to efficiently extract temporal features for classification tasks. However, due to the complexity of training, these three deep learning methods have been less used in early crop classification. In this work, we attempted to combine them with an incremental classification method to avoid the need for training optimal architectures and hyper-parameters for data from each time series. First, we trained 1D CNNs, LSTM RNNs, and GRU RNNs based on the full images' time series to attain three classifiers with optimal architectures and hyper-parameters. Then, starting at the first time point, we performed an incremental classification process to train each classifier using all of the previous data, and obtained a classification network with all parameter values (including the hyper-parameters) at each time point. Finally, test accuracies of each time point were assessed for each crop type to determine the optimal time series length. A case study was conducted in Suixi and Leizhou counties of Zhanjiang City, China. To verify the effectiveness of this method, we also implemented the classic random forest (RF) approach. The results were as follows: (i) 1D CNNs achieved the highest Kappa coefficient (0.942) of the four classifiers, and the highest value (0.934) in the GRU RNNs time series was attained earlier than with other classifiers; (ii) all three deep learning methods and the RF achieved F measures above 0.900 before the end of growth seasons of banana, eucalyptus, second-season paddy rice, and sugarcane; while, the 1D CNN classifier was the only one that could obtain an F-measure above 0.900 for pineapple before harvest. All results indicated the effectiveness of the solution combining the deep learning models with the incremental classification approach for early crop classification. This method is expected to provide new perspectives for early mapping of croplands in cloudy areas. © 2019 by the authors.","Early crop classification; Gated recurrent unit; Incremental classification; Long short-term memory; One-dimensional CNNs; RNNs; Sentinel-1A; Synthetic aperture radar","Brain; Crops; Decision trees; Deep learning; Food supply; Image classification; Image enhancement; Long short-term memory; Memory architecture; Network architecture; Radar imaging; Remote sensing; Synthetic aperture radar; Time series; Classification approach; Classification networks; Convolutional neural network; Crop classification; Gated recurrent unit; High spatial resolution; RNNs; Sentinel-1; Classification (of information)"
"Automated extraction of antarctic glacier and ice shelf fronts from Sentinel-1 imagery using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074639701&doi=10.3390%2frs11212529&partnerID=40&md5=7900ec7964628a64b9930031fb67a2d9","Sea level rise contribution from the Antarctic ice sheet is influenced by changes in glacier and ice shelf front position. Still, little is known about seasonal glacier and ice shelf front fluctuations as the manual delineation of calving fronts from remote sensing imagery is very time-consuming. The major challenge of automatic calving front extraction is the low contrast between floating glacier and ice shelf fronts and the surrounding sea ice. Additionally, in previous decades, remote sensing imagery over the often cloud-covered Antarctic coastline was limited. Nowadays, an abundance of Sentinel-1 imagery over the Antarctic coastline exists and could be used for tracking glacier and ice shelf front movement. To exploit the available Sentinel-1 data, we developed a processing chain allowing automatic extraction of the Antarctic coastline from Seninel-1 imagery and the creation of dense time series to assess calving front change. The core of the proposed workflow is a modified version of the deep learning architecture U-Net. This convolutional neural network (CNN) performs a semantic segmentation on dual-pol Sentinel-1 data and the Antarctic TanDEM-X digital elevation model (DEM). The proposed method is tested for four training and test areas along the Antarctic coastline. The automatically extracted fronts deviate on average 78 m in training and 108 m test areas. Spatial and temporal transferability is demonstrated on an automatically extracted 15-month time series along the Getz Ice Shelf. Between May 2017 and July 2018, the fronts along the Getz Ice Shelf show mostly an advancing tendency with the fastest moving front of DeVicq Glacier with 726 ± 20 m/yr. © 2019 by the authors.","Antarctica; Calving front; Coastline; Convolutional neural network; Deep learning; Getz Ice Shelf; Glacier front; Glacier terminus; Semantic segmentation; U-Net","Convolution; Deep learning; Deep neural networks; Extraction; Landforms; Neural networks; Sea ice; Sea level; Semantics; Surveying; Time series; Antarctica; Calving front; Coastline; Convolutional neural network; Ice shelves; Semantic segmentation; Remote sensing"
"High-resolution remote sensing imagery classification of imbalanced data using multistage sampling method and deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074662020&doi=10.3390%2frs11212523&partnerID=40&md5=0c82e7b20c4c8c2f7f7e2b6af6995fc6","Class imbalance is a key issue for the application of deep learning for remote sensing image classification because a model generated by imbalanced samples training has low classification accuracy for minority classes. In this study, an accurate classification approach using the multistage sampling method and deep neural networkswas proposed to classify imbalanced data. We first balance samples by multistage sampling to obtain the training sets. Then, a state-of-the-artmodel is adopted by combining the advantages of atrous spatial pyramid pooling (ASPP) and Encoder-Decoder for pixel-wise classification, which are two different types of fully convolutional networks (FCNs) that can obtain contextual information of multiple levels in the Encoder stage. The details and spatial dimensions of targets are restored using such information during the Decoder stage. We employ four deep learning-based classification algorithms (basic FCN, FCN-8S, ASPP, and Encoder-Decoder with ASPP of our approach) on multistage training sets (original, MUS1, and MUS2) of WorldView-3 images in southeastern Qinghai-Tibet Plateau and GF-2 images in northeastern Beijing for comparison. The experiments show that, compared with existing sets (original,MUS1, and identical) and existing method (cost weighting), theMUS2 training set of multistage sampling significantly enhance the classification performance for minority classes. Our approach shows distinct advantages for imbalanced data. © 2019 by the authors.","ASPP; Classification; Deep learning; Encoder-decoder; High-resolution remote sensing image; Imbalanced data; Multistage sampling","Decoding; Deep learning; Deep neural networks; Image classification; Remote sensing; Signal encoding; ASPP; Encoder-decoder; High resolution remote sensing images; Imbalanced data; Multistage sampling; Classification (of information)"
"A deep learning approach for early wildfire detection from hyperspectral satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078039682&doi=10.1109%2fRITAPP.2019.8932740&partnerID=40&md5=ecd6fb61135e161e6c12e80464e6f543","Wildfires are getting more severe and destructive. Due to their fast-spreading nature, wildfires are often detected when already beyond control and consequently cause billion-scale effects in a very short time. Governments are looking for remote sensing methods for early wildfire detection, avoiding billion-dollar losses of damaged properties. The aim of this study was to develop an autonomous and intelligent system built on top of imagery data streams, which is available from around-the-clock satellites, to monitor and prevent fire hazards from becoming disasters. However, satellite data pose unique challenges for image processing techniques, including temporal dependencies across time steps, the complexity of spectral chan-nels, and adversarial conditions such as cloud and illumination. In this paper, we propose a novel wildfire detection method that utilises satellite images in an advanced deep learning architecture for locating wildfires at pixel level. The detection outputs are further visualised in an interactive dashboard that allows wildfire mitigation specialists to deeply analyse regions of interest in the world-map. Our system is built and tested on the Geostationary Operational Environmental Satellites (GOES-16) streaming data source. Empirical evaluations show the superiorperformance of our approach over the baselines with 94% F1- score and 1.5 times faster detections as well as its robustness against different types of wildfires and adversarial conditions. © 2019 IEEE.",,"Disaster prevention; Fire hazards; Fires; Geostationary satellites; Image processing; Intelligent robots; Intelligent systems; Maps; Remote sensing; Early wildfire detections; Empirical evaluations; Geostationary operational environmental satellites; Hyperspectral satellite; Image processing technique; Learning architectures; Regions of interest; Wildfire detection; Deep learning"
"Adaptive Multiscale Deep Fusion Residual Network for Remote Sensing Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074453327&doi=10.1109%2fTGRS.2019.2921342&partnerID=40&md5=a7260b37fa39d59d1a5e766c14d6a6fa","With the development of remote sensing imaging technology, remote sensing images with high-resolution and complex structure can be acquired easily. The classification of remote sensing images is always a hot and challenging problem. In order to improve the performance of remote sensing image classification, we propose an adaptive multiscale deep fusion residual network (AMDF-ResNet). The AMDF-ResNet consists of a backbone network and a fusion network. The backbone network including several residual blocks generates multiscale hierarchy features, which contain semantic information from low to high levels. In the fusion network, the adaptive feature fusion module proposed can emphasize useful information and suppress useless information by learning the weights, which represent the importance of the features. The AMDF-ResNet can make full use of the multiscale hierarchy features and the extracted feature is discriminative. In addition, we propose a samples selection method named important samples selection strategy (ISSS). Based on superpixels segmentation result, gradient information and spatial distribution are used as two references to determine the selection numbers and select samples. Compared with the random selection strategy, training samples selected by ISSS are more representative and diverse. The experimental results on four data sets demonstrate that the AMDF-ResNet and ISSS are effective. © 1980-2012 IEEE.","Deep learning (DL); feature extraction; image classification; multispectral (MS) images; remote sensing","Deep learning; Feature extraction; Image classification; Image enhancement; Image fusion; Imaging techniques; Information use; Semantics; Classification of remote sensing image; Gradient informations; Multiscale hierarchies; Multispectral images; Remote sensing image classification; Remote sensing images; Remote sensing imaging; Superpixels segmentations; Remote sensing; artificial neural network; data acquisition; image classification; imaging method; machine learning; performance assessment; remote sensing; spectral analysis"
"Remote Sensing Image Superresolution Using Deep Residual Channel Attention","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074462732&doi=10.1109%2fTGRS.2019.2924818&partnerID=40&md5=7c0f3f4e693eb16d983733bc78ce07c2","The current trend in remote sensing image superresolution (SR) is to use supervised deep learning models to effectively enhance the spatial resolution of airborne and satellite-based optical imagery. Nonetheless, the inherent complexity of these architectures/data often makes these methods very difficult to train. Despite these recent advances, the huge amount of network parameters that must be fine-tuned and the lack of suitable high-resolution remotely sensed imagery in actual operational scenarios still raise some important challenges that may become relevant limitations in the existent earth observation data production environments. To address these problems, we propose a new remote sensing SR approach that integrates a visual attention mechanism within a residual-based network design in order to allow the SR process to focus on those features extracted from land-cover components that require more computations to be superresolved. As a result, the network training process is significantly improved because it aims at learning the most relevant high-frequency information while the proposed architecture allows neglecting the low-frequency features extracted from spatially uninformative earth surface areas by means of several levels of skip connections. Our experimental assessment, conducted using the University of California at Merced and GaoFen-2 remote sensing image collections, three scaling factors, and eight different SR methods, demonstrates that our newly proposed approach exhibits competitive performance in the task of superresolving remotely sensed imagery. © 1980-2012 IEEE.","Deep learning; remote sensing; single-image superresolution (SR); visual attention (VA)","Behavioral research; Deep learning; Image enhancement; Network architecture; Optical resolving power; Satellite imagery; Competitive performance; Experimental assessment; High-frequency informations; High-resolution remotely sensed imageries; Single images; University of California; Visual Attention; Visual attention mechanisms; Remote sensing; complexity; image resolution; land cover; network design; remote sensing; satellite altimetry; spatial resolution; training"
"A multi-scale approach for remote sensing scene classification based on feature maps selection and region representation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074672465&doi=10.3390%2frs11212504&partnerID=40&md5=9d34b7af0cd5b60c8541e613066ed190","Scene classification is one of the bases for automatic remote sensing image interpretation. Recently, deep convolutional neural networks have presented promising performance in high-resolution remote sensing scene classification research. In general, most researchers directly use raw deep features extracted from the convolutional networks to classify scenes. However, this strategy only considers single scale features, which cannot describe both the local and global features of images. In fact, the dissimilarity of scene targets in the same category may result in convolutional features being unable to classify them into the same category. Besides, the similarity of the global features in different categories may also lead to failure of fully connected layer features to distinguish them. To address these issues, we propose a scene classification method based on multi-scale deep feature representation (MDFR), which mainly includes two contributions: (1) region-based features selection and representation; and (2) multi-scale features fusion. Initially, the proposed method filters the multi-scale deep features extracted from pre-trained convolutional networks. Subsequently, these features are fused via two efficient fusion methods. Our method utilizes the complementarity between local features and global features by effectively exploiting the features of different scales and discarding the redundant information in features. Experimental results on three benchmark high-resolution remote sensing image datasets indicate that the proposed method is comparable to some state-of-the-art algorithms. © 2019 by the authors.","Convolutional neural networks; deep learning; Multi-scale deep feature representation; Remote sensing image scene classification","Convolution; Deep learning; Deep neural networks; Feature extraction; Neural networks; Remote sensing; Convolutional neural network; Feature representation; High resolution remote sensing; High resolution remote sensing images; Multi-scale approaches; Remote sensing image interpretations; Remote sensing images; State-of-the-art algorithms; Classification (of information)"
"Deep learning-generated nighttime reflectance and daytime radiance of the midwave infrared band of a geostationary satellite","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075368575&doi=10.3390%2frs11222713&partnerID=40&md5=64db655d556509fce31f13c62551d5c9","Midwave infrared (MWIR) band of 3.75 μm is important in satellite remote sensing in many applications. This band observes daytime reflectance and nighttime radiance according to the Earth's and the Sun's effects. This study presents an algorithm to generate no-present nighttime reflectance and daytime radiance at MWIR band of satellite observation by adopting the conditional generative adversarial nets (CGAN) model. We used the daytime reflectance and nighttime radiance data in the MWIR band of the meteoritical imager (MI) onboard the Communication, Ocean and Meteorological Satellite (COMS), as well as in the longwave infrared (LWIR; 10.8 μm) band of the COMS/MI sensor, from 1 January to 31 December 2017. This model was trained in a size of 1024 × 1024 pixels in the digital number (DN) from 0 to 255 converted from reflectance and radiance with a dataset of 256 images, and validated with a dataset of 107 images. Our results show a high statistical accuracy (bias = 3.539, root-mean-square-error (RMSE) = 8.924, and correlation coefficient (CC) = 0.922 for daytime reflectance; bias = 0.006, RMSE = 5.842, and CC = 0.995 for nighttime radiance) between the COMS MWIR observation and artificial intelligence (AI)-generated MWIR outputs. Consequently, our findings from the real MWIR observations could be used for identification of fog/low cloud, fire/hot-spot, volcanic eruption/ash, snow and ice, low-level atmospheric vector winds, urban heat islands, and clouds. © 2019 by the authors.","Deep learning; Midwave infrared; Radiance; Reflectance; Satellite remote sensing","Geostationary satellites; Infrared radiation; Mean square error; Reflection; Remote sensing; Volcanoes; Correlation coefficient; Mid wave infrared (MWIR); Midwave infrared bands; Radiance; Root mean square errors; Satellite observations; Satellite remote sensing; Statistical accuracy; Deep learning"
"Contour Refinement and EG-GHT-Based Inshore Ship Detection in Optical Remote Sensing Image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074449643&doi=10.1109%2fTGRS.2019.2921242&partnerID=40&md5=216235b8d2667200b2804b05275bb0db","Inshore ship detection becomes challenging in high-resolution optical remote sensing image (RSI) because inshore ships are often incomplete and deformed due to the poor imaging condition and shadow of ship superstructure, and there are various interferences in harbor. A contour refinement and the improved generalized Hough transform (GHT)-based inshore ship detection scheme is proposed for RSI with complex harbor scenes. First, the suspected region of ships (SRS) is located in the entire RSI according to the line segments of ship body and docks. The contours in each SRS are then refined to repair the damaged ship head contour (SHC) using the convex set characteristics of ship head and subsequently reduce non-SHC by curvature filtering. In each refined SRS, equal frequency quantification instead of equal width quantification for R-Table construction and Gini coefficient-based decision criterion combining the number and distribution of votes are proposed to improve GHT (i.e., EG-GHT) and to extract SHCs as candidate targets. The false candidates are removed according to pixel proportion described by the structured binarization feature. Applying the border scoring strategy, the best candidates with the largest score among all the overlapped bounding boxes are selected as the final detection targets. Using the public RSIs with various cases, including turbid water, cloud occlusion, ships moored together, and ships with the different sizes, experimental results demonstrate the proposed scheme outperforms state-of-the-art contour-based methods and deep learning-based methods in terms of precision-recall rate and average precision, respectively. © 1980-2012 IEEE.","Border scoring; curvature filtering; equal frequency quantification; generalized Hough transform (GHT); Gini coefficient; inshore ship detection; structured binarization feature (SBF)","Deep learning; Feature extraction; Hough transforms; Remote sensing; Set theory; Ships; Binarizations; Border scoring; equal frequency quantification; Generalized Hough transform; Gini coefficients; Ship detection; Shipbuilding; curvature; detection method; image analysis; optical method; precision; remote sensing; transform"
"Deep learning for multi-modal classification of cloud, shadow and land cover scenes in PlanetScope and Sentinel-2 imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072255321&doi=10.1016%2fj.isprsjprs.2019.08.018&partnerID=40&md5=54e8e96f185a53eec10cfc12250b11b2","With the increasing availability of high-resolution satellite imagery it is important to improve the efficiency and accuracy of satellite image indexing, retrieval and classification. Furthermore, there is a need for utilizing all available satellite imagery in identifying general land cover types and monitoring their changes through time irrespective of their spatial, spectral, temporal and radiometric resolutions. Therefore, in this study, we developed deep learning models able to efficiently and accurately classify cloud, shadow and land cover scenes in different high-resolution (&lt;10 m) satellite imagery. Specifically, we trained deep convolutional neural network (CNN) models to perform multi-label classification of multi-modal, high-resolution satellite imagery at the scene level. Multi-label classification at the scene level (a.k.a. image indexing), as opposed to the pixel level, allows for faster performance, higher accuracy (although at the cost of detail) and higher generalizability. We investigated the generalization ability (i.e. cross-dataset and geographic independence) of individual and ensemble CNN models trained on multi-modal satellite imagery (i.e. PlanetScope and Sentinel-2). The models trained on PlanetScope imagery collected over the Amazon performed well when applied to PlanetScope and Sentinel-2 imagery collected over the Wet Tropics of Australia with an F2 score of 0.72 and 0.69, respectively. Similarly, PlanetScope-based CNN models trained on imagery collected over the Wet Tropics of Australia performed well when applied to Sentinel-2 imagery with an F2 score of 0.76, and the reverse scenario resulted in the same F2 score of 0.76. This suggests that our CNN models have high cross-dataset generalization ability and are suitable for classifying cloud, shadow and land cover classes in satellite imagery with resolutions from 3 m (PlanetScope) to 10 m (Sentinel-2). The performance of our CNN models was also comparable to the state-of-the-art methods (i.e. Sen2Cor and MACCS) developed specifically for classifying cloud and shadow classes in Sentinel-2 imagery. Finally, we show the potential of our CNN models to mask cloud and shadow contaminated areas from PlanetScope- and Sentinel-2-derived NDVI time-series. © 2019 The Authors","CNN; Deep learning; Ensemble; Indexing; MACCS; Multi-label; Multi-modal; PlanetScope; Remote sensing; Scene classification; Sen2Cor; Sentinel-2","Classification (of information); Deep learning; Deep neural networks; Image classification; Image enhancement; Image segmentation; Indexing (of information); Neural networks; Remote sensing; Tropics; Ensemble; MACCS; Multi-label; Multi-modal; PlanetScope; Scene classification; Sen2Cor; Sentinel-2; Satellite imagery; accuracy assessment; artificial neural network; cloud; image classification; image resolution; land cover; learning; NDVI; pixel; remote sensing; satellite imagery; Sentinel; spatiotemporal analysis; Amazonia; Australia"
"Extraction of Buildings in Remote Sensing Imagery with Deep Belief Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077790182&doi=10.1109%2fAITC.2019.8921039&partnerID=40&md5=c0ac089ec6092cdf03ae509714569677","In land use analysis, the extraction of buildings from remote sensing imagery is an important problem. This work is difficult to obtain the spectral features from buildings due to high intra-class and low inter-class variation of buildings. In the paper, a patch-based deep belief network (PBDBN) architecture is used for the extraction of buildings from remote sensing datasets. And low-level building features (e.g compacted contours) of adjacent regions are combined with Deep Belief Network (DBN) features during the post-processing stage for obtaining better performance. The experimental results are demonstrated on Massachusetts buildings dataset to express the performance of PBDBN and it is compared with other method on the same dataset. © 2019 IEEE.","Building Extraction; Deep Belief Network; Deep Learning; Segmentation","Buildings; Deep learning; Extraction; Image segmentation; Land use; Building extraction; Deep belief network (DBN); Deep belief networks; Land-use analysis; Massachusetts; Post-processing stages; Remote sensing imagery; Spectral feature; Remote sensing"
"Remote Sensor Design for Visual Recognition with Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074461118&doi=10.1109%2fTGRS.2019.2925813&partnerID=40&md5=58b4aaa7eef46b5bd541c3f7557957a8","While deep learning technologies for computer vision have developed rapidly since 2012, modeling of remote sensing systems has remained focused around human vision. In particular, remote sensing systems are usually constructed to optimize sensing cost-quality tradeoffs with respect to human image interpretability. While some recent studies have explored remote sensing system design as a function of simple computer vision algorithm performance, there has been little work relating this design to the state of the art in computer vision: Deep learning with convolutional neural networks. We develop experimental systems to conduct this analysis, showing results with modern deep learning algorithms and recent overhead image data. Our results are compared to standard image quality measurements based on human visual perception, and we conclude not only that machine and human interpretability differ significantly but also that computer vision performance is largely self-consistent across a range of disparate conditions. This paper is presented as a cornerstone for a new generation of sensor design systems that focus on computer algorithm performance instead of human visual perception. © 2019 IEEE.","Convolutional neural network (CNN); deep learning; image system design; remote sensing; satellite imagery; transfer learning","Computer vision; Convolution; Deep learning; Deep neural networks; Image quality; Learning algorithms; Neural networks; Satellite imagery; Systems analysis; Vision; Algorithm performance; Computer vision algorithms; Convolutional neural network; Human visual perception; Image interpretability; Image system design; Remote sensing system; Transfer learning; Remote sensing; algorithm; artificial neural network; computer vision; design method; experimental study; image analysis; remote sensing; sensor; supervised learning; visual analysis"
"Lifting scheme-based deep neural network for remote sensing scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075393596&doi=10.3390%2frs11222648&partnerID=40&md5=dfacea3af638fcfeeac639853e9cbe26","Recently, convolutional neural networks (CNNs) achieve impressive results on remote sensing scene classification, which is a fundamental problem for scene semantic understanding. However, convolution, the most essential operation in CNNs, restricts the development of CNN-based methods for scene classification. Convolution is not efficient enough for high-resolution remote sensing images and limited in extracting discriminative features due to its linearity. Thus, there has been growing interest in improving the convolutional layer. The hardware implementation of the JPEG2000 standard relies on the lifting scheme to perform wavelet transform (WT). Compared with the convolution-based two-channel filter bank method of WT, the lifting scheme is faster, taking up less storage and having the ability of nonlinear transformation. Therefore, the lifting scheme can be regarded as a better alternative implementation for convolution in vanilla CNNs. This paper introduces the lifting scheme into deep learning and addresses the problems that only fixed and finite wavelet bases can be replaced by the lifting scheme, and the parameters cannot be updated through backpropagation. This paper proves that any convolutional layer in vanilla CNNs can be substituted by an equivalent lifting scheme. A lifting scheme-based deep neural network (LSNet) is presented to promote network applications on computational-limited platforms and utilize the nonlinearity of the lifting scheme to enhance performance. LSNet is validated on the CIFAR-100 dataset and the overall accuracies increase by 2.48% and 1.38% in the 1D and 2D experiments respectively. Experimental results on the AID which is one of the newest remote sensing scene dataset demonstrate that 1D LSNet and 2D LSNet achieve 2.05% and 0.45% accuracy improvement compared with the vanilla CNNs respectively. © 2019 by the authors.","CNN; Convolution; Lifting scheme; Scene classification","Backpropagation; Convolution; Neural networks; Remote sensing; Semantics; Wavelet analysis; Wavelet transforms; Convolutional neural network; Discriminative features; Hardware implementations; High resolution remote sensing images; Lifting schemes; Non-linear transformations; Scene classification; Two-channel filter banks; Deep neural networks"
"CISPNet: Automatic detection of remote sensing images from google earth in complex scenes based on context information scene perception","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075245451&doi=10.3390%2fapp9224836&partnerID=40&md5=e8487bd9494fdc259169f586b359d19a","The ability to detect small targets and the speed of the target detector are very important for the application of remote sensing image detection, and in this paper, we propose an effective and efficient method (named CISPNet) with high detection accuracy and compact architecture. In particular, according to the characteristics of the data, we apply a context information scene perception (CISP) module to obtain the contextual information for targets of different scales and use k-means clustering to set the aspect ratios and size of the default boxes. The proposed method inherits the network structure of Single Shot MultiBox Detector (SSD) and introduces the CISP module into it. We create a dataset in the Pascal Visual Object Classes (VOC) format, annotated with the three types of detection targets, aircraft, ship, and oiltanker. Experimental results on our remote sensing image dataset as well as the Northwestern Polytechnical University very-high-resolution (NWPU VRH-10) dataset demonstrate that the proposed CISPNet performs much better than the original SSD and other detectors especially for small objects. Specifically, our network can achieve 80.34% mean average precision (mAP) at the speed of 50.7 frames per second (FPS) with the input size 300 × 300 pixels on the remote sensing image dataset. On extended experiments, the performance of CISPNet in fuzzy target detection in remote sensing image is better than that of SSD. © 2019 by the authors.","Context feature scene perception; Deep convolutional neural network; Deep learning; Real-time target detection; Remote sensing images",
"Triplanar Imaging of 3-D GPR Data for Deep-Learning-Based Underground Object Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078465711&doi=10.1109%2fJSTARS.2019.2953505&partnerID=40&md5=f7a13d84cb073e672cb73579446e5e7f","This article proposes a deep-learning-based underground object classification technique that uses triplanar ground-penetrating radar images consisting of B-, C-, and D-scan images. Although multichannel ground-penetrating radar (GPR) systems provide three-dimensional (3-D) information about underground objects, there is currently no suitable technique available for processing 3-D data as opposed to 2-D images. In this article, a triplanar deep convolutional neural network technique is proposed for use in processing 3-D GPR data for use in automatized underground object classification. The proposed method was validated experimentally using 3-D GPR road scanning data obtained from urban roads in Seoul, South Korea. In addition, the classification performance of the method was compared to that of a conventional method that uses only B-scan-images. The results of the validation and comparison tests reveal that the classification performance of the proposed technique is notably better than that of the conventional B-scan-image-based method and that its use results in decrease misclassification ratios. © 2008-2012 IEEE.","D-scan image; ground-penetrating radar (GPR); triplanar images; underground object classification","C (programming language); Data handling; Deep neural networks; Geological surveys; Geophysical prospecting; Neural networks; Object detection; Radar imaging; Classification performance; Convolutional neural network; Ground penetrating radar (GPR); Multi-channel ground-penetrating radars; Object classification; Scan images; triplanar images; Underground object detections; Ground penetrating radar systems; artificial neural network; data processing; detection method; ground penetrating radar; image analysis; image classification; remote sensing; road transport; satellite data; urban transport; Seoul [Seoul (ADS)]; Seoul [South Korea]; South Korea"
"Complex Background SAR Target Recognition Based on Convolution Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083498534&doi=10.1109%2fAPSAR46974.2019.9048279&partnerID=40&md5=a34cc7671e28e160c858d63638c35789","Deep learning networks are widely being applied to remote sensing image recognition and have achieved promising results. In this paper, we researched the influence of background with different scattering characteristics for synthetic aperture radar (SAR) target recognition based on convolutional neural network (CNN). Firstly, a two-parameter CFAR image segmentation method based on Weibull distribution was used to extracted SAR target and its shadow. And then, SAR datasets with road, farmland and grassland background environment is synthesized to analyze the CNN classifier. Experiments results show that the method by mixing training sets with different background together can improve the recognization rate when the backgrounds are complex. © 2019 IEEE.","classification; complex background; convolutional neural network; image segmentation; SAR","Classification (of information); Complex networks; Convolution; Convolutional neural networks; Deep learning; Image recognition; Image segmentation; Radar imaging; Remote sensing; Synthetic aperture radar; Weibull distribution; Background environment; Complex background; Convolution neural network; Learning network; Remote sensing images; Scattering char-acteristics; Segmentation methods; Target recognition; Radar target recognition"
"DNNs as Applied to Electromagnetics, Antennas, and Propagation - A Review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075007342&doi=10.1109%2fLAWP.2019.2916369&partnerID=40&md5=0002a0c63c3df8d01201d22969649018","A review of the most recent advances in deep learning (DL) as applied to electromagnetics (EM), antennas, and propagation is provided. It is aimed at giving the interested readers and practitioners in EM and related applicative fields some useful insights on the effectiveness and potentialities of deep neural networks (DNNs) as computational tools with unprecedented computational efficiency. The range of considered applications includes forward/inverse scattering, direction-of-arrival estimation, radar and remote sensing, and multi-input/multi-output systems. Appealing DNN-based solutions concerned with localization, human behavior monitoring, and EM compatibility are reported as well. Some final remarks are drawn along with the indications on future trends according to the authors' viewpoint. © 2002-2011 IEEE.","Antennas; convolutional neural networks; deep learning (DL); deep neural networks (DNNs); electromagnetics (EM); propagation; radar; remote sensing (RS); scattering","Antennas; Backpropagation; Behavioral research; Computational efficiency; Electromagnetic compatibility; Forward scattering; MIMO systems; Neural networks; Radar; Radar antennas; Remote sensing; Scattering; Wave propagation; Computational tools; Convolutional neural network; Direction of arrival estimation; Electromagnetics; Future trends; Human behaviors; Multi-input/multi-output systems; Deep neural networks"
"Efficiently utilizing complex-valued PolSAR image data via a multi-task deep learning framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071967232&doi=10.1016%2fj.isprsjprs.2019.09.002&partnerID=40&md5=8fd3d4e4a2f109582ff3e58add52a963","Convolutional neural networks (CNNs) have been widely used to improve the accuracy of polarimetric synthetic aperture radar (PolSAR) image classification. However, in most studies, the difference between PolSAR images and optical images is rarely considered. Most of the existing CNNs are not tailored for the task of PolSAR image classification, in which complex-valued PolSAR data have been simply equated to real-valued data to fit the optical image processing architectures and avoid complex-valued operations. This is one of the reasons CNNs unable to perform their full capabilities in PolSAR classification. To solve the above problem, the objective of this paper is to develop a tailored CNN framework for PolSAR image classification, which can be implemented from two aspects: Seeking a better form of PolSAR data as the input of CNNs and building matched CNN architectures based on the proposed input form. In this paper, considering the properties of complex-valued numbers, amplitude and phase of complex-valued PolSAR data are extracted as the input for the first time to maintain the integrity of original information while avoiding immature complex-valued operations. Then, a multi-task CNN (MCNN) architecture is proposed to match the improved input form and achieve better classification results. Furthermore, depthwise separable convolution is introduced to the proposed architecture in order to better extract information from the phase information. Experiments on three PolSAR benchmark datasets not only prove that using amplitude and phase as the input do contribute to the improvement of PolSAR classification, but also verify the adaptability between the improved input form and the well-designed architectures. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural networks; Deep learning; Depthwise separable convolutions; Multi-task learning; Polarimetric synthetic aperture radar (PolSAR) classification; Two-stream architecture","Architecture; Classification (of information); Complex networks; Computer architecture; Convolution; Deep learning; Deep neural networks; Geometrical optics; Image enhancement; Network architecture; Neural networks; Optical data processing; Polarimeters; Synthetic aperture radar; Classification results; Convolutional neural network; Extract informations; Multitask learning; Optical image processing; Polarimetric synthetic aperture radars; Proposed architectures; Two-stream; Image classification; accuracy assessment; amplitude; benchmarking; complexity; experimental study; image classification; image processing; satellite data; synthetic aperture radar"
"Informed Region Selection for Efficient UAV-based Object Detectors: Altitude-aware Vehicle Detection with CyCAR Dataset","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081160472&doi=10.1109%2fIROS40897.2019.8967722&partnerID=40&md5=e32526d8b8dc72e68691a7998f8f740e","Deep Learning-based object detectors enhance the capabilities of remote sensing platforms, such as Unmanned Aerial Vehicles (UAVs), in a wide spectrum of machine vision applications. However, the integration of deep learning introduces heavy computational requirements, preventing the deployment of such algorithms in scenarios that impose low-latency constraints during inference, in order to make mission-critical decisions in real-time. In this paper, we address the challenge of efficient deployment of region-based object detectors in aerial imagery, by introducing an informed methodology for extracting candidate detection regions (proposals). Our approach considers information from the UAV on-board sensors, such as flying altitude and light-weight computer vision filters, along with prior domain knowledge to intelligently decrease the number of region proposals by eliminating false-positives at an early stage of the computation, reducing significantly the computational workload while sustaining the detection accuracy. We apply and evaluate the proposed approach on the task of vehicle detection. Our experiments demonstrate that state-of-the-art detection models can achieve up to 2.6x faster inference by employing our altitude-aware data-driven methodology. Alongside, we introduce and provide to the community a novel vehicle-annotated and altitude-stamped dataset of real UAV imagery, captured at numerous flying heights under a wide span of traffic scenarios. © 2019 IEEE.",,"Aerial photography; Aircraft detection; Antennas; Computer vision; Deep learning; Inference engines; Intelligent robots; Remote sensing; Unmanned aerial vehicles (UAV); Computational requirements; Computational workload; Detection accuracy; Mission-critical decisions; On-board sensors; Region selections; Remote sensing platforms; Vehicle detection; Object detection"
"Optimization of ship target detection algorithm based on random forest and regional convolutional network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081169693&doi=10.1109%2fEEI48997.2019.00088&partnerID=40&md5=65250b546a776af1f7ad51092e8d60c4","Target detection can assist in detecting the position of the target ship, which is an important part of the intelligent ship visual aid system. With the development and perfection of deep learning, the convolutional neural network technology has been continuously optimized. And it can automatically learn and extract features of objects in images, providing stronger distinguishing power and representation ability. In this paper, various optimization algorithms of convolutional neural networks are compared. Aiming at the problem of unbalanced ship targets in remote sensing images of near-port areas, a ship target detection algorithm based on random forest and Faster-RCNN is proposed. The random forest algorithm is used for model optimization due to its insensitivity to multi-collinearity. The positive effect of the optimized algorithm on accuracy is verified through experiments. © 2019 IEEE.","Deep learning; Random forest; Regional convolutional network; Ship target detection algorithm","Convolution; Convolutional neural networks; Decision trees; Deep learning; Remote sensing; Ships; Signal detection; Collinearity; Convolutional networks; Model optimization; Optimization algorithms; Optimized algorithms; Random forest algorithm; Remote sensing images; Ship targets; Random forests"
"Multimodal and multi-model deep fusion for fine classification of regional complex landscape areas using ZiYuan-3 imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075390048&doi=10.3390%2frs11222716&partnerID=40&md5=8cdd3f8fd10b098377095d0035ae96e3","Land cover classification (LCC) of complex landscapes is attractive to the remote sensing community but poses great challenges. In complex open pit mining and agricultural development landscapes (CMALs), the landscape-specific characteristics limit the accuracy of LCC. The combination of traditional feature engineering and machine learning algorithms (MLAs) is not sufficient for LCC in CMALs. Deep belief network (DBN) methods achieved success in some remote sensing applications because of their excellent unsupervised learning ability in feature extraction. The usability of DBN has not been investigated in terms of LCC of complex landscapes and integrating multimodal inputs. A novel multimodal and multi-model deep fusion strategy based on DBN was developed and tested for fine LCC (FLCC) of CMALs in a 109.4 km2 area of Wuhan City, China. First, low-level and multimodal spectral-spatial and topographic features derived from ZiYuan-3 imagery were extracted and fused. The features were then input into a DBN for deep feature learning. The developed features were fed to random forest and support vector machine (SVM) algorithms for classification. Experiments were conducted that compared the deep features with the softmax function and low-level features with MLAs. Five groups of training, validation, and test sets were performed with some spatial auto-correlations. A spatially independent test set and generalized McNemar tests were also employed to assess the accuracy. The fused model of DBN-SVM achieved overall accuracies (OAs) of 94.74% ± 0.35% and 81.14% in FLCC and LCC, respectively, which significantly outperformed almost all other models. From this model, only three of the twenty land covers achieved OAs below 90%. In general, the developed model can contribute to FLCC and LCC in CMALs, and more deep learning algorithm-based models should be investigated in future for the application of FLCC and LCC in complex landscapes. © 2019 by the authors.","Complex landscape; DBN; Deep learning; Fine classification; Fusion; Machine learning algorithm; Multimodal; Remote sensing; ZiYuan-3","Complex networks; Decision trees; Deep learning; Fusion reactions; Image classification; Learning systems; Machine learning; Open pit mining; Remote sensing; Support vector machines; Complex landscape; Deep belief network (DBN); Land cover classification; Machine learning algorithm (MLAs); Multi-modal; Remote sensing applications; Support vector machine algorithm; ZiYuan-3; Learning algorithms"
"A novel deep framework for change detection of multi-source heterogeneous images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078752811&doi=10.1109%2fICDMW.2019.00034&partnerID=40&md5=e8c0376743f173d187aa5508a32ae0de","Change detection of remote sensing images is to detect changes in multiple images (at least a pair of images) obtained at different time points. However, almost all existing change detection methods based on deep learning need to be trained repeatedly on different datasets (especially on heterogeneous datasets) to obtain satisfactory results. Besides, many of these methods have to be trained on large numbers of samples acquired by classical pre-classification methods. In this paper, a novel change detection framework based on meta-learning, called MLCD, is proposed. We model the change detection problem as a one-shot learning problem, although there are no labels for reference in reality. Inspired by active learning, we present two sample selection strategies to find a few representative samples. Based on the key idea of meta-learning, our framework mainly consists of a modified convolutional neural network and a graph neural network, and it is capable of learning to compare samples in the embedded feature space. Meanwhile, it only needs to be trained once with very few samples and can be directly transferred to test changes on other heterogeneous images. Experimental results on both Synthetic Aperture Radar (SAR) and multispectral images demonstrate the effectiveness and superiority of our framework over state-of-the-art methods. © 2019 IEEE.","Change detection; Graph neural network; Meta-learning; Remote sensing images","Data mining; Neural networks; Remote sensing; Space optics; Synthetic aperture radar; Change detection; Classification methods; Convolutional neural network; Graph neural networks; Heterogeneous datasets; Metalearning; Remote sensing images; State-of-the-art methods; Deep learning"
"Unsupervised Change Detection Based on a Unified Framework for Weighted Collaborative Representation with RDDL and Fuzzy Clustering","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074454481&doi=10.1109%2fTGRS.2019.2923643&partnerID=40&md5=511919b245865ce32e290a1188d7748a","In this paper, we propose a novel unsupervised change detection method of remote sensing (RS) images based on a unified framework for weighted collaborative representation (WCR) with robust deep dictionary learning (RDDL) and fuzzy clustering. Specifically, WCR is employed to collaboratively represent neighborhood features with lower computational complexity, for which the RDDL model is built to learn more effective and representative overcomplete dictionary and enhance the robustness against the noise and outliers. Meanwhile, in order to make the resulting collaborative coefficients more beneficial for clustering, the unified framework for WCR with RDDL and fuzzy clustering is designed. By doing so, our framework not only precludes the utilization of third-party clustering algorithm, but also achieves better detection performance. Subsequently, the spatial constraint is enforced on the membership matrix to yield the updated one for further improving the accuracy of change detection. Finally, a binary change mask (CM) is achieved by assigning the pixels into the changed and unchanged classes. Experiments are performed on five pairs of RS images, and experimental results demonstrate the effectiveness of the proposed method. © 2019 IEEE.","Collaborative representation; deep learning 23 (DL); dictionary learning; fuzzy clustering; remote sensing (RS) 24 image; unsupervised change detection","Deep learning; Fuzzy clustering; Remote sensing; Collaborative representations; Detection performance; Dictionary learning; Membership matrix; Over-complete dictionaries; Remote sensing images; Spatial constraints; Unsupervised change detection; Clustering algorithms; cluster analysis; detection method; fuzzy mathematics; machine learning; numerical model; remote sensing; satellite imagery; unsupervised classification"
"Evolving larger convolutional layer kernel sizes for a settlement detection deep-learner on summit","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078172508&doi=10.1109%2fDLS49591.2019.00010&partnerID=40&md5=437aa3df29a91950112275e2ccbb19fc","Deep-learner hyper-parameters, such as kernel sizes, batch sizes, and learning rates, can significantly influence the quality of trained models. The state of the art for finding optimal hyper-parameters generally uses a brute force, grid search approach, random search, or Bayesian-based optimization among other techniques. We applied an evolutionary algorithm to optimize kernel sizes for a convolutional neural network used to detect settlements in satellite imagery. Usually convolutional layer kernel sizes are small - typically one, three, or five - but we found that the system converged at, or near, kernel sizes of nine for the last convolutional layer, and that this occurred for multiple runs using two different datasets. Moreover, the larger kernel sizes had fewer false positives than the 3x3 kernel sizes found as optimal via a brute force uniform grid search. This suggests that this large kernel size may be leveraging patterns found in larger areal features in the source imagery, and that this may be generalized as possible guidance for similar remote sensing deep-learning tasks. © 2019 IEEE.","Deep learning; Evolutionary algorithms; Hyper-parameters; Optimization; Remote sensing; Settlement mapping","Convolution; Dynamic light scattering; Evolutionary algorithms; Neural networks; Optimization; Remote sensing; Satellite imagery; Supercomputers; Convolutional neural network; False positive; Hyper-parameter; Large kernel sizes; Learning rates; Learning tasks; Random searches; State of the art; Deep learning"
"Classifying street spaces with street view images for a spatial indicator of urban functions","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075896826&doi=10.3390%2fsu11226424&partnerID=40&md5=1d401c257eb95338232d9d20a7b421cd","Streets, as one type of land use, are generally treated as developed or impervious areas in most of the land-use/land-cover studies. This coarse classification substantially understates the value of streets as a type of public space with the most complexity. Street space, being an important arena for urban vitality, is valued by various dimensions, such as transportation, recreation, aesthetics, public health, and social interactions. Traditional remote sensing approaches taking a sky viewpoint cannot capture these dimensions not only due to the resolution issue but also the lack of a citizen viewpoint. The proliferation of street view images provides an unprecedented opportunity to characterize street spaces from a citizen perspective at the human scale for an entire city. This paper aims to characterize and classify street spaces based on features extracted from street view images by a deep learning model of computer vision. A rule-based clustering method is devised to support the empirically generated classification of street spaces. The proposed classification scheme of street spaces can serve as an indirect indicator of place-related functions if not a direct one, once its relationship with urban functions is empirically tested and established. This approach is empirically applied to Beijing city to demonstrate its validity. © 2019 by the authors.","Deep learning; Spatial indicator of urban functions; Street view images; Streetscape classification","algorithm; image classification; land cover; land use; numerical method; numerical model; public space; remote sensing; satellite imagery; urban area; Beijing [China]; China"
"Fast super-resolution of 20 m Sentinel-2 bands using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075367036&doi=10.3390%2frs11222635&partnerID=40&md5=c70e6bcf1ebf436f61ec5327ae6aacf7","Images provided by the ESA Sentinel-2 mission are rapidly becoming the main source of information for the entire remote sensing community, thanks to their unprecedented combination of spatial, spectral and temporal resolution, as well as their associated open access policy. Due to a sensor design trade-off, images are acquired (and delivered) at different spatial resolutions (10, 20 and 60 m) according to specific sets of wavelengths, with only the four visible and near infrared bands provided at the highest resolution (10 m). Although this is not a limiting factor in general, many applications seem to emerge in which the resolution enhancement of 20 m bands may be beneficial, motivating the development of specific super-resolution methods. In this work, we propose to leverage Convolutional Neural Networks (CNNs) to provide a fast, upscalable method for the single-sensor fusion of Sentinel-2 (S2) data, whose aim is to provide a 10 m super-resolution of the original 20 m bands. Experimental results demonstrate that the proposed solution can achieve better performance with respect to most of the state-of-the-art methods, including other deep learning based ones with a considerable saving of computational burden. © 2019 by the authors.","Convolutional neural network; Data fusion; Landcover classification; Multi-resolution analysis; Pansharpening","Convolution; Data fusion; Deep learning; Economic and social effects; Infrared devices; Neural networks; Optical resolving power; Remote sensing; Computational burden; Convolutional neural network; Land-cover classification; Pan-sharpening; Resolution enhancement; State-of-the-art methods; Superresolution methods; Visible and near infrared; Sensor data fusion"
"Aquaculture area extraction and vulnerability assessment in Sanduao based on richer convolutional features network model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068992196&doi=10.1007%2fs00343-019-8265-z&partnerID=40&md5=3dfa7b21f32a0b1ff5e1cb95b5080694","Sanduao is an important sea-breeding bay in Fujian, South China and holds a high economic status in aquaculture. Quickly and accurately obtaining information including the distribution area, quantity, and aquaculture area is important for breeding area planning, production value estimation, ecological survey, and storm surge prevention. However, as the aquaculture area expands, the seawater background becomes increasingly complex and spectral characteristics differ dramatically, making it difficult to determine the aquaculture area. In this study, we used a high-resolution remote-sensing satellite GF-2 image to introduce a deep-learning Richer Convolutional Features (RCF) network model to extract the aquaculture area. Then we used the density of aquaculture as an assessment index to assess the vulnerability of aquaculture areas in Sanduao. The results demonstrate that this method does not require land and water separation of the area in advance, and good extraction can be achieved in the areas with more sediment and waves, with an extraction accuracy >93%, which is suitable for large-scale aquaculture area extraction. Vulnerability assessment results indicate that the density of aquaculture in the eastern part of Sanduao is considerably high, reaching a higher vulnerability level than other parts. © 2019, Chinese Society for Oceanology and Limnology, Science Press and Springer-Verlag GmbH Germany, part of Springer Nature.","aquaculture area; deep learning; high-resolution remote sensing; Richer Convolutional Features (RCF) network model; vulnerability assessment",
"RealPoint3D: Generating 3D point clouds from a single image of complex scenarios","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075389013&doi=10.3390%2frs11222644&partnerID=40&md5=421dd33e66c4193ed4d8d974a9ea7673","Generating 3D point clouds from a single image has attracted full attention from researchers in the field of multimedia, remote sensing and computer vision. With the recent proliferation of deep learning, various deep models have been proposed for the 3D point cloud generation. However, they require objects to be captured with absolutely clean backgrounds and fixed viewpoints, which highly limits their application in the real environment. To guide 3D point cloud generation, we propose a novel network, RealPoint3D, to integrate prior 3D shape knowledge into the network. Taking additional 3D information, RealPoint3D can handle 3D object generation from a single real image captured from any viewpoint and complex background. Specifically, provided a query image, we retrieve the nearest shape model from a pre-prepared 3D model database. Then, the image, together with the retrieved shape model, is fed into RealPoint3D to generate a fine-grained 3D point cloud. We evaluated the proposed RealPoint3D on the ShapeNet dataset and ObjectNet3D dataset for the 3D point cloud generation. Experimental results and comparisons with state-of-the-art methods demonstrate that our framework achieves superior performance. Furthermore, our proposed framework works well for real images in complex backgrounds (the image has the remaining objects in addition to the reconstructed object, and the reconstructed object may be occluded or truncated) with various viewing angles. © 2019 by the authors.","3D reconstruction; Deep learning; Indoor scenario; Point cloud generation; Single image","3D modeling; Complex networks; Deep learning; Multimedia systems; Query processing; Remote sensing; 3D reconstruction; Complex background; Indoor scenario; Point cloud; Real environments; Reconstructed objects; Single images; State-of-the-art methods; Image reconstruction"
"Convolutional neural network based regression for leaf water content estimation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078846459&doi=10.1109%2fINTELLECT47034.2019.8954985&partnerID=40&md5=5d235f03b0fee46e90494f7b24045f71","With the advancements in precision farming, crop sensing is gaining importance for timely crop health management. Leaf water content (LWC) is key component to determine vegetation health and nourishment. Timely estimation of LWC could save us from hazardous damage by pre-planning: drought stress on plants, irrigation and prediction of woodland fire. The retrieval of LWC from visible to shortwave infrared (VSWIR: 0.39 to 2.5 μm) mid- and thermal-infrared (MIR and TIR: 2.50 to 14.0 μm) windows of electromagnetic spectrum has been investigated using different statistical algorithms. Deep learning is modernizing the fast growing field of machine learning and image processing. The convolutional neural network (CNN) is ultramodern technique of deep learning that learns and extracts features directly from data. This research is focused on the extraction of different features of different plant species by using CNN for Regression. The modeled CNN architecture automatically detects prominent features to estimate LWC in plant species from its reflectance spectra, recorded for varying amount of LWC. Previous methods applied on same dataset yielded accuracy of 93% and Root Mean Square Error (RMSE) of 7.1, however, CNN resulted in better and swift results with an accuracy of 98.4% and RMSE of 4.183. This study helps in identifying the important spectral regions for quantifying water stresses in vegetation. The outcomes of this study can enable the future space missions to foresee water content of different plant species on the basis of their spectral signatures for illustrating vegetation stresses. © 2019 IEEE.","Convolutional neural network; Deep learning; Hyperspectral imaging; Leaf water content; Regression; Remote sensing","Convolution; Crops; Deep learning; Deep neural networks; Hyperspectral imaging; Mean square error; Regression analysis; Remote sensing; Spectroscopy; Vegetation; Convolutional neural network; Electromagnetic spectra; Leaf water content; Reflectance spectrum; Regression; Root mean square errors; Short wave infrared; Statistical algorithm; Neural networks"
"End-to-End DSM Fusion Networks for Semantic Segmentation in High-Resolution Aerial Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074452773&doi=10.1109%2fLGRS.2019.2907009&partnerID=40&md5=02af3aa543a91ad0ba04b6428435e71b","Semantic segmentation in high-resolution aerial images is a fundamental research problem in remote sensing field for its wide range of applications. However, it is difficult to distinguish regions with similar spectral features using only multispectral data. Recent research studies have indicated that the introduction of multisource information can effectively improve the robustness of segmentation method. In this letter, we use digital surface models (DSMs) information as a complementary feature to further improve the semantic segmentation results. To this end, we propose a lightweight and simple DSM fusion (DSMF) branch structure module. Compared with the existing feature extraction structures, proposed DSMF module is simple and can be easily applied to other networks. In addition, we investigate four fusion strategies based on DSMF module to explore the optimal feature fusion strategy and four end-to-end DSMFNets are designed according to the corresponding strategies. We evaluate our models on International Society for Photogrammetry and Remote Sensing Vaihingen data set and all DSMFNets achieve promising results. In particular, DSMFNet-1 achieves an overall accuracy of 91.5% on the test data set. © 2004-2012 IEEE.","Convolutional neural networks (CNNs); deep learning; high-resolution aerial images; semantic segmentation","Antennas; Deep learning; Deep neural networks; Neural networks; Remote sensing; Semantic Web; Semantics; Statistical tests; Complementary features; Convolutional neural network; Digital surface models; High-resolution aerial images; International society; Multi-source informations; Segmentation methods; Semantic segmentation; Image segmentation; accuracy assessment; aerial survey; artificial neural network; data assimilation; data set; image analysis; machine learning; photogrammetry; remote sensing; segmentation; spectral resolution; Baden-Wurttemberg; Germany; Vaihingen an der Enz"
"Automated method of road extraction from aerial images using a deep convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075236889&doi=10.3390%2fapp9224825&partnerID=40&md5=a020ca3b64c859ce03c25653e04c0f22","Updating road networks using remote sensing imagery is among the most important topics in city planning, traffic management and disaster management. As a good alternative to manual methods, which are considered to be expensive and time consuming, deep learning techniques provide great improvements in these regards. One of these techniques is the use of deep convolution neural networks (DCNNs). This study presents a road segmentation model consisting of a skip connection of U-net and residual blocks (ResBlocks) in the encoding part and convolution layers (Conv. layer) in the decoding part. Although the model uses fewer residual blocks in the encoding part and fewer convolution layers in the decoding part, it produces better image predictions in comparison with other state-of-the-art models. This model automatically and efficiently extracts road networks from high-resolution aerial imagery in an unexpansive manner using a small training dataset. © 2019 by the authors.","Automated road extraction; Deep neural network; Dice similarity coefficient; Remote sensing imagery",
"Retrieval of chemical oxygen demand through modified capsule network based on hyperspectral data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075237744&doi=10.3390%2fapp9214620&partnerID=40&md5=2a8bc611dc88f1c5e76988598268d1ca","This study focuses on the retrieval of chemical oxygen demand (COD) in the Baiyangdian area in North China, using a modified capsule network. Herein, the capsule model was modified to analyze the regression relationship between 1-D hyperspectral data and COD values. The results indicate there is a statistically significant correlation between COD and the hyperspectral data. The accuracy of the capsule network was compared with the results obtained from using a traditional back-propagation neural network (BP) method. The capsule network achieved superior accuracy with fewer iterations, compared with the BP algorithm. An R2 value of 0.78 was obtained against measured COD values retrieved using the capsule network method, compared with a value of 0.42 for the BP algorithm retrievals. This suggests the capsule network method has great potential to solve regression problems in the field of remote sensing. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Capsule network; Chemical oxygen demand; Deep learning; Remote sensing; Water pollution",
"Patch similarity convolutional neural network for urban flood extent mapping using bi-temporal satellite multispectral imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074659859&doi=10.3390%2frs11212492&partnerID=40&md5=0567a54fb9973feb2d5a2be72b16fa7b","Urban flooding is a major natural disaster that poses a serious threat to the urban environment. It is highly demanded that the flood extent can be mapped in near real-time for disaster rescue and relief missions, reconstruction efforts, and financial loss evaluation. Many efforts have been taken to identify the flooding zones with remote sensing data and image processing techniques. Unfortunately, the near real-time production of accurate flood maps over impacted urban areas has not been well investigated due to three major issues. (1) Satellite imagery with high spatial resolution over urban areas usually has nonhomogeneous background due to different types of objects such as buildings, moving vehicles, and road networks. As such, classical machine learning approaches hardly can model the spatial relationship between sample pixels in the flooding area. (2) Handcrafted features associated with the data are usually required as input for conventional flood mapping models, which may not be able to fully utilize the underlying patterns of a large number of available data. (3) High-resolution optical imagery often has varied pixel digital numbers (DNs) for the same ground objects as a result of highly inconsistent illumination conditions during a flood. Accordingly, traditional methods of flood mapping have major limitations in generalization based on testing data. To address the aforementioned issues in urban flood mapping, we developed a patch similarity convolutional neural network (PSNet) using satellite multispectral surface reflectance imagery before and after flooding with a spatial resolution of 3 meters. We used spectral reflectance instead of raw pixel DNs so that the influence of inconsistent illumination caused by varied weather conditions at the time of data collection can be greatly reduced. Such consistent spectral reflectance data also enhance the generalization capability of the proposed model. Experiments on the high resolution imagery before and after the urban flooding events (i.e., the 2017 Hurricane Harvey and the 2018 Hurricane Florence) showed that the developed PSNet can produce urban flood maps with consistently high precision, recall, F1 score, and overall accuracy compared with baseline classification models including support vector machine, decision tree, random forest, and AdaBoost, which were often poor in either precision or recall. The study paves the way to fuse bi-temporal remote sensing images for near real-time precision damage mapping associated with other types of natural hazards (e.g., wildfires and earthquakes). © 2019 by the authors.","Bi-temporal multispectral; Convolutional neural network; Deep learning; Flood mapping; Natural hazards; Patch similarity","Adaptive boosting; Convolution; Data handling; Decision trees; Deep learning; Deep neural networks; Disasters; Hazards; Hurricanes; Image resolution; Internet protocols; Losses; Mapping; Neural networks; Optical data processing; Pixels; Reflection; Remote sensing; Satellite imagery; Support vector machines; Convolutional neural network; Flood mapping; Multi-spectral; Natural hazard; Patch similarity; Floods"
"Spotting insects from satellites: Modeling the presence of culicoides imicola through deep CNNs","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084825336&doi=10.1109%2fSITIS.2019.00036&partnerID=40&md5=fb4dd0786f24a62977a8ca16da323ab7","Nowadays, Vector-Borne Diseases (VBDs) raise a severe threat for public health, accounting for a considerable amount of human illnesses. Recently, several surveillance plans have been put in place for limiting the spread of such diseases, typically involving on-field measurements. Such a systematic and effective plan still misses, due to the high costs and efforts required for implementing it. Ideally, any attempt in this field should consider the triangle vectors-host-pathogen, which is strictly linked to the environmental and climatic conditions. In this paper, we exploit satellite imagery from Sentinel-2 mission, as we believe they encode the environmental factors responsible for the vector's spread. Our analysis - conducted in a data-driver fashion - couples spectral images with ground-truth information on the abundance of Culicoides imicola. In this respect, we frame our task as a binary classification problem, underpinning Convolutional Neural Networks (CNNs) as being able to learn useful representation from multi-band images. Additionally, we provide a multi-instance variant, aimed at extracting temporal patterns from a short sequence of spectral images. Experiments show promising results, providing the foundations for novel supportive tools, which could depict where surveillance and prevention measures could be prioritized. © 2019 IEEE.","Bluetongue; Convolutional Neural Networks; Culicoides imicola; Deep Learning; Entomological surveillance; Europe; Infectious disease; Remote-sensing; Satellite imagery; Sentinels; Vector-borne diseases","Convolutional neural networks; Health risks; Satellite imagery; Spectroscopy; Binary classification problems; Climatic conditions; Environmental factors; Field measurement; Multi-band images; Prevention measures; Temporal pattern; Vector-borne disease; Network security"
"Bringing automated, remote-sensed, machine learning methods to monitoring crop landscapes at scale","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075203611&doi=10.1111%2fagec.12531&partnerID=40&md5=bb9529d96fa46284e3a781f03a435aad","This article provides an overview of how recent advances in machine learning and the availability of data from earth observing satellites can dramatically improve our ability to automatically map croplands over long periods and over large regions. It discusses three applications in the domain of crop monitoring where machine learning (ML) approaches are beginning to show great promise. For each application, it highlights machine learning challenges, proposed approaches, and recent results. The article concludes with discussion of major challenges that need to be addressed before ML approaches will reach their full potential for this problem of great societal relevance. © 2019 International Association of Agricultural Economists","deep learning; machine learning; monitoring crop landscapes; N5; Q1; remote sensing","automation; crop performance; geological mapping; machine learning; observational method; remote sensing; satellite altimetry"
"Unsupervised Clustering of Seismic Signals Using Deep Convolutional Autoencoders","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074539916&doi=10.1109%2fLGRS.2019.2909218&partnerID=40&md5=ea3d0a6c00426dbeeb7dd936ba020ec0","In this letter, we use deep neural networks for unsupervised clustering of seismic data. We perform the clustering in a feature space that is simultaneously optimized with the clustering assignment, resulting in learned feature representations that are effective for a specific clustering task. To demonstrate the application of this method in seismic signal processing, we design two different neural networks consisting primarily of full convolutional and pooling layers and apply them to: 1) discriminate waveforms recorded at different hypocentral distances and 2) discriminate waveforms with different first-motion polarities. Our method results in precisions that are comparable to those recently achieved by supervised methods, but without the need for labeled data, manual feature engineering, and large training sets. The applications we present here can be used in standard single-site earthquake early warning systems to reduce the false alerts on an individual station level. However, the presented technique is general and suitable for a variety of applications including quality control of the labeling and classification results of other supervised methods. © 2004-2012 IEEE.","Clustering; deep learning; neural networks; seismic signal; unsupervised learning; waveform discrimination","Convolution; Deep learning; Multilayer neural networks; Neural networks; Seismic design; Seismic waves; Seismology; Signal processing; Supervised learning; Unsupervised learning; Classification results; Clustering; Earthquake early warning systems; Feature representation; Seismic signal processing; Seismic signals; Unsupervised clustering; Waveform discrimination; Deep neural networks; artificial neural network; design method; earthquake event; quality control; remote sensing; seismic data; unsupervised classification; waveform analysis"
"Oil spill segmentation in SAR images using convolutional neural networks. A comparative analysis with clustering and logistic regression algorithms.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071650215&doi=10.1016%2fj.asoc.2019.105716&partnerID=40&md5=58dd1d99ef5a3767d52b2472137465c6","Synthetic aperture radar (SAR) images are a valuable source of information for the detection of marine oil spills. For their effective analysis, it is important to have segmentation algorithms that can delimit possible oil spill areas. This article addresses the application of clustering, logistic regression and convolutional neural network algorithms for the detection of oil spills in Envisat and Sentinel-1 satellite images. Large oil spills do not occur frequently so that the identification of a pixel as oil is relatively uncommon. Metrics based on Precision–Recall curves have been employed because they are useful for problems with an imbalance in the number of samples from the classes. Although logistic regression and clustering algorithms can be considered useful for oil spill segmentation, the combination of convolutional techniques and neural networks achieves the best results with low computing time. A convolutional neural network has been integrated into a decision support system in order to facilitate decision-making and data analysis of possible oil spill events. © 2019 Elsevier B.V.","Deep learning; Image segmentation; Oil spill; Remote sensing; SAR","Convolution; Decision making; Decision support systems; Deep learning; Geodetic satellites; Image segmentation; Marine pollution; Marine radar; Neural networks; Oil spills; Radar imaging; Regression analysis; Remote sensing; Synthetic aperture radar; Comparative analysis; Convolutional neural network; Effective analysis; Logistic regression algorithms; Logistic regressions; Number of samples; Segmentation algorithms; Synthetic aperture radar (SAR) images; Clustering algorithms"
"A labelled ocean SAR imagery dataset of ten geophysical phenomena from Sentinel-1 wave mode","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070104608&doi=10.1002%2fgdj3.73&partnerID=40&md5=73fb3ac0d2339ab1d9dc345fd1f45b8e","The Sentinel-1 mission is part of the European Copernicus program aiming at providing observations for Land, Marine and Atmosphere Monitoring, Emergency Management, Security and Climate Change. It is a constellation of two (Sentinel-1 A and B) Synthetic Aperture Radar (SAR) satellites. The SAR wave mode (WV) routinely collects high-resolution SAR images of the ocean surface during day and night and through clouds. In this study, a subset of more than 37,000 SAR images is labelled corresponding to ten geophysical phenomena, including both oceanic and meteorologic features. These images cover the entire open ocean and are manually selected from Sentinel-1A WV acquisitions in 2016. For each image, only one prevalent geophysical phenomenon with its prescribed signature and texture is selected for labelling. The SAR images are processed into a quick-look image provided in the formats of PNG and GeoTIFF as well as the associated labels. They are convenient for both visual inspection and machine learning-based methods exploitation. The proposed dataset is the first one involving different oceanic or atmospheric phenomena over the open ocean. It seeks to foster the development of strategies or approaches for massive ocean SAR image analysis. A key objective was to allow exploiting the full potential of Sentinel-1 WV SAR acquisitions, which are about 60,000 images per satellite per month and freely available. Such a dataset may be of value to a wide range of users and communities in deep learning, remote sensing, oceanography and meteorology. © 2019 The Authors. Geoscience Data Journal published by Royal Meteorological Society and John Wiley & Sons Ltd.","manual labelling; ocean surface phenomena; Sentinel-1 wave mode; Synthetic aperture radar",
"Context pyramidal network for stereo matching regularized by disparity gradients","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072713822&doi=10.1016%2fj.isprsjprs.2019.09.012&partnerID=40&md5=ea2cd7dbab4921749bcc0a39516a6c8e","Also after many years of research, stereo matching remains to be a challenging task in photogrammetry and computer vision. Recent work has achieved great progress by formulating dense stereo matching as a pixel-wise learning task to be resolved with a deep convolutional neural network (CNN). However, most estimation methods, including traditional and deep learning approaches, still have difficulty to handle real-world challenging scenarios, especially those including large depth discontinuity and low texture areas. To tackle these problems, we investigate a recently proposed end-to-end disparity learning network, DispNet (Mayer et al., 2015), and improve it to yield better results in these problematic areas. The improvements consist of three major contributions. First, we use dilated convolutions to develop a context pyramidal feature extraction module. A dilated convolution expands the receptive field of view when extracting features, and aggregates more contextual information, which allows our network to be more robust in weakly textured areas. Second, we construct the matching cost volume with patch-based correlation to handle larger disparities. We also modify the basic encoder-decoder module to regress detailed disparity images with full resolution. Third, instead of using post-processing steps to impose smoothness in the presence of depth discontinuities, we incorporate disparity gradient information as a gradient regularizer into the loss function to preserve local structure details in large depth discontinuity areas. We evaluate our model in terms of end-point-error on several challenging stereo datasets including Scene Flow, Sintel and KITTI. Experimental results demonstrate that our model decreases the estimation error compared with DispNet on most datasets (e.g. we obtain an improvement of 46% on Sintel) and estimates better structure-preserving disparity maps. Moreover, our proposal also achieves competitive performance compared to other methods. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Dilated convolution; Gradient regularizer; Stereo matching; Structure preserving","Convolution; Deep neural networks; Neural networks; Stereo vision; Textures; Competitive performance; Contextual information; Convolutional neural network; Dense stereo matching; Photogrammetry and computer visions; Regularizer; Stereo matching; Structure-preserving; Stereo image processing; algorithm; artificial neural network; equipment; estimation method; field of view; image analysis; image resolution; photogrammetry"
"Building scene recognition based on deep multiple instance learning convolutional neural network using high resolution remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078479445&doi=10.1145%2f3369318.3369324&partnerID=40&md5=7af05a347c3f7d31f1894a0e265d9f6b","As the typical man-made objects, buildings play a special role in remote sensing images. This paper proposes a building scene recognition method based on deep multiple instance convolutional neural network. Scene classification is formulated as a multiple instance learning (MIL) problem in which local scene regions are regarded as instances and assigned with different labels. Through a trainable MIL pooling based on a spatial attention mechanism to select the most relevant instances adaptively and produce the scene-level predictions. Experimental results on UCM data set and small targets data set collected based on the GF-2 show that the method can improve the accuracy of building scene recognition using high resolution remote sensing image. Particularly, it solves the problem that it is difficult to detect the small buildings correctly and provide a new idea of scene recognition of buildings. © 2019 Association for Computing Machinery.","Building scene recognition; Convolutional neural network(CNN); Multiple instance learning(MIL); Small building targets","Buildings; Convolution; Image enhancement; Neural networks; Remote sensing; Video signal processing; Convolutional neural network; High resolution remote sensing images; Multiple instances; Multiple-instance learning; Remote sensing images; Scene classification; Scene recognition; Small buildings; Deep learning"
"AENN: A GENERATIVE ADVERSARIAL NEURAL NETWORK for WEATHER RADAR ECHO EXTRAPOLATION","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081576080&doi=10.5194%2fisprs-archives-XLII-3-W9-89-2019&partnerID=40&md5=7a7d74925921a90f0fb6f0ed61e37bab","Weather radar echo is one of the fundamental data for meteorological workers to weather systems identification and classification. Through the technique of weather radar echo extrapolation, the future short-term weather conditions can be predicted and severe convection storms can be warned. However, traditional extrapolation methods cannot offer accurate enough extrapolation results since their modeling capacity is limited, the recent deep learning based methods make some progress but still remains a problem of blurry prediction when making deeper extrapolation, which may due to they choose the mean square error as their loss function and that will lead to losing echo details. To address this problem and make a more realistic and accurate extrapolation, we propose a deep learning model called Adversarial Extrapolation Neural Network (AENN), which is a Generative Adversarial Network (GAN) structure and consist of a conditional generator and two discriminators, echo-frame discriminator and echo-sequence discriminator. The generator and discriminators are trained alternately in an adversarial way to make the final extrapolation results be realistic and accurate. To evaluate the model, we conduct experiments on extrapolating 0.5h, 1h, and 1.5h imminent future echoes, the results show that our proposed AENN can achieve the expected effect and outperforms other models significantly, which has a powerful potential application value for short-term weather forecasting. © 2020 Authors.","Adversarial Training; Deep Learning; Generative Adversarial Network; Radar Echo Extrapolation; Recurrent Neural Network; Short-term Weather Forecasting; Weather Radar","Deep learning; Deep neural networks; Extrapolation; Learning systems; Mean square error; Meteorological radar; Recurrent neural networks; Remote sensing; Adversarial networks; Expected effects; Extrapolation methods; Learning-based methods; Radar echoes; Severe convections; Short term; Weather systems; Weather forecasting"
"Artificial neural networks optimization and convolution neural networks to classifying images in remote sensing: A review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078340188&doi=10.1145%2f3372938.3372945&partnerID=40&md5=d07d7e3b029ef3888f4505ae7fc4d273","One of important functions of remote sensing data is producing the land-use/land-cover maps. Image classification is one of important applications for remote sensing imaginary. Machine learning (ML) techniques are the most widely used for this purpose in recent years. With the advent of computer vision thus, the need to deal with a large amount of data and avoiding any data redundancy, the deep learning techniques were appeared. Deep learning (DL) is a branch of machine learning that imitates the human brain structure and depends on the artificial neural networks (ANNs). Optimization of the neural networks is necessary for reduce the loss functions and avoiding any redundancy data in the training set, thus raise the accuracy. Genetic algorithms (GA) are the most widely used in the neural networks optimization, which considered as fully connected neural networks. Convolution neural networks (CNNs) are a branch of the artificial neural networks that are saving the computing cost and processing time. Thus, this paper presents a review of the deep learning algorithms specially the artificial neural networks, the genetic algorithm and the convolution neural networks. This paper also introduces a comparative study between the genetic algorithm and the convolution neural networks method. This comparison based on the overall accuracy (OA) and the kappa coefficient. This comparison shows that there are many conditions can affect the classifier accuracy. The results demonstrate that the CNNs algorithms are more accurate than the GA and in the other hand, the CNNs algorithms have lower computing cost. © 2019 Association for Computing Machinery.","ANNs; CNNs; Deep Learning; GA; Image Classification; Machine Learning; Optimization; Remote Sensing","Big data; Convolution; Gallium; Genetic algorithms; Image classification; Internet of things; Land use; Learning algorithms; Learning systems; Machine learning; Neural networks; Optimization; Redundancy; Remote sensing; ANNs; CNNs; Comparative studies; Convolution neural network; Fully connected neural network; Learning techniques; Overall accuracies; Remote sensing data; Deep learning"
"Accurate semantic segmentation in remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082678677&doi=10.1145%2f3373509.3373535&partnerID=40&md5=38c4fd06379d073a13157ffe417b55d7","Thanks to recent development in CNNs and deep learning, solid improvements have been made in semantic segmentation, however, most of the previous works in semantic segmentation are for automatic driving and do not fully taken into account the specific difficulties that exist in high resolution remote sensing imagery. One of such difficulties is that objects are small, crowded and intra-class scale difference in remote sensing imagery. To tackle with this challenging task, we have proposed a novel architecture which adopts encoder-decoder structure, multiscale dilated convolution with spatial attention and separable convolution (Global Attention Pyramid) and channel attention decoder (Attention Decoder). The proposed Global Attention Pyramid module solves these problems by enlarging receptive field without reducing resolution of feature maps and pixel-level attention. And the proposed Attention Decoder module solves these problems by providing global context to select category localization details. We tested our network on two satellite imagery datasets and acquired remarkably good results for both datasets especially for small objects. And our new network improves the performance from 0.6341 to 0.6510 in DEEPGLOBE road extraction dataset. © 2019 Association for Computing Machinery.","Neural networks.; Remote sensing image; Semantic segmentation","Automobile drivers; Channel coding; Convolution; Decoding; Deep learning; Image enhancement; Image segmentation; Neural networks; Pattern recognition; Satellite imagery; Semantics; Small satellites; Automatic driving; Category localization; High resolution remote sensing imagery; Novel architecture; Remote sensing imagery; Remote sensing images; Semantic segmentation; Spatial attention; Remote sensing"
"Building extraction using mask scoring R-Cnn network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074813801&doi=10.1145%2f3331453.3361644&partnerID=40&md5=8a53d3321bc7094f75a4fe36913c5812","Extracting buildings from high resolution remotely sensed images is very practical, which can be applied to urban modeling and so on. The development of computer vision has become better, and the accuracy of recognition of convolutional neural networks has exceeded the accuracy of recognition of human eyes. In this paper, we used a deep convolutional neural network in remote sensing to achieve building extraction. The method in this paper is not based on semantic segmentation, but instance segmentation, which considered each building as an independent individual to achieve building extraction. The results showed that the proposed method is able to extract buildings with high accuracy. © 2019 Association for Computing Machinery.","Building extraction; Deep learning; Instance segmentation","Convolution; Deep learning; Deep neural networks; Extraction; Neural networks; Remote sensing; Semantics; Building extraction; CNN network; Convolutional neural network; Extracting buildings; High resolution remotely sensed images; High-accuracy; Semantic segmentation; Urban model; Buildings"
"Attention based convolutional neural network for building extraction from very high resolution remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083205574&doi=10.5194%2fisprs-archives-XLII-4-W18-507-2019&partnerID=40&md5=10be15913d3eeeac9fbb4a434eea7198","Buildings are a major element in the formation of cities and are essential for urban mapping. The precise extraction of buildings from remote sensing data has become a significant topic and has received much attention in recent years. The recently developed convolutional neural networks have shown effective and superior performance to perform well on learning high-level and discriminative features in extracting buildings because of the outstanding feature learning and end-to-end pixel labelling abilities. However, it is difficult to use the features of different levels with a certain degree of importance that is appropriate to deep learning networks. To tackle this problem, a network based on U-Nets and the attention mechanism block was proposed. The network contains an encoder part and a decoder part and a spatial attention module. The special architecture presented in this article enhances the propagation of features and effectively utilizes the features at various levels to reduce errors. The other remarkable thing is that attention module blocks only lead to a minimal increase in model complexity. We effectively demonstrate an improvement of building extraction accuracy on challenging Potsdam and Vaihingen benchmark datasets. The results of this paper show that the proposed architecture improves building extraction in very high resolution remote sensing images compared to previous models. © 2019 H. R. Hosseinpoor.","Attention mechanism; Building extraction; Fully convolutional neural networks; U-Net","Backpropagation; Buildings; Convolution; Deep learning; Extraction; Image enhancement; Network architecture; Remote sensing; Attention mechanisms; Benchmark datasets; Building extraction; Discriminative features; Extracting buildings; Proposed architectures; Remote sensing data; Very high resolution; Convolutional neural networks"
"Aerial point cloud classification with deep learning and machine learning algorithms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083281009&doi=10.5194%2fisprs-archives-XLII-4-W18-843-2019&partnerID=40&md5=8fec1b532bc198cfcbbbfefab20c9de9","With recent advances in technology, 3D point clouds are getting more and more frequently requested and used, not only for visualization needs but also e.g. by public administrations for urban planning and management. 3D point clouds are also a very frequent source for generating 3D city models which became recently more available for many applications, such as urban development plans, energy evaluation, navigation, visibility analysis and numerous other GIS studies. While the main data sources remained the same (namely aerial photogrammetry and LiDAR), the way these city models are generated have been evolving towards automation with different approaches. As most of these approaches are based on point clouds with proper semantic classes, our aim is to classify aerial point clouds into meaningful semantic classes, e.g. ground level objects (GLO, including roads and pavements), vegetation, buildings' facades and buildings' roofs. In this study we tested and evaluated various machine learning algorithms for classification, including three deep learning algorithms and one machine learning algorithm. In the experiments, several hand-crafted geometric features depending on the dataset are used and, unconventionally, these geometric features are used also for deep learning. © 2019 E. Özdemir et al.","Classification; Deep learning; Geometric features; Machine learning; Point cloud; Urban areas","Antennas; Deep learning; Learning systems; Photogrammetry; Public administration; Remote sensing; Semantics; Three dimensional computer graphics; Urban growth; 3D city models; 3D point cloud; Aerial photogrammetry; Energy evaluation; Geometric feature; Semantic class; Urban development; Visibility analysis; Learning algorithms"
"Application of machine and deep learning strategies for the classification of heritage point clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082301790&doi=10.5194%2fisprs-archives-XLII-4-W18-447-2019&partnerID=40&md5=52b291d5cf6a2b05c53f0ab939b93f14","The use of heritage point cloud for documentation and dissemination purposes is nowadays increasing. The association of semantic information to 3D data by means of automated classification methods can help to characterize, describe and better interpret the object under study. In the last decades, machine learning methods have brought significant progress to classification procedures. However, the topic of cultural heritage has not been fully explored yet. This paper presents a research for the classification of heritage point clouds using different supervised learning approaches (Machine and Deep learning ones). The classification is aimed at automatically recognizing architectural components such as columns, facades or windows in large datasets. For each case study and employed classification method, different accuracy metrics are calculated and compared. © 2019 E. Grilli et al.","Classification; Cultural Heritage; Deep Learning; Machine Learning; Point Clouds","Classification (of information); Large dataset; Learning systems; Remote sensing; Semantics; Architectural components; Automated classification; Classification methods; Classification procedure; Cultural heritages; Machine learning methods; Semantic information; Supervised learning approaches; Deep learning"
"A deep learning framework for roads network damage assessment using post-earthquake lidar data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079068973&doi=10.5194%2fisprs-archives-XLII-4-W18-955-2019&partnerID=40&md5=7dc25e03dd3116a969f280bab28181fc","Roads network are the most important parts of urban infrastructures, which can cause difficulty to the city whenever they undergo a problem. This paper aims to provide and implement a deep learning-based method to determine the status of the streets network after an earthquake using LiDAR point cloud. The proposed framework composes of three main phases: (1) Deep features of LiDAR data are extracted using a Convolutional Neural Network (CNN). (2) The extracted features are used in a multilayer perceptron (MLP) neural network in which debris areas inside the road network are detected. (3) The amount of debris in each road is applied to damage index for classifying the road segments into blocked or un-blocked. To evaluate the efficiency of the proposed framework, LiDAR point cloud of the Port-au-Prince, Haiti after the 2010 Haiti earthquake was used. The overall accuracy of more than 97% proved the high performance of this framework for debris detection. Moreover, analyzing damage assessment of 37 road segments based on the detected debris and comparing to a visually generated damaged map, 31 of the road segments were correctly labelled as either blocked or un-blocked. © 2019 S. T. Seydi.","Conventional Neural Network; Deep learning; Earthquake; LiDAR Point Cloud; Roads damage map","Convolutional neural networks; Damage detection; Debris; Earthquakes; Multilayer neural networks; Optical radar; Remote sensing; Roads and streets; Damage assessments; Haiti earthquakes; Learning frameworks; Learning-based methods; Lidar point clouds; Multilayer perceptron neural networks; Overall accuracies; Urban infrastructure; Deep learning"
"Hyperspectral image classification by exploiting convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083161258&doi=10.5194%2fisprs-archives-XLII-4-W18-535-2019&partnerID=40&md5=68ddcaa4f19d93ce4200f1cb603e4b8f","High spectral dimensionality of hyperspectral images makes them useful data resources for earth observation in many remote sensing applications. In this case, the convolutional neural network (CNN) can help to extract deep and robust features from hyperspectral images. The main goal of this paper is to use deep learning concept to extract deep features from hyperspectral datasets to achieve better classification results. In this study, after pre-processing step, data is fed to a CNN in order to extract deep features. Extracted features are then imported in a multi-layer perceptron (MLP) network as our selected classifier. Obtained classification accuracies, based on training sample size, vary from 94.3 to 97.17% and 92.35 to 98.14% for Salinas and Pavia datasets, respectively. These results expressed more than 10% improvements compared to the classic MLP classification technique. © 2019 B. Hosseiny et al.","Convolutional Neural Networks; Deep learning; Feature extraction; Hyperspectral Image","Classification (of information); Convolution; Data handling; Deep learning; Image classification; Remote sensing; Spectroscopy; Classification accuracy; Classification results; Classification technique; Earth observations; Multi layer perceptron; Pre-processing step; Remote sensing applications; Training sample; Convolutional neural networks"
"Robust building footprint extraction from big multi-sensor data using deep competition network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083206180&doi=10.5194%2fisprs-archives-XLII-4-W18-615-2019&partnerID=40&md5=b678866d2303058f0a6df841855820ae","Building footprint extraction (BFE) from multi-sensor data such as optical images and light detection and ranging (LiDAR) point clouds is widely used in various fields of remote sensing applications. However, it is still challenging research topic due to relatively inefficient building extraction techniques from variety of complex scenes in multi-sensor data. In this study, we develop and evaluate a deep competition network (DCN) that fuses very high spatial resolution optical remote sensing images with LiDAR data for robust BFE. DCN is a deep superpixelwise convolutional encoder-decoder architecture using the encoder vector quantization with classified structure. DCN consists of five encoding-decoding blocks with convolutional weights for robust binary representation (superpixel) learning. DCN is trained and tested in a big multi-sensor dataset obtained from the state of Indiana in the United States with multiple building scenes. Comparison results of the accuracy assessment showed that DCN has competitive BFE performance in comparison with other deep semantic binary segmentation architectures. Therefore, we conclude that the proposed model is a suitable solution to the robust BFE from big multi-sensor data. © 2019 M. Khoshboresh Masouleh.","Big Multi-Sensor Data; Building Extraction; Cloud Computing; Colab; Convolutional Network; Deep Learning","Convolution; Decoding; Extraction; Geometrical optics; Network architecture; Optical radar; Remote sensing; Semantics; Signal encoding; Binary representations; Binary segmentation; Building extraction; Convolutional encoders; Light detection and ranging; Optical remote sensing; Remote sensing applications; Very high spatial resolutions; Data mining"
"A convolutional neural network for flood mapping using sentinel-1 and srtm dem data: Case study in poldokhtar-Iran","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083253997&doi=10.5194%2fisprs-archives-XLII-4-W18-527-2019&partnerID=40&md5=4f50c8b3f266c8c947da20d3ad192713","Flood contributes a key role in devastating natural and man-made areas. Floods usually are occurred when there is a considerable number of clouds in the sky making optic data useless. Synthetic aperture radar (SAR) images can be a valuable data source in earth observation tasks. The most important characteristic of the radar image is its ability to penetrate the cloud and dust. Therefore, monitoring earth in cloudy or rainy weather can be available by this kind of dataset. In the last few years by improving machine learning methods and development of convolutional neural networks in remote sensing applications we are facing with extremely high improvement in classification tasks. In this paper, we use dual-polarized VV and VH backscatter values of Sentinel-1 and Shuttle Radar Topography Mission (SRTM) digital elevation model (DEM) dataset in a proposed convolutional neural network to generate a land cover map of a flooded area before and after happening. Obtained classification results vary between 93.3% to 98.5% for different training sizes. By comparing the generated classified maps, flooded areas of each class can be extracted. © 2019 B. Hosseiny et al.","Change detection; Classification; Deep learning; Satellite images; Synthetic aperture radar","Convolution; Floods; Learning systems; Radar imaging; Remote sensing; Space-based radar; Surveying; Synthetic aperture radar; Topography; Tracking radar; Classification results; Classification tasks; Digital elevation model; Earth observations; Machine learning methods; Remote sensing applications; Shuttle radar topography mission; Synthetic aperture radar (SAR) images; Convolutional neural networks"
"Urban vision development in order to monitor wheelchair users based on the yolo algorithm","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083186739&doi=10.5194%2fisprs-archives-XLII-4-W18-25-2019&partnerID=40&md5=ca2e6b486e019a6a020897da58d2b252","Disability has been one of the most important problems of social communities throughout the ages. As population and urbanization have grown dramatically over recent years, this problem has more and more created the gap between people with disabilities and ordinary people in terms of access to resources, social services and social partnerships. Therefore, this study attempts to demonstrate the ratio of presence of wheelchair users in a community compared to the total population of the same community and evaluate their patterns of presence in different conditions, for example, various weather conditions. For this purpose, we used the You Look Only Once version 3 (YOLOv3) algorithm which is a multilayer deep learning object detection tool to analyze and extract wheelchair users from three different sets of images taken by a camera located in an intersection proximate to a rehabilitation center in Quebec, Canada. The results show that the proportion of wheelchair users in the sample community is 7.4%, while the population with mobility disabilities in Canada is 9.6%. © 2019 A. Ahmadi et al.","Artificial Intelligence; Disability; Urban Vision; Wheelchair Detection; YOLOv3","Deep learning; Object detection; Remote sensing; Access to resources; Detection tools; Ordinary people; People with disabilities; Quebec , Canada; Social communities; Social partnership; Wheelchair users; Wheelchairs"
"Virtual training sample generation by generative adversarial networks for hyperspectral images classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083198692&doi=10.5194%2fisprs-archives-XLII-4-W18-63-2019&partnerID=40&md5=ace3f94edf92f9c576b53b1548f816a0","Convolutional Neural Networks (CNNs) as a well-known deep learning technique has shown a remarkable performance in visual recognition applications. However, using such networks in the area of hyperspectral image classification is a challenging and time-consuming process due to the high dimensionality and the insufficient training samples. In addition, Generative Adversarial Networks (GANs) has attracted a lot of attentions in order to generate virtual training samples. In this paper, we present a new classification framework based on integration of multi-channel CNNs and new architecture for generator and discriminator of GANs to overcome Small Sample Size (SSS) problem in hyperspectral image classification. Further, in order to reduce the computational cost, the methods related to the reduction of subspace dimension were proposed to obtain the dominant feature around the training sample to generate meaningful training samples from the original one. The proposed framework overcomes SSS and overfitting problem in classifying hyperspectral images. Based on the experimental results on real and well-known hyperspectral benchmark images, our proposed strategy improves the performance compared to standard CNNs and conventional data augmentation strategy. The overall classification accuracy in Pavia University and Indian Pines datasets was 99.8% and 94.9%, respectively. © 2019 T. Alipourfard.","Convolutional Neural Network; Deep Learning; Generative Adversarial Networks; Hyperspectral Image","Benchmarking; Classification (of information); Convolutional neural networks; Deep learning; E-learning; Image enhancement; Remote sensing; Sampling; Silicon compounds; Spectroscopy; Adversarial networks; Classification accuracy; Classification framework; High dimensionality; Images classification; Learning techniques; Over fitting problem; Small sample size problems; Image classification"
"Building outline extraction from aerial images using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083229977&doi=10.5194%2fisprs-archives-XLII-4-W18-57-2019&partnerID=40&md5=b7a1d216af70f6ca1828ea25e6702794","Automatic detection and extraction of buildings from aerial images are considerable challenges in many applications, including disaster management, navigation, urbanization monitoring, emergency responses, 3D city mapping and reconstruction. However, the most important problem is to precisely localize buildings from single aerial images where there is no additional information such as LiDAR point cloud data or high resolution Digital Surface Models (DSMs). In this paper, a Deep Learning (DL)-based approach is proposed to localize buildings, estimate the relative height information, and extract the buildings' boundaries using a single aerial image. In order to detect buildings and extract the bounding boxes, a Fully Connected Convolutional Neural Network (FC-CNN) is trained to classify building and non-building objects. We also introduced a novel Multi-Scale Convolutional-Deconvolutional Network (MS-CDN) including skip connection layers to predict normalized DSMs (nDSMs) from a single image. The extracted bounding boxes as well as predicted nDSMs are then employed by an Active Contour Model (ACM) to provide precise boundaries of buildings. The experiments show that, even having noises in the predicted nDSMs, the proposed method performs well on single aerial images with different building shapes. The quality rate for building detection is about 86% and the RMSE for nDSM prediction is about 4 m. Also, the accuracy of boundary extraction is about 68%. Since the proposed framework is based on a single image, it could be employed for real time applications. © 2019 F. Alidoost et al.","Active Contour Models; Building Detection; Deep Learning; Depth Prediction; Selective Search","Air navigation; Antennas; Buildings; Convolution; Convolutional neural networks; Deep learning; Disaster prevention; Disasters; Extraction; Maintenance; Remote sensing; Active contour model; Automatic Detection; Boundary extraction; Digital surface models; Disaster management; Lidar point cloud datum; Outline extractions; Real-time application; Image processing"
"CNN-based feature-level fusion of very high resolution aerial imagery and lidar data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083224798&doi=10.5194%2fisprs-archives-XLII-4-W18-279-2019&partnerID=40&md5=ce71e9bdb64f09fb7657204091528d5d","Land-cover classification of Remote Sensing (RS) data in urban area has always been a challenging task due to the complicated relations between different objects. Recently, fusion of aerial imagery and light detection and ranging (LiDAR) data has obtained a great attention in RS communities. Meanwhile, convolutional neural network (CNN) has proven its power in extracting high-level (deep) descriptors to improve RS data classification. In this paper, a CNN-based feature-level framework is proposed to integrate LiDAR data and aerial imagery for object classification in urban area. In our method, after generating low-level descriptors and fusing them in a feature-level fusion by layer-stacking, the proposed framework employs a novel CNN to extract the spectral-spatial features for classification process, which is performed using a fully connected multilayer perceptron network (MLP). The experimental results revealed that the proposed deep fusion model provides about 10% improvement in overall accuracy (OA) in comparison with other conventional feature-level fusion techniques. © 2019 S. Daneshtalab et al.","Aerial Imagery; Convolutional Neural Network (CNN); Deep Learning; Feature Extraction; Feature Fusion; LiDAR","Aerial photography; Antennas; Convolutional neural networks; Optical radar; Remote sensing; Classification process; Feature level fusion; Land cover classification; Light detection and ranging; Low level descriptors; Multilayer perceptron network (MLP); Object classification; Very high resolution; Classification (of information)"
"Identification and classification of desert grassland species based on A-Clenet5 [基于A-Clenet5的荒漠化草原草种识别与分类]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076534203&doi=10.16136%2fj.joel.2019.10.0075&partnerID=40&md5=bc4dc2fa6b40a6a676f3f2d4c74b1eb8","Grassland degradation has become a major ecological problem facing human beings. One of signs is the change of grassland vegetation community structure. The hyperspectral recognition and classification of grassland grasses is the basis and premise of remote sensing for large-area and high-precision grassland degradation monitoring and treatment. Due to the randomness of the distribution of natural grassland species and the complexity of the remote sensing image cloud system, the problem of low recognition accuracy of grass species has not been satisfactorily resolved. This study proposes the A-Clent5 method for the first time based on deep learning, collects typical pasture hyperspectral data under natural light in the natural grassland. Then the data is preprocessed, and the feature mining and data classification are performed by the A-Cletet5 method. The research results show that the recognition accuracy of this method is 92. 18%, which realizes the identification of grassland typical grass species based on hyperspectral, which provides the possibility of high-precision remote sensing monitoring of grassland degradation. © 2019, Science Press in China. All right reserved.","A-Clenet5; Deeplearning; Feature mining; Grass species classification; Hyperspectral",
"Method for Solving Echo Time of Pulse Laser Ranging Based on Deep Learning [基于深度学习的脉冲激光测距回波时刻解算方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075166372&doi=10.3788%2fCJL201946.1010001&partnerID=40&md5=ce301cb20327385f5a9af0e5726e61cf","To improve the application adaptability of the method used for solving the echo time of pulse laser ranging, this study transforms the echo time solving problem into a waveform classification problem and uses a novel deep learning method to solve the echo time. Further, a one-dimensional convolutional neural network model is trained by simulating the sample echo data containing different distances, signal amplitudes, waveform shapes, and noises with a time resolution of 0.1 ns, and a classification accuracy of 99.85% is obtained using the sample test set. Using the deep learning method and the Gaussian fitting method to process the airborne lidar echo data, the correlation coefficient of the wall surface sweep measurement results is 0.99981. Further, the plane fitting residuals of the field flight test data are approximately 20 mm; the effects of the two methods are observed to be equivalent. The results denote that the proposed method can satisfy the requirements for solving the echo time of airborne pulse laser ranging and can improve the solution accuracy and adapt to several application scenarios. © 2019, Chinese Lasers Press. All right reserved.","Convolutional neural network; Deep learning; Echo time solving; Lidar; Pulsed laser ranging; Remote sensing","Classification (of information); Convolution; Deep learning; Deep neural networks; Neural networks; Optical radar; Remote sensing; Application scenario; Classification accuracy; Convolutional neural network; Correlation coefficient; Echo time; Pulsed laser ranging; Solution accuracy; Sweep measurements; Pulsed lasers"
"A multiscale object detection approach for remote sensing images based on MSE-DenseNet and the dynamic anchor assignment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075732587&doi=10.1080%2f2150704X.2019.1633486&partnerID=40&md5=a66d29eb853ccaf0ab28b0d90e939f26","Object detection has played an important role in the remote sensing (RS) image understanding domain, but it still suffers from challenges, including the complexity of the image background, the variety of objects and differences of target scales. Although many studies applying deep learning methods to object detection have been presented, effective methods of multiscale object detection in RS images are still lacking. In this paper, we propose a novel object detection approach based on the Multiscale-SELU-DenseNet (MSE-DenseNet) and the dynamic anchor assignment (DAA) strategy. We fuse a highly modified DenseNet and a feature pyramid network into the MSE-DenseNet to extract multi-level feature descriptions of RS images, and then we use them to detect objects on various scales. To further improve the detection performance, we propose a DAA strategy for assigning prior anchor boxes. The experimental results show that our proposed method outperforms state-of-the-art detection methods on the VHR-10 dataset and maintains a real-time detection speed. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",,"Deep learning; Object recognition; Remote sensing; Detection approach; Detection methods; Detection performance; Feature description; Learning methods; Real-time detection; Remote sensing images; State of the art; Object detection; anchor; complexity; data set; detection method; remote sensing; satellite imagery"
"County-level soybean yield prediction using deep CNN-LSTM model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073105002&doi=10.3390%2fs19204363&partnerID=40&md5=84114e71df5c8420644b5ed606c37723","Yield prediction is of great significance for yield mapping, crop market planning, crop insurance, and harvest management. Remote sensing is becoming increasingly important in crop yield prediction. Based on remote sensing data, great progress has been made in this field by using machine learning, especially the Deep Learning (DL) method, including Convolutional Neural Network (CNN) or Long Short-Term Memory (LSTM). Recent experiments in this area suggested that CNN can explore more spatial features and LSTM has the ability to reveal phenological characteristics, which both play an important role in crop yield prediction. However, very few experiments combining these two models for crop yield prediction have been reported. In this paper, we propose a deep CNN-LSTM model for both end-of-season and in-season soybean yield prediction in CONUS at the county-level. The model was trained by crop growth variables and environment variables, which include weather data, MODIS Land Surface Temperature (LST) data, and MODIS Surface Reflectance (SR) data; historical soybean yield data were employed as labels. Based on the Google Earth Engine (GEE), all these training data were combined and transformed into histogram-based tensors for deep learning. The results of the experiment indicate that the prediction performance of the proposed CNN-LSTM model can outperform the pure CNN or LSTM model in both end-of-season and in-season. The proposed method shows great potential in improving the accuracy of yield prediction for other crops like corn, wheat, and potatoes at fine scales in the future. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","CNN-LSTM; County-level; Google Earth Engine; Soybean; Yield prediction","Crops; Deep learning; Engines; Forecasting; Land surface temperature; Radiometers; Remote sensing; CNN-LSTM; County level; Google earths; Soybean; Yield prediction; Long short-term memory; article; convolutional neural network; deep learning; harvest; histogram; long short term memory network; nonhuman; plant growth; potato; prediction; season; soybean; weather; wheat"
"A Combined Deep Learning Model for the Scene Classification of High-Resolution Remote Sensing Image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077744985&doi=10.1109%2fLGRS.2019.2902675&partnerID=40&md5=5f50dc4b57bae2eb74b58b0b9137bc90","Deep learning now plays an important role in solving complex problems in computer vision fields. The highly challenging high-resolution remote sensing image scene classification problem can also be solved using deep learning methods. The most commonly used method of deep learning is the convolutional neural network model. In this letter, based on deep learning, a combined model named Inception-long short-term memory (LSTM) is proposed. First, we combine the deep learning feature extracted from the pretrained Inception-V3 model with a hand-crafted feature: the GIST feature. The different features are then combined and input into the batch normalization (BN) layer. Second, the BN layer plays the role of the bridge to combine the InceptionV3 model with the LSTM model, which features a softmax classifier. The LSTM model is used to analyze the features and classify the different high-resolution remote sensing scene images. The proposed model, as a whole, can be uniformly trained. Three different datasets - the NWPU-RESISC45 dataset, the UC Merced dataset, and the SIRI-WHU dataset - were used to verify the effectiveness of the proposed model. The results show that the proposed Inception-LSTM model shows an outstanding performance in the scene classification task. © 2019 IEEE.","GIST; Inception-V3; long short-term memory (LSTM); pretraining; scene classification","Brain; Classification (of information); Deep learning; Image classification; Remote sensing; Complex problems; Convolutional neural network; GIST; High resolution remote sensing; High resolution remote sensing images; Inception-V3; Pre-training; Scene classification; Long short-term memory; artificial neural network; computer vision; data set; image classification; machine learning; numerical method; numerical model; remote sensing; satellite imagery; spectral resolution"
"Multi-resolution Object Detection and Data Fusion for Large-scale Remote Sensing Images based on Deep Learning Method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081159000&doi=10.1109%2fIMCEC46724.2019.8983902&partnerID=40&md5=1d91ea6c6c0f937e423c7ce75309db2b","With the development of aircraft and telemetry satellites, the acquisition of high-resolution remote sensing images becomes easier. Object detection in large-scale remote sensing images has become a valuable problem. At present, the object detector trained in the natural scene has achieved quite good performance, but in the remote sensing scene, the detection result is not unacceptable because the ground object size is too small, dense, and the background is complicated. In order to make the object detector based on deep learning method operating more accurately in large-scale remote sensing images, this paper proposes an algorithm for multi-resolution detection and fusion of data on remote sensing images. The algorithm divides the original large-scale remote sensing image into a plurality of sub-images according to certain parameter settings, and fuses the results after detecting the objects on each sub-image. The process will be adaptively performed multiple times and adopt different resolutions. Finally, all the obtained objects bounding boxes are subjected to non-maximum suppression processing to obtain the final detection result. © 2019 IEEE.","data fusion; deep learning; multi-resolution; object detection; remote sensing image","Data fusion; Deep learning; Image fusion; Information management; Learning systems; Object detection; Object recognition; Different resolutions; High resolution remote sensing images; Learning methods; Non-maximum suppression; Object detectors; Parameter setting; Remote sensing images; Resolution detection; Remote sensing"
"Remote sensing image aircraft detection based on feature fusion across deep learning framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082863723&doi=10.1109%2fICSESS47205.2019.9040808&partnerID=40&md5=ae9bb8963045cd4e6a875e1ab392b792","The detection of remote sensing image aircraft targets based on deep learning has practical and important significance in the fields of military reconnaissance and disaster rescue. As a typical representative of the two mainstream detection algorithms, YOLOv3 and Faster-R-CNN have good detection effects on remote sensing image aircraft targets. However, for low quality remote sensing images, the two detection algorithms also have the phenomenons of omission and false detection. To deal with this, this paper proposes a target detection algorithm (YF-R-CNN) for ""Separate training, joint detection, which realizes the cross-platform detection feature fusion of YOLOv3 based on darknet framework and Faster-R-CNN based on tensorflow framework, effectively alleviating the problems of existing algorithms. The experimental results show that the detection accuracy of YF-R-CNN algorithm reaches 94.8%, which is 3.7% and 3.1% higher than YOLOv3 and Faster-R-CNN respectively. The detection accuracy is obviously improved, and the algorithm has better flexibility and robustness. © 2019 IEEE.","Aircraft detection; Component; Feature fusion; Remote sensing image","Aircraft accidents; Deep learning; Feature extraction; Image fusion; Military photography; Remote sensing; Signal detection; Software engineering; Training aircraft; Component; Detection accuracy; Detection algorithm; Feature fusion; Learning frameworks; Military reconnaissance; Remote sensing images; Target detection algorithm; Aircraft detection"
"Deep Learning for Remote Sensing Image Super-Resolution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080145619&doi=10.1109%2fUEMCON47517.2019.8993047&partnerID=40&md5=736354c0ac5bc10838c078f6109ef3a9","The aim of image super-Resolution (SR) is to enhance image resolution while still retain the integrity of the original image. There are many ongoing types of research on image super-resolution for natural images, but any a few on remote sensing images. In this paper, we proposed deep learning-based image super-resolution techniques, including convolutional neural network (CNN) and generative adversarial network (GAN) to enhance the resolution of remote sensing images by a factor 4. In CNN, it learns an end to end mapping from low-resolution image to high-resolution image whereas, in GAN, the model learns the mapping guided by the GAN loss and gives the sharper appearance in high-resolution images. Our experimental results show that visually GAN models perform well but are inferior to other models in terms of image quality metrics, whereas quantitatively CNN models outperform other super-resolution models. © 2019 IEEE.","CNN; Deep Learning; GAN; Machine Learning; Remote sensing image; Super resolution","Convolutional neural networks; Image enhancement; Image resolution; Learning systems; Mapping; Mobile telecommunication systems; Optical resolving power; Remote sensing; Ubiquitous computing; Adversarial networks; High resolution image; Image quality metrics; Image super resolutions; Low resolution images; Remote sensing images; Super resolution; Super-resolution models; Deep learning"
"High-resolution optical remote sensing imagery change detection through deep transfer learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077768479&doi=10.1117%2f1.JRS.13.046512&partnerID=40&md5=dbc84f4874631b134ede2edfaa300ff7","Change detection is a challenging task that has received much attention in the remote sensing field. Whereas numerous remote sensing change detection methods have been developed, the efficiency of these approaches is insufficient to meet the real-world applications' requirements. Recently, deep learning methods have been largely used for remote sensing imagery change detection, most of these approaches are limited by their training dataset. However, adapting a pretrained convolutional neural network (CNN) on an image classification task to change detection is extremely challenging. An automatic land cover/use change detection approach based on fast and accurate frameworks for optical high-resolution remote sensing imagery is proposed. The fast framework is designed for applications that require immediate results with less complexity. The accurate framework is designed for applications that require high levels of precision, it decomposes large images into small processing blocks and forwards them into CNN. The proposed frameworks can learn transferable features from one task to another and escape the use of the expensive and inaccurate handcrafted features and the requirements of the big training dataset. A number of experiments were carried out to validate the proposed approach on three real bitemporal images. The experimental results illustrate the superiority of the proposed approach over other state-of-the-art methods. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","change detection; convolutional neural network; deep learning; hypervectors; remote sensing, hyperfeatures","Convolution; Deep learning; Deep neural networks; Neural networks; Change detection; Convolutional neural network; High resolution remote sensing imagery; hypervectors; Optical remote-sensing imagery; Remote sensing imagery; State-of-the-art methods; Transfer learning; Remote sensing"
"A hierarchical airport detection method using spatial analysis and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073437231&doi=10.3390%2frs11192204&partnerID=40&md5=319bd490e1e6b1a24055bfa079e43347","Airports have a profound impact on our lives, and uncovering their distribution around the world has great significance for research and development. However, existing airport databases are incomplete and have a high cost of updating. Thus, a fast and automatic worldwide airport detection method can be of significance for global airport detection at regular intervals. However, previous airport detection studies are usually based on single remote sensing (RS) imagery, which seems an overwhelming burden for worldwide airport detection with traversal searching. Thus, we propose a hierarchical airport detection method consisting of broad-scale extraction of worldwide candidate airport regions based on spatial analysis of released RS products, including impervious surfaces from FROM-GLC10 (fine resolution observation and monitoring of global land cover 10) product, building distribution from OSMs (open street maps) and digital surface model from AW3D30 (ALOS World 3D-30 m). Moreover, narrow-scale aircraft detection was initially conducted by the Faster R-CNN (regional-convolutional neural networks) deep learning method. To avoid overestimation of background regions by Faster R-CNN, a second CNN classifier is used to refine the class labeling with negative samples. Specifically, our research focuses on target airports with at least 2 km length in three experimental regions. Results show that spatial analysis reduced the possible regions to 0.56% of the total area of 75,691 km2. The initial aircraft detection by Faster R-CNN had a mean user's accuracy of 88.90% and ensured that all the aircrafts could be detected. Then, by introducing the CNN reclassifier, the user's accuracy of aircraft detection was significantly increased to 94.21%. Finally, through an experienced threshold of aircraft number, 19 of the total 20 airports were detected correctly. Our results reveal the overall workflow is reliable for automatic and rapid airport detection around the world with the help of released RS products. This research promotes the application and progression of deep learning. © 2019 by the authors.","Airports; Deep learning (DL); Object-based target detection;; Remote sensing (RS); Spatial analysis","Aircraft; Aircraft detection; Airports; Neural networks; Remote sensing; Spatial variables measurement; Airport detections; Building distribution; Convolutional neural network; Digital surface models; Impervious surface; Object based; Research and development; Spatial analysis; Deep learning"
"THE POTENTIAL of BUILDING DETECTION from SAR and LIDAR USING DEEP LEARNING","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083270202&doi=10.5194%2fisprs-archives-XLII-4-W16-489-2019&partnerID=40&md5=cd70066c41ea71fd4f9d0f32989b84fc","The introduction of airborne Synthetic Aperture Radar (SAR) approach has successfully addressed several challenges for mapping and surveying applications Unlike other conventional sensors, airborne SAR mapping approach offers practicality and significant cost savings for the nation minimizing the need for ground control points on the ground in addition to providing high-resolution, day-and-night, cloud coverage and weather independent images, which in turn provides faster turnaround times for creation of large area geospatial data. Up-to-date building map is necessary to guide the decision making in many fields to understand the urban dynamics such as in disaster management, population estimation, planning and many other applications. Whilst mapping and surveying work using airborne SAR have started to capture many interest among surveyors, professionals and practitioners abroad, Malaysia however is still lacking behind in term of the knowledge and the usage of this technology together with Deep Learning, Machine Learning approach especially in building extraction for topographic mapping and urban planning and development. Deep learning is a subset of the machine learning algorithm. Recently, Deep Learning has been proposed to solve traditional artificial intelligent problems. In order to develop a sustainable national geospatial infrastructure for years to come, the integration between airborne SAR and other sensors as such LIDAR is therefore essential in Malaysia and in high demand for urban planning and management. Thus, this paper reviews current techniques and future trends of multi-sources Remote Sensing for building extraction. © 2019 Z. Nordin et al.","building extraction; deep learning; feature extraction; lidar; orthophoto; synthetic aperture radar (SAR)","Decision making; Disaster prevention; Disasters; Extraction; Learning algorithms; Learning systems; Mapping; Optical radar; Remote sensing; Rock mechanics; Surveys; Synthetic aperture radar; Turnaround time; Urban planning; Airborne synthetic aperture radars; Artificial intelligent; Conventional sensors; Geospatial infrastructure; Ground control points; Machine learning approaches; Planning and development; Population estimations; Deep learning"
"Post-disaster building database updating using automated deep learning: An integration of pre-disaster OpenStreetMap and multi-temporal satellite data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074215826&doi=10.3390%2frs11202427&partnerID=40&md5=c31262911bca16e75d7019c9c3bcd91a","First responders and recovery planners need accurate and quickly derived information about the status of buildings as well as newly built ones to both help victims and to make decisions for reconstruction processes after a disaster. Deep learning and, in particular, convolutional neural network (CNN)-based approaches have recently become state-of-the-art methods to extract information from remote sensing images, in particular for image-based structural damage assessment. However, they are predominantly based on manually extracted training samples. In the present study, we use pre-disaster OpenStreetMap building data to automatically generate training samples to train the proposed deep learning approach after the co-registration of the map and the satellite images. The proposed deep learning framework is based on the U-net design with residual connections, which has been shown to be an effective method to increase the efficiency of CNN-based models. The ResUnet is followed by a Conditional Random Field (CRF) implementation to further refine the results. Experimental analysis was carried out on selected very high resolution (VHR) satellite images representing various scenarios after the 2013 Super Typhoon Haiyan in both the damage and the recovery phases in Tacloban, the Philippines. The results show the robustness of the proposed ResUnet-CRF framework in updating the building map after a disaster for both damage and recovery situations by producing an overall F1-score of 84.2%. © 2019 by the authors.","Building database update; Convolutional neural network; Damage assessment; Deep learning; Multi-temporal satellite imagery; OpenStreetMap; Post-disaster; Recovery assessment; Super Typhoon Haiyan; U-Net","Buildings; Computer system recovery; Convolution; Damage detection; Deep learning; Deep neural networks; Disasters; Hurricanes; Maintenance; Neural networks; Random processes; Recovery; Remote sensing; Sampling; Satellite imagery; Structural analysis; Convolutional neural network; Damage assessments; Multi-temporal; OpenStreetMap; Post disasters; Super Typhoon Haiyan; Image processing"
"Remote Sensing Image Translation Using Spatial-Frequency Consistency GAN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079126856&doi=10.1109%2fCISP-BMEI48845.2019.8965788&partnerID=40&md5=9af2d14fc5e18a46cd691ccc0749f441","Synthesis aperture radar(SAR) is a powerful technique for remote sensing. The major advantage of SAR is all-weather and all-day imaging. However, SAR images are too hard for ordinary people. Motivated by the development of deep learning, it is available for the image translation from SAR images to optical images. Unfortunately, existed image translation methods ignore unique imaging of remote sensing images. Under unique imaging, remote sensing images are noisy and low resolution. Based on the observation, we propose our method, SFGAN, for remote sensing image translation using a generative adversarial network, which is trained on SEN1-2 Dataset. Our experiments compare our method with some state-of-the-art methods. Results show that the proposed method preserves more linear features under various area and has a higher PSNR. © 2019 IEEE.","Generative adversarial network; Image translation; Remote sensing","Biomedical engineering; Deep learning; Geometrical optics; Image processing; Remote sensing; Synthetic aperture radar; Adversarial networks; Image translation; Linear feature; Low resolution; Ordinary people; Remote sensing images; Spatial frequency; State-of-the-art methods; Radar imaging"
"Efficient object detection framework and hardware architecture for remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074203690&doi=10.3390%2frs11202376&partnerID=40&md5=116aa6bc34e9b7597bc537128c9c2e63","Object detection in remote sensing images on a satellite or aircraft has important economic and military significance and is full of challenges. This task requires not only accurate and efficient algorithms, but also high-performance and low power hardware architecture. However, existing deep learning based object detection algorithms require further optimization in small objects detection, reduced computational complexity and parameter size. Meanwhile, the general-purpose processor cannot achieve better power efficiency, and the previous design of deep learning processor has still potential for mining parallelism. To address these issues, we propose an efficient context-based feature fusion single shot multi-box detector (CBFF-SSD) framework, using lightweight MobileNet as the backbone network to reduce parameters and computational complexity, adding feature fusion units and detecting feature maps to enhance the recognition of small objects and improve detection accuracy. Based on the analysis and optimization of the calculation of each layer in the algorithm, we propose efficient hardware architecture of deep learning processor with multiple neural processing units (NPUs) composed of 2-D processing elements (PEs), which can simultaneously calculate multiple output feature maps. The parallel architecture, hierarchical on-chip storage organization, and the local register are used to achieve parallel processing, sharing and reuse of data, and make the calculation of processor more efficient. Extensive experiments and comprehensive evaluations on the public NWPU VHR-10 dataset and comparisons with some state-of-the-art approaches demonstrate the effectiveness and superiority of the proposed framework. Moreover, for evaluating the performance of proposed hardware architecture, we implement it on Xilinx XC7Z100 field programmable gate array (FPGA) and test on the proposed CBFF-SSD and VGG16 models. Experimental results show that our processor are more power efficient than general purpose central processing units (CPUs) and graphics processing units (GPUs), and have better performance density than other state-of-the-art FPGA-based designs. © 2019 by the authors.","Convolutional neural networks (CNNs); Deep learning; Hardware architecture; Object detection; Processor; Remote sensing image","Aircraft detection; Complex networks; Computational complexity; Computer graphics; Deep learning; Deep neural networks; Digital storage; Field programmable gate arrays (FPGA); General purpose computers; Graphics processing unit; Learning algorithms; Military photography; Network architecture; Neural networks; Object recognition; Parallel architectures; Program processors; Remote sensing; Convolutional neural network; Efficient object detections; General purpose processors; Hardware architecture; Object detection algorithms; Processor; Remote sensing images; State-of-the-art approach; Object detection"
"Road extraction from unmanned aerial vehicle remote sensing images based on improved neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072581031&doi=10.3390%2fs19194115&partnerID=40&md5=b14c753f959d01ba1ad47f3263b8a978","Roads are vital components of infrastructure, the extraction of which has become a topic of significant interest in the field of remote sensing. Because deep learning has been a popular method in image processing and information extraction, researchers have paid more attention to extracting road using neural networks. This article proposes the improvement of neural networks to extract roads from Unmanned Aerial Vehicle (UAV) remote sensing images. D-Linknet was first considered for its high performance; however, the huge scale of the net reduced computational efficiency. With a focus on the low computational efficiency problem of the popular D-LinkNet, this article made some improvements: (1) Replace the initial block with a stem block. (2) Rebuild the entire network based on ResNet units with a new structure, allowing for the construction of an improved neural network D-Linknetplus. (3) Add a 1 × 1 convolution layer before DBlock to reduce the input feature maps, reducing parameters and improving computational efficiency. Add another 1 × 1 convolution layer after DBlock to recover the required number of output channels. Accordingly, another improved neural network B-D-LinknetPlus was built. Comparisons were performed between the neural nets, and the verification were made with the Massachusetts Roads Dataset. The results show improved neural networks are helpful in reducing the network size and developing the precision needed for road extraction. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural net; Image processing; Road; UAV sensors","Antennas; Computational efficiency; Convolution; Deep learning; Efficiency; Feature extraction; Image enhancement; Image processing; Remote sensing; Roads and streets; Unmanned aerial vehicles (UAV); Input features; Massachusetts; Network size; Network-based; Output channels; Remote sensing images; Road; Road extraction; Neural networks"
"Bisupervised network with pyramid pooling module for land cover classification of satellite remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077779862&doi=10.1117%2f1.JRS.13.048502&partnerID=40&md5=4baeff95637dfd9cae0c8e78160c3129","Land cover classification is a significant task in remote sensing that aims at land cover monitoring and adjustment. Instance segmentation in deep learning has been widely used in land cover classification. However, this method requires high quality, labor-intensive pixel-level annotations. A bisupervised pipeline is proposed especially for datasets with ambiguous samples, which reduces the requirement of labeling accuracy. In order to supervise learning with ambiguous annotation samples, the pipeline has two losses to feedback, namely main loss and auxiliary loss. The main loss is still responsible for the expected output. According to the difference degree between ambiguous samples, categories are constructed artificially. The auxiliary loss generates feedback through categories, sharing decoder layers of main output during the training process. In the prediction process, auxiliary loss is used to update the main loss results. The neural network adopts the structure of U-Net with the pyramid pooling module, in which the multiscale feature is used in the feature extraction process. We also compare five different backbones and choose Inception-V3 as the backbone to improve the feature extraction capabilities of network encoders. The use of transposed convolution instead of traditional upsampling in decoders can improve the segmentation details. Our bisupervise network obtains Jaccard index of 83.936% and F1-score of 91.17% on Gaofen-2 imagery dataset. The results demonstrate that the proposed method can achieve better classification performance in datasets with ambiguous annotations. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","convolution neural network; deep learning; image segmentation; remote sensing","Classification (of information); Convolution; Decoding; Deep learning; Deep neural networks; Extraction; Feature extraction; Image classification; Image segmentation; Network coding; Pipelines; Satellite imagery; Classification performance; Convolution neural network; Difference degrees; Extraction capability; Labeling accuracies; Land cover classification; Multi-scale features; Satellite remote sensing; Remote sensing"
"Super-resolution reconstruction of remote sensing images based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073737863&doi=10.1117%2f1.JRS.13.4.046502&partnerID=40&md5=ab2b448ee16d64400b129dcb02dac574","A method of super-resolution reconstruction of remote sensing images based on convolutional neural network is proposed to address the problems of low-resolution and poor visual quality of remote sensing images. In this method, a sample database with high-resolution (HR) and low-resolution (LR) remote sensing images was constructed. A convolutional neural network was then obtained by determining the intrinsic relationship between HR and LR remote sensing images in the sample database. Multiple pairs of HR and LR images were selected from the sample database and sent into a six-layer convolutional neural network. The experimental results showed that compared with other learning-based methods, such as the fast super-resolution convolutional neural network (FSRCNN), the image quality obtained by our method is improved both subjectively and objectively. Moreover, the training time was ∼17 % less than in the FSRCNN method. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","convolutional neural network; deep learning; remote sensing image; super-resolution reconstruction","Convolution; Database systems; Deep learning; Deep neural networks; Image enhancement; Image reconstruction; Multilayer neural networks; Network layers; Optical resolving power; Convolutional neural network; High resolution; Learning-based methods; Remote sensing images; Sample Database; Super resolution; Super resolution reconstruction; Visual qualities; Remote sensing"
"Semantic segmentation of static and dynamic structures of marina satellite images using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077041124&doi=10.1109%2fBRACIS.2019.00129&partnerID=40&md5=7cf39aeb5b043899e10bcbb5b9a227c5","Advances in remote sensing and machine learning technologies allow a better understanding of satellite images and therefore, our planet. Many applications of semantic segmentation were designed to operate in urban areas. However, fewer applications have been proposed for sea-land regions. In this paper, we propose a semantic segmentation approach for sea-land regions using visible satellite images. Static and dynamic structures, as well as water, are segmented by a Convolutional Neural Network based on the U-Net architecture. The model is evaluated with real satellite images, and qualitative and quantitative results are presented. The model achieves attractive results that support a possible application in autonomous marine navigation. © 2019 IEEE.","Convolutional Neural Network; Remote Sensor Imagery; Sea-land Semantic Segmentation","Convolution; Image segmentation; Intelligent systems; Marine navigation; Neural networks; Remote sensing; Satellites; Semantics; Convolutional neural network; Dynamic structure; Machine learning technology; NET architecture; Quantitative result; Remote sensors; Satellite images; Semantic segmentation; Deep learning"
"Target detection method for optical remote sensing imagery [光学遥感图像目标检测方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076678407&doi=10.3969%2fj.issn.1001-506X.2019.10.02&partnerID=40&md5=732a450cbd5374f6479033191565be3b","Faster region-based convolutional neural network (Faster RCNN), a two-stage object detection model, proposes candidate regions through a region proposal network and coalesces the two stages of region proposal and classification to a single network, which makes most computation be conducted in the graphic processing unit. Therefore, it has both high detection speed and accuracy on many public datasets. However, when we directly use Faster RCNN to conduct target detection on the remote sensing imageries, its performance is not ideal enough. In this work, the influences of pooling and target scale on region proposal are analyzed, and an optimized region proposal network is proposed to address the problems in the region proposal process. In addition, an individualized generation strategy for training samples which can avoid the generation of invalid foreground samples is introduced to speed up the training process. We evaluate the proposed detection method on the remote sensing image dataset. The result shows that the proposed model improves the recall rate and detection accuracy of multi-scale targets. Moreover, the experiments also demonstrate that the training of the model is rapid and high-efficient. © 2019, Editorial Office of Systems Engineering and Electronics. All right reserved.","Convolutional neural network (CNN); Deep learning; Remote sensing imagery; Target detection","Convolution; Deep learning; Deep neural networks; Graphics processing unit; Neural networks; Object detection; Object recognition; Target tracking; Convolutional neural network; Detection accuracy; Detection methods; Graphic processing units; Optical remote-sensing imagery; Remote sensing imagery; Remote sensing images; Training process; Remote sensing"
"Dynamic multicontext segmentation of remote sensing images based on convolutional networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075747890&doi=10.1109%2fTGRS.2019.2913861&partnerID=40&md5=047fc661fb6f028a20e51999a4970fea","Semantic segmentation requires methods capable of learning high-level features while dealing with large volume of data. Toward such goal, convolutional networks can learn specific and adaptable features based on the data. However, these networks are not capable of processing a whole remote sensing image, given its huge size. To overcome such limitation, the image is processed using fixed size patches. The definition of the input patch size is usually performed empirically (evaluating several sizes) or imposed (by network constraint). Both strategies suffer from drawbacks and could not lead to the best patch size. To alleviate this problem, several works exploited multicontext information by combining networks or layers. This process increases the number of parameters, resulting in a more difficult model to train. In this paper, we propose a novel technique to perform semantic segmentation of remote sensing images that exploits a multicontext paradigm without increasing the number of parameters while defining, in training time, the best patch size. The main idea is to train a dilated network with distinct patch sizes, allowing it to capture multicontext characteristics from heterogeneous contexts. While processing these varying patches, the network provides a score for each patch size, helping in the definition of the best size for the current scenario. A systematic evaluation of the proposed algorithm is conducted using four high-resolution remote sensing data sets with very distinct properties. Our results show that the proposed algorithm provides improvements in pixelwise classification accuracy when compared to the state-of-the-art methods. © 1980-2012 IEEE.","Convolutional networks (ConvNets); deep learning; multicontext; multiscale; remote sensing; semantic segmentation","Convolution; Deep learning; Image segmentation; Network layers; Semantics; Convolutional networks; High resolution remote sensing; multicontext; multiscale; Pixelwise classification; Semantic segmentation; State-of-the-art methods; Systematic evaluation; Remote sensing; accuracy assessment; algorithm; artificial neural network; genetic algorithm; image analysis; image classification; image processing; remote sensing; satellite data; satellite imagery; segmentation"
"Object detection in remote sensing imagery based on convolutional neural networks with suitable scale features [遥感影像目标的尺度特征卷积神经网络识别法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075073191&doi=10.11947%2fj.AGCS.2019.20180393&partnerID=40&md5=97f5032df9931f04cbc00de6afeb6f83","Object detection and recognition in high spatial resolution remote sensing images (HSRI) is an important part of image information automatic extraction, analysis and understanding in high resolution earth observation system. The robustness and universality of traditional object detection and recognition algorithms using artificial design object feature are poor. To solve these problems, object detection and recognition in HSRI based on convolutional neural networks (CNN) with suitable scale features is proposed. Firstly, the suitable scale of the region of interest (ROI) of object is obtained by statistic the scale range of object in HSRI in the process of training and testing of CNN. Then, a CNN framework for object detection and recognition in HSRI is designed according to the suitable object ROI scale. The mean average precision (mAP) of the proposed CNN framework and Faster-RCNN is tested using the WHU-RSone data set. The experimental results show that the mAP of ZF model and VGG-16 model of the proposed CNN framework are 8.17% and 8.31% higher than that of Faster R-CNN ZF model and Faster R-CNN VGG-16 model, respectively. The proposed CNN framework can obtain good object detection and recognition results. © 2019, Surveying and Mapping Press. All right reserved.","Convolutional neural network; Deep learning; High resolution remote sensing image; Object detection and recognition; Object scale","Convolution; Deep learning; Deep neural networks; Feature extraction; Image segmentation; Neural networks; Object recognition; Remote sensing; Convolutional neural network; Earth observation systems; High resolution remote sensing images; High spatial resolution; Object detection and recognition; Object scale; Remote sensing imagery; The region of interest (ROI); Object detection; algorithm; artificial neural network; detection method; machine learning; numerical model; precision; remote sensing; satellite imagery; scale effect; spectral resolution"
"DE-Net: Deep encoding network for building extraction from high-resolution remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074187425&doi=10.3390%2frs11202380&partnerID=40&md5=07d36117980b4c91067020f1ca2510cb","Deep convolutional neural networks have promoted significant progress in building extraction from high-resolution remote sensing imagery. Although most of such work focuses on modifying existing image segmentation networks in computer vision, we propose a new network in this paper, Deep Encoding Network (DE-Net), that is designed for the very problem based on many lately introduced techniques in image segmentation. Four modules are used to construct DE-Net: the inception-style downsampling modules combining a striding convolution layer and a max-pooling layer, the encoding modules comprising six linear residual blocks with a scaled exponential linear unit (SELU) activation function, the compressing modules reducing the feature channels, and a densely upsampling module that enables the network to encode spatial information inside feature maps. Thus, DE-Net achieves state-of-the-art performance on the WHU Building Dataset in recall, F1-Score, and intersection over union (IoU) metrics without pre-training. It also outperformed several segmentation networks in our self-built Suzhou Satellite Building Dataset. The experimental results validate the effectiveness of DE-Net on building extraction from aerial imagery and satellite imagery. It also suggests that given enough training data, designing and training a network from scratch may excel fine-tuning models pre-trained on datasets unrelated to building extraction. © 2019 by the authors.","Building extraction; Deep learning; Fully convolutional network; High-resolution remote sensing imagery","Aerial photography; Antennas; Buildings; Convolution; Deep learning; Deep neural networks; Encoding (symbols); Extraction; Image segmentation; Neural networks; Remote sensing; Satellite imagery; Signal sampling; Activation functions; Building extraction; Convolutional networks; Convolutional neural network; High resolution remote sensing imagery; Satellite buildings; Spatial informations; State-of-the-art performance; Network coding"
"High resolution image classification of urban areas based on convolution neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079280207&doi=10.1109%2fICMCCE48743.2019.00100&partnerID=40&md5=801c895cd39d14e5eb5a2c237164dbbe","In the process of urbanization, land is gradually reduced, and artificial targets(concrete, asphalt, stone paving, et al.) are increased. Remote sensing technology is one of the main means to analyze and obtain data of urban spatial pattern. The classification method and accuracy of remote sensing image directly affect the accuracy of information extraction. In this paper, a method of deep learning - convolutional neural network is used to classify in urban areas high-resolution remote sensing images. Firstly, the basic principle of CNN method are introduced, and parameters suitable for this image classification are selected. In this study, the model structure of CNN is set to four layers, and more abstract expression features are obtained from bottom layer to top layer, and compared with traditional classification methods. Classification accuracy of CNNs is 85.3%, Indistinguishable the artificial surface features in urban areas have very good classification results using CNNs. © 2019 IEEE.","Classification; Convolutional neural networks(CNNs); High spatial resolution remote sensing; Urban areas","Classification (of information); Convolution; Convolutional neural networks; Deep learning; Remote sensing; Accuracy of information; Classification accuracy; Classification results; Convolution neural network; High resolution remote sensing images; High spatial resolution; Remote sensing technology; Urban areas; Image classification"
"Semantic segmentation and segmentation refinement using machine learning case study: Water turbidity segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077499784&doi=10.1109%2fICARES.2019.8914551&partnerID=40&md5=66e7e807d22f7d68063a33afd1bff417","Classical methods for image segmentation such as pixel thresholding, clustering, region growing, maximum likelihood have been used regularly and relied on for a long time. However, these classical methods have limitations, particularly on images where there are many overlapping pixel values between features, which is common in remote sensing images. The advent of machine learning, in particular, deep learning in computer vision and image analysis, has gained interest in the remote sensing field. Current deep learning architecture has been able to achieve high accuracy for image recognition, object detection, and segmentation. This study performed image segmentation on the coastal area with high water turbidity using Landsat-8 images. Currently, the standard tool to derive water turbidity data from Landsat-8 images is the level-2 plugin of SEADAS software. However, due to its rigorous processing method, the processing time using SEADAS Level-2 Plugin is quite long; for example, processing one Landsat-8 image took around 8 hours. As a consequence, the amount of time needed to process multiple images is increasing. Deep learning has advantages once the model trained, the inference or prediction process is quite fast. Therefore it has the potential to be used as a complementary tool to predict and segment high turbidity areas, because in deep learning. In this study, we implemented U-Net architecture with ResNet connection and used Generative-Adversarial Network (GAN) to refined segmentation results. © 2019 IEEE.","CNN; Deep learning; GAN; Image segmentation; Machine learning; Remote sensing","Deep learning; Image recognition; Learning systems; Machine learning; Maximum likelihood; Network architecture; Object detection; Pixels; Processing; Remote sensing; Semantics; Turbidity; Adversarial networks; Complementary tools; Learning architectures; Overlapping pixels; Prediction process; Remote sensing images; Segmentation results; Semantic segmentation; Image segmentation"
"Real-time object detection in remote sensing images based on visual perception and memory reasoning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073718319&doi=10.3390%2felectronics8101151&partnerID=40&md5=40dee009b1f2f12b82c9852d6c10f3b1","Aiming at the real-time detection of multiple objects and micro-objects in large-scene remote sensing images, a cascaded convolutional neural network real-time object-detection framework for remote sensing images is proposed, which integrates visual perception and convolutional memory network reasoning. The detection framework is composed of two fully convolutional networks, namely, the strengthened object self-attention pre-screening fully convolutional network (SOSA-FCN) and the object accurate detection fully convolutional network (AD-FCN). SOSA-FCN introduces a self-attention module to extract attention feature maps and constructs a depth feature pyramid to optimize the attention feature maps by combining convolutional long-term and short-term memory networks. It guides the acquisition of potential sub-regions of the object in the scene, reduces the computational complexity, and enhances the network’s ability to extract multi-scale object features. It adapts to the complex background and small object characteristics of a large-scene remote sensing image. In AD-FCN, the object mask and object orientation estimation layer are designed to achieve fine positioning of candidate frames. The performance of the proposed algorithm is compared with that of other advanced methods on NWPU_VHR-10, DOTA, UCAS-AOD, and other open datasets. The experimental results show that the proposed algorithm significantly improves the efficiency of object detection while ensuring detection accuracy and has high adaptability. It has extensive engineering application prospects. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Neural network; Object detection; Remote sensing images; Visual perception",
"Recovery of urban 3D road boundary via multi-source data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070902701&doi=10.1016%2fj.isprsjprs.2019.08.010&partnerID=40&md5=dd55bba2e664b6502b5c93e38b5a9917","The mapping of road boundaries provides critical information about roads for urban road traffic safety. This paper presents a deep learning-based framework for recovering 3D road boundary using multi-source data, which include mobile laser scanning (MLS) point clouds, spatial trajectory data, and remote sensing images. The proposed road recovery method uses extracted 3D road boundaries from MLS point clouds as inputs. First, after automatic erroneous boundary removal, a CNN-based boundary completion model completes road boundaries. Then, to refine the imperfect road boundaries, road centerlines generated from dynamic taxi GPS trajectory data and remote sensing images are used as completion guidance for a generative adversarial nets model to obtain more accurate and complete road boundaries. Finally, after associating a sequence of taxi GPS recorded trajectory points with the correct 3D road boundaries, inherent geometric road characteristics and road dynamic information are extracted from the complete boundaries and taxi GPS trajectory data, respectively. The testing dataset contains two urban road MLS datasets, and the KITTI dataset. The experimental results on point clouds from different sensors demonstrate our proposed method is effective and promising for recovering 3D road boundary and extracting road characteristics. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","3D road boundary; GPS trajectory data; Mobile laser scanning; Multi-source data; Point clouds; Remote sensing image","Deep learning; Global positioning system; Laser applications; Recovery; Remote sensing; Statistical tests; Taxicabs; Traffic control; Trajectories; 3D road boundary; Gps trajectories; Laser scanning; Multisource data; Point cloud; Remote sensing images; Roads and streets; accuracy assessment; artificial neural network; GPS; laser method; learning; recovery method; remote sensing; road traffic; satellite imagery; transportation safety; urban area"
"Toward Generalized Change Detection on Planetary Surfaces with Convolutional Autoencoders and Transfer Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076096857&doi=10.1109%2fJSTARS.2019.2936771&partnerID=40&md5=c4d75875924b14ee6bf96600b8399d1a","Ongoing planetary exploration missions are returning large volumes of image data. Identifying surface changes in these images, e.g., new impact craters, is critical for investigating many scientific hypotheses. Traditional approaches to change detection rely on image differencing and manual feature engineering. These methods can be sensitive to irrelevant variations in illumination or image quality and typically require before and after images to be coregistered, which itself is a major challenge. Additionally, most prior change detection studies have been limited to remote sensing images of earth. We propose a new deep learning approach for binary patch-level change detection involving transfer learning and nonlinear dimensionality reduction using convolutional autoencoders. Our experiments on diverse remote sensing datasets of Mars, the moon, and earth show that our methods can detect meaningful changes with high accuracy using a relatively small training dataset despite significant differences in illumination, image quality, imaging sensors, coregistration, and surface properties. We show that the latent representations learned by a convolutional autoencoder yield the most general representations for detecting change across surface feature types, scales, sensors, and planetary bodies. © 2008-2012 IEEE.","Change detection algorithms; earth; machine learning; mars; moon; neural networks; remote sensing; supervised learning; unsupervised learning","Convolution; Deep learning; Earth (planet); Feature extraction; Image quality; Interplanetary spacecraft; Learning systems; Moon; Neural networks; Remote sensing; Supervised learning; Unsupervised learning; Change detection algorithms; Change detection studies; mars; Nonlinear dimensionality reduction; Planetary-exploration missions; Remote sensing images; Scientific Hypothesis; Traditional approaches; Machine learning; algorithm; detection method; Mars; Moon; planetary atmosphere; remote sensing; satellite data; satellite imagery"
"Underwater and airborne monitoring of marine ecosystems and debris","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077780931&doi=10.1117%2f1.JRS.13.044509&partnerID=40&md5=e2115e4477e6d13a115a68646f2b9fb4","Advancing the sustainable use and conservation of marine environments is urgent. Tons of debris including macro- and microplastics generated on land are entering the oceans, marine resources are decreasing, and many species are facing extinction. Though satellite remote sensing techniques are commonly used for global environmental monitoring, it is still difficult to detect small objects such as floating debris on the vast ocean surface, and the ecosystems deep in the oceans where light does not reach are unobservable. An autonomous monitoring system consisting of optimally controlled robots is required for acquiring spatiotemporally rich marine data. However, object detection in marine environments, which is a necessary function the robots should have for underwater and aerial monitoring, has not been extensively studied. Here, we argue that state-of-the-art deep-learning-based object detection works well for monitoring underwater ecosystems and marine debris. We found that by using the deep-learning object-detection algorithm YOLO v3, underwater sea life and debris floating on the ocean surface can be detected with mean average precision of 69.6% and 77.2%, respectively. We anticipate our results to be a starting point for developing tools for enabling safe and precise acquisition of marine data to elucidate and utilize this last frontier. © The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.","autonomous systems; deep network; environmental monitoring; marine debris; marine ecosystems","Antennas; Aquatic ecosystems; Debris; Deep learning; Marine biology; Monitoring; Object detection; Object recognition; Oceanography; Remote sensing; Small satellites; Airborne monitoring; Autonomous monitoring; Autonomous systems; Environmental Monitoring; Global environmental monitoring; Marine debris; Marine environment; Satellite remote sensing; Ecosystems"
"Scene Context-Driven Vehicle Detection in High-Resolution Aerial Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074131030&doi=10.1109%2fTGRS.2019.2912985&partnerID=40&md5=ad943625f921726b4325a1bd7dcfe35f","As the spatial resolution of remote sensing images is improving gradually, it is feasible to realize 'scene-object' collaborative image interpretation. Unfortunately, this idea is not fully utilized in vehicle detection from high-resolution aerial images, and most of the existing methods may be promoted by considering the variability of vehicle spatial distribution in different image scenes and treating vehicle detection tasks scene-specific. With this motivation, a scene context-driven vehicle detection method is proposed in this paper. At first, we perform scene classification using the deep learning method and, then, detect vehicles in roads and parking lots separately through different vehicle detectors. Afterward, we further optimize the detection results using different postprocessing rules according to different scene types. Experimental results show that the proposed approach outperforms the state-of-the-art algorithms in terms of higher detection accuracy rate and lower false alarm rate. © 1980-2012 IEEE.","Aerial image; deep learning; scene context-driven; vehicle detection","Deep learning; Image resolution; Mobile antennas; Remote sensing; Vehicles; Aerial images; High-resolution aerial images; Image interpretation; Remote sensing images; Scene classification; scene context-driven; State-of-the-art algorithms; Vehicle detection; Image enhancement; algorithm; detection method; image analysis; image classification; image resolution; remote sensing; spatial distribution; spatial resolution"
"Low-power neural networks for semantic segmentation of satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082481030&doi=10.1109%2fICCVW.2019.00302&partnerID=40&md5=b842a38853f4b62967478bd441a9fc3f","Semantic segmentation methods have made impressive progress with deep learning. However, while achieving higher and higher accuracy, state-of-the-art neural networks overlook the complexity of architectures, which typically feature dozens of millions of trainable parameters. Consequently, these networks requires high computational resources and are mostly not suited to perform on edge devices with tight resource constraints, such as phones, drones, or satellites. In this work, we propose two highly compact neural network architectures for semantic segmentation of images, which are up to 100 000 times less complex than state-of-the-art architectures while approaching their accuracy. To decrease the complexity of existing networks, our main ideas consist in exploiting lightweight encoders and decoders with depth-wise separable convolutions and decreasing memory usage with the removal of skip connections between encoder and decoder. Our architectures are designed to be implemented on a basic FPGA such as the one featured on the Intel Altera Cyclone V family of SoCs. We demonstrate the potential of our solutions in the case of binary segmentation of remote sensing images, in particular for extracting clouds and trees from RGB satellite images. © 2019 IEEE.","Cloud; Compact; FPGA; Neural-Network; Onboard; Satellite; Segmentation","Binary trees; Clouds; Complex networks; Computer vision; Decoding; Deep learning; Field programmable gate arrays (FPGA); Image segmentation; Low power electronics; Neural networks; Remote sensing; Satellites; Semantic Web; Semantics; Signal encoding; Binary segmentation; Compact; Computational resources; Encoders and decoders; Onboard; Remote sensing images; Resource Constraint; Semantic segmentation; Network architecture"
"Automatic building extraction based on high resolution aerial images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074868898&doi=10.1109%2fEITCE47263.2019.9094824&partnerID=40&md5=7c5d576856eaf7427f4185f9ff5efa88","Building extraction is a hotspot of research. Buildings in remote sensing images are numerous, and buildings have different distribution patterns in different regions, which means it is difficult to extract buildings by one traditional method for all areas. Deep learning plays more and more important part of computer version and image processing, which has made some progress in target detection and identification, especially in medical graphic recognition. In this paper, we compared three common datasets, and selected the most suitable one. Mask R-CNN is applied to this dataset. We provided a better method and this method can be used in different areas when dataset is rich enough for training. The method in this paper just need high resolution aerial images to extract buildings automatically without any other steps, because identification and extraction are achieved in one network. The research showed: the dataset we selected is suitable; Mask R-CNN can be used in high resolution remotely images; Mask R-CNN is good at building extraction, and greatly reduce the loss of classification and leakage. Therefore, this paper provided a suitable method for automatic building extraction. © 2019 IEEE.","Building extraction; High resolution remotely image; Mask R-CNN","Antennas; Classification (of information); Convolutional neural networks; Deep learning; Extraction; Medical imaging; Remote sensing; Automatic building extraction; Building extraction; Computer versions; Detection and identifications; Different distributions; Graphic recognition; High-resolution aerial images; Remote sensing images; Buildings"
"Combining a deep convolutional neural network with transfer learning for ship classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082131375&doi=10.1109%2fICICTA49267.2019.00011&partnerID=40&md5=242d13799f3f301aba4af59b18394fbe","the classification of ship targets is essential for maritime search and rescue, fishing vessel monitoring, and other maritime supervision. Aiming at the problems of low recognition accuracy due to insufficient image resolution of the ship, a high-resolution remote sensing image ship target recognition method based on deep convolutional neural network is proposed. First VGG19 network model is used as the basic network, and then the transfer learning technology is applied to initialize parameters. The model is trained by ship target data set. Finally we use Adam method to optimize this model. The experimental results show that the final recognition accuracy is 95.8%, and it is proved higher accuracy compared with other classification methods. © 2019 IEEE.","High-resolution remote sensing images; Ship Target recognition; Transfer learning","Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Image resolution; Remote sensing; Ships; Classification methods; High resolution remote sensing images; Learning technology; Network modeling; Recognition accuracy; Search and rescue; Ship classification; Ship targets; Transfer learning"
"Multitask learning for large-scale semantic change detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071711233&doi=10.1016%2fj.cviu.2019.07.003&partnerID=40&md5=992d7ceef83be6a1231249502a999400","Change detection is one of the main problems in remote sensing, and is essential to the accurate processing and understanding of the large scale Earth observation data available. Most of the recently proposed change detection methods bring deep learning to this context, but change detection labelled datasets which are openly available are still very scarce, which limits the methods that can be proposed and tested. In this paper we present the first large scale very high resolution semantic change detection dataset, which enables the usage of deep supervised learning methods for semantic change detection with very high resolution images. The dataset contains coregistered RGB image pairs, pixel-wise change information and land cover information. We then propose several supervised learning methods using fully convolutional neural networks to perform semantic change detection. Most notably, we present a network architecture that performs change detection and land cover mapping simultaneously, while using the predicted land cover information to help to predict changes. We also describe a sequential training scheme that allows this network to be trained without setting a hyperparameter that balances different loss functions and achieves the best overall results. © 2019 Elsevier Inc.","Fully convolutional networks; High resolution Earth observation; Multitask learning; Remote sensing; Semantic change detection","Convolution; Large dataset; Machine learning; Network architecture; Neural networks; Observatories; Remote sensing; Semantics; Supervised learning; Change detection; Convolutional networks; Convolutional neural network; Earth observations; Land cover informations; Multitask learning; Supervised learning methods; Very high resolution (VHR) image; Deep learning"
"TCANet for domain adaptation of hyperspectral images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073422005&doi=10.3390%2frs11192289&partnerID=40&md5=20464513e5483438da812078c0d056f5","The use of Convolutional Neural Networks (CNNs) to solve Domain Adaptation (DA) image classification problems in the context of remote sensing has proven to provide good results but at high computational cost. To avoid this problem, a deep learning network for DA in remote sensing hyperspectral images called TCANet is proposed. As a standard CNN, TCANet consists of several stages built based on convolutional filters that operate on patches of the hyperspectral image. Unlike the former, the coefficients of the filter are obtained through Transfer Component Analysis (TCA). This approach has two advantages: firstly, TCANet does not require training based on backpropagation, since TCA is itself a learning method that obtains the filter coefficients directly from the input data. Second, DA is performed on the fly since TCA, in addition to performing dimensional reduction, obtains components that minimize the difference in distributions of data in the different domains corresponding to the source and target images. To build an operating scheme, TCANet includes an initial stage that exploits the spatial information by providing patches around each sample as input data to the network. An output stage performing feature extraction that introduces sufficient invariance and robustness in the final features is also included. Since TCA is sensitive to normalization, to reduce the difference between source and target domains, a previous unsupervised domain shift minimization algorithm consisting of applying conditional correlation alignment (CCA) is conditionally applied. The results of a classification scheme based on CCA and TCANet show that the DA technique proposed outperforms other more complex DA techniques. © 2019 by the authors.","Classification; Correlation alignment; Domain adaptation; Hyperspectral; TCA","Backpropagation algorithms; Classification (of information); Convolution; Deep learning; Input output programs; Neural networks; Spectroscopy; Classification scheme; Conditional correlation; Convolutional neural network; Dimensional reduction; Domain adaptation; HyperSpectral; Minimization algorithms; Spatial informations; Remote sensing"
"Fusion of convolutional neural network and statistical features for texture classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078154460&doi=10.1109%2fWINCOM47513.2019.8942469&partnerID=40&md5=2032c10f918c6b06b2588ba1325f6fcb","Texture is a fundamental characteristic of many types of images, especially those with significant rotation, scale illumination, and viewpoint change. Texture image classification is one of the challenging problems that have various applications such as remote sensing, material recognition, and computer-aided medical diagnosis, etc. Various Computer vision techniques have been used. More recently, Deep learning architectures demonstrated impressive results. This paper aims to investigate combining two feature extraction methods: Handcrafted-based and CNN-based in a two-stream neural network architecture. We believe that Statistical features could enhance the performance of the CNN architecture, especially in the case of small datasets. To test our approach we used two challenging datasets, the Describable Textures Dataset (DTD) and Flicker Material Database (FMD). Results showed that our two-stream neural network which has an image as a first stream and a statistical feature vector as a second stream achieve better results than a Convolutional neural network achieved with just the RGB image as input. The Xception network [9] combined with SIFT-FV demonstrated an accuracy superiority for both datasets. © 2019 IEEE.","CNN; Computer vision; Deep learning; Feature extraction; Texture classification and retrieval; Visual attributes","Classification (of information); Computer aided diagnosis; Computer vision; Convolution; Deep learning; Extraction; Feature extraction; Medical imaging; Medical problems; Mobile telecommunication systems; Network architecture; Neural networks; Remote sensing; Statistical tests; Textures; Wireless networks; Computer vision techniques; Convolutional neural network; Feature extraction methods; Fundamental characteristics; Learning architectures; Material recognition; Texture classification; Visual attributes; Image texture"
"Conditional generative adversarial networks (cGANs) for near real-time precipitation estimation from multispectral GOES-16 satellite imageries-PERSIANN-cGAN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073408806&doi=10.3390%2frs11192193&partnerID=40&md5=ee563f287877925796524d0b1da5cda2","In this paper, we present a state-of-the-art precipitation estimation framework which leverages advances in satellite remote sensing as well as Deep Learning (DL). The framework takes advantage of the improvements in spatial, spectral and temporal resolutions of the Advanced Baseline Imager (ABI) onboard the GOES-16 platform along with elevation information to improve the precipitation estimates. The procedure begins by first deriving a Rain/No Rain (R/NR) binary mask through classification of the pixels and then applying regression to estimate the amount of rainfall for rainy pixels. A Fully Convolutional Network is used as a regressor to predict precipitation estimates. The network is trained using the non-saturating conditional Generative Adversarial Network (cGAN) and Mean Squared Error (MSE) loss terms to generate results that better learn the complex distribution of precipitation in the observed data. Common verification metrics such as Probability Of Detection (POD), False Alarm Ratio (FAR), Critical Success Index (CSI), Bias, Correlation and MSE are used to evaluate the accuracy of both R/NR classification and real-valued precipitation estimates. Statistics and visualizations of the evaluation measures show improvements in the precipitation retrieval accuracy in the proposed framework compared to the baseline models trained using conventional MSE loss terms. This framework is proposed as an augmentation for PERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network- Cloud Classification System) algorithm for estimating global precipitation. © 2019 by the authors.","Convolutional neural networks (CNNs); Generative adversarial networks (GANs); Machine learning; Multispectral satellite imagery; Precipitation","Classification (of information); Convolution; Deep learning; Image enhancement; Learning algorithms; Learning systems; Mean square error; Neural networks; Pixels; Precipitation (chemical); Rain; Remote sensing; Adversarial networks; Cloud classification systems; Convolutional neural network; Multispectral satellite imagery; Precipitation estimation; Precipitation estimation from remotely sensed information; Precipitation retrievals; Probability of detection; Satellite imagery"
"Special section guest editorial: Change detection using multi-source remotely sensed imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073421795&doi=10.3390%2frs11192216&partnerID=40&md5=d311ab7118bfcb3e055efb823a3ad2f1","This special issue hosts papers on change detection technologies and analysis in remote sensing, including multi-source sensors, advanced machine learning technologies for change information mining, and the utilization of these technologies in a variety of geospatial applications. The presented results showed improved results when multi-source remote sensed data was used in change detection. © 2019 by the authors.","Change detection; Deep learning; Image segmentation; Multi-scale; Multi-source remote sensing","Deep learning; Engineering education; Image segmentation; Change detection; Geospatial applications; Information mining; Machine learning technology; Multi-scale; Multi-Sources; Remote sensed data; Remotely sensed imagery; Remote sensing"
"Wetland classification based on a new efficient generative adversarial network and Jilin-1 satellite image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074199313&doi=10.3390%2frs11202455&partnerID=40&md5=d3315428da4167b779f2d029c6fc58d8","Recent studies have shown that deep learning methods provide useful tools for wetland classification. However, it is difficult to perform species-level classification with limited labeled samples. In this paper, we propose a semi-supervised method for wetland species classification by using a new efficient generative adversarial network (GAN) and Jilin-1 satellite image. The main contributions of this paper are twofold. First, the proposed method, namely ShuffleGAN, requires only a small number of labeled samples. ShuffleGAN is composed of two neural networks (i.e., generator and discriminator), which perform an adversarial game in the training phase and ShuffleNet units are added in both generator and discriminator to obtain speed-accuracy tradeoff. Second, ShuffleGAN can perform species-level wetland classification. In addition to distinguishing the wetland areas from non-wetlands, different tree species located in the wetland are also identified, thus providing a more detailed distribution of the wetland land-covers. Experiments are conducted on the Haizhu Lake wetland data acquired by the Jilin-1 satellite. Compared with existing GAN, the improvement in overall accuracy (OA) of the proposed ShuffleGAN is more than 2%. This work can not only deepen the application of deep learning in wetland classification but also promote the study of fine classification of wetland land-covers. © 2019 by the authors.","Classification; Deep learning; Generative adversarial networks; Remote sensing; Wetland","Classification (of information); Deep learning; Image classification; Remote sensing; Satellites; Supervised learning; Adversarial networks; Learning methods; Overall accuracies; Satellite images; Semi-supervised method; Species classification; Speed accuracy; Wetland classification; Wetlands"
"Lossless compression for hyperspectral image using deep recurrent neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072013140&doi=10.1007%2fs13042-019-00937-2&partnerID=40&md5=03ff3f732102ccf08b2ac8e56212b22a","With the rapid development of hyperspectral remote sensing technology, the spatial resolution and spectral resolution of hyperspectral images are continually increasing, resulting in a continual increase in the scale of hyperspectral data. At present, hyperspectral lossless compression technology has reached a bottleneck. Simultaneously, the rise of deep learning has provided us with new ideas. Therefore, this paper examines the use of deep learning for the lossless compression of hyperspectral images. In view of the differential pulse code modulation (DPCM) method being insufficient for predicting spectral band information, the proposed method, called C-DPCM-RNN, uses a deep recurrent neural network (RNN) to improve the traditional DPCM method and improve the generalization ability and prediction accuracy of the model. The final experimental result shows that C-DPCM-RNN achieves better compression on a set of calibrated AVIRIS test images provided by the Multispectral and Hyperspectral Data Compression Working Group of the Consultative Committee for Space Data Systems in 2006. C-DPCM-RNN overcomes the limits of traditional methods in its performance on uncalibrated AVIRIS test images. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Hyperspectral images; Lossless compression",
"Reservoir Facies Classification using Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081663447&doi=10.1109%2fTENGARSS48957.2019.8976038&partnerID=40&md5=45b0c36ac9f2b133cbab8f00e3b7a58f","Distribution of Reservoir facies classes in a hydrocarbon reservoir is an important aspect for geologists to decide the location to drill a production well. Facies classes are based on rock characteristics that can indicate the locations of good quality sand, shale and hence the presence of hydrocarbon. However, due to the heterogeneous nature of earth subsurface, gathering facies information is challenging. Seismic data when properly correlated with the facies classes at the well locations can be suitable for effectively generating the facies information across the reservoir. Machine learning models had a great contribution in this field to successfully understand the reservoir characteristics from seismic data. However, advanced modelling techniques like deep learning still have not been explored in depth. In this paper, we put emphasis to understand the relation of seismic and facies classes using Convolution Neural Network (CNN) models, a popular deep learning model that takes into consideration the spatial relation present in the input data. CNN variations are adapted to improve the mapping relation of seismic to facies classes. When the models are compared with the traditional Machine Learning models, the CNN provided improved mapping relation as compared to the conventional models. More specifically, ResNet, an adaptation of CNN architecture with residual connections, outperformed all the other models with a Precision, Sensitivity and Specificity of 0.61, 0.69 and 0.84 respectively. © 2019 IEEE.","Convolutional Neural Networks (CNNs); Deep Learning; Facies Classification; Machine Learning; Seismic","Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Geology; Geophysical prospecting; Hydrocarbons; Location; Mapping; Remote sensing; Seismic response; Seismic waves; Convolution neural network; Hydrocarbon reservoir; Machine learning models; Modelling techniques; Reservoir characteristic; Rock characteristics; Seismic; Sensitivity and specificity; Learning systems"
"Deep hyperspectral prior: Single-image denoising, inpainting, super-resolution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082459903&doi=10.1109%2fICCVW.2019.00477&partnerID=40&md5=520f13e0337701168dd87bd5e505539d","Deep learning algorithms have demonstrated state-of-the-art performance in various tasks of image restoration. This was made possible through the ability of CNNs to learn from large exemplar sets. However, the latter becomes an issue for hyperspectral image processing where datasets commonly consist of just a few images. In this work, we propose a new approach to denoising, inpainting, and super-resolution of hyperspectral image data using intrinsic properties of a CNN without any training. The performance of the given algorithm is shown to be comparable to the performance of trained networks, while its application is not restricted by the availability of training data. This work is an extension of original 'deep prior' algorithm to hyperspectral imaging domain and 3D-convolutional networks. © 2019 IEEE.","3D CNN; Deep image prior; Deep learning; Denoising; Hyperspectral imaging; Image generation; Inpainting; Remote sensing; Single image; Super resolution","Computer vision; Deep learning; Hyperspectral imaging; Image reconstruction; Learning algorithms; Optical resolving power; Remote sensing; Spectroscopy; Three dimensional computer graphics; 3D CNN; De-noising; Image generations; Image priors; Inpainting; Single images; Super resolution; Image denoising"
"Void Filling of Digital Elevation Models with Deep Generative Models","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076575363&doi=10.1109%2fLGRS.2019.2902222&partnerID=40&md5=e5222a7aed6097a3c7c16d12cdfee1f2","In recent years, advances in machine learning algorithms, cheap computational resources, and the availability of big data have spurred the deep learning revolution in various application domains. In particular, supervised learning techniques in image analysis have led to a superhuman performance in various tasks, such as classification, localization, and segmentation, whereas unsupervised learning techniques based on increasingly advanced generative models have been applied to generate high-resolution synthetic images indistinguishable from real images. In this letter, we consider a state-of-the-art machine learning model for image inpainting, namely, a Wasserstein Generative Adversarial Network based on a fully convolutional architecture with a contextual attention mechanism. We show that this model can be successfully transferred to the setting of digital elevation models for the purpose of generating semantically plausible data for filling voids. Training, testing, and experimentation are done on GeoTIFF data from various regions in Norway, made openly available by the Norwegian Mapping Authority. © 2019 IEEE.","Digital elevation models (DEMs); predictive models; remote sensing; unsupervised learning","Deep learning; Digital instruments; Geomorphology; Image segmentation; Learning algorithms; Remote sensing; Supervised learning; Surveying; Unsupervised learning; Adversarial networks; Attention mechanisms; Computational resources; Digital elevation model; Generative model; Machine learning models; Predictive models; Synthetic images; Machine learning; algorithm; computer simulation; digital elevation model; image analysis; image resolution; machine learning; performance assessment; segmentation; supervised learning; unsupervised classification; Norway"
"Detection of Fishing Boats in SAR Image using Linear Moments and Machine Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081648570&doi=10.1109%2fTENGARSS48957.2019.8976043&partnerID=40&md5=9c0403258950394679570b36d6f1b398","Synthetic Aperture Radar is a popular instrument used in maritime surveillance due to its all weather imaging capabilities and ability to discriminate objects from its backscatter. One of the important concern in maritime safety is detection of fishing boats for rescue and other strategic operations. Popular approaches such as, Constant False Alaram Rate (CFAR) and its variations, distribution based approaches such as K-Wishart, Guass, G0 and Deep Learning approaches have been widely employed but with limitations such as false detection or no detection due to non-convergence or high computational requirements. Keeping in view of the above, in this article we propose a simple approach based on Linearized Moments in association with machine learning methods to detect fishing boats. Our approaches is examined on a data set acquired from a Airborne SAR platform with frequencies in L and C bands operating in multiple polarization modes and having a spatial resolution of 1.2m. The proposed approach showed better accuracy in identifying fishing boats from Sea clutter with an area under curve (AUC) of 0.98 using a C4.5 decision tree classifier. Our study had shown that Linearized Moments can better model the Sea clutter thus improving the ability of machine classifers to detect fishing boats with improved accuracies. © 2019 IEEE.","Airborne SAR; C4.5; Classification; Clustering; Fishing Boats; K-Means; KNN; Linearized Moments; SVM","Classification (of information); Clutter (information theory); Decision trees; Deep learning; Fisheries; Geology; K-means clustering; Learning systems; Linearization; Radar imaging; Remote sensing; Support vector machines; Synthetic aperture radar; Airborne SAR; C4.5; Clustering; Fishing boats; K-means; Linearized Moments; Fishing vessels"
"Mapping dead forest cover using a deep convolutional neural network and digital aerial photography","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073641369&doi=10.1016%2fj.isprsjprs.2019.07.010&partnerID=40&md5=c94438353307dba3885efd7b976cf97e","Tree mortality is an important forest ecosystem variable having uses in many applications such as forest health assessment, modelling stand dynamics and productivity, or planning wood harvesting operations. Because tree mortality is a spatially and temporally erratic process, rates and spatial patterns of tree mortality are difficult to estimate with traditional inventory methods. Remote sensing imagery has the potential to detect tree mortality at spatial scales required for accurately characterizing this process (e.g., landscape, region). Many efforts have been made in this sense, mostly using pixel- or object-based methods. In this study, we explored the potential of deep Convolutional Neural Networks (CNNs) to detect and map tree health status and functional type over entire regions. To do this, we built a database of around 290,000 photo-interpreted trees that served to extract and label image windows from 20 cm-resolution digital aerial images, for use in CNN training and evaluation. In this process, we also evaluated the effect of window size and spectral channel selection on classification accuracy, and we assessed if multiple realizations of a CNN, generated using different weight initializations, can be aggregated to provide more robust predictions. Finally, we extended our model with 5 additional classes to account for the diversity of landcovers found in our study area. When predicting tree health status only (live or dead), we obtained test accuracies of up to 94%, and up to 86% when predicting functional type only (broadleaf or needleleaf). Channel selection had a limited impact on overall classification accuracy, while window size increased the ability of the CNNs to predict plant functional type. The aggregation of multiple realizations of a CNN allowed us to avoid the selection of suboptimal models and help to remove much of the speckle effect when predicting on new aerial images. Test accuracies of plant functional type and health status were not affected in the extended model and were all above 95% for the 5 extra classes. Our results demonstrate the robustness of the CNN for between-scene variations in aerial photography and also suggest that this approach can be applied at operational level to map tree mortality across extensive territories. © 2019 The Authors","Convolutional neural network; Deep learning; Ensemble learning; Machine learning; Remote sensing; Tree mortality","Aerial photography; Antennas; Convolution; Deep learning; Deep neural networks; Ecosystems; Forecasting; Health; Learning systems; Neural networks; Photographic equipment; Plants (botany); Remote sensing; Classification accuracy; Convolutional neural network; Ensemble learning; Forest health assessment; Plant functional type; Remote sensing imagery; Tree mortality; Weight initialization; Forestry; aerial photography; artificial neural network; database; ensemble forecasting; forest cover; forest ecosystem; forest health; morality; remote sensing; speckle; stand dynamics"
"Learning Discriminative Compact Representation for Hyperspectral Imagery Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078267810&doi=10.1109%2fTGRS.2019.2919938&partnerID=40&md5=2fbe2639872d23ac46a01096fbf01bb4","Abundant spectral information of hyperspectral images (HSIs) has shown an obvious advantage in improving the performance of classification in the remote sensing domain. However, this is paid by the expensive consumption on the computation, transmission, as well as storage of HSIs. To address this problem, we propose to learn the discriminative compact representation for HSIs classification, which not only greatly reduces the data redundancy in the image but also preserves the discriminative information required for pixelwise classification in HSIs. To this end, we present a multi-task deep learning framework, which integrates HSIs autoencoding and classification into a two-branch deep neural network for jointly learning. In the network, we employ an encoder block to learn the compact representation of the input HSI via compression in the spectral domain. Being fed with the compact representation, the autoencoding branch then employs a decoder block to reconstruct the input HSI, while the classification branch utilizes a classifier block to predict the label for each pixel. Through end-to-end joint learning, the compact representation is not only informative enough to accurately reconstruct the original HSI but also discriminative enough to appropriately label each pixel with the trained classier. Sufficient experimental results on four HSIs classification data sets demonstrate the effectiveness of the proposed framework. © 1980-2012 IEEE.","Autoencoding; band selection; compact representation learning; hyperspectral images (HSIs) classification","Deep neural networks; Digital storage; Image classification; Image enhancement; Pixels; Remote sensing; Spectroscopy; Autoencoding; Band selection; Compact representation; Hyperspectral imagery classifications; Learning frameworks; Pixelwise classification; Spectral domains; Spectral information; Classification (of information); image classification; image processing; multispectral image; pixel; remote sensing; satellite imagery; spectral analysis"
"A convolutional neural network classifier identifies tree species in mixed-conifer forest from hyperspectral imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073458685&doi=10.3390%2frs11192326&partnerID=40&md5=b4e5bbd4ade66c2e9b67326f460a0045","In this study, we automate tree species classification and mapping using field-based training data, high spatial resolution airborne hyperspectral imagery, and a convolutional neural network classifier (CNN). We tested our methods by identifying seven dominant trees species as well as dead standing trees in a mixed-conifer forest in the Southern Sierra Nevada Mountains, CA (USA) using training, validation, and testing datasets composed of spatially-explicit transects and plots sampled across a single strip of imaging spectroscopy. We also used a three-band 'Red-Green-Blue' pseudo true-color subset of the hyperspectral imagery strip to test the classification accuracy of a CNN model without the additional non-visible spectral data provided in the hyperspectral imagery. Our classifier is pixel-based rather than object based, although we use three-dimensional structural information from airborne Light Detection and Ranging (LiDAR) to identify trees (points > 5 m above the ground) and the classifier was applied to image pixels that were thus identified as tree crowns. By training a CNN classifier using field data and hyperspectral imagery, we were able to accurately identify tree species and predict their distribution, as well as the distribution of tree mortality, across the landscape. Using a window size of 15 pixels and eight hidden convolutional layers, a CNN model classified the correct species of 713 individual trees from hyperspectral imagery with an average F-score of 0.87 and F-scores ranging from 0.67-0.95 depending on species. The CNN classification model performance increased from a combined F-score of 0.64 for the Red-Green-Blue model to a combined F-score of 0.87 for the hyperspectral model. The hyperspectral CNN model captures the species composition changes across ~700 meters (1935 to 2630 m) of elevation from a lower-elevation mixed oak conifer forest to a higher-elevation fir-dominated coniferous forest. High resolution tree species maps can support forest ecosystem monitoring and management, and identifying dead trees aids landscape assessment of forest mortality resulting from drought, insects and pathogens. We publicly provide our code to apply deep learning classifiers to tree species identification from geospatial imagery and field training data. © 2019 by the authors.","Convolutional neural networks; Deep learning; Hyperspectral imagery; Species distribution modeling","Classification (of information); Convolution; Deep learning; Deep neural networks; Ecosystems; Image classification; Neural networks; Optical radar; Pixels; Population distribution; Remote sensing; Spectroscopy; Well testing; Classification accuracy; Convolutional neural network; High spatial resolution; Hyper-spectral imageries; Light detection and ranging; Species distribution modeling; Structural information; Tree species identifications; Forestry"
"Visual Attention-Driven Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077965031&doi=10.1109%2fTGRS.2019.2918080&partnerID=40&md5=46d9350e51b85edab3bcddb9b1e519e5","Deep neural networks (DNNs), including convolutional neural networks (CNNs) and residual networks (ResNets) models, are able to learn abstract representations from the input data by considering a deep hierarchy of layers that perform advanced feature extraction. The combination of these models with visual attention techniques can assist with the identification of the most representative parts of the data from a visual standpoint, obtained through more detailed filtering of the features extracted by the operational layers of the network. This is of significant interest for analyzing remotely sensed hyperspectral images (HSIs), characterized by their very high spectral dimensionality. However, few efforts have been conducted in the literature in order to adapt visual attention methods to remotely sensed HSI data analysis. In this paper, we introduce a new visual attention-driven technique for the HSI classification. Specifically, we incorporate attention mechanisms to a ResNet in order to better characterize the spectral-spatial information contained in the data. Our newly proposed method calculates a mask that is applied to the features obtained by the network in order to identify the most desirable ones for classification purposes. Our experiments, conducted using four widely used HSI data sets, reveal that the proposed deep attention model provides competitive advantages in terms of classification accuracy when compared to other state-of-the-art methods. © 1980-2012 IEEE.","Deep learning (DL); feature extraction; hyperspectral image (HSI) classification; residual neural networks; visual attention","Behavioral research; Classification (of information); Competition; Data mining; Deep neural networks; Extraction; Feature extraction; Multilayer neural networks; Network layers; Remote sensing; Spectroscopy; Abstract representation; Attention mechanisms; Classification accuracy; Competitive advantage; Convolutional neural network; State-of-the-art methods; Visual Attention; Visual attention methods; Image classification; artificial neural network; detection method; image analysis; image classification; satellite data"
"A hybrid OSVM-OCNN method for crop classification from fine spatial resolution remotely sensed imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074202873&doi=10.3390%2frs11202370&partnerID=40&md5=19dae5fb26ea38a329fd8c7592be35d1","Accurate information on crop distribution is of great importance for a range of applications including crop yield estimation, greenhouse gas emission measurement and management policy formulation. Fine spatial resolution (FSR) remotely sensed imagery provides new opportunities for crop mapping at a detailed level. However, crop classification from FSR imagery is known to be challenging due to the great intra-class variability and low inter-class disparity in the data. In this research, a novel hybrid method (OSVM-OCNN) was proposed for crop classification from FSR imagery, which combines a shallow-structured object-based support vector machine (OSVM) with a deep-structured object-based convolutional neural network (OCNN). Unlike pixel-wise classification methods, the OSVM-OCNN method operates on objects as the basic units of analysis and, thus, classifies remotely sensed images at the object level. The proposed OSVM-OCNN harvests the complementary characteristics of the two sub-models, the OSVM with effective extraction of low-level within-object features and the OCNN with capture and utilization of high-level between-object information. By using a rule-based fusion strategy based primarily on the OCNN's prediction probability, the two sub-models were fused in a concise and effective manner. We investigated the effectiveness of the proposed method over two test sites (i.e., S1 and S2) that have distinctive and heterogeneous patterns of different crops in the Sacramento Valley, California, using FSR Synthetic Aperture Radar (SAR) and FSR multispectral data, respectively. Experimental results illustrated that the new proposed OSVM-OCNN approach increased markedly the classification accuracy for most of crop types in S1 and all crop types in S2, and it consistently achieved the most accurate accuracy in comparison with its two object-based sub-models (OSVM and OCNN) as well as the pixel-wise SVM (PSVM) and CNN (PCNN) methods. Our findings, thus, suggest that the proposed method is as an effective and efficient approach to solve the challenging problem of crop classification using FSR imagery (including from different remotely sensed platforms). More importantly, the OSVM-OCNN method is readily generalisable to other landscape classes and, thus, should provide a general solution to solve the complex FSR image classification problem. © 2019 by the authors.","crop mapping; Decision fusion; Deep learning; FSR remotely sensed imagery; Object-based image classification","Crops; Deep learning; Greenhouse gases; Image resolution; Mapping; Neural networks; Pixels; Remote sensing; Support vector machines; Synthetic aperture radar; Classification accuracy; Complementary characteristics; Convolutional neural network; Crop mapping; Decision fusion; Object-based image classification; Prediction probabilities; Remotely sensed imagery; Image classification"
"Vision-Based Method for Automatic Quantification of Parkinsonian Bradykinesia","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073663155&doi=10.1109%2fTNSRE.2019.2939596&partnerID=40&md5=c2065aab1386ccaf674494d570024c6d","Non-volitional discontinuation of motion, namely bradykinesia, is a common motor symptom among patients with Parkinson's disease (PD). Evaluating bradykinesia severity is an important part of clinical examinations on PD patients in both diagnosis and monitoring phases. However, subjective evaluations from different clinicians often show low consistency. The research works that explore objective quantification of bradykinesia are mostly based on highly-integrated sensors. Although these sensor-based methods demonstrate applaudable performance, it is unrealistic to promote them for wide use because the special devices they require are far from popularized in daily lives. In this paper, we take advantage of computer vision and machine learning technologies, proposing a vision-based method to automatically and objectively quantify bradykinesia severity. Three bradykinesia-related items are investigated in our study: finger tapping, hand clasping and hand pro/supination. In our method, human pose estimation technology is utilized to extract kinematic characteristics and supervised-learning-based classifiers are employed to generate score ratings. Clinical experiment on 60 patients shows that the scoring accuracy of our method over 360 examination videos is 89.7%, which is competitive with other related works. The devices our method requires are only a camera for instrumentation and a laptop for data processing. Therefore, our method can produce reliable assessment results on Parkinsonian bradykinesia with minimal device requirement, showing great potential of realizing long-term remote monitoring on patients' condition. © 2001-2011 IEEE.","automatic quantification; human pose estimation; Parkinsonian bradykinesia","Data handling; Diagnosis; Machine learning; Automatic quantification; Clinical experiments; Human pose estimations; Kinematic characteristics; Machine learning technology; Parkinsonian bradykinesia; Patients' conditions; Subjective evaluations; Remote patient monitoring; adult; Article; artificial neural network; body position; bradykinesia; deep learning; dimensional measurement accuracy; disease duration; disease severity assessment; feature extraction algorithm; female; finger tapping test; hand clasping; human; information processing; kinematics; machine learning; major clinical study; male; mathematical model; middle aged; Parkinson disease; remote sensing; scoring system; supination; support vector machine; task performance; three dimensional imaging; training; two-dimensional imaging; Unified Parkinson Disease Rating Scale; upper limb; validation process"
"Agricultural remote sensing image cultivated land extraction technology based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074367146&partnerID=40&md5=a1c45e0dcdfc5e64459299761433d893","Land resources are the material basis for human survival and development. The characteristics of China’s land resources are “one more than three less”, that is, the total amount is large, the amount of cultivated land per capita is small, the quality of cultivated land is small, and the reserve resources that can be developed are few. The quantity and quality of cultivated land are related to China’s food security. Cultivated land protection has always been the core of land resource management. The acquisition of the quantity of cultivated land and its distribution information is the premise to achieve this goal. Remote sensing technology can objectively obtain the information of cultivated land from the spatial scale of wide area and local area. Remote sensing images have been widely used in large-scale land use surveys and remote sensing monitoring services. However, due to the details of their own ambiguity, large amount of data and large differences within the class, the difficulty of automatic interpretation of images has increased. This makes the actual business work still mainly based on manual visual interpretation, and lacks a highly automated and streamlined working style. Deep learning is a research field that has gradually emerged in recent years. At present, deep learning has become a mainstream research tool in the fields of speech recognition, image recognition, image classification, and target detection. Extracting the information of cultivated land from remote sensing images is a problem of image recognition and classification. Therefore, using deep learning to extract cultivated land from agricultural remote sensing images is a very feasible research program. Based on the deep learning theory, this paper sorts out the research results in the field of remote sensing image classification and deep learning at home and abroad, then preprocesses and labels the remote sensing image to make a training sample set, and finally uses convolutional neural network (CNN). For the classifier, the agricultural remote sensing image was extracted from cultivated land. © 2019, Universidad del Zulia. All rights reserved.","Agricultural Remote Sensing Image; Deep Learning; Feature Extraction; Image Processing",
"Environmental monitoring of mountain orchards based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074351143&partnerID=40&md5=2ebdf5ba78a4023e7c6ff78c1b386b22","�“Deep Learning + agriculture” is the main way of modern agricultural development, and agricultural environmental monitoring is an important research field of modern agriculture. The transformation of agricultural production mode from traditional coarse-grained operation to fine-grained operation has played a guiding and promoting role in the modernization of traditional agriculture. It is an effective way to rapidly change the limitations of traditional agricultural monitoring at the present stage to integrate the Internet of Things, sensing and communication technology to realize the digital information monitoring of agricultural production. It can effectively reduce the cost of collecting basic agricultural information and improve production efficiency and output. In order to optimize the monitoring of environmental pollution, it is necessary to recognize the target area of environmental pollution effectively. This paper proposes a method of environmental pollution target area recognition based on deep learning and UAV remote sensing technology. The method uses UAV airborne space scanning method to collect images in mountain orchards, and then carries out edge contour detection and image segmentation of contaminated areas on the collected environmental images. The deep learning algorithm is used to mark and recognize the image of the polluted area adaptability, and the image detection and three-dimensional area recognition of the polluted area are realized. The method has high accuracy in target recognition of Mountain Orchard environmental pollution area and high accuracy in partition recognition of environmental pollution area. © 2019, Universidad del Zulia. All rights reserved.","Deep Learning; Environmental Monitoring; Mountain Orchards; Unmanned Aerial Vehicle",
"Study on rainfall-runoff simulation and prediction in lake basin based on dynamic data-driven deep recurrent network [动态数据驱动模式下的湖泊流域降雨径流模拟]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073170708&doi=10.11834%2fjrs.20198027&partnerID=40&md5=e586b7d5ccf96a1057ed3d34060e23bc","Performing rainfall-runoff simulation and prediction in a lake basin is a time-series analysis problem in complex systems. Existing mechanistic and identification methods for model selection have advantages and disadvantages. Mechanism model has a clear physical explanation, but it requires professional data for support and the model-solving process is complicated. Identification model is flexible and its solution is simple, but it has difficulty building a universal model and the accuracy of the model is low. Existing simulation models are also based mostly on static data, which cannot effectively use real-time observation data from sensor networks to improve simulation uncertainty. This study aims to improve the problem of traditional identification models being incapable of effectively using timing information, which results in low simulation accuracy. Moreover, it establishes a simulation and simulation framework for dynamic feedback and adaptive adjustment between observation and numerical simulation. This research proposes a dynamic data-driven model based on deep recurrent neural network, named dynamic data-driven time sequence model, which consists of a multilayered long short-term memory loop body and a fully connected layer. The proposed model incorporates runoff remote sensing rainfall data and ground station observation rainfall data as static data input and recent ground station actual runoff observation data as dynamic data input to simulate catchment runoff process. Several cases of multiple sub-river runoff into Poyang Lake indicate that in static data-driven mode, with TRMM_3B42_V7 precipitation as input, the ENS accuracy of DTSM is 10 percentage points higher or more than that of the mechanism model. The cases also indicate that in static data-driven mode, with the fusion precipitation from TRMM_3B42_V7 and ground station as input, the ENS accuracy of DTSM is 29 percentage points higher or more than that of the mechanism model. Lastly, the dynamic data-driven model can further improve the accuracy of simulation compared with the static-driven model, and the improvement is substantial in the basin with lower accuracy in static data-driven mode. The model based on deep recurrent neural network can effectively extract the timing information from the data. The dynamic data-driven model can make adaptive adjustments to improve simulation uncertainty. Based on the above two reasons, the DTSM proposed in this paper can achieve the same or a better simulation accuracy than existing representative mechanism models. At the same time, DTSM is flexible and the solving process is simple. © 2019, Science Press. All right reserved.","Data-driven simulation; Deep learning; Remote sensing; Runoff simulation; Sensor network; TRMM rainfall","Catchments; Deep learning; Deep neural networks; Dynamics; Lakes; Multilayer neural networks; Rain; Rain gages; Remote sensing; Runoff; Sensor networks; Time series analysis; Data-driven simulation; Identification method; Identification model; Rainfall-runoff simulations; Real time observation; Runoff simulation; Simulation framework; Simulation uncertainty; Recurrent neural networks"
"Deep feature learning versus shallow feature learning systems for joint use of airborne thermal hyperspectral and visible remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063597880&doi=10.1080%2f01431161.2019.1597310&partnerID=40&md5=523a05de1c931b85573293a1965d31ba","Recently, the development of various remote sensing sensors has provided more reliable information and data for identification of different ground classes. Accordingly, multisensory fusion techniques are applied to enhance the process of information extraction from complementary airborne and spaceborne remote sensing data. Most of previous research in the literature has focused on the extraction of shallow features from a specific sensor and on classification of the resulted feature space using decision fusion systems. In recent years, Deep Learning (DL) algorithms have drawn a lot of attention in the machine learning area and have had different remote sensing applications, especially on data fusion. This study presents two different feature-learning strategies for the fusion of hyperspectral thermal infrared (HTIR) and visible remote sensing data. First, a Deep Convolutional Neural Network (DCNN)-Support Vector Machine (SVM) was utilized on the features of two datasets to provide the class labels. To validate the results with other learning strategies, a shallow feature model was used, as well. This model was based on feature fusion and decision fusion that classified and fused the two datasets. A co-registered thermal infrared hyperspectral (HTIR) and Fine Resolution Visible (Vis) RGB imagery was available from Quebec of Canada to examine the effectiveness of the proposed method. Experimental results showed that, except for the computational time, the proposed deep learning model outperformed shallow feature-based strategies in the classification performance that was based on its accuracy. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",,"Classification (of information); Data fusion; Data mining; Deep neural networks; Infrared radiation; Neural networks; Remote sensing; Space optics; Support vector machines; Classification performance; Convolutional neural network; Deep feature learning; Feature learning system; Multi-sensory fusion; Remote sensing applications; Remote sensing sensors; Spaceborne remote sensing; Machine learning; algorithm; artificial neural network; data set; image classification; machine learning; remote sensing; spectral analysis; spectral resolution; Canada; Quebec [Canada]; Quebec [Quebec (PRV)]"
"Fusion of feature based and deep learning methods for classification of MMS point clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074684086&doi=10.5194%2fisprs-archives-XLII-2-W16-235-2019&partnerID=40&md5=558d427196abe08558a6682714cfd7fc","This work proposes an approach for semantic classification of an outdoor-scene point cloud acquired with a high precision Mobile Mapping System (MMS), with major goal to contribute to the automatic creation of High Definition (HD) Maps. The automatic point labeling is achieved by utilizing the combination of a feature-based approach for semantic classification of point clouds and a deep learning approach for semantic segmentation of images. Both, point cloud data, as well as the data from a multi-camera system are used for gaining spatial information in an urban scene. Two types of classification applied for this task are: 1) Feature-based approach, in which the point cloud is organized into a supervoxel structure for capturing geometric characteristics of points. Several geometric features are then extracted for appropriate representation of the local geometry, followed by removing the effect of local tendency for each supervoxel to enhance the distinction between similar structures. And lastly, the Random Forests (RF) algorithm is applied in the classification phase, for assigning labels to supervoxels and therefore to points within them. 2) The deep learning approach is employed for semantic segmentation of MMS images of the same scene. To achieve this, an implementation of Pyramid Scene Parsing Network is used. Resulting segmented images with each pixel containing a class label are then projected onto the point cloud, enabling label assignment for each point. At the end, experiment results are presented from a complex urban scene and the performance of this method is evaluated on a manually labeled dataset, for the deep learning and feature-based classification individually, as well as for the result of the labels fusion. The achieved overall accuracy with fusioned output is 0.87 on the final test set, which significantly outperforms the results of individual methods on the same point cloud. The labeled data is published on the TUM-PF Semantic-Labeling-Benchmark. © Authors 2019. CC BY 4.0 License.","Classification; Deep learning; HD Maps; Mobile mapping; Point clouds; Segmentation; Urban scene","Decision trees; Deep learning; Digital television; Geometry; Image segmentation; Mapping; Remote sensing; Semantics; Feature based approaches; Feature-based classification; Geometric characteristics; Mobile mapping; Mobile mapping systems; Point cloud; Semantic classification; Urban scenes; Classification (of information)"
"Events recognition for a semi-automatic annotation of soccer videos: A study based deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074686495&doi=10.5194%2fisprs-archives-XLII-2-W16-135-2019&partnerID=40&md5=6b7640b1477d50bcf67c2b2a2fec03bc","In this work, we propose an efficient way of web video annotation in soccer domain. To achieve this, it is necessary to enjoy different architectures of deep learning. We aim at realising a system of annotation able to recognise several events from information of the object that is the ball in our case, in order to fuse them as a part of actions in video. We propose to use Deep Neural Network (DNN) to detect ball and actions. However, Mask R-CNN can play a very important role for features extracted as an output using a training network on ImageNet dataset. The Mask R-CNN is chosen as a method using different CNN as backbone (convolutional Neural Network) ResNet50, ResNet101 and ResNet152, VGG16, VGG 19. We experimentally verify the effectiveness of the proposed method in the test phase. © Authors 2019. CC BY 4.0 License.","CNN; Deep Learning; Event detection; Mask R-CNN; RNN; Soccer Video Annotation","Deep learning; Multimedia systems; Neural networks; Remote sensing; Sports; Convolutional neural network; Event detection; Semi-automatic annotation; Soccer domain; Soccer video; Training network; Web video; Deep neural networks"
"A deep learning approach for urban underground objects detection from vehicle-borne ground penetrating radar data in real-time","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074711187&doi=10.5194%2fisprs-archives-XLII-2-W16-293-2019&partnerID=40&md5=2aa218b78748af33cc82488f8528374d","GPRs (Ground Penetrating Radar) are widely adopted in underground space survey and mapping, because of their advantages of fast data acquisition, convenience, high imaging resolution and NDT (Non Destructive Testing) inspection. However, at present, the automation of the GPR data post-processing is low and the identification of underground objects needs expert interpretation. The heavy manual interpretation labor limits the GPR applications in large-scale urban scenarios. According to the latest research, it is still an unsolved problem to detect targets or defects in GPR data automatically and needs further exploration. In this paper, we propose a deep learning method for real-time detection of underground targets from GPR data. Seven typical targets in urban underground space are identified and labelled to construct the training dataset. The constructed dataset is consist of 489 labelled samples including rainwater wells, cables, metal/nonmetal pipes, sparse/dense steel reinforcement, voids. The training dataset is further augmented to produce more samples. DarkNet53 convolutional neural network (CNN) is trained using the constructed training dataset including realistic data and augmented data to extract features of the buried objects. And then the end-to-end YOLO detection framework is used to classify and locate the seven specific categories buried targets in the GPR data in real time. Experiments show that the automatic real-time detection method proposed in this paper can effectively detect the buried objects in the ground penetrating radar image in real time at Shenzhen test site (typical urban road scene). © Authors 2019. CC BY 4.0 License.","Convolutional Neural Network; Deep Learning; Ground Penetrating Radar; Object Detection; Urban Space Security","Convolution; Data acquisition; Data handling; Deep learning; Deep neural networks; Geological surveys; Ground penetrating radar systems; Neural networks; Nondestructive examination; Object detection; Radar imaging; Remote sensing; Signal detection; Space-based radar; Tracking radar; Urban growth; Convolutional neural network; Data post processing; Fast data acquisition; Ground Penetrating Radar; Non destructive testing; Steel reinforcements; Urban spaces; Urban underground spaces; Geophysical prospecting"
"Remote sensing scene classification using multiple pyramid pooling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074682701&doi=10.5194%2fisprs-archives-XLII-2-W16-279-2019&partnerID=40&md5=e699af5e25ace661a6d0a299189614f2","Remote sensing image scene classification has gained remarkable attention, due to its versatile use in different applications like geospatial object detection, ground object information extraction, environment monitoring and etc. The scene not only contains the information of the ground objects, but also includes the spatial relationship between the ground objects and the environment. With rapid growth of the amount of remote sensing image data, the need for automatic annotation methods for image scenes is more urgent. This paper proposes a new framework for high resolution remote sensing images scene classification based on convolutional neural network. To eliminate the requirement of fixed-size input image, multiple pyramid pooling strategy is equipped between convolutional layers and fully connected layers. Then, the fixed-size features generated by multiple pyramid pooling layer was extended to one-dimension fixed-length vector and fed into fully connected layers. Our method could generate a fixed-length representation regardless of image size, at the same time get higher classification accuracy. On UC-Merced and NWPU-RESISC45 datasets, our framework achieved satisfying accuracies, which is 93.24% and 88.62% respectively. © Authors 2019. CC BY 4.0 License.","Convolutional Neural Network; Deep Learning; Multiple Pyramid Pooling; Remote Sensing; Scene Classification","Classification (of information); Convolution; Deep learning; Deep neural networks; Image classification; Neural networks; Object detection; Object recognition; Classification accuracy; Convolutional neural network; Environment monitoring; High resolution remote sensing images; Multiple Pyramid Pooling; Remote sensing images; Scene classification; Spatial relationships; Remote sensing"
"Lithological classification using multi-sensor data and convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074715182&doi=10.5194%2fisprs-archives-XLII-2-W16-55-2019&partnerID=40&md5=ccb8dcf9757085bc7d48e03322d083ee","Deep learning has been used successfully in computer vision problems, e.g. image classification, target detection and many more. We use deep learning in conjunction with ArcGIS to implement a model with advanced convolutional neural networks (CNN) for lithological mapping in the Mount Isa region (Australia). The area is ideal for spectral remote sensing as there is only sparse vegetation and besides freely available Sentinel-2 and ASTER data, several geophysical datasets are available from exploration campaigns. By fusing the data and thus covering a wide spectral range as well as capturing geophysical properties of rocks, we aim at improving classification accuracies and support geological mapping. We also evaluate the performance of the sensors on their own compared to a joint use as the Sentinel-2 satellites are relatively new and as of now there exist only few studies for geological applications. We developed an end-to-end deep learning model using Keras and Tensorflow that consists of several convolutional, pooling and deconvolutional layers. Our model was inspired by the family of U-Net architectures, where low-level feature maps (encoders) are concatenated with high-level ones (decoders), which enables precise localization. This type of network architecture was especially designed to effectively solve pixel-wise classification problems, which is appropriate for lithological classification. We spatially resampled and fused the multi-sensor remote sensing data with different bands and geophysical data into image cubes as input for our model. Pre-processing was done in ArcGIS and the final, fine-tuned model was imported into a toolbox to be used on further scenes directly in the GIS environment. The tool classifies each pixel of the multiband imagery into different types of rocks according to a defined probability threshold. Results highlight the power of using Sentinel-2 in conjunction with ASTER data with accuracies of 75% in comparison to only 70%and 73% for ASTER or Sentinel-2 data alone. These results are similar but examining the different classes shows that there aresignificant improvements for classes such as dolerite or carbonate sediments that are not that widely distributed in the area. Adding geophysical datasets reduced accuracies to 60%, probably due to an order of magnitude difference in spatial resolution. In comparison, Random Forest (RF) and Support Vector Machines (SVMs) that were trained on the same data only achieve accuracies of 46 % and 36 % respectively. Most insecurity is due to labelling errors and labels with mixed lithologies. However, results show that the U-Netmodel is a powerful alternative to other classifiers for medium-resolution multispectral data. © Authors 2019. CC BY 4.0 License.","ArcGIS; ASTER; Deep Learning; Geology; Lithological Classification; Sentinel-2","Convolution; Decision trees; Deep learning; Geographic information systems; Geology; Geophysics; Lithology; Network architecture; Neural networks; Photomapping; Pixels; Remote sensing; Support vector machines; ArcGIS; ASTER; Classification accuracy; Computer vision problems; Convolutional neural network; Geological applications; Sentinel-2; Support vector machine (SVMs); Classification (of information)"
"Video image target recognition and geolocation method for UAV based on landmarks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074685045&doi=10.5194%2fisprs-archives-XLII-2-W16-285-2019&partnerID=40&md5=a1f1cd5800f0b7c5adbe01b5ec9cabc3","Relying on landmarks for robust geolocation of drone and targets is one of the most important ways in GPS-denied environments. For small drones，there is no direct orientation capability without high-precision IMU. This paper presents an automated real-time matching and geolocation algorithm between video keyframes and landmark database based on the integration of visual SALM and YOLOv3 deep learning network method. The algorithm mainly extracts the landmarks from the drone video keyframe images to improve target geolocation accuracy, and designs different processing scheme of the keyframes which contains rich and spare landmarks. For feature extraction matching, we improved ORB feature extraction strategy, and obtained a more uniformly distributed feature points than original ORB feature extraction. In the three groups of top-down drone video images experiments, the 100m, 200m, and 300m of the case were carried out to verify the robustness of the algorithm and being compared with GPS surveying data. The results show that the features of keyframe landmarks in the top-down video images within 300m are stable to match the landmark database, the geolocation accuracy is controlled within 0.8m, and it has good accuracy. © Authors 2019. CC BY 4.0 License.","GPS-denied Environment; Landmarks; Target Detection; Target Geolocation; UAV Video","Aircraft detection; Deep learning; Drones; Extraction; Feature extraction; Global positioning system; Remote sensing; Target drones; Target tracking; Distributed features; Geolocation algorithm; Landmark database; Landmarks; Orientation-capability; Real-time matching; Target geolocation; Uav videos; Image enhancement"
"Super-resolution for sentinel-2 images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074682230&doi=10.5194%2fisprs-archives-XLII-2-W16-95-2019&partnerID=40&md5=95ff6610fd7dfda236bb9906d67685e7","Obtaining Sentinel-2 imagery of higher spatial resolution than the native bands while ensuring that output imagery preserves the original radiometry has become a key issue since the deployment of Sentinel-2 satellites. Several studies have been carried out on the upsampling of 20m and 60m Sentinel-2 bands to 10 meters resolution taking advantage of 10m bands. However, how to super-resolve 10m bands to higher resolutions is still an open problem. Recently, deep learning-based techniques has become a de facto standard for single-image super-resolution. The problem is that neural network learning for super-resolution requires image pairs at both the original resolution (10m in Sentinel-2) and the target resolution (e.g., 5m or 2.5m). Since there is no way to obtain higher resolution images for Sentinel-2, we propose to consider images from others sensors having the greatest similarity in terms of spectral bands, which will be appropriately pre-processed. These images, together with Sentinel-2 images, will form our training set. We carry out several experiments using state-of-the-art Convolutional Neural Networks for single-image super-resolution showing that this methodology is a first step toward greater spatial resolution of Sentinel-2 images. © Authors 2019. CC BY 4.0 License.","Convolutional neural network; Deep learning; Image enhancement; Optical images; Sentinel-2; Super-resolution","Convolution; Deep learning; Deep neural networks; Geometrical optics; Image resolution; Neural networks; Optical resolving power; Remote sensing; Convolutional neural network; Higher resolution; Higher resolution images; Neural network learning; Optical image; Sentinel-2; Spatial resolution; Super resolution; Image enhancement"
"Dead wood detection based on semantic segmentation of VHR aerial CIR imagery using optimized FCN-Densenet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074717369&doi=10.5194%2fisprs-archives-XLII-2-W16-127-2019&partnerID=40&md5=340b9aa17b569424d57f4d06d1c89091","The assessment of the forests' health conditions is an important task for biodiversity, forest management, global environment monitoring, and carbon dynamics. Several research works were proposed to evaluate the state condition of a forest based on remote sensing technology. Concerning existing technologies, employing traditional machine learning approaches to detect the dead wood in aerial colour-infrared (CIR) imagery is one of the major trends due to its spectral capability to explicitly capturing vegetation health conditions. However, the complicated scene with background noise restricted the accuracy of existing approaches as those detectors normally utilized hand-crafted features. Currently, deep neural networks are widely used in computer vision tasks and prove that features learnt by the model itself perform much better than the hand-crafted features. The semantic image segmentation is a pixel-level classification task, which is best suitable to dead wood detection in very high resolution (VHR) mode because it enables the model to identify and classify very dense and detailed components on the tree objects. In this paper, an optimized FCN-DenseNet is proposed to detect dead wood (i.e. standing dead tree and fallen tree) in a complicated temperate forest environment. Since the appearance of dead trees generally occupies greatly different scales and sizes; several pooling procedures are employed to extract multi-scale features and dense connection is employed to enhance the inline connection among the scales. Our proposed deep neural network is evaluated over VHR CIR imagery (GSD-10cm) captured in a natural temperate forest in Bavarian national forest park, Germany, which has undergone on-site bark beetle attack. The results show that the boundary of dead trees can be accurately segmented, and the classification are performed with a high accuracy, even though only one labelled image with moderate size is used for training the deep neural network. © Authors 2019. CC BY 4.0 License.","Dead Wood Detection; Deep Learning; Semantic Segmentation","Antennas; Biodiversity; Deep learning; Deep neural networks; Image segmentation; Remote sensing; Semantics; Wood; Classification tasks; Dead wood; Machine learning approaches; Multi-scale features; Remote sensing technology; Semantic image segmentations; Semantic segmentation; Very high resolution; Forestry"
"Automated chain for large-scale 3D reconstruction of urban scenes from satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074702317&doi=10.5194%2fisprs-archives-XLII-2-W16-243-2019&partnerID=40&md5=4190ab4052ee2317c4622d6c3ca9e8cb","Automatic city modeling from satellite imagery is a popular yet challenging topic in remote sensing, driven by numerous applications such as telecommunications, defence and urban mamagement. In this paper, we present an automated chain for large-scale 3D reconstruction of urban scenes with a Level of Detail 1 from satellite images. The proposed framework relies on two key ingredient. First, from a stereo pair of images, we estimate a digital terrain model and a digital height model, by using a novel set of feature descriptors based on multiscale morphological analysis. Second, inspired by recent works in machine learning, we extract in an automatic way contour polygons of buildings, by adopting a fully convolutional network U-Net followed by a polygonization of the predicted mask of buildings. We demonstrate the potential of our chain by reconstructing in an automated way different areas of the world. © Authors 2019. CC BY 4.0 License.","3D Reconstruction; Deep Learning; Satellite Images; Stereo Pair; U-Net","Automation; Deep learning; Image analysis; Remote sensing; Satellite imagery; Stereo image processing; 3D reconstruction; Convolutional networks; Digital height models; Digital terrain model; Feature descriptors; Morphological analysis; Satellite images; Stereo pair; Image reconstruction"
"Futuregan: Anticipating the future frames of video sequences using spatio-temporal 3D convolutions in progressively growing GANs","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074715200&doi=10.5194%2fisprs-archives-XLII-2-W16-3-2019&partnerID=40&md5=975bf3bf4421bd473b6b080bce333339","We introduce a new encoder-decoder GAN model, FutureGAN, that predicts future frames of a video sequence conditioned on a sequence of past frames. During training, the networks solely receive the raw pixel values as an input, without relying on additional constraints or dataset specific conditions. To capture both the spatial and temporal components of a video sequence, spatio-temporal 3d convolutions are used in all encoder and decoder modules. Further, we utilize concepts of the existing progressively growing GAN (PGGAN) that achieves high-quality results on generating high-resolution single images. The FutureGAN model extends this concept to the complex task of video prediction. We conducted experiments on three different datasets, MovingMNIST, KTH Action, and Cityscapes. Our results show that the model learned representations to transform the information of an input sequence into a plausible future sequence effectively for all three datasets. The main advantage of the FutureGAN framework is that it is applicable to various different datasets without additional changes, whilst achieving stable results that are competitive to the state-of-the-art in video prediction. The code to reproduce the results of this paper is publicly available at https://github.com/TUM-LMF/FutureGAN. © Authors 2019. CC BY 4.0 License.","Deep Learning; Generative Adversarial Networks; Generative Modeling; Video Prediction","Convolution; Decoding; Deep learning; Forecasting; Remote sensing; Signal encoding; Adversarial networks; Encoder-decoder; Generative model; High resolution; Spatio temporal; State of the art; Video prediction; Video sequences; Video recording"
"PCCT: A point cloud classification tool to create 3D training data to adjust and develop 3D convnet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074712635&doi=10.5194%2fisprs-archives-XLII-2-W16-35-2019&partnerID=40&md5=67ab26d359bad1604d91aa0167b785a0","Point clouds give a very detailed and sometimes very accurate representation of the geometry of captured objects. In surveying, point clouds captured with laser scanners or camera systems are an intermediate result that must be processed further. Often the point cloud has to be divided into regions of similar types (object classes) for the next process steps. These classifications are very time-consuming and cost-intensive compared to acquisition. In order to automate this process step, conventional neural networks (ConvNet), which take over the classification task, are investigated in detail. In addition to the network architecture, the classification performance of a ConvNet depends on the training data with which the task is learned. This paper presents and evaluates the point clould classification tool (PCCT) developed at HCU Hamburg. With the PCCT, large point cloud collections can be semi-automatically classified. Furthermore, the influence of erroneous points in three-dimensional point clouds is investigated. The network architecture PointNet is used for this investigation. © Authors 2019. CC BY 4.0 License.","ConvNet; Deep learning; Semantic labeling; TLS; Training data","Classification (of information); Deep learning; Remote sensing; Semantics; Surveying instruments; Thallium; Classification performance; Classification tasks; Classification tool; Convnet; Intermediate results; Semantic labeling; Three-dimensional point clouds; Training data; Network architecture"
"SEN12MS &ndash; A CURATED DATASET of GEOREFERENCED MULTI-SPECTRAL SENTINEL-1/2 IMAGERY for DEEP LEARNING and DATA FUSION","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084684118&doi=10.5194%2fisprs-annals-IV-2-W7-153-2019&partnerID=40&md5=a594132c1ed18fcccd585a917a8d3bba","The availability of curated large-scale training data is a crucial factor for the development of well-generalizing deep learning methods for the extraction of geoinformation from multi-sensor remote sensing imagery. While quite some datasets have already been published by the community, most of them suffer from rather strong limitations, e.g. regarding spatial coverage, diversity or simply number of available samples. Exploiting the freely available data acquired by the Sentinel satellites of the Copernicus program implemented by the European Space Agency, as well as the cloud computing facilities of Google Earth Engine, we provide a dataset consisting of 180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land cover maps. With all patches being fully georeferenced at a 10 m ground sampling distance and covering all inhabited continents during all meteorological seasons, we expect the dataset to support the community in developing sophisticated deep learning-based approaches for common tasks such as scene classification or semantic segmentation for land cover mapping. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Data Fusion; Dataset; Deep Learning; Machine Learning; Multi-Spectral Imagery; Optical Remote Sensing; Remote Sensing; Sentinel-1; Sentinel-2; Synthetic Aperture Radar (SAR)","Classification (of information); Data fusion; Earth (planet); Learning systems; Remote sensing; Semantics; Space optics; Space-based radar; Synthetic aperture radar; European Space Agency; Ground sampling distances; Land cover mapping; Learning-based approach; Remote sensing imagery; Scene classification; Semantic segmentation; Synthetic aperture radar (SAR) images; Deep learning"
"EVALUATION of DEEP LEARNING TECHNIQUES for DEFORESTATION DETECTION in the AMAZON FOREST","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082307899&doi=10.5194%2fisprs-annals-IV-2-W7-121-2019&partnerID=40&md5=7479fcc000aae894be5c261885fd1fb0","Deforestation is one of the main causes of biodiversity reduction, climate change among other destructive phenomena. Thus, early detection of deforestation processes is of paramount importance. Motivated by this scenario, this work presents an evaluation of methods for automatic deforestation detection, specifically Early Fusion (EF) Convolutional Network, Siamese Convolutional Network (S-CNN) and the well-known Support Vector Machine (SVM), taken as the baseline. These methods were evaluated in a region of the Brazilian Legal Amazon (BLA). Two Landsat 8 images acquired in 2016 and 2017 were used in our experiments. The impact of training set size was also investigated. The Deep Learning-based approaches clearly outperformed the SVM baseline in our approaches, both in terms of F1-score and Overall Accuracy, with a superiority of S-CNN over EF. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Amazon Rainforest; Convolutional Neural Networks; Deep learning; Deforestation; Image Stacking","Biodiversity; Climate change; Convolution; Convolutional neural networks; Deforestation; Petroleum reservoir evaluation; Remote sensing; Support vector machines; Amazon forests; Biodiversity reductions; Convolutional networks; Evaluation of methods; Learning techniques; Learning-based approach; Overall accuracies; Training sets; Deep learning"
"A COMPARATIVE ANALYSIS of UNSUPERVISED and SEMI-SUPERVISED REPRESENTATION LEARNING for REMOTE SENSING IMAGE CATEGORIZATION","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084680670&doi=10.5194%2fisprs-annals-IV-2-W7-167-2019&partnerID=40&md5=2f3aaf589a6bc7f1f43f3889776ca17b","This work aims at investigating unsupervised and semi-supervised representation learning methods based on generative adversarial networks for remote sensing scene classification. The work introduces a novel approach, which consists in a semi-supervised extension of a prior unsupervised method, known as MARTA-GAN. The proposed approach was compared experimentally with two baselines upon two public datasets, UC-MERCED and NWPU-RESISC45. The experiments assessed the performance of each approach under different amounts of labeled data. The impact of fine-tuning was also investigated. The proposed method delivered in our analysis the best overall accuracy under scarce labeled samples, both in terms of absolute value and in terms of variability across multiple runs. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Deep Learning; Generative Adversarial Networks; Representation Learning; Semi-supervised Learning","Image analysis; Learning systems; Semi-supervised learning; Absolute values; Adversarial networks; Comparative analysis; Learning methods; Overall accuracies; Remote sensing images; Scene classification; Unsupervised method; Remote sensing"
"DEEP RESIDUAL LEARNING for SINGLE-IMAGE SUPER-RESOLUTION of MULTI-SPECTRAL SATELLITE IMAGERY","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084643909&doi=10.5194%2fisprs-annals-IV-2-W7-189-2019&partnerID=40&md5=2a603b108ce7e232383eeb2009482bf8","Analyzing optical remote sensing imagery depends heavily on their spatial resolution. At the same time, this data is adversely affected by fixed sensor parameters and environmental influences. Methods for increasing the quality of such data and concomitantly optimizing its information content are, thus, in high demand. In particular, single-image super-resolution (SISR) approaches aim to achieve this goal solely by observing the individual images. We propose to adapt a generic deep residual neural network architecture for SISR to deal with the special properties of remote sensing satellite imagery, especially taking into account the different spatial resolutions of individual Sentinel-2 bands, i.e., ground sampling distances of 20 m and 10 m. As a result, this method is able to increase the perceived resolution of the 20 m channels and mesh all spectral bands. Experimental evaluation and ablation studies on large datasets have shown superior performance compared to the state-of-the-art and that the model is not bound by its capacity. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Convolutional Neural Networks; Deep Learning; Remote Sensing; Residual Learning; Sentinel-2; Single-Image Super-Resolution","Image analysis; Large dataset; Network architecture; Optical resolving power; Remote sensing; Satellite imagery; Environmental influences; Experimental evaluation; Ground sampling distances; Information contents; Optical remote-sensing imagery; Remote sensing satellites; Spatial resolution; Special properties; Deep learning"
"A SEMI-SUPERVISED APPROACH to SAR-OPTICAL IMAGE MATCHING","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084674944&doi=10.5194%2fisprs-annals-IV-2-W7-71-2019&partnerID=40&md5=3c423b8a249fc553528011643c729645","Matching synthetic aperture radar (SAR) and optical remote sensing imagery is a key first step towards exploiting the complementary nature of these data in data fusion frameworks. While numerous signal-based approaches to matching have been proposed, they often fail to perform well in multi-sensor situations. In recent years deep learning has become the go-to approach for solving image matching in computer vision applications, and has also been adapted to the case of SAR-optical image matching. However, the hitherto proposed techniques still fail to match SAR and optical imagery in a generalizable manner. These limitations are largely due to the complexities in creating large-scale datasets of corresponding SAR and optical image patches. In this paper we frame the matching problem within semi-supervised learning, and use this as a proxy for investigating the effects of data scarcity on matching. In doing so we make an initial contribution towards the use of semi-supervised learning for matching SAR and optical imagery. We further gain insight into the non-complementary nature of commonly used supervised and unsupervised loss functions, as well as dataset size requirements for semi-supervised matching. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Deep Learning; Deep Matching; Image Matching; Optical Remote Sensing; Semisupervised Learning; Synthetic Aperture Radar (SAR)","Data fusion; Deep learning; Geometrical optics; Image matching; Large dataset; Remote sensing; Semi-supervised learning; Synthetic aperture radar; Computer vision applications; Data scarcity; Large-scale datasets; Loss functions; Matching problems; Optical imagery; Optical remote-sensing imagery; Semi-supervised; Radar imaging"
"IMAGE-TO-IMAGE TRANSLATION for ENHANCED FEATURE MATCHING, IMAGE RETRIEVAL and VISUAL LOCALIZATION","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084672809&doi=10.5194%2fisprs-annals-IV-2-W7-111-2019&partnerID=40&md5=f80cb9abbd1eebbc825d347b56232dcb","The performance of machine learning and deep learning algorithms for image analysis depends significantly on the quantity and quality of the training data. The generation of annotated training data is often costly, time-consuming and laborious. Data augmentation is a powerful option to overcome these drawbacks. Therefore, we augment training data by rendering images with arbitrary poses from 3D models to increase the quantity of training images. These training images usually show artifacts and are of limited use for advanced image analysis. Therefore, we propose to use image-to-image translation to transform images from a rendered domain to a captured domain. We show that translated images in the captured domain are of higher quality than the rendered images. Moreover, we demonstrate that image-to-image translation based on rendered 3D models enhances the performance of common computer vision tasks, namely feature matching, image retrieval and visual localization. The experimental results clearly show the enhancement on translated images over rendered images for all investigated tasks. In addition to this, we present the advantages utilizing translated images over exclusively captured images for visual localization. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","3D Models; Convolutional Neural Networks; Data Augmentation; Feature Matching; Generative Adversarial Networks; Image Retrieval; Image-to-Image Translation; Visual Localization","Deep learning; Image analysis; Image retrieval; Learning algorithms; Quality control; Remote sensing; Rendering (computer graphics); Three dimensional computer graphics; Annotated training data; Data augmentation; Feature matching; Image translation; Rendered images; Training data; Training image; Visual localization; Image enhancement"
"DATA AUGMENTATION APPROACHES for SATELLITE IMAGE SUPER-RESOLUTION","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084668861&doi=10.5194%2fisprs-annals-IV-2-W7-47-2019&partnerID=40&md5=1042fc0c6a0dcf8cb89a572c5ded4620","Data augmentation is a well known technique that is frequently used in machine learning tasks to increase the number of training instances and hence decrease model over-fitting. In this paper we propose a data augmentation technique that can further boost the performance of satellite image super resolution tasks. A super-resolution convolutional neural network (SRCNN) was adopted as a state-of-the-art deep learning model to test the proposed data augmentation technique. Different augmentation techniques were studied to investigate their relative importance and accuracy gains. We categorized the augmentation methods into instance based and channel based augmentation methods. The former refers to the standard approach of creating new data instances through applying image transformations to the original images such as adding artificial noise, rotations and translations to training samples, while in the latter we fuse auxiliary channels (or custom bands) with each training instance, which helps the model learn useful representations. Fusing auxiliary derived channels to a satellite image RGB combination can be seen as a spectral-spatial fusion process as we explain later. Several experiments were carried out to evaluate the efficacy of the proposed fusion-based augmentation method compared with traditional data augmentation techniques such as rotation, flip and noisy training inputs. The reconstruction quality of the high resolution output was quantitatively evaluated using Peak-Signal-To-Noise-Ratio (PSNR) and qualitatively through visualisation of test samples before and after super-resolving. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Convolutional Neural Networks; Data Augmentation; Deep Learning; Landsat-8; Super Resolution","Convolutional neural networks; Deep learning; Optical resolving power; Quality control; Remote sensing; Satellites; Signal to noise ratio; Augmentation methods; Augmentation techniques; Auxiliary channel; Data augmentation; High-resolution output; Image transformations; Peak signal to noise ratio; Reconstruction quality; Learning systems"
"SEMANTIC SEGMENTATION of MANMADE LANDSCAPE STRUCTURES in DIGITAL TERRAIN MODELS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084680018&doi=10.5194%2fisprs-annals-IV-2-W7-87-2019&partnerID=40&md5=7a3f4ad99d1c67cde2d969246ba458b2","We explore the use of semantic segmentation in Digital Terrain Models (DTMS) for detecting manmade landscape structures in archaeological sites. DTM data are stored and processed as large matrices of depth 1 as opposed to depth 3 in RGB images. The matrices usually contain continuous real-valued information upper bound of which is not fixed, such as distance or height from a reference surface. This is different from RGB images that contain integer values in a fixed range of 0 to 255. Additionally, RGB images are usually stored in smaller multidimensional matrices, and are more suitable as inputs for a neural network while the large DTMs are necessary to be split into smaller sub-matrices to be used by neural networks. Thus, while the spatial information of pixels in RGB images are important only locally within a single image, for DTM data, they are important locally, within a single sub-matrix processed for neural network, and also globally, in relation to the neighboring sub-matrices. To cope with the two differences, we apply min-max normalization to each input matrix fed to the neural network, and use a slightly modified version of DeepLabv3+ model for semantic segmentation. We show that with the architecture change, and the preprocessing, better results are achieved. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Deep Learning; Digital Terrain Models; Laser Scanning; Object Detection; Semantic Segmentation","Matrix algebra; Remote sensing; Semantics; Archaeological site; Digital terrain model; Integer values; Man-made landscapes; Min-max normalizations; Multi-dimensional matrices; Semantic segmentation; Spatial informations; Color image processing"
"SEMANTIC LABELING and REFINEMENT of LIDAR POINT CLOUDS USING DEEP NEURAL NETWORK in URBAN AREAS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075333743&doi=10.5194%2fisprs-annals-IV-2-W7-63-2019&partnerID=40&md5=272df6b14c8f144b06e00e9ef510f9cd","In this paper, we propose a framework for obtaining semantic labels of LiDAR point clouds and refining the classification results by combining a deep neural network with a graph-structured smoothing technique. In general, the goal of the semantic scene analysis is to assign a semantic label to each point in the point cloud. Although various related researches have been reported, due to the complexity of urban areas, the semantic labeling of point clouds in urban areas is still a challenging task. In this paper, we address the issues of how to effectively extract features from each point and its local surrounding and how to refine the initial soft labels by considering contextual information in the spatial domain. Specifically, we improve the effectiveness of classification of point cloud in two aspects. Firstly, instead of utilizing handcrafted features as input for classification and refinement, the local context of a point is embedded into deep dimensional space and classified via a deep neural network (PointNet++), and simultaneously soft labels are obtained as initial results for next refinement. Secondly, the initial label probability set is improved by taking the context both in the spatial domain into consideration by constructing a graph structure, and the final labels are optimized by a graph cuts algorithm. To evaluate the performance of our proposed framework, experiments are conducted on a mobile laser scanning (MLS) point cloud dataset. We demonstrate that our approach can achieve higher accuracy in comparison to several commonly-used state-of-the-art baselines. The overall accuracy of our proposed method on TUM dataset can reach 85.38% for labeling eight semantic classes. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","deep learning; MLS; optimization; Point clouds; semantic labeling","Graph algorithms; Graph structures; Graphic methods; Image segmentation; Optical radar; Remote sensing; Semantic Web; Semantics; Classification results; Contextual information; Dimensional spaces; Lidar point clouds; Overall accuracies; Semantic labeling; Smoothing techniques; State of the art; Deep neural networks"
"Landslide mapping from multi-sensor data through improved change detection-based Markov random field","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067615144&doi=10.1016%2fj.rse.2019.111235&partnerID=40&md5=2c0cdfc40c2065f66a6a73c43eb15b7d","Accurate landslide inventory mapping is essential for quantitative hazard and risk assessment. Although multi-temporal change detection techniques have contributed greatly to landslide inventory preparation, it is still challenging to generate quality change detection images (CDIs) for accurate landslide mapping. The recently proposed change detection-based Markov random field (CDMRF) provides an effective approach for rapid mapping of landslides with minimum user interventions. However, when CDI is generated by change vector analysis (CVA) alone, the CDMRF method may suffer from noise especially when the pre- and post-event remote sensing images are acquired under different atmospheric, illumination, and phenological conditions. This paper improved such CDMRF approach by integrating normalized difference vegetation index (NDVI), principal component analysis (PCA), and independent component analysis (ICA) generated CDIs with MRF for landslide inventory mapping from multi-sensor data. To justify the effectiveness and applicability, the improved methods were applied to map rainfall-, typhoon-, and earthquake-triggered landslides from the pre- and post-event satellite images acquired by very high resolution QuickBird, high resolution FORMOSAT-2, and moderate resolution Sentinel-2. Moreover, they were tested on pre-event Landsat-8 and post-event Sentinel-2 datasets, indicating that they are operational for landslide inventory mapping from combined multi-temporal and multi-sensor data. The results demonstrate that the improved δNDVI-, PCA-, and ICA-based approaches perform much better than CVA-based CDMRF in terms of completeness, correctness, Kappa coefficient, and F-measures. To the best of our knowledge, it is the first time that NDVI, PCA, and ICA are integrated with MRF for landslide inventory mapping from multi-sensor data. It is anticipated that this research can be a starting point for developing new change detection techniques that can readily generate quality CDI and for applying advanced machine learning algorithms (e.g., deep learning) to automatic detection of natural hazards from multi-sensor time series data. © 2019 The Authors","Change detection; Independent component analysis; Landslide inventory mapping; Markov random field (MRF); Multi-sensor; NDVI; Principal component analysis","Deep learning; Hazards; Image acquisition; Image enhancement; Independent component analysis; Landslides; Learning algorithms; Machine learning; Magnetorheological fluids; Mapping; Markov processes; Remote sensing; Risk assessment; Change detection; Landslide inventories; Markov Random Fields; Multi sensor; NDVI; Principal component analysis; data set; detection method; earthquake trigger; hazard assessment; image resolution; landslide; machine learning; mapping method; principal component analysis; remote sensing; risk assessment; satellite data; satellite imagery; satellite sensor"
"Quantitative comparison between neural network- And SGM-based stereo matching","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085035125&doi=10.5194%2fisprs-archives-XLII-1-W2-17-2019&partnerID=40&md5=be7653e0aecb4229a4eb6deaa1541e1d","Over the last decades, various methods for three-dimensional detection of the environment have been developed and successfully used. This work considers classical stereo methods, which can determine depth information by the means of correspondence analysis on the basis of two pictures of a scene. Recently, neural networks have been used to solve correspondence analysis. These procedures came first places on corresponding benchmarks and are ahead of many already established solutions. In this work, images captured by the ZED camera are evaluated for accuracy of the depth maps generated by several approaches. This includes modern methods based on neural networks. © Authors 2019.","Deep learning; Stereo matching; ZED camera","Remote sensing; Correspondence analysis; Depth information; Depth Map; Quantitative comparison; Stereo matching; Stereo method; Three dimensional detection; Stereo image processing"
"Identifying urban villages from city-wide satellite imagery leveraging mask r-Cnn","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072880805&doi=10.1145%2f3341162.3355269&partnerID=40&md5=be8b4e7fb45fbc1a24c5f515edc7d1ec","Urban villages emerge with the rapid urbanization process in many developing countries, and bring serious social and economic challenges to urban authorities, such as overcrowding and low living standards. A comprehensive understanding of the locations and regional boundaries of urban villages in a city is crucial for urban planning and management, especially when urban authorities need to renovate these regions. Traditional methods greatly rely on surveys and investigations of city planners, which consumes substantial time and human labor. In this work, we propose a low-cost and automatic framework to accurately identify urban villages from high-resolution remote sensing satellite imagery. Specifically, we leverage the Mask Regional Convolutional Neural Network (Mask-RCNN) model for end-to-end urban village detection and segmentation. We evaluate our framework on the city-wide satellite imagery of Xiamen, China. Results show that our framework successfully detects 87.18% of the urban villages in the city, and accurately segments their regional boundaries with an IoU of 74.48%. © 2019 Association for Computing Machinery.","Deep Learning; Image Segmentation; Mask-RCNN; Urban Computing; Urban Village","Deep learning; Developing countries; Image segmentation; Neural networks; Remote sensing; Rural areas; Ubiquitous computing; Wearable computers; City planners; Convolutional neural network; Economic challenges; High resolution remote sensing; Living standards; Rapid urbanization process; Urban computing; Urban Village; Satellite imagery"
"Research on building extraction of multi-temporal remote sensing image based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087106142&doi=10.1364%2fFIO.2019.JW3A.102&partnerID=40&md5=29d0c7462759ee971c899ac0095b8153","The building texture features of multi-temporal remote sensing images were introduced into the VGG neural network. Building extraction accuracy of multi-temporal images can reach 90.6%, which is significantly higher than that of single-phase images. © OSA 2019. The Author(s).",,"Extraction; Image processing; Phase shifters; Remote sensing; Textures; Building extraction; Multi-temporal image; Multi-temporal remote sensing; Single phase; Texture features; Deep learning"
"Using vehicle synthesis generative adversarial networks to improve vehicle detection in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072573692&doi=10.3390%2fijgi8090390&partnerID=40&md5=f57165d2787a747dea2e2c6df41cae6d","Vehicle detection based on very high-resolution (VHR) remote sensing images is beneficial in many fields such as military surveillance, traffic control, and social/economic studies. However, intricate details about the vehicle and the surrounding background provided by VHR images require sophisticated analysis based on massive data samples, though the number of reliable labeled training data is limited. In practice, data augmentation is often leveraged to solve this conflict. The traditional data augmentation strategy uses a combination of rotation, scaling, and flipping transformations, etc., and has limited capabilities in capturing the essence of feature distribution and proving data diversity. In this study, we propose a learning method named Vehicle Synthesis Generative Adversarial Networks (VS-GANs) to generate annotated vehicles from remote sensing images. The proposed framework has one generator and two discriminators, which try to synthesize realistic vehicles and learn the background context simultaneously. The method can quickly generate high-quality annotated vehicle data samples and greatly helps in the training of vehicle detectors. Experimental results show that the proposed framework can synthesize vehicles and their background images with variations and different levels of details. Compared with traditional data augmentation methods, the proposed method significantly improves the generalization capability of vehicle detectors. Finally, the contribution of VS-GANs to vehicle detection in VHR remote sensing images was proved in experiments conducted on UCAS-AOD and NWPU VHR-10 datasets using up-to-date target detection frameworks. © 2019 by the authors.","Data augmentation; Deep learning; Generative adversarial network; Remote sensing; Vehicle detection",
"Hyperspectral imagery classification with deep metric learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065777929&doi=10.1016%2fj.neucom.2019.05.019&partnerID=40&md5=4c51b5cfd24638e77047b97f00328279","The high dimensionality of hyperspectral imagery often introduces challenge for the conventional data analysis techniques. In order to improve the classification performance of hyperspectral imagery, metric learning is often introduced to assign small distances between samples from the same class and large distances from different class. However, most of the traditional metric learning methods only adopt linear transformations, which cannot capture the complex nonlinear relationships between high dimensional samples. Inspired by the successful application of deep learning for the classification of hyperspectral imagery. In this paper, the deep neural network is introduced to learn the discriminating metric for the classification of hyperspectral images. In order to improve the reliability of classification results, the spectral and spatial information are combined with weighted classification probabilities. Experimental results demonstrate that the proposed method achieves satisfactory classification performance when compared with other metric learning methods or deep models. © 2019 Elsevier B.V.","Classification; Deep metric learning; Feature fusion; Hyperspectral imagery","Deep neural networks; Image classification; Image enhancement; Linear transformations; Mathematical transformations; Remote sensing; Spectroscopy; Classification performance; Classification results; Data analysis techniques; Feature fusion; Hyper-spectral imageries; Hyperspectral imagery classifications; Metric learning; Non-linear relationships; Classification (of information); article; deep learning; human; human experiment; imagery; probability; reliability"
"Survey of deep-learning approaches for remote sensing observation enhancement","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072555757&doi=10.3390%2fs19183929&partnerID=40&md5=857b0d03e174ccd53dc03eb73ca52949","Deep Learning, and Deep Neural Networks in particular, have established themselves as the new norm in signal and data processing, achieving state-of-the-art performance in image, audio, and natural language understanding. In remote sensing, a large body of research has been devoted to the application of deep learning for typical supervised learning tasks such as classification. Less yet equally important effort has also been allocated to addressing the challenges associated with the enhancement of low-quality observations from remote sensing platforms. Addressing such channels is of paramount importance, both in itself, since high-altitude imaging, environmental conditions, and imaging systems trade-offs lead to low-quality observation, as well as to facilitate subsequent analysis, such as classification and detection. In this paper, we provide a comprehensive review of deep-learning methods for the enhancement of remote sensing observations, focusing on critical tasks including single and multi-band super-resolution, denoising, restoration, pan-sharpening, and fusion, among others. In addition to the detailed analysis and comparison of recently presented approaches, different research avenues which could be explored in the future are also discussed. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Deep learning; Denoising; Earth observations; Fusion; Generative adversarial networks; Pan-sharpening; Satellite imaging; Super-resolution","Chemical detection; Data handling; Deep learning; Deep neural networks; Economic and social effects; Fusion reactions; Neural networks; Optical resolving power; Quality control; Adversarial networks; Convolutional neural network; De-noising; Earth observations; Pan-sharpening; Satellite imaging; Super resolution; Remote sensing"
"Deep learning based dense matching for aerial remote sensing images [基于深度学习的航空遥感影像密集匹配]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073675920&doi=10.11947%2fj.AGCS.2019.20180247&partnerID=40&md5=c363241ad8aab3500caab5f7cefd3a5c","This work studied that the application of deep learning based stereo methods in aerial remote sensing images, including its performance evaluation, the comparison with classical methods and generalization ability estimation.Three convolution neural networks are applied, MC-CNN(matching cost convolutional neural network), GC-Net(geometry and context network) and DispNet(disparity estimation network), on aerial stereo image pairs. The results are compared with SGM (semi-global matching) and a commercial software SURE. Secondly, the generalization ability of the MC-CNN and GC-Net are evaluated with models pretrained on other datasets. Finally, fine tuning on a small number of target training data with pretrained models are compared to direct training. Three sets of aerial images and two open-source street data sets are used for test. Experiments show that: firstly, deep learning methods perform slightly better than traditional methods; secondly, both GC-Net and MC-CNN have demonstrated good generalization ability, and can get satisfactory 3PE (3-pixel-error) results on aerial images using a model pretrained on available stereo benchmarks; thirdly, when the training samples in target dataset are insufficient, the strategy of fine-tuning on a pretrained model can improve the effect of direct training. © 2019, Surveying and Mapping Press. All right reserved.","Aerial images; Convolutional neural network; Deep learning; Dense matching; Stereo matching","Antennas; Convolution; Deep learning; Deep neural networks; Image enhancement; Neural networks; Open source software; Remote sensing; Aerial images; Aerial remote sensing; Convolution neural network; Convolutional neural network; Dense matching; Disparity estimations; Generalization ability; Stereo matching; Stereo image processing; aerial photography; artificial neural network; data set; estimation method; experimental study; performance assessment; remote sensing; software; stereo image"
"High resolution remote sensing image ship target detection technology based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073066017&doi=10.1007%2fs11801-019-9003-7&partnerID=40&md5=95380477c5663d58384d1850b647261e","With the development of China’s high-resolution special projects and the rapid development of commercial satellite, the resolution of the mainstream satellite remote sensing images has reached the sub-meter level. Ship target detection in high-resolution remote sensing images has always been the focus and hotspot in image understanding. Real-time and effective detection of ships play an extremely important role in marine transportation, military operations and so on. Firstly, the full-factor ship target sample library of high-resolution image is synthetically prepared. Then, based on the Faster R-CNN framework and Resnet model, optimize the parameters of the model to achieve accurate results. The simulation results show that the detection model trained in this paper has the highest recall rate of 98.01% and false alarm rate of 0.83%. It can be applied to the practical application of ship detection in remote sensing images. © 2019, Tianjin University of Technology and Springer-Verlag GmbH Germany, part of Springer Nature.",,"Military operations; Object recognition; Remote sensing; Ships; Commercial satellites; Detection models; Detection technology; High resolution image; High resolution remote sensing images; Marine transportation; Remote sensing images; Satellite remote sensing; Deep learning"
"Deep learning with multi-scale feature fusion in remote sensing for automatic oceanic eddy detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054181910&doi=10.1016%2fj.inffus.2018.09.006&partnerID=40&md5=ec61a80d4b72d48d197fc67c0d3d594a","Oceanic eddies are ubiquitous in global oceans and play a major role in ocean energy transfer and nutrients distribution, thus being significant for understanding ocean current circulation and marine climate change. They are characterized by a combination of high-speed vertical rotations and horizontal movements, leading to irregular three-dimensional spiral structures. While the ability to detect eddies automatically and remotely is crucial to monitoring important spatial–temporal dynamics, existing methods are inaccurate because eddies are highly dynamic and the underlying physical processes are not well understood. Typically, remote sensing is used to detect eddies based on physical parameters, geometrics or other handcrafted features. In this paper, we show how Deep Learning may be used to reliably extract higher-level features and then fuse multi-scale features to identify eddies, regardless of their structures and scales. We learn eddy features using two principal component analysis convolutional layers, then perform a non-linear transformation of the features through a binary hashing layer and block-wise histograms. To handle the difficult problem of spatial variability across synthetic aperture radar (SAR) images, we introduce a spatial pyramid model to allow multi-scale features fusion. Finally, a linear support vector machine classifier recognizes the eddies. Our method, dubbed DeepEddy, is benchmarked against a dataset of 20,000 SAR image samples, achieving a 97.8 ± 1% accuracy of detection. © 2018 Elsevier B.V.","Deep learning; Eddy detection; Feature fusion; Remote sensing; SAR images","Climate change; Energy transfer; Feature extraction; Image fusion; Linear transformations; Mathematical transformations; Nutrients; Oceanography; Principal component analysis; Radar imaging; Remote sensing; Synthetic aperture radar; Eddy detection; Feature fusion; Horizontal movements; Linear Support Vector Machines; Multi-scale features; Non-linear transformations; SAR Images; Synthetic aperture radar (SAR) images; Deep learning"
"Transferred deep learning-based change detection in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071920788&doi=10.1109%2fTGRS.2019.2909781&partnerID=40&md5=21505dc5c7717970ab405abe221abd6f","Supervised deep neural networks (DNNs) have been extensively used in diverse tasks. Generally, training such DNNs with superior performance requires a large amount of labeled data. However, it is time-consuming and expensive to manually label the data, especially for tasks in remote sensing, e.g., change detection. The situation motivates us to resort to the existing related images with labels, from which the concept of change can be adapted to new images. However, the distributions of the related labeled images (source domain) and unlabeled new images (target domain) are similar but not identical. It impedes a change detection model learned from source domains being well applied to the target domain. In this paper, we propose a transferred deep learning-based change detection framework to solve this problem. It consists of pretraining and fine-tuning stages. In the pretraining process, we propose two tasks to be learned simultaneously, namely, change detection for the source domain with labels and reconstruction of the unlabeled target data. The auxiliary task aims to reconstruct the difference image (DI) for the target domain. DI is an effective feature, such that the auxiliary task is of much relevance to change detection. The lower layers are shared between these two tasks in the training process. It mitigates the distribution discrepancy between the source and target domains and makes the concept of change from the source domain adapt to the target domain. In addition, we evaluate three modes of the U-net architecture to merge the information for a pair of patches. To fine-tune the change detection network (CDN) for the target domain, two strategies are exploited to select the pixels that have a high possibility of being correctly classified by an unsupervised approach. The proposed method demonstrates an excellent capacity for adapting the concept of change from the source domain to the target domain. It outperforms the state-of-the-art change detection methods via experimental results on real remote sensing data sets. © 1980-2012 IEEE.","Adaptation; change detection; deep neural networks (DNNs); reconstruction; remote sensing","Image reconstruction; Remote sensing; Adaptation; Change detection; Difference images; NET architecture; Remote sensing data; Remote sensing images; Training process; Unsupervised approaches; Deep neural networks"
"Deep learning for hyperspectral image classification: An overview","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071550451&doi=10.1109%2fTGRS.2019.2907932&partnerID=40&md5=e1b9c8ef66b0c4631fc7ce8638790f77","Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework that divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral-spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments. © 1980-2012 IEEE.","Classification; deep learning; feature extraction; hyperspectral image (HSI)","Classification (of information); Extraction; Feature extraction; Hyperspectral imaging; Image classification; Machine learning; Remote sensing; Spectroscopy; Classification methods; Classification performance; Complex characteristics; Hyperspectral Data; Machine learning methods; Nonlinear problems; Nonlinear relations; Spectral information; Deep learning"
"Deep Learning for Building Density Estimation in Remotely Sensed Imagery [Uzaktan Algilanan Görüntülerde Bina Yogunlugu Kestirimi Ǐin Derin Ögrenme]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076199572&doi=10.1109%2fUBMK.2019.8907133&partnerID=40&md5=fdbc25723e0681d9ba3e1ddb7bb7222b","This paper is about point-wise estimation of building density from remote sensing optical imagery using deep learning methods. Convolutional neural network (CNN) based deep learning approaches are used for this work. Pre-trained VGG-16 and FCN-8s deep architectures are adapted to the problem and fine-tuned with additional training. Estimated values are used to generate building heat maps in urban areas. Comparative simulation results of the two architectures reveal that accurate density estimation is possible without the need for detailed maps of building locations during supervised training. © 2019 IEEE.","building density estimation; deep learning; remote sensing","Buildings; Network architecture; Neural networks; Remote sensing; Building densities; Comparative simulation; Convolutional neural network; Deep architectures; Density estimation; Learning approach; Remotely sensed imagery; Supervised trainings; Deep learning"
"Deep Learning for Atmospheric Cloud Image Segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075630466&doi=10.1109%2fELIT.2019.8892285&partnerID=40&md5=94eb9b4bb57110766e6002812fbb04eb","As a result of the work, the main segmentation methods were analyzed when applied to images of atmospheric clouds obtained by remote sensing methods. It is proposed the approach which is a further development of the deep learning model based on CNN class U-net. The quality of cloud image segmentation using various methods based on the Intersection over Union (IoU) criterion is presented. The evaluations of the advantages and disadvantages of the proposed segmentation method are provided. Experimental studies have shown the feasibility of using neural network segmentation with deep learning, which allows to localize the clouds in the image. The results can be used in the systems of monitoring and classification of the regions of Ukraine on the distribution of cloud masses in the seasons based on images of satellite weather maps. © 2019 IEEE.","atmospheric clouds; deep learning; Intersection over Union; remote sensing; U-net","Image segmentation; Remote sensing; Learning models; Network segmentation; Segmentation methods; Ukraine; Weather maps; Deep learning"
"Remote sensing single-image superresolution based on a deep compendium model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081923103&doi=10.1109%2fLGRS.2019.2899576&partnerID=40&md5=06d39416819f4c80a74552c120b633cc","This letter introduces a novel remote sensing single-image superresolution (SR) architecture based on a deep efficient compendium model. The current deep learning-based SR trend stands for using deeper networks to improve the performance. However, this practice often results in the degradation of visual results. To address this issue, the proposed approach harmonizes several different improvements on the network design to achieve state-of-the-art performance when superresolving remote sensing imagery. On the one hand, the proposal combines residual units and skip connections to extract more informative features on both local and global image areas. On the other hand, it makes use of parallelized 1×1 convolutional filters (network in network) to reconstruct the superresolved result while reducing the information loss through the network. Our experiments, conducted using seven different SR methods over the well-known UC Merced remote sensing data set, and two additional GaoFen-2 test images, show that the proposed model is able to provide competitive advantages. © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.","Deep learning (DL); Remote sensing; Superresolution (SR)","Competition; Deep learning; Image enhancement; Optical resolving power; Statistical tests; Architecture-based; Competitive advantage; Information loss; Remote sensing data; Remote sensing imagery; State-of-the-art performance; Superresolution; Superresolving; Remote sensing; algorithm; image resolution; numerical model; remote sensing; satellite data; satellite imagery"
"Deep Learning for Super-Resolution of Unregistered Multi-Temporal Satellite Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075676262&doi=10.1109%2fWHISPERS.2019.8920910&partnerID=40&md5=d0959582745d97b432076a638e5cf77a","Recently, convolutional neural networks (CNN) have been successfully applied to many remote sensing tasks. However, deep learning for multi-image superresolution from multitemporal imagery has received little attention so far. We propose a residual CNN that exploits both spatial and temporal correlations in the low-resolution image set by using 3D convolutional layers to combine multiple images from the same scene. The experiments have been carried out using a dataset of PROBA-V satellite ground images, composed of several low-resolution and high-resolution images taken at different times from instruments on the same platform, in the context of a challenge issued by the European Space Agency. © 2019 IEEE.","convolutional neural networks; Multi-image superresolution; multi-temporal images","Convolution; Hyperspectral imaging; Neural networks; Optical resolving power; Remote sensing; Space optics; Spectroscopy; Convolutional neural network; High resolution image; Low resolution images; Multi-images; Multi-temporal image; Multi-temporal imageries; Multi-temporal satellite images; Spatial and temporal correlation; Deep learning"
"Sea Fog Detection Using U-Net Deep Learning Model Based on Modis Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077603268&doi=10.1109%2fWHISPERS.2019.8920979&partnerID=40&md5=403146ae08d8b3eb57b84b0d1cd84392","Sea fog can have both negative and positive impacts on humans life. At present, remote sensing has become the main means of long-term and large-scale observation of sea fog. With the improvement of spectral resolution and increase of data volume, the traditional threshold method is simple and convenient as the main method of current sea fog detection, but it's not flexible and accurate enough which causes people need a more automated and intelligent algorithm to achieve efficient sea fog detection. In this article, we use the U-Net deep learning model to construct the sea fog detection model for MODIS multi-spectral images. The main steps include? (1) Data preprocessing, including the PCA method for dimensionality reduction of data; (2) Manual samples extraction with CALIPSO data assist; (3) Construction and training of U-Net sea fog detection model. The experimental results show that the U-Net model can effectively and machine learning method has good potential in sea fog detection. © 2019 IEEE.","CALIPSO; Deep Learning; Detection; MODIS; Sea fog","Error detection; Fog; Hyperspectral imaging; Radiometers; Remote sensing; Spectroscopy; CALIPSO; Data preprocessing; Dimensionality reduction; Intelligent Algorithms; Machine learning methods; MODIS; Multispectral images; Sea fog; Deep learning"
"A review on IoT deep learning UAV systems for autonomous obstacle detection and collision avoidance","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072642705&doi=10.3390%2frs11182144&partnerID=40&md5=0491c21500f8a5c37a0b93e191c79c58","Advances in Unmanned Aerial Vehicles (UAVs), also known as drones, offer unprecedented opportunities to boost a wide array of large-scale Internet of Things (IoT) applications. Nevertheless, UAV platforms still face important limitations mainly related to autonomy and weight that impact their remote sensing capabilities when capturing and processing the data required for developing autonomous and robust real-time obstacle detection and avoidance systems. In this regard, Deep Learning (DL) techniques have arisen as a promising alternative for improving real-time obstacle detection and collision avoidance for highly autonomous UAVs. This article reviews the most recent developments on DL Unmanned Aerial Systems (UASs) and provides a detailed explanation on the main DL techniques. Moreover, the latest DL-UAV communication architectures are studied and their most common hardware is analyzed. Furthermore, this article enumerates the most relevant open challenges for current DL-UAV solutions, thus allowing future researchers to define a roadmap for devising the new generation affordable autonomous DL-UAV IoT solutions. © 2019 by the authors.","Autonomous UAV; Collision avoidance; Deep learning; Drone; Image processing; Large-scale datasets; Obstacle detection; Remote sensing; UAS; UAV","Aircraft detection; Antennas; Collision avoidance; Data handling; Drones; Image processing; Internet of things; Large dataset; Obstacle detectors; Real time systems; Remote sensing; Unmanned aerial vehicles (UAV); Autonomous UAV; Autonomous UAVs; Communication architectures; Large scale Internet; Large-scale datasets; Obstacle detection; UAV platform; Unmanned aerial systems; Deep learning"
"An ensemble learning approach for the classification of remote sensing scenes based on covariance pooling of CNN features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075605937&doi=10.23919%2fEUSIPCO.2019.8902561&partnerID=40&md5=0acb5a82ef417ac4f302dec11c045554","This paper aims at presenting a novel ensemble learning approach based on the concept of covariance pooling of CNN features issued from a pretrained model. Starting from a supervised classification algorithm, named multilayer stacked covariance pooling (MSCP), which exploits simultaneously second order statistics and deep learning features, we propose an alternative strategy which employs an ensemble learning approach among the stacked convolutional feature maps. The aggregation of multiple learning algorithm decisions, produced by different stacked subsets, permits to obtain a better predictive classification performance. An application for the classification of large scale remote sensing images is next proposed. The experimental results, conducted on two challenging datasets, namely UC Merced and AID datasets, improve the classification accuracy while maintaining a low computation time. This confirms, besides the interest of exploiting second order statistics, the benefit of adopting an ensemble learning approach. © 2019 IEEE","Covariance pooling; Ensemble learning approach; Multilayer feature maps; Pretrained CNN models; Remote sensing scene classification","Classification (of information); Deep learning; Multilayers; Remote sensing; Signal processing; CNN models; Covariance pooling; Ensemble learning approach; Feature map; Scene classification; Learning algorithms"
"Predicting poverty index using deep learning on remote sensing and household data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073718431&doi=10.35940%2fijrte.C3918.098319&partnerID=40&md5=89987b23ebb5f499bf64c79a2b45a857","The main challenge for approving and implementing policies aiming at sustainable development of country is correct prediction of socioeconomic condition. Deep learning algorithms in recent researches have been identified as potential resource to be applied in this domain. Another challenge is availability of sufficient amount of data which is solved using transfer learning in Convolutional Neural Network (CNN). We used pre-trained Inception Net-v3 and Ridge regression model to estimate poverty level using publicly available dataset comprising of daylight images, nightlight images and survey data. Each cluster of samples contains households between 1-28. Its mean is 21.09, median 21 and a standard deviation is 1.36. Proposed deep learning inspired model estimates wealth-score for 28393 clusters with an r value i.e. Pearson Correlation Coefficient of 0.73, signifying r2 value i.e. Coefficient of determination of 0.54. It shows that daytime satellite images, nightlight intensity and demographic data available can be utilized for precise evaluations about the spatial scattering of monetary prosperity crosswise over different nations. © BEIESP.","Convolutional Neural Network; Daylight; Nightlight; Regression; Satellite Images; Survey Data",
"Enhanced feature representation in detection for optical remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072642849&doi=10.3390%2frs11182095&partnerID=40&md5=a8f04aeb75d26e52a01688ba19bda043","In recent years, deep learning has led to a remarkable breakthrough in object detection in remote sensing images. In practice, two-stage detectors perform well regarding detection accuracy but are slow. On the other hand, one-stage detectors integrate the detection pipeline of two-stage detectors to simplify the detection process, and are faster, but with lower detection accuracy. Enhancing the capability of feature representation may be a way to improve the detection accuracy of one-stage detectors. For this goal, this paper proposes a novel one-stage detector with enhanced capability of feature representation. The enhanced capability benefits from two proposed structures: dual top-down module and dense-connected inception module. The former efficiently utilizes multi-scale features from multiple layers of the backbone network. The latter both widens and deepens the network to enhance the ability of feature representation with limited extra computational cost. To evaluate the effectiveness of proposed structures, we conducted experiments on horizontal bounding box detection tasks on the challenging DOTA dataset and gained 73.49% mean Average Precision (mAP), achieving state-of-the-art performance. Furthermore, our method ran significantly faster than the best public two-stage detector on the DOTA dataset. © 2019 by the authors.","One-stage detector; Receptive field; Remote sensing; Top down module","Deep learning; Image enhancement; Object detection; Remote sensing; Bounding box detections; Feature representation; Multi-scale features; Optical remote sensing; Receptive fields; Remote sensing images; State-of-the-art performance; Topdown; Feature extraction"
"Corrections to: Recognizing Global Reservoirs From Landsat 8 Images: A Deep Learning Approach (IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (2019) 12:9 DOI: 10.1109/JSTARS.2019.2929601)","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073171889&doi=10.1109%2fJSTARS.2019.2939941&partnerID=40&md5=9b50525a617eaf28515a1a521c871d62","In [1], the affiliation for Y. Hong should be updated as follows: Y. Hong is with the School of Civil Engineering and Environmental Science, University of Oklahoma, Norman, OK 73019 USA and also with GIS, Peking University, Beijing 100871, China (e-mail: yanghong@ou.edu). The bio of Y. Hong should be updated as follows: Yang Hong received the B.S. andM.S. degrees from PekingUniversity, Beijing, China, in 1996 and 1999, respectively, and the Ph.D. degree from the University of Arizona, Tucson, AZ, USA, in 2003. His research interests include radar/satellite remote sensing of thewater cycle, hydrology and water resource, hydrometeorology, and hydroclimatology. © 2019 IEEE.",,
"Development and evaluation of a deep learning model for real-time ground vehicle semantic segmentation from UAV-based thermal infrared imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069676654&doi=10.1016%2fj.isprsjprs.2019.07.009&partnerID=40&md5=d8b4eeb8559ce9ef54ee3602c006ddaa","Real-time unmanned aerial vehicles (UAVs)-based thermal infrared images processing, due to high spatial resolution and knowledge of the various infrared radiant energy level distribution of solid bodies, has important applications such as monitoring and control of the various phenomena in different natural situations. One of these applications is monitoring the ground vehicles in cities by using detection or semantic segmentation of them in the thermal images. In this research, our purpose is to improve the performance of deep learning combined model by using Gaussian-Bernoulli Restricted Boltzmann Machine (GB-RBM) specifications for the segmentation of the ground vehicles from UAV-based thermal infrared imagery. The proposed model is studied in three steps. First, designing the proposed model by using an encoder-decoder structure and addition of extracted features from convolutional layers and restricted Boltzmann machine in the network. Second, the implementation of the research goals on four sets of UAV-based thermal infrared imagery named NPU_CS_UAV_IR_DATA that was collected from some streets of China by using FLIR TAU2 thermal infrared sensor in 2017. Finally, analyzing the performance of the proposed model by using five state-of-the-art models in semantic segmentation. The results evaluated the performance of the proposed model as a robust model with the average precision and average processing time of approximately 0.97, and 19.73 s for all datasets, respectively. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Gaussian-Bernoulli Restricted Boltzmann Machine; Ground vehicle; Semantic segmentation; UAV-based thermal infrared imagery","Antennas; Ground vehicles; Image enhancement; Image segmentation; Infrared detectors; Infrared imaging; Infrared radiation; Semantics; Unmanned aerial vehicles (UAV); Bernoulli; High spatial resolution; Monitoring and control; Restricted boltzmann machine; Semantic segmentation; Thermal infrared imagery; Thermal infrared images; Thermal infrared sensors; Deep learning; algorithm; image processing; infrared imagery; machinery; model; satellite imagery; satellite sensor; segmentation; spatial resolution; unmanned vehicle; China"
"A Summary of Super-Resolution for Satellite Videos Via Learning-Based Methods","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077572399&doi=10.1109%2fWHISPERS.2019.8920882&partnerID=40&md5=541507c93cc3f4dfa64c3f00efd7d221","With the development of remote sensing techniques, remote sensing data can be obtained with higher spatial, higher spectral, and higher temporal resolution. In addition, to get higher spatial resolution, super-resolution for increasing spatial resolution is getting special attention. In this paper, we will focus on some classical learning-based superresolution methods to investigate the adaptability for satellite videos with low imaging quality. Methods include sparse representation, collaborative representation, and deep learning methods. Experiments show that learning-based methods can perform well for single-frame super-resolution for satellite videos. Methods based on deep learning show higher PSNR and SSIM. And multi-frame super-resolution will be good for moving objects. However, it may also bring negative influence for a stationary scene, which is caused by low satellite video quality, such as winkling noise, a vibration of a camera, overexposure of metals. © 2019 IEEE.","Deep Learning.; Dictionary Learning; Satellite Videos; Super-Resolution","Hyperspectral imaging; Image resolution; Optical resolving power; Remote sensing; Satellites; Spectroscopy; Collaborative representations; Dictionary learning; Learning-based methods; Remote sensing techniques; Single frame super resolutions; Sparse representation; Super resolution; Superresolution methods; Deep learning"
"Semisupervised Hyperspectral Image Classification Using Deep Features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073166053&doi=10.1109%2fJSTARS.2019.2921033&partnerID=40&md5=2213a5e036cbbb87384c60a088b4f6fe","As in other remote-sensing applications, collecting ground-truth information from the earth's surface is expensive and time-consuming process for hyperspectral imaging. In this study, a deep learning-based semisupervised learning framework is proposed to solve this small labeled sample size problem. The main contribution of this study is the construction of a deep learning model for each hyperspectral sensor type that can be used for data obtained from these sensors. In the proposed framework, the ""trained base model"" is obtained with any dataset from a hyperspectral sensor, and fine-tuned and evaluated with another dataset. In this way, a general deep model is developed for extracting deep features which can be linearly classified or clustered. The system is evaluated with three different clustering techniques, the modified k-means, subtractive, and mean-shift clustering, for selecting initial representative labeled training samples comparatively. Another contribution of this study is to exploit the labeled and unlabeled sample information with linear transductive support vector machines. The proposed semisupervised learning framework is proven by the experimental results using different number of small sample sizes. © 2019 IEEE.","Deep features; deep learning (DL); fine-tuning; hyperspectral images; semisupervised learning (SSL); transductive support vector machines (TSVM)","Hyperspectral imaging; Image classification; K-means clustering; Remote sensing; Sampling; Spectroscopy; Supervised learning; Support vector machines; Clustering techniques; Deep features; Fine tuning; Mean-Shift Clustering; Remote sensing applications; Semi- supervised learning; Semi-supervised learning (SSL); Transductive support vector machine; Deep learning; cluster analysis; data set; image classification; numerical model; remote sensing; sampling; supervised learning; support vector machine"
"A Tool for Bridge Detection in Major Infrastructure Works Using Satellite Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074879481&doi=10.1109%2fWVC.2019.8876942&partnerID=40&md5=b78fa0ae3dfc45bd30b9b85c2d6913b2","The identification of bridges in major infrastructure works is crucial to provide information about the status of these constructions and support possible decision-making processes. Typically, this identification is performed by human agents that must detect the bridges into large-scale datasets, analyzing image by image, a time-consuming task. In this paper, we propose a novel tool to perform bridge detection and identification in large-scale remote sensing datasets. This tool implements a deep learning-based algorithm, the Faster R-CNN (Regions with CNN features), a technique that is the current state-of-the-art for many object detection and identification applications. Since deep training usually requires a lot of data, we also created a bridge image dataset, composed of remote sensing images from around the globe. The proposed tool was encapsulated into an ArcGIS plugin in order to facilitate its use by non-programmer users. © 2019 IEEE.","Bridge Detection; Deep Learning; Remote Sensing","Computer vision; Decision making; Deep learning; Large dataset; Object detection; Remote sensing; Bridge detections; Decision making process; Detection and identifications; Large-scale datasets; Learning-based algorithms; Remote sensing images; Satellite images; Time-consuming tasks; Bridges"
"Discriminative Spectral-Spatial Attention-Aware Residual Network for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077556205&doi=10.1109%2fWHISPERS.2019.8921022&partnerID=40&md5=b4b457726e80a4fe3d5ff037bf8dab1e","Convolutional neural networks (CNNs) have been widely used in remote sensing image analysis, significantly improving the state-of-the-art. In this paper, we present a novel deep residual network based on spectral-spatial attention (DS2 A-RN) for classification of hyperspectral images. First, we propose an efficient residual block allowing 3D cube inputs and consisting of spectral attention and spatial attention to simultaneously model the explicit relationship between spectral bands and neighboring pixels. Second, a center loss is introduced to combine with softmax loss to enable our model to learn discriminative features by encouraging inter-class separability and intra-class compactness. We evaluate our method for three real hyperspectral images and compare with many existing deep learning methods, showing that the proposed method can achieve state-of-the-art classification performance. © 2019 IEEE.","attention mechanism; center loss; Deep residual networks; hyperspectral image classification","Deep learning; Hyperspectral imaging; Image enhancement; Neural networks; Remote sensing; Software architecture; Spectroscopy; Attention mechanisms; Classification performance; Convolutional neural network; Discriminative features; Learning methods; Remote sensing images; Spatial attention; State of the art; Image classification"
"Automatic design of convolutional neural network for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072048106&doi=10.1109%2fTGRS.2019.2910603&partnerID=40&md5=fc79e971967559ed57834db8bcec661e","Hyperspectral image (HSI) classification is a core task in the remote sensing community, and recently, deep learning-based methods have shown their capability of accurate classification of HSIs. Among the deep learning-based methods, deep convolutional neural networks (CNNs) have been widely used for the HSI classification. In order to obtain a good classification performance, substantial efforts are required to design a proper deep learning architecture. Furthermore, the manually designed architecture may not fit a specific data set very well. In this paper, the idea of automatic CNN for the HSI classification is proposed for the first time. First, a number of operations, including convolution, pooling, identity, and batch normalization, are selected. Then, a gradient descent-based search algorithm is used to effectively find the optimal deep architecture that is evaluated on the validation data set. After that, the best CNN architecture is selected as the model for the HSI classification. Specifically, the automatic 1-D Auto-CNN and 3-D Auto-CNN are used as spectral and spectral-spatial HSI classifiers, respectively. Furthermore, the cutout is introduced as a regularization technique for the HSI spectral-spatial classification to further improve the classification accuracy. The experiments on four widely used hyperspectral data sets (i.e., Salinas, Pavia University, Kennedy Space Center, and Indiana Pines) show that the automatically designed data-dependent CNNs obtain competitive classification accuracy compared with the state-of-the-art methods. In addition, the automatic design of the deep learning architecture opens a new window for future research, showing the huge potential of using neural architectures' optimization capabilities for the accurate HSI classification. © 1980-2012 IEEE.","Convolutional neural network (CNN); deep learning; hyperspectral image (HSI) classification; neural architecture search (NAS)","Classification (of information); Convolution; Deep learning; Deep neural networks; Gradient methods; Network architecture; Neural networks; Optimization; Remote sensing; Space platforms; Spectroscopy; Classification accuracy; Classification performance; Convolutional neural network; Neural architectures; Optimization capabilities; Regularization technique; Spectral-spatial classification; State-of-the-art methods; Image classification"
"Nighttime reflectance generation in the visible band of satellites","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072642592&doi=10.3390%2frs11182087&partnerID=40&md5=d821229aba4b2411be92dd0c16ca0e8b","Visible (VIS) bands, such as the 0.675 μmband in geostationary satellite remote sensing, have played an important role in monitoring and analyzing weather and climate change during the past few decades with coarse spatial and high temporal resolution. Recently, many deep learning techniques have been developed and applied in a variety of applications and research fields. In this study, we developed a deep-learning-based model to generate non-existent nighttime VIS satellite images using the Conditional Generative Adversarial Nets (CGAN) technique. For our CGAN-based model training and validation, we used the daytime image data sets of reflectance in the Communication, Ocean and Meteorological Satellite / Meteorological Imager (COMS/MI) VIS (0.675 μm) band and radiance in the longwave infrared (10.8 μm) band of the COMS/MI sensor over five years (2012 to 2017). Our results show high accuracy (bias = -2.41 and root mean square error (RMSE) = 36.85 during summer, bias = -0.21 and RMSE = 33.02 during winter) and correlation (correlation coefficient (CC) = 0.88 during summer, CC = 0.89 during winter) of values between the observed images and the CGAN-generated images for the COMS VIS band. Consequently, our CGAN-based model can be effectively used in a variety of meteorological applications, such as cloud, fog, and typhoon analyses during daytime and nighttime. © 2019 by the authors.","CGAN; Deep learning; Infrared; Radiance; Reflectance; Satellite remote sensing; Visible",
"Cloudmaskgan: A Content-Aware Unpaired Image-To-Image Translation Algorithm for Remote Sensing Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076808836&doi=10.1109%2fICIP.2019.8803161&partnerID=40&md5=85cd9ed66f80dc7774b1b9497e8d5429","Cloud segmentation is a vital task in applications that uti-lize satellite imagery. A common obstacle in using deep learning-based methods for this task is the insufficient number of images with their annotated ground truths. This work presents a content-aware unpaired image-to-image translation algorithm. It generates synthetic images with different land cover types from original images, while preserving the locations and the intensity values of the cloud pixels. Therefore, no manual annotation of ground truth in these images is required. The visual and numerical evaluations of the generated images by the proposed method prove that their quality is better than that of competitive algorithms. © 2019 IEEE.","Cloud detection; Landsat 8; remote sensing imagery; unpaired image-to-image translation",
"Cascaded Detection Framework Based on a Novel Backbone Network and Feature Fusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073168088&doi=10.1109%2fJSTARS.2019.2924086&partnerID=40&md5=6a3ec634b4c25529f53d59ef8cf72306","Due to the ability of powerful feature representation, deep-learning-based object detection has attracted considerable research attention, and many methods have been proposed for remote sensing images. However, there are still some problems that need to be addressed. In this paper, a novel and effective detection framework based on faster region-based convolutional neural network is designed. Specifically, first, in order to locate the boundaries of large objects and find the missing small objects, DetNet is incorporated into the detection framework as the backbone network. DetNet fixes the spatial resolution in deep layers and adopts dilated bottleneck with convolution projection to increase the divergence between input and output feature maps. Then, the proposed framework uses the backbone network to extract the scene features and region features simultaneously, which are both mapped to feature vectors and then fused together. The feature fusion operation can improve the feature representation of the generated region. Last, to improve the performance of localization, the cascade structure is adopted in the framework. The cascade structure has multiple phases and every phase has independent classifier and regressor. The results obtained from the previous phase are used as the regions of interest in the next phase. Therefore, the multiphase detector can increase the detection accuracy phase by phase. Comprehensive evaluations on a public ten-class object detection dataset demonstrate the effectiveness of the proposed framework. Moreover, ablation experiments are also implemented to show the respective influence of different parts of the framework on the performance improvement. © 2019 IEEE.","Convolutional neural network (CNN); feature fusion; object detection; remote sensing images","Classification (of information); Convolution; Deep learning; Neural networks; Object detection; Object recognition; Remote sensing; Ablation experiments; Comprehensive evaluation; Convolutional neural network; Feature fusion; Feature representation; Independent classifiers; Regions of interest; Remote sensing images; Feature extraction; artificial neural network; data set; detection method; image analysis; pattern recognition; remote sensing; spatial resolution; vector"
"Citrus rootstock evaluation utilizing UAV-based remote sensing and artificial intelligence","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069614009&doi=10.1016%2fj.compag.2019.104900&partnerID=40&md5=5ee599b62adc573e23687cfd54e650e2","The implementation of breeding methods requires the creation of a large and genetically diverse training population. Large-scale experiments are needed for the rapid acquisition of phenotypic data to explore the correlation between genomic and phenotypic information. Traditional sensing technologies for field surveys and field phenotyping rely on manual sampling and are time consuming and labor intensive. Since availability of personnel trained for phenotyping is a major problem, small UAVs (unmanned aerial vehicles) equipped with various sensors can simplify the surveying procedure, decrease data collection time, and reduce cost. In this study, we evaluated the phenotypic characteristics of sweet orange trees grafted on 25 rootstock cultivars with different influences on plant growth and productivity utilizing a UAV-based high throughput phenotyping system. Data collected by UAV were compared with data collected manually according to standard horticultural procedures. The UAV-based technique was able to detect and count citrus trees with high precision (99.9%) in an orchard of 4931 trees and estimate tree canopy size with a high correlation (R = 0.84) with the manual collected data. No correlation of UAV-based data and manually collected data was observed for yield. The reason for the observed deviation is the influence of different rootstock cultivars on yield efficiency. Despite the low vigor-inducing effect of some rootstocks, they are highly productive, whilst others are high in vigor but produce less fruit. Our study demonstrates the high accuracy of the UAV technique to assess tree size. When using these techniques, it is essential to recognize the limitations imposed by the biological system. © 2019 The Authors","Citrus; Deep learning; Machine learning; Neural networks; Precision agriculture; Rootstock; Smart agriculture; UAV","Antennas; Artificial intelligence; Deep learning; Deep neural networks; Forestry; Learning systems; Neural networks; Precision agriculture; Remote sensing; Surveys; Unmanned aerial vehicles (UAV); Citrus; Citrus rootstocks; High-throughput phenotyping; Large scale experiments; Rapid acquisition; Rootstock; Sensing technology; Smart agricultures; Trees (mathematics); accuracy assessment; artificial intelligence; artificial neural network; deciduous tree; experimental study; field survey; genomics; machine learning; phenotype; precision agriculture; remote sensing; rootstock; unmanned vehicle; Citrus; Citrus sinensis"
"Deep Panchromatic Image Guided Residual Interpolation for Multispectral Image Demosaicking","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077585398&doi=10.1109%2fWHISPERS.2019.8920868&partnerID=40&md5=ed7dce14c4012c0030882393f1fbae20","Snapshot multispectral imaging based on multispectral filter arrays (MSFA) has gained popularity recently for its size and speed. To process these multispectral images, demosaicking is the most crucial and challenging step to reduce artifacts in both spatial and spectral domain. In this work, a novel ResNet based deep learning model is first proposed to reconstruct the full-resolution panchromatic image from MSFA mosaic image. Then, the reconstructed deep panchromatic image (DPI) is deployed as the guide to recover the full-resolution multispectral image using a two-pass guided residual interpolation method. Experiment results demonstrate that the proposed method outperforms the state-of-the-art conventional and deep learning demosaicking methods both qualitatively and quantitatively. © 2019 IEEE.","deep learning; guided residual interpolation; image demosaicking; multispectral filter array","Deep learning; Hyperspectral imaging; Interpolation; Remote sensing; Spectroscopy; Demosaicking; Interpolation method; Multi-spectral filter arrays; Multispectral images; Multispectral imaging; Panchromatic images; Spectral domains; State of the art; Image reconstruction"
"Optimal parameter selection in hyperspectral classification based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078030379&doi=10.1109%2fICFSP48124.2019.8938098&partnerID=40&md5=4dd65eb890c3f827ba5d15e12b9be9ba","Classification is a key technique in hyperspectral image (HSI) applications. Deep learning algorithms, which exhibit strong modeling and representational capabilities, have been successfully adopted in fields such as image and language processing. And convolutional neural networks (CNNs) have been used for HSI classification and some interesting results have been obtained. Owing to local connection and weight sharing, the number of parameters is reduced to some extent, but there are still many parameters and the deeper the network, the larger is the number of parameters. The network performance is strongly influenced by the parameter settings. To obtain the optimal CNN parameters for HSI classification, this paper proposes a classification method based on a CNN with parameter tuning (CNN-PT). The network parameters are tuned in turn according to the unique variable principle. Simulation results show that the proposed CNN-PT method has considerable potential for HSI classification compared to previous methods. © 2019 IEEE.","Artificial intelligence; Classification; Deep learning; Parameter estimation; Remote sensing image; Unique variable","Artificial intelligence; Classification (of information); Convolution; Deep learning; Learning algorithms; Modeling languages; Neural networks; Parameter estimation; Remote sensing; Spectroscopy; Classification methods; Convolutional neural network; Hyper-spectral classification; Language processing; Optimal parameter selection; Remote sensing images; Representational capabilities; Unique variable; Signal processing"
"Deep residual autoencoder with multiscaling for semantic segmentation of land-use images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072648350&doi=10.3390%2frs11182142&partnerID=40&md5=9228c70312108baefb0c2dac670e8cd9","Semantic segmentation is a fundamental means of extracting information from remotely sensed images at the pixel level. Deep learning has enabled considerable improvements in efficiency and accuracy of semantic segmentation of general images. Typical models range from benchmarks such as fully convolutional networks, U-Net, Micro-Net, and dilated residual networks to the more recently developed DeepLab 3+. However, many of these models were originally developed for segmentation of general or medical images and videos, and are not directly relevant to remotely sensed images. The studies of deep learning for semantic segmentation of remotely sensed images are limited. This paper presents a novel flexible autoencoder-based architecture of deep learning that makes extensive use of residual learning and multiscaling for robust semantic segmentation of remotely sensed land-use images. In this architecture, a deep residual autoencoder is generalized to a fully convolutional network in which residual connections are implemented within and between all encoding and decoding layers. Compared with the concatenated shortcuts in U-Net, these residual connections reduce the number of trainable parameters and improve the learning efficiency by enabling extensive backpropagation of errors. In addition, resizing or atrous spatial pyramid pooling (ASPP) can be leveraged to capture multiscale information from the input images to enhance the robustness to scale variations. The residual learning and multiscaling strategies improve the trained model's generalizability, as demonstrated in the semantic segmentation of land-use types in two real-world datasets of remotely sensed images. Compared with U-Net, the proposed method improves the Jaccard index (JI) or the mean intersection over union (MIoU) by 4-11% in the training phase and by 3-9% in the validation and testing phases. With its flexible deep learning architecture, the proposed approach can be easily applied for and transferred to semantic segmentation of land-use variables and other surface variables of remotely sensed images. © 2019 by the authors.","Atrous spatial pyramid pooling; Autoencoder; Multiscale; Remotely sensed land-use images; Residual learning; Semantic segmentation","Backpropagation; Convolution; Deep learning; Efficiency; Image segmentation; Land use; Medical imaging; Network architecture; Remote sensing; Semantics; Auto encoders; Multiscale; Remotely sensed land-use images; Residual learning; Semantic segmentation; Spatial pyramids; Image enhancement"
"Incorporating Spectral Unmixing in Satellite Imagery Semantic Segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076818136&doi=10.1109%2fICIP.2019.8803372&partnerID=40&md5=1c773e528785f94479dae8ebbd64f90b","Land-cover classification to distinguish physical covers of Earth's surface is one of the critical tasks in remote sensing. Although deep learning-based approaches have shown remarkable performance in semantic segmentation, they require a massive amount of training data. Thus, the generalization capability of these approaches is of great importance, especially in working with satellite images when the amount of available labeled data is quite limited. In this paper, we propose incorporating spectral unmixing methods to obtain powerful representations of spectral information for semantic segmentation of satellite images. We show that land-cover classification performance can be enhanced by this proper extraction of features as input to the deep learning-based model. The experimental results demonstrate promising potential improvements in terms of segmentation accuracy. In addition, qualitative assessments show a higher confidence level of the proposed framework in predicting a label for a given pixel. © 2019 IEEE.","deep learning; Satellite image; semantic segmentation; spectral unmixing",
"Land Cover Classification for Satellite Images Through 1D CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077566080&doi=10.1109%2fWHISPERS.2019.8921180&partnerID=40&md5=2d800fd370c2703f3164941ce5e5df84","Land cover classification of satellite imagery can provide significant information for many applications, including surface analysis, environmental monitoring, building reconstruction, etc. Land cover classification has been generally performed using unmixing-based or shallow/deep learning approaches, among which the unmixing-based approaches suffer from stability issues due to the complex intrinsic properties of the data, deep learning-based approaches like 2D CNN requires large labeled training set which is often unavailable in satellite images and small ground truth collection leads to spatial discontinuities (as shown in Fig. 1), making 2D CNN approaches unviable. In this paper, we first propose a 1D convolution neural network-based framework applied to each pixel in the spectral domain where we extract descriptive local features for improved classification. Experimental results demonstrate superior classification accuracy through comparison with traditional unmixing-based and neural network methods using just limited number of training samples. © 2019 IEEE.","1D CNN; Deep Learning; Land Cover Classification; Satellite Image; Spectral Unmixing","Classification (of information); Deep learning; Hyperspectral imaging; Remote sensing; Satellite imagery; Small satellites; Spectroscopy; Surface analysis; 1D CNN; Classification accuracy; Convolution neural network; Environmental Monitoring; Land cover classification; Learning-based approach; Satellite images; Spectral unmixing; Image classification"
"Road detection and centerline extraction via deep recurrent convolutional neural network U-Net","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072047852&doi=10.1109%2fTGRS.2019.2912301&partnerID=40&md5=76d0a46b34165f41e126a7605db0f485","Road information extraction based on aerial images is a critical task for many applications, and it has attracted considerable attention from researchers in the field of remote sensing. The problem is mainly composed of two subtasks, namely, road detection and centerline extraction. Most of the previous studies rely on multistage-based learning methods to solve the problem. However, these approaches may suffer from the well-known problem of propagation errors. In this paper, we propose a novel deep learning model, recurrent convolution neural network U-Net (RCNN-UNet), to tackle the aforementioned problem. Our proposed RCNN-UNet has three distinct advantages. First, the end-to-end deep learning scheme eliminates the propagation errors. Second, a carefully designed RCNN unit is leveraged to build our deep learning architecture, which can better exploit the spatial context and the rich low-level visual features. Thereby, it alleviates the detection problems caused by noises, occlusions, and complex backgrounds of roads. Third, as the tasks of road detection and centerline extraction are strongly correlated, a multitask learning scheme is designed so that two predictors can be simultaneously trained to improve both effectiveness and efficiency. Extensive experiments were carried out based on two publicly available benchmark data sets, and nine state-of-the-art baselines were used in a comparative evaluation. Our experimental results demonstrate the superiority of the proposed RCNN-UNet model for both the road detection and the centerline extraction tasks. © 1980-2012 IEEE.","Recurrent convolutional neural network (RCNN); road centerline extraction; road detection; U-Net","Antennas; Backpropagation; Convolution; Deep neural networks; Remote sensing; Roads and streets; Centerline extraction; Comparative evaluations; Convolution neural network; Convolutional neural network; Effectiveness and efficiencies; Learning architectures; Road detection; Road information extractions; Recurrent neural networks"
"Superpixel based land cover classification of VHR satellite image combining multi-scale CNN and scale parameter estimation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068144209&doi=10.1007%2fs12145-019-00383-2&partnerID=40&md5=2f3bf2704e766d964be1a31ec719eb1e","Traditional classification methods, which use low-level features, have failed to gain satisfactory classification results of very high spatial resolution (VHR) remote sensing images. Even though per-pixel classification method based on convolutional neural network (CNN) (Per-pixel CNN) achieved higher accuracy with the help of high-level features, this method still has limitations. Per-superpixel classification method based on CNN (Per-superpixel CNN) overcomes the limitations of per-pixel CNN, however, there are still some scale related issues in per-superpixel CNN needed to be explored and addressed. Firstly, in order to avoid the misclassification of complex land cover objects caused by scale effect, the per-superpixel classification method combining multi-scale CNN (Per-superpixel MCNN) is proposed. Secondly, this paper analyzes how scale parameter of CNN impacts the classification accuracy and involves spatial statistics to pre-estimate scale parameter in per-superpixel CNN. This paper takes two VHR remote sensing images as experimental data, and employs two superpixel segmentation algorithms to classify urban and suburban land covers. The experimental results show that per-superpixel MCNN can effectively avoid misclassification in complex urban area compared with per-superpixel classification method combining single-scale CNN (Per-superpixel SCNN). Series of classification results also show that using the pre-estimated scale parameter can guarantee high classification accuracy, thus arbitrary nature of scale estimation can be avoided to some extent. Additionally, through discussion of the influence of accuracy evaluation method in CNN classification, it is stressed that random selection of ground truth validation points from study area is recommended and more responsibly other than using part of a reference dataset. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; High spatial resolution remote sensing image; Land cover classification; OBIA; Scale parameter estimation","accuracy assessment; algorithm; artificial neural network; image classification; image resolution; land cover; parameter estimation; pixel; remote sensing; satellite imagery; segmentation; spatial resolution; urban area"
"Deep Convolutional Networks for Snapshot Hypercpectral Demosaicking","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077595508&doi=10.1109%2fWHISPERS.2019.8921273&partnerID=40&md5=d6c8bb81c7397c585bb9df86bb251882","In this paper we introduce a novel demosaicking algorithm for snashot hyperspectral images. Snapshot cameras allow real-time hyper spectral imaging in uncontrolled environments. However, snapshot cameras with K bands capture only a single frequency band per pixel leaving the rest, K - 1 values, at the mercy of demosaicking algorithms. Demosaicking is a very challenging problem as interpolating unknown values in a highly under-sampled cube of radiances is likely to lead to aliasing artifacts in the interpolated output values. Most of the existing demosaicking approaches are hand crafted interpolation based methods. Inspired by the Deep Learning based breakthroughs in various areas of computer vision, we propose a snapshot hyperspectral demosaicking approach based on deep convolutinal networks. Experiments on the CAVE and Hytexila datasets show that our proposed method outperforms the state-of-the-art methods. © 2019 IEEE.","Deep Learning; Demosaicking; Hyperpsectral Imaging","Cameras; Deep learning; Interpolation; Remote sensing; Spectroscopy; Aliasing artifacts; Convolutional networks; Demosaicking; Output values; Single frequency; State-of-the-art methods; Under sampled; Unknown values; Hyperspectral imaging"
"Transfer Learning for Fine-Grained Crop Disease Classification Based on Leaf Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077597779&doi=10.1109%2fWHISPERS.2019.8921213&partnerID=40&md5=bcdf402d46e91bd18ef5bc17f8cbf5e4","In this paper, convolutional neural network models and transfer learning techniques were employed to perform automated early plant disease detection and diagnosis using simple leaf images of healthy and diseased plants taken in situ and in controlled environment. This paper presents a faster technique to perform fine-tuning of state-of-art models by using a concept of freezing blocks of layers instead of individual layers. Models were trained and tested on public dataset of 23,352 images from 28 classes incorporating 15 different crop species. Six different model architectures and their variants were trained, with the best performance attaining an accuracy of 99.74% obtained by fine-tuning deep learning model previously trained on ImageNet. The significantly high success rate and computationally efficient property on small amount of data makes this model a very useful early warning tool capable of being deployed as integrated plant disease identification system in portable and resource constrained devices to operate in real cultivation conditions. These techniques were tested on other datasets and was proven to be highly efficient for fine-grained dataset. © 2019 IEEE.","Classification; Fine-Tuning; Plant Disease; Transfer learning","Classification (of information); Crops; Cultivation; Deep learning; Diagnosis; Image classification; Neural networks; Plants (botany); Remote sensing; Spectroscopy; Tuning; Computationally efficient; Controlled environment; Convolutional neural network; Cultivation conditions; Fine tuning; Plant disease; Resourceconstrained devices; Transfer learning; Hyperspectral imaging"
"Generative and Encoded Anomaly Detectors","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077525167&doi=10.1109%2fWHISPERS.2019.8920850&partnerID=40&md5=3f4bcb4cd8d3ac0e3e3c01b812e67ee6","We present two fully unsupervised deep learning approaches for hyperspectral anomaly detection. In one approach we formulate the anomaly detection problem as an adversarial game where a generator network learns the distribution of the hyperspectral background pixels comprising a single hyperspectral image and the output of the corresponding discriminator network yields a detection statistic. The other approach formulates the detection statistic as the error between an input hyperspectral pixel and the reconstruction of that pixel by an autoencoder network trained on the image. Both methods leverage a sub-sampling scheme that allows for unsupervised training and testing on the same data set. Our approaches are validated on a four-class synthetic hyperspectral data set and compared to a statistical approach (RX) and a geometric approach (skelton kernel principal component analysis). The proposed Generative Anomaly Detector algorithm achieves top performance on the data set while the autoencoder detection scheme also demonstrates performance gains relative to the comparison algorithms. Benefits and drawbacks of the approaches are discussed and highlight the many potential directions for future work. © 2019 IEEE.","Anomaly Detection; Autoencoder; Generative Adversarial Networks; Hyperspectral Imaging","Anomaly detection; Deep learning; Pixels; Principal component analysis; Remote sensing; Spectroscopy; Statistical tests; Adversarial networks; Auto encoders; Geometric approaches; Hyperspectral anomaly detection; Hyperspectral backgrounds; Kernel principal component analyses (KPCA); Statistical approach; Unsupervised training; Hyperspectral imaging"
"UAV-g 2019: Unmanned aerial vehicles in geomatics","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081912440&doi=10.3390%2fdrones3030074&partnerID=40&md5=8d8288b6c7cede0dccf6477296b4b1de","Unmanned aerial vehicle in geomatics (UAV-g) is a well-established scientific event dedicated to UAVs in geomatics and remote sensing. In the different editions of the journal, new scientific challenges have increased their synergy with adjacent domains, such as robotics and computer vision, thereby increasing the impact of this conference. The 2019 edition has been hosted by the University of Twente (The Netherlands) and has attracted about 300 participants for the full three-day program. Researchers from 36 different countries (from all continents) have presented 89 accepted papers in 17 oral and 2 poster sessions. The presented papers covered multi-disciplinary topics, such as photogrammetry, natural resources monitoring, autonomous navigation, and deep learning. All these contributions have in common the use of UAV platforms for the innovative acquisition and processing of the acquired data and information extracted from the surrounding environment. © 2019 by the author. Licensee MDPI, Basel, Switzerland.","Computer vision; Drones; Photogrammetry; Remote sensing; Robotics; UAV",
"A Pixel Level Scaled Fusion Model to Provide High Spatial-Spectral Resolution for Satellite Images Using LSTM Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077559994&doi=10.1109%2fWHISPERS.2019.8921269&partnerID=40&md5=05be7266faee713984b84933674ee973","Pixel-level fusion of satellite images coming from multiple sensors allows for an improvement in the quality of the acquired data both spatially and spectrally. In particular, multispectral and hyperspectral images have been fused to generate images with a high spatial and spectral resolution. In literature, there are several approaches for this task, nonetheless, those techniques still present a loss of relevant spatial information during the fusion process. This work presents a multi scale deep learning model to fuse multispectral and hyperspectral data, each with high-spatial-and-low-spectral resolution (HSaLS) and low-spatial-and-high-spectral resolution (LSaHS) respectively. As a result of the fusion scheme, a high-spatial-and-spectral resolution image (HSaHS) can be obtained. In order of accomplishing this result, we have developed a new scalable high spatial resolution process in which the model learns how to transition from low spatial resolution to an intermediate spatial resolution level and finally to the high spatial-spectral resolution image. This step-by-step process reduces significantly the loss of spatial information. The results of our approach show better performance in terms of both the structural similarity index and the signal to noise ratio. © 2019 IEEE.","Data Fusion; hyperspectral image; Long Short Term Memory; multispectral image; Pixel level; Super resolution","Data fusion; Deep learning; Hyperspectral imaging; Image enhancement; Image fusion; Image resolution; Pixels; Remote sensing; Signal to noise ratio; Spectral resolution; Spectroscopy; High spatial resolution; High spectral resolution; Multispectral images; Pixel level; Spatial informations; Spatial resolution; Structural similarity indices; Super resolution; Long short-term memory"
"Rainfall Estimation From Ground Radar and TRMM Precipitation Radar Using Hybrid Deep Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074073975&doi=10.1029%2f2019GL084771&partnerID=40&md5=c5942c3712a4a7e58ba98588e559a903","Remote sensing of precipitation is critical for regional, continental, and global water and climate research. This study develops a deep learning mechanism to link between point-wise rain gauge measurements, ground-based, and spaceborne radar reflectivity observations. Two neural network models are designed to construct a hybrid rainfall system, where the ground radar is used to bridge the scale gaps between rain gauge and satellite. The first model is trained for ground radar using rain gauge data as target labels, whereas the second model is for spaceborne Tropical Rainfall Measuring Mission (TRMM) Precipitation Radar (PR) using ground radar estimates as training labels. Data from 1 year of observations in Florida during 2009 are utilized to illustrate the application of this hybrid rainfall system. Validation using independent data in 2009, as well as 2-year comparison against the standard PR products, demonstrates the promising performance and generality of this innovative rainfall algorithm. © 2019. American Geophysical Union. All Rights Reserved.","ground radar; hybrid system; neural network; rain gauge; rainfall estimation; TRMM PR","Deep neural networks; Hybrid systems; Neural networks; Radar measurement; Radar target recognition; Rain; Remote sensing; Space applications; Space-based radar; Ground radars; Learning mechanism; Neural network model; Precipitation radar; Rain gauges; Rainfall algorithms; Rainfall estimations; Tropical rainfall measuring missions; Rain gages; artificial neural network; climate forcing; precipitation intensity; radar altimetry; raingauge; reflectivity; SIR; TRMM; Florida [United States]; United States"
"Hyperspectral imagery classification based on semi-supervised 3-D deep neural network and adaptive band selection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064135871&doi=10.1016%2fj.eswa.2019.04.006&partnerID=40&md5=8aba703ab61fd4be18d3ffca6bfce9cf","This paper proposes a novel approach based on adaptive dimensionality reduction (ADR) and a semi-supervised 3-D convolutional neural network (3-D CNN) for the spectro-spatial classification of hyperspectral images (HSIs). It tackles the problem of curse of dimensionality and the limited number of training samples by selecting the most relevant spectral bands. The selected bands should be informative, discriminative and distinctive. They are fed into a semi-supervised 3-D CNN feature extractor, then a linear regression classifier to produce the classification map. In fact, the proposed semi-supervised 3-D CNN model seeks to extract the deep spectral and spatial features based on convolutional encoder-decoder to enhance the HSI classification. It uses several 3-D convolution and max-pooling layers to extract these features from the selected relevant bands. The main advantage of the proposed approach is to reduce the high dimensionality of HSI, preserve the relevant spectro-spatial information and enhance the classification using few labeled training samples. Experimental studies are carried out on three real HSI data sets: Indian Pines, Pavia University, and Salinas. The obtained results show that the proposed approach performs better than other deep learning-based methods including CNN-based methods, and significantly improves the classification accuracy of HSIs. © 2019","Adaptive dimensionality reduction; Convolutional neural network (CNN); Deep learning; Hyperspectral imagery classification","Classification (of information); Convolution; Deep learning; Deep neural networks; Neural networks; Remote sensing; Sampling; Spectroscopy; Adaptive Band Selection; Classification accuracy; Convolutional encoders; Convolutional neural network; Curse of dimensionality; Dimensionality reduction; Hyperspectral imagery classifications; Learning-based methods; Image classification"
"Knowledge Transfer via Convolution Neural Networks for Multi-Resolution Lawn Weed Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077628723&doi=10.1109%2fWHISPERS.2019.8920832&partnerID=40&md5=fe28df4aa00d84687b70ee2f815a54f8","Weed identification and classification are essential and challenging tasks for site-specific weed control. Object-based image analysis making use of spatial information is adopted in this study for the weed classification because the spectral similarity between the weeds and crop is high. With the availability of a wide range of sensors, it is likely to capture weed imagery at various altitudes and with different specifications of the sensor. In this paper, we propose a novel method using transfer learning to deal with multi-resolution images from various sensors via Convolutional Neural Networks (CNN). CNN trained for a typical image data set and the trained weights are transferred to other data sets of different resolutions. In this way, the new data sets can be classified by fine-tuning the network using a small number of training samples, which reduces the need of big data to train the model. To avoid over-fitting during the fine-tuning, small deep learning architecture is proposed and investigated using the parameters of the initial layers of pre-trained model. The sizes of training samples are investigated for their impact on the performance of fine-tuning. Experiments were conducted with field data, which show that the proposed method outperforms the direct training method in terms of recognition accuracy and computation cost. © 2019 IEEE.","Convolutional Neural Network (CNN); Hyperspectral images; Resolution; Transfer Learning; Weed Mapping","Classification (of information); Convolution; Deep learning; Knowledge management; Neural networks; Optical resolving power; Remote sensing; Sampling; Spectroscopy; Tuning; Weed control; Convolution neural network; Convolutional neural network; Different resolutions; Learning architectures; Multiresolution images; Object based image analysis; Transfer learning; Weed mappings; Hyperspectral imaging"
"Urban greenery and mental wellbeing in adults: Cross-sectional mediation analyses on multiple pathways across different greenery measures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067848081&doi=10.1016%2fj.envres.2019.108535&partnerID=40&md5=8a8f226068a9d740eb88e5bb4d959443","Background: Multiple mechanisms have been proposed to explain how greenery in the vicinity of people's homes enhances their mental health and wellbeing. Mediation studies, however, focus on a limited number of mechanisms and rely on remotely sensed greenery measures, which do not accurately capture how neighborhood greenery is perceived on the ground. Objective: To examine: 1) how streetscape and remote sensing-based greenery affect people's mental wellbeing; 2) whether and, if so, to what extent the associations are mediated by physical activity, stress, air quality and noise, and social cohesion; and 3) whether differences in the mediation across the streetscape greenery and NDVI exposure metrics occurred. Methods: We used a population sample of 1029 adult residents of the metropolis of Guangzhou, China, from 2016. Mental wellbeing was quantified by the World Health Organization Well-Being Index (WHO-5). Two objective greenery measures were extracted at the neighborhood level: 1) streetscape greenery from street view data via a convolutional neural network, and 2) the normalized difference vegetation index (NDVI) from Landsat 8 remote sensing images. Single and multiple mediation analyses with multilevel regressions were conducted. Results: Streetscape and NDVI greenery were weakly and positively, but not significantly, correlated. Our regression results revealed that streetscape greenery and NDVI were, individually and jointly, positively associated with mental wellbeing. Significant partial mediators for the streetscape greenery were physical activity, stress, air quality and noise, and social cohesion; together, they explained 62% of the association. For NDVI, only physical activity and social cohesion were significant partial mediators, accounting for 22% of the association. Conclusions: Mental health and wellbeing and both streetscape and satellite-derived greenery seem to be both directly correlated and indirectly mediated. Our findings signify that both greenery measures capture different aspects of natural environments and may contribute to people's wellbeing by means of different mechanisms. © 2019 Elsevier Inc.","China; Deep learning; Green space; Mediation; Mental wellbeing; Street view data","adult; air quality; algorithm; greenspace; Landsat; mental health; NDVI; neighborhood; physical activity; remote sensing; satellite data; satellite imagery; World Health Organization; adult; air quality; article; China; deep learning; female; human; human experiment; human tissue; male; mediator; neighborhood; noise; physical activity; psychological well-being; resident; satellite imagery; stress; vegetation; World Health Organization; air pollution; China; cross-sectional study; demography; environmental planning; mental health; China; Guangdong; Guangzhou; Adult; Air Pollution; China; Cross-Sectional Studies; Environment Design; Humans; Mental Health; Residence Characteristics"
"High-Resolution Aerial Images Semantic Segmentation Using Deep Fully Convolutional Network with Channel Attention Mechanism","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073162731&doi=10.1109%2fJSTARS.2019.2930724&partnerID=40&md5=2c305dd0e3b25a23990dbf9987f5c2ea","Semantic segmentation is one of the fundamental tasks in understanding high-resolution aerial images. Recently, convolutional neural network (CNN) and fully convolutional network (FCN) have achieved excellent performance in general images' semantic segmentation tasks and have been introduced to the field of aerial images. In this paper, we propose a novel deep FCN with channel attention mechanism (CAM-DFCN) for high-resolution aerial images' semantic segmentation. The CAM-DFCN architecture follows the mode of encoder-decoder. In the encoder, two identical deep residual networks are both divided into multiple levels and acted on spectral images and auxiliary data, respectively. Then, the feature map concatenation is carried out at each level. In the decoder, the channel attention mechanism (CAM) is introduced to automatically weigh the channels of feature maps to perform feature selection. On the one hand, the CAM follows the concatenated feature maps at each level to select more discriminative features for classification. On the other hand, the CAM is used to further weigh the semantic information and spatial location information in the adjacent-level concatenated feature maps for more accurate predictions. We evaluate the proposed CAM-DFCN by using two benchmarks (the Potsdam set and the Vaihingen set) provided by the International Society for Photogrammetry and Remote Sensing. Experimental results show that the proposed method has considerable improvement. © 2008-2012 IEEE.","Channel attention mechanism (CAM); convolutional neural networks (CNNs); deep learning; fully convolutional networks (FCNs); high-resolution aerial images; semantic segmentation","Antennas; Channel coding; Convolution; Decoding; Deep learning; Deep neural networks; Feature extraction; Neural networks; Remote sensing; Semantic Web; Semantics; Signal encoding; Spectroscopy; Attention mechanisms; Convolutional networks; Convolutional neural network; High-resolution aerial images; Semantic segmentation; Image segmentation; aerial survey; artificial neural network; experimental study; image analysis; image resolution; machine learning; remote sensing; spatiotemporal analysis"
"Multi-Task Learning for Segmentation of Building Footprints with Deep Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075927226&doi=10.1109%2fICIP.2019.8803050&partnerID=40&md5=f9a088a1eb1bdadc377176a39ed6d994","The increased availability of high-resolution satellite imagery allows to sense very detailed structures on the surface of our planet. Access to such information opens up new directions in the analysis of remote sensing imagery. While deep neural networks have achieved significant advances in semantic segmentation of high-resolution images, most of the existing approaches tend to produce predictions with poor boundaries. In this paper, we address the problem of preserving semantic segmentation boundaries in high-resolution satellite imagery by introducing a novel multi-task loss. The loss leverages multiple output representations of the segmentation mask and biases the network to focus more on pixels near boundaries. We evaluate our approach on the large-scale Inria Aerial Image Labeling Dataset which contains high-resolution images. Our results show that we are able to outperform state-of-the-art methods by 9.8% on the Intersection over Union (IoU) metric without any additional post-processing steps. Source code and all models will be available under https://github.com/bbischke/MultiTaskBuildingSegmentation. © 2019 IEEE.","Building Extraction; Deep Learning; Multi Task Learning; Satellite Imagery; Semantic Segmentation",
"A New Pansharpening Method Using Objectness Based Saliency Analysis and Saliency Guided Deep Residual Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076814025&doi=10.1109%2fICIP.2019.8803477&partnerID=40&md5=e448cf5b1ec4f7327e85e27d798f2f20","Pansharpening is a fundamental and crucial task in the remote sensing community. For remote sensing images, there is a significant difference in demands for spatial and spectral resolution in different regions. From this perspective, we propose a new pansharpening method using objectness based saliency analysis and saliency guided deep residual network to boost the fusion accuracy. We first develop an objectness based saliency analysis by incorporating texture feature and objectness measurements to estimate saliency values in images and thereby help discriminate different demands for spatial improvement and spectral preservation. Inspired by the impressive performance of deep learning, we subsequently construct a saliency guided deep residual network to implement pansharpening. In addition, in order to produce images with subtler details, we design a new loss function, the normalized mean square error, particularly for the pansharpening task. Experiments support the superiority of our proposal over six competing methods. © 2019 IEEE.","deep residual network; Image fusion; normalized mean square error; pansharpening; saliency",
"Convolutional neural network for GF-2 image stand type classification [基于卷积神经网络的高分二号影像林分类型分类]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074230681&doi=10.13332%2fj.1000-1522.20180342&partnerID=40&md5=5afe224bc64281d8d14594bb93392b2e","The classification of stand type based on remote sensing imagery is an important application in modern forestry. In recent years, many studies have explored this territory using multiple data sources and classification algorithm. Convolutional neural network (CNN), a new neural network algorithm, has higher accuracy in pattern recognition, scene classification and objective detection because of its unique structure and deep learning technology. The purpose of this paper is to propose a convolutional neural network system tailored for GF-2 (a high-resolution multispectral remote sensing data) applied in stand type classification on pixel-level. We chose different image patch size (i.e. m = 5, 7, 9 and 11) for building CNN and multilayer perceptrons (MLP) as benchmark in tensorflow (an open source library of machine intelligence), to train and compare classification accuracy of model. Experimental results showed that the CNN (m = 9) outperformed MLP and CNNs (m = 5, 7 and 11) by 10.91%, 6.55%, 1.3% and 2.54%, respectively, in overall classification accuracy. And the CNN (m = 9) alleviates the effect of salt-and-pepper and boundary uncertainties greatly in visual assessment. CNN can fully exploit the spatial features of images while utilizing the spectral features of high-resolution images to improve classification accuracy. And on remote sensing image classification based on CNN, selecting the appropriate image patch size according to the data source and the features of the objective is the key measure to improve the classification accuracy and classification effect. © 2019, Editorial Department of Journal of Beijing Forestry University. All right reserved.","CNN; GF-2; Image patch size; MLP; Stand type",
"A Framework for An Artificial Neural Network Enabled Single Pixel Hyperspectral Imager","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077525959&doi=10.1109%2fWHISPERS.2019.8921054&partnerID=40&md5=c212f58dbb94c3fa4d0540797dfb830d","Compressive Sensing enables improvement of acquisition of a variety of signals in various applications with little to no discernible loss in terms of recovered image quality. The current work proposes a signal processing framework for the acquisition and fast reconstruction of compressively sampled hyperspectral images using an artificial neural network architecture. This ANN-based approach is capable of performing a fast reconstruction by avoiding the requirement of solving a computationally intensive image-specific optimization problem. The proposed framework contributes to advance singlepixel hyperspectral imaging device methodologies, which enable a significant reduction in device mechanical complexity, imaging rate, and cost. Our experiments demonstrate that a hyperspectral image can be reconstructed using only 10% of the samples without compromising classification performance. Specifically, the results show that classification performance of the compressively sampled hyperspectral image recovered using artificial neural networks is equal or higher to that of those obtained using current scanning hyperspectral imaging platforms. © 2019 IEEE.","compressive sensing; deep learning; hyperspectral imaging; remote sensing","Compressed sensing; Deep learning; Image compression; Image enhancement; Image reconstruction; Network architecture; Neural networks; Remote sensing; Spectroscopy; Classification performance; Compressive sensing; Fast reconstruction; Hyperspectral imagers; Mechanical complexity; Optimization problems; Single pixel; Hyperspectral imaging"
"Multi-Scale Dilated Residual Convolutional Neural Network for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077524982&doi=10.1109%2fWHISPERS.2019.8921284&partnerID=40&md5=7b4e126e5b75a0496259f4ca0eb93e0e","Recently, deep Convolutional Neural Networks (CNNs) have been extensively studied for hyperspectral image classification. It has undergone significant improvement as compared to conventional classification methods. Yet, there are not much studies have been taken on sub-sampled ground truth dataset in CNN. This paper exploits CNN-based method along with multi-scale and dilated convolution with residual connection concepts for hyperspectral image classification on exclusive real time data set. Two raw and one standard full ground truth Pavia University datasets are used to characterize the performance. Out of raw exclusive datasets, one was taken over urban areas of Ahmedabad, India under ISRO-NASA joint initiative for HYperSpectral Imaging (HYSI) programme, and the other was collected using Hypersec VNIR integrated camera of our institute surroundings from the rooftop of the building. © 2019 IEEE.","Convolutional Neural Network; Deep Learning; Hyperspectral Imaging; Multi-scale Dilated Residual Learning","Classification (of information); Convolution; Deep learning; Deep neural networks; Image classification; NASA; Neural networks; Remote sensing; Spectroscopy; Ahmedabad , india; Conventional classification methods; Convolutional neural network; Ground truth; Ground-truth dataset; Integrated cameras; Multi-scale Dilated Residual Learning; Real-time data; Hyperspectral imaging"
"Hyperspectral and Multispectral Image Fusion Based on Deep Attention Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077523554&doi=10.1109%2fWHISPERS.2019.8920825&partnerID=40&md5=866324eb402f0eabc5dd332264bcf902","Hyperspectral (HS) images have rich spectral information and can provide attribute information. High spatial resolution images, such as multispectral (MS) images and panchromatic (PAN) images, can provide fine geometric features. Thus, the fusion of the two images can achieve information complementarity and increase the accuracy and reliability of information. In this paper, we propose a hyperspectral and multispectral image fusion method based on deep attention network. Our model consists of two parts. One is the fusion network, which is used to fuse images. The other part is the spatial attention network, which is used to extract tiny textures and enhance the spatial structure. Experimental results compared with some state-of-the-art methods illustrate that our method is outstanding in both visual and numerical results. © 2019 IEEE.","deep learning; Hyperspectral image; image fusion; multi-spectral image; spatial attention","Deep learning; Hydraulic structures; Hyperspectral imaging; Numerical methods; Remote sensing; Spectroscopy; Textures; Attribute information; High spatial resolution images; Multi-spectral image fusions; Multispectral images; Panchromatic (Pan) image; Reliability of information; Spatial attention; State-of-the-art methods; Image fusion"
"SSSDET: Simple Short and Shallow Network for Resource Efficient Vehicle Detection in Aerial Scenes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076810987&doi=10.1109%2fICIP.2019.8803262&partnerID=40&md5=db515aabc2fa313dc0f12653a34ae024","Detection of small-sized targets is of paramount importance in many aerial vision-based applications. The commonly deployed low cost unmanned aerial vehicles (UAVs) for aerial scene analysis are highly resource constrained in nature. In this paper we propose a simple short and shallow network (SSSDet) to robustly detect and classify small-sized vehicles in aerial scenes. The proposed SSSDet is up to 4× faster, requires 4.4× less FLOPs, has 30× less parameters, requires 31× less memory space and provides better accuracy in comparison to existing state-of-the-art detectors. Thus, it is more suitable for hardware implementation in real-time applications. We also created a new airborne image dataset (ABD) by annotating 1396 new objects in 79 aerial images for our experiments. The effectiveness of the proposed method is validated on the existing VEDAI, DLR-3K, DOTA and Combined dataset. The SSSDet outperforms state-of-the-art detectors in term of accuracy, speed, compute and memory efficiency. © 2019 IEEE.","aerial scene; deep learning; real-time; remote sensing; vehicle detection",
"Spectral-spatial classification of hyperspectral images using CNNs and approximate sparse multinomial logistic regression","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075602708&doi=10.23919%2fEUSIPCO.2019.8902983&partnerID=40&md5=6197b6ff8f3bd9cdc9078abeb72ee45b","We propose a technique for training convolutional neural networks (CNNs) in which the convolutional layers are trained using a gradient descent based method and the classification layer is trained using a second order method called approximate sparse multinomial logistic regression (ASMLR) which also provides a spatial smoothing procedure that increases the classification accuracy for hyperspectral images. ASMLR performs well on hyperspectral images, and CNNs are known to give good results in many applications such as image classification and object recognition. Thus, the proposed technique allows us to improve the performance of CNNs by training the whole network with an end-to-end framework. This approach takes advantage of convolutional layers for spectral feature extraction, and of the softmax classification layer for feature selection with sparsity constraints, and an intrinsic learning rate adjustment mechanism. In classification, we also use a spatial smoothing method. The proposed method was evaluated on two hyperspectral images for spectral-spatial land cover classification, and the results have shown that it outperforms the CNN and the ASMLR classifiers when they are used separately. © 2019 IEEE","Convolutional neural networks; Deep learning; Hyperspectral image classification; Logistic regression; Remote sensing","Convolution; Deep learning; Deep neural networks; Feature extraction; Gradient methods; Hyperspectral imaging; Multilayer neural networks; Object recognition; Regression analysis; Remote sensing; Spectroscopy; Classification accuracy; Convolutional neural network; Land cover classification; Logistic regressions; Multinomial logistic regression; Second-order methods; Spectral feature extraction; Spectral-spatial classification; Image classification"
"Farmland Recognition of High Resolution Multispectral Remote Sensing Imagery using Deep Learning Semantic Segmentation Method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074788137&doi=10.1145%2f3357777.3357788&partnerID=40&md5=851e62156d6c13081804061d37b8eb09","Farmland mapping is an important step for estimating grain yields. However extraction of farmland from multispectral remote sensing images (RSIs) is still a challenging work, as farmland is located on not only plains but also mountains, which displays divergent and confusing characteristics in RSIs. To solve the problem of lacking the multispectral remote sensing image dataset for pretraining, we extend Deep Feature Aggregation Net (DFANet) with fewer network parameters, a semantic segmentation network, to automatically map farmland from 3-band to multispectral images in a pixel-wise strategy. In this network, we first utilize more information aggregation. The fully-connected attention module is then replaced by a proposed convolution attention module. Finally, a new proposed decoder is used to recover the details of the feature map. Experimental results show that the model with multispectral RSIs outperforms the baselines. © 2019 Association for Computing Machinery. ACM ISBN","Agriculture; Deep Learing; Farmland Recognition; High-resolution Multispectral Image; Remote Sensing","Agriculture; Artificial intelligence; Farms; Image segmentation; Pattern recognition; Remote sensing; Semantics; Deep Learing; Farmland Recognition; Feature aggregation; Information aggregation; Multispectral images; Multispectral remote sensing image; Multispectral remote sensing imagery; Semantic segmentation; Deep learning"
"PRAI 2019 - Proceedings of 2019 International Conference on Pattern Recognition and Artificial Intelligence","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074764338&partnerID=40&md5=e180d4dcf807ae3dfebba71f41cb3373","The proceedings contain 19 papers. The topics discussed include: sentimental analysis of chinese new social media for stock market information; safety monitoring of power industrial control terminals based on data cleaning; incorporating singular value decomposition in user-based collaborative filtering technique for a movie recommendation system: a comparative study; a neural network model of the NBA most valued player selection prediction; mining the relationship between crimes, weather and tweets; a secure privacy preserving proxy re-encryption scheme for IoT security using near-ring; farmland recognition of high resolution multispectral remote sensing imagery using deep learning semantic segmentation method; and generating adversarial samples with convolutional neural network.",,
"ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073789576&partnerID=40&md5=54d8d5a3ce34d3abd683b2a71bd236ff","The proceedings contain 18 papers. The topics discussed include: documenting Nea Paphos for conservation and management; T’minga’ community participation for the maintenance of vernacular heritage buildings in the historical center of Cuenca, Ecuador; classification of archaeological sites for heritage management inventory: the case of the ancient synagogues in the Galilee; BIM mixed reality tool for the inspection of heritage buildings; project ANQA: presenting the built heritage of Damascus, Syria through digitally-assisted storytelling; to be part of... architecture, decoration or iconography. documenting Azulejo as integrated heritage; and multi-task deep learning with incomplete training samples for the image-based prediction of variables describing silk fabrics.",,
"Deep learning-based analysis of the relationships between climate change and crop yield in China","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074258846&doi=10.5194%2fisprs-archives-XLII-3-W8-93-2019&partnerID=40&md5=438c12346e233533796b48b404174f66","Climate change is an important factor in vegetation growth, and it is very significant to understand the relationship between climate change and rice yield. China is a food-importing country whose grain consumption is higher than grain production, and which relies on imports of rice, soybean, wheat and other grains. Therefore, in order to secure food security for 1.6 billion people in China, it is necessary to grasp the relationship between climate change and rice yield. In this study, 16 administrative districts in China were selected and designated as study area. This study used annual rice production from the USDA (United States Department of Agriculture) for each of China's major administrative regions from 1979 to 2009, as well as average climate data from July to August, which were meteorological observations collected from the CRU (Climate Research Unit). Using this data, the rice crop was increased in 10 administrative regions in China and the reduction in rice harvest in 6 administrative areas was confirmed. The relationship between selected rice production and climate change was nonlinear and modelled using a deep neural network, and the validation statistics showed that the performance of DNN was 32-33% better than that of MLR (multiple linear regression). Therefore, a more quantitative analysis of the relationship between climate change and rice yield changes has been made possible through our prediction model. This study is expected to contribute to better food self-sufficiency in China and forecast future grain yields. © 2019 International Society for Photogrammetry and Remote Sensing.","Climate Change; Deep Learning; Food Security; Grain Yield; Rice Yield","Climate models; Crops; Deep learning; Deep neural networks; Disaster prevention; Disasters; Food supply; Grain (agricultural product); Linear regression; Food security; Grain yield; Meteorological observation; Multiple linear regressions; Prediction model; Rice yield; United states department of agricultures; Vegetation growth; Climate change"
"Multi-purpose chestnut clusters detection using deep learning: A preliminary approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074279645&doi=10.5194%2fisprs-archives-XLII-3-W8-1-2019&partnerID=40&md5=4ee1aae7fbc5ac3b731daee62020d0f2","In the early 1980′s, the European chestnut tree (Castanea sativa, Mill.) assumed an important role in the Portuguese economy. Currently, the Trás-os-Montes region (Northeast of Portugal) concentrates the highest chestnuts production in Portugal, representing the major source of income in the region (€50M-€60M). The recognition of the quality of the Portuguese chestnut varieties has increasing the international demand for both industry and consumer-grade segments. As result, chestnut cultivation intensification has been witnessed, in such a way that widely disseminated monoculture practices are currently increasing environmental disaster risks. Depending on the dynamics of the location of interest, monocultures may lead to desertification and soil degradation even if it encompasses multiple causes and a whole range of consequences or impacts. In Trás-os-Montes, despite the strong increase in the cultivation area, phytosanitary problems, such as the chestnut ink disease (Phytophthora cinnamomi) and the chestnut blight (Cryphonectria parasitica), along with other threats, e.g. chestnut gall wasp (Dryocosmus kuriphilus) and nutritional deficiencies, are responsible for a significant decline of chestnut trees, with a real impact on production. The intensification of inappropriate agricultural practices also favours the onset of phytosanitary problems. Moreover, chestnut trees management and monitoring generally rely on in-field time-consuming and laborious observation campaigns. To mitigate the associated risks, it is crucial to establish an effective management and monitoring process to ensure crop cultivation sustainability, preventing at the same time risks of desertification and land degradation. Therefore, this study presents an automatic method that allows to perform chestnut clusters identification, a key-enabling task towards the achievement of important goals such as production estimation and multi-temporal crop evaluation. The proposed methodology consists in the use of Convolutional Neural Networks (CNNs) to classify and segment the chestnut fruits, considering a small dataset acquired based on digital terrestrial camera. © 2019 International Society for Photogrammetry and Remote Sensing.","Chestnut; Chestnut Detection; Chestnut Tree; CNN; Convolutional Neural Networks; Deep Learning; DL; Rough Segmentation; Tiling Segmentation; Xception","Classification (of information); Climatology; Convolution; Crops; Cultivation; Deep learning; Deep neural networks; Disaster prevention; Disasters; Forestry; Neural networks; Plants (botany); Sustainable development; Chestnut; Chestnut Tree; Convolutional neural network; Rough segmentation; Xception; Fruits"
"Uav-based structural damage mapping-results from 6 years of research in two european projects","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074255951&doi=10.5194%2fisprs-archives-XLII-3-W8-187-2019&partnerID=40&md5=66a12b530d6bbdd88119eca8b62486ef","Structural disaster damage detection and characterisation is one of the oldest remote sensing challenges, and the utility of virtually every type of active and passive sensor deployed on various air-and spaceborne platforms has been assessed. The proliferation and growing sophistication of UAV in recent years has opened up many new opportunities for damage mapping, due to the high spatial resolution, the resulting stereo images and derivatives, and the flexibility of the platform. We have addressed the problem in the context of two European research projects, RECONASS and INACHUS. In this paper we synthesize and evaluate the progress of 6 years of research focused on advanced image analysis that was driven by progress in computer vision, photogrammetry and machine learning, but also by constraints imposed by the needs of first responder and other civil protection end users. The projects focused on damage to individual buildings caused by seismic activity but also explosions, and our work centred on the processing of 3D point cloud information acquired from stereo imagery. Initially focusing on the development of both supervised and unsupervised damage detection methods built on advanced texture features and basic classifiers such as Support Vector Machine and Random Forest, the work moved on to the use of deep learning. In particular the coupling of image-derived features and 3D point cloud information in a Convolutional Neural Network (CNN) proved successful in detecting also subtle damage features. In addition to the detection of standard rubble and debris, CNN-based methods were developed to detect typical façade damage indicators, such as cracks and spalling, including with a focus on multi-temporal and multi-scale feature fusion. We further developed a processing pipeline and mobile app to facilitate near-real time damage mapping. The solutions were tested in a number of pilot experiments and evaluated by a variety of stakeholders. © 2019 International Society for Photogrammetry and Remote Sensing.","CNN; Computer vision; Drone; First responder; INACHUS; Machine learning; Point clouds; RECONASS","Computer vision; Couplings; Decision trees; Deep learning; Disaster prevention; Disasters; Drones; Feature extraction; Learning systems; Machine learning; Mapping; Neural networks; Pipeline processing systems; Remote sensing; Stereo image processing; Structural analysis; Support vector machines; Textures; Unmanned aerial vehicles (UAV); Convolutional neural network; European research project; First responders; High spatial resolution; INACHUS; Multi-scale features; Point cloud; RECONASS; Damage detection"
"Drone based near real-time human detection with geographic localization","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074302596&doi=10.5194%2fisprs-archives-XLII-3-W8-49-2019&partnerID=40&md5=e5163d3cae6867dda4715514f9b6cb8a","Detection of humans, e.g. for search and rescue operations has been enabled by the availability of compact, easy to use cameras and drones. On the other hand, aerial photogrammetry techniques for inspection applications allow for precise geographic localization and the generation of an overview orthomosaic and 3D terrain model. The proposed solution is based on nadir drone imagery and combines both deep learning and photogrammetric algorithms to detect people and position them with geographical coordinates on an overview orthomosaic and 3D terrain map. The drone image processing chain is fully automated and near real-time and therefore allows search and rescue teams to operate more efficiently in difficult to reach areas. © 2019 International Society for Photogrammetry and Remote Sensing.","Deep Learning; Drones; Geographic Information Systems (GIS); Human Detection; Photogrammetry","3D modeling; Antennas; Deep learning; Disaster prevention; Disasters; Drones; Geographic information systems; Image processing; Photogrammetry; Three dimensional computer graphics; 3D terrain model; Aerial photogrammetry; Fully automated; Geographical coordinates; Human detection; Image processing chains; Search and rescue; Search and rescue operations; Aircraft detection"
"Using virtual scenarios to produce machine learnable environments for wildfire detection and segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074278302&doi=10.5194%2fisprs-archives-XLII-3-W8-9-2019&partnerID=40&md5=392a4d162ab7b4300d6a2c58839a047d","Today's climatic proneness to extreme conditions together with human activity have been triggering a series of wildfire-related events that put at risk ecosystems, as well as animal and vegetal patrimony, while threatening dwellers nearby rural or urban areas. When intervention teams-firefighters, civil protection, police-acknowledge these events, usually they have already escalated to proportions hardly controllable mainly due wind gusts, fuel-like solo conditions, among other conditions that propitiate fire spreading. Currently, there is a wide range of camera-capable sensing systems that can be complemented with useful location data-for example, unmanned aerial systems (UAS) integrated cameras and IMU/GPS sensors, stationary surveillance systems-and processing components capable of fostering wildfire events detection and monitoring, thus providing accurate and faithful data for decision support. Precisely in what concerns to detection and monitoring, Deep Learning (DL) has been successfully applied to perform tasks involving classification and/or segmentation of objects of interest in several fields, such as Agriculture, Forestry and other similar areas. Usually, for an effective DL application, more specifically, based on imagery, datasets must rely on heavy and burdensome logistics to gather a representative problem formulation. What if putting together a dataset could be supported in customizable virtual environments, representing faithful situations to train machines, as it already occurs for human training in what regards some particular tasks (rescue operations, surgeries, industry assembling, etc.)? This work intends to propose not only a system to produce faithful virtual environments to complement and/or even supplant the need for dataset gathering logistics while eventually dealing with hypothetical proposals considering climate change events, but also to create tools for synthesizing wildfire environments for DL application. It will therefore enable to extend existing fire datasets with new data generated by human interaction and supervision, viable for training a computational entity. To that end, a study is presented to assess at which extent data virtually generated data can contribute to an effective DL system aiming to identify and segment fire, bearing in mind future developments of active monitoring systems to timely detect fire events and hopefully provide decision support systems to operational teams. © 2019 International Society for Photogrammetry and Remote Sensing.","Corsican Fire Dataset; Deep Learning (DL); Feature Pyramid Network (FPN); Fire Detection; Fire Monitoring; Fire Virtual Simulation; Virtual-to-Real Learning Transference","Antennas; Artificial intelligence; Cameras; Climate change; Decision support systems; Deep learning; Disaster prevention; Disasters; Fire protection; Forestry; Security systems; Virtual reality; Corsican Fire Dataset; Feature pyramid; Fire detection; Fire Monitoring; Virtual simulations; Virtual-to-Real Learning Transference; Fires"
"Detection and monitoring of beach litter using uav image and deep neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074290689&doi=10.5194%2fisprs-archives-XLII-3-W8-55-2019&partnerID=40&md5=020f9f11c0daf4f913f4eaa7172f4c9e","Beach litter destroys marine ecosystems and creates aesthetic discomfort that lowers the value of the beach. In order to solve this beach litter problem, it is necessary to study the generation and distribution pattern of waste and the cause of the inflow. However, the data for the study are only sample data collected in some areas of the beach. Also, most of the data covers only the total amount of beach litter. UAV(Unmanned Aerial Vehicle) and Deep Neural Network can be effectively used to detect and monitor beach litter. Using UAV, it is possible to easily survey the entire beach. The Deep Neural Network can also identify the type of coastal litter. Therefore, using UAV and Deep Neural Network, it is possible to acquire spatial information by type of beach litter. This paper proposes a Beach litter detection algorithm based on UAV and Deep Neural Network and a Beach litter monitoring process using it. It also offers optimal shooting altitude and film duplication to detect small beach litter such as plastic bottles and styrofoam pieces found on the beach. In this study, DJI Mavic 2 Pro was used. The camera on the UAV is a 1-inch CMOS with a resolution of 20MP. The images obtained through UAV are produced as orthoimages and input into a pre-trained neural network algorithm. The Deep Neural Network used for Beach litter detection removed the Fully Connected Layer from the Convolutional Neural Network for semantic segmentation. © 2019 International Society for Photogrammetry and Remote Sensing.","Deep Learning; Marine Debris; Marine Pollution; Neural Network; Unmanned Aerial Vehicle","Aircraft detection; Antennas; Beaches; Deep learning; Disaster prevention; Disasters; Ecosystems; Marine pollution; Multilayer neural networks; Neural networks; Plastic bottles; Semantics; Unmanned aerial vehicles (UAV); Convolutional neural network; Detection algorithm; Distribution patterns; Marine debris; Semantic segmentation; Spatial informations; Trained neural networks; UAV (unmanned aerial vehicle); Deep neural networks"
"AUTOMATIC DAMAGE DETECTION of STONE CULTURAL PROPERTY BASED on DEEP LEARNING ALGORITHM","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072156050&doi=10.5194%2fisprs-archives-XLII-2-W15-639-2019&partnerID=40&md5=3505573fab52965ab80f4e24ca9a21df","Outdoor stone cultural properties are continuously affected by the external environment such as wind, rain, and earthquakes. These cause damage to the cultural properties by not only threatening structural stability but also damaging the aesthetic value. Quick detection of these damages is important to enable appropriate preservation treatment in terms of cultural property conservation management. Even though conventional manual damage detection methods are widely used, they are limited by manpower, cost, and other external conditions. In this paper, we propose a system that automatically detects and classifies damage occurring in cultural properties using deep-learning technique to settle these drawbacks. In detail, the damages are classified into four types (i.e., crack, loss, detachment, biological colonization) based on Faster region-based convolutional neural network (R-CNN) algorithm. In addition, we construct an image dataset of stone damage, which is collected by the regular report of the National Designated Cultural Property in 2017 conducted by the Cultural Heritage Administration of S. Korea, and augment its dataset to enhance damage detection performance. From the experiment conducted, we achieved an average confidence score of 94.6&thinsp;% or more on the 20 test images. © 2019 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Damage classification; Damage detection; Deep learning; Disaster management","Bioinformatics; Deep learning; Disaster prevention; Disasters; Image enhancement; Learning algorithms; Neural networks; Stability; Conservation management; Convolutional neural network; Damage classification; Detection performance; Disaster management; External conditions; External environments; Structural stabilities; Damage detection"
"DEEP LEARNING for SEMANTIC SEGMENTATION of 3D POINT CLOUD","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072167750&doi=10.5194%2fisprs-archives-XLII-2-W15-735-2019&partnerID=40&md5=4451f7ae78332a88f1344a7dc4863375","Cultural Heritage is a testimony of past human activity, and, as such, its objects exhibit great variety in their nature, size and complexity; from small artefacts and museum items to cultural landscapes, from historical building and ancient monuments to city centers and archaeological sites. Cultural Heritage around the globe suffers from wars, natural disasters and human negligence. The importance of digital documentation is well recognized and there is an increasing pressure to document our heritage both nationally and internationally. For this reason, the three-dimensional scanning and modeling of sites and artifacts of cultural heritage have remarkably increased in recent years. The semantic segmentation of point clouds is an essential step of the entire pipeline; in fact, it allows to decompose complex architectures in single elements, which are then enriched with meaningful information within Building Information Modelling software. Notwithstanding, this step is very time consuming and completely entrusted on the manual work of domain experts, far from being automatized. This work describes a method to label and cluster automatically a point cloud based on a supervised Deep Learning approach, using a state-of-the-art Neural Network called PointNet++. Despite other methods are known, we have choose PointNet++ as it reached significant results for classifying and segmenting 3D point clouds. PointNet++ has been tested and improved, by training the network with annotated point clouds coming from a real survey and to evaluate how performance changes according to the input training data. It can result of great interest for the research community dealing with the point cloud semantic segmentation, since it makes public a labelled dataset of CH elements for further tests. © 2019 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Classification; Deep Learning; Point Cloud; Segmentation; Synthetic Dataset","Classification (of information); Complex networks; Disasters; Image segmentation; Semantics; Statistical tests; Building Information Modelling; Complex architectures; Digital documentation; Point cloud; Research communities; Semantic segmentation; Synthetic Dataset; Three-dimensional scanning; Deep learning"
"GEOMETRIC FEATURES ANALYSIS for the CLASSIFICATION of CULTURAL HERITAGE POINT CLOUDS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072213156&doi=10.5194%2fisprs-archives-XLII-2-W15-541-2019&partnerID=40&md5=a8b053f80e414a7e45b6f0d0a10df787","In the last years, the application of artificial intelligence (Machine Learning and Deep Learning methods) for the classification of 3D point clouds has become an important task in modern 3D documentation and modelling applications. The identification of proper geometric and radiometric features becomes fundamental to classify 2D/3D data correctly. While many studies have been conducted in the geospatial field, the cultural heritage sector is still partly unexplored. In this paper we analyse the efficacy of the geometric covariance features as a support for the classification of Cultural Heritage point clouds. To analyse the impact of the different features calculated on spherical neighbourhoods at various radius sizes, we present results obtained on four different heritage case studies using different features configurations. © 2019 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Covariance features; Cultural Heritage; Point clouds; Random Forest","Artificial intelligence; Decision trees; Deep learning; Geometry; Three dimensional computer graphics; 3-D documentation; 3D point cloud; Covariance features; Cultural heritages; Geometric feature; Learning methods; Point cloud; Random forests; Classification (of information)"
"ARCHITECTURE RECOGNITION by MEANS of CONVOLUTIONAL NEURAL NETWORKS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072178360&doi=10.5194%2fisprs-archives-XLII-2-W15-77-2019&partnerID=40&md5=bf9e09827bedf63a2b1adab6f7a1389d","The use of mobile computing technologies can change the experience of visiting cultural sites by making vast digital heritage collections accessible on site. The spread of machine learning technologies on mobile devices is encouraging the interaction of artificial intelligence with the shape of the built environment. However, while some research already applies deep learning image recognition in an urban context, the literature on how to develop effective neural networks to detect architectural features is still limited, as well as the availability of architecture-related datasets. This work presents the steps and results of the prototype development of a mobile app to perform monument recognition using convolutional neural networks. The tool allows users to interact with the physical space and access a digital archive of texts, models, images and other data. © 2019 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Architectural Heritage; Artificial Intelligence; Convolutional Neural Networks; Deep Learning; Image Classification; Machine Learning; Mobile Computing","Architecture; Artificial intelligence; Convolution; Deep learning; Deep neural networks; Image classification; Image recognition; Learning systems; Machine learning; Mobile computing; Neural networks; Architectural features; Architectural heritage; Built environment; Convolutional neural network; Digital archives; Machine learning technology; Mobile computing technology; Prototype development; Network architecture"
"ARCHITECTURAL HERITAGE RECOGNITION in HISTORICAL FILM FOOTAGE USING NEURAL NETWORKS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072165734&doi=10.5194%2fisprs-archives-XLII-2-W15-343-2019&partnerID=40&md5=6239ad1b6ad164909fbfc7df2f9cc892","Researching historical archives for material suitable for photogrammetry is essential for the documentation and 3D reconstruction of Cultural Heritage, especially when this heritage has been lost or transformed over time. This research presents an innovative workflow which combines the photogrammetric procedure with Machine Learning for the processing of historical film footage. A Neural Network is trained to automatically detect frames in which architectural heritage appears. These frames are subsequently processed using photogrammetry and finally the resulting model is assessed for metric quality. This paper proposes best practises in training and validation on a Cultural Heritage asset. The algorithm was tested through a case study of the Tour Saint Jacques in Paris for which an entirely new dataset was created. The findings are encouraging both in terms of saving human effort and of improvement of the photogrammetric survey pipeline. This new tool can help researchers to better manage and organize historical information. © 2019 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Cultural Heritage; Deep Learning; Historical Video Classification; Neural Networks; Photogrammetric Workflow; TensorFlow","Deep learning; Deep neural networks; Motion pictures; Neural networks; 3D reconstruction; Architectural heritage; Cultural heritages; Historical archive; Historical information; Photogrammetric Workflow; TensorFlow; Video classification; Photogrammetry"
"Super-resolution reconstruction algorithms based on fusion of deep learning mechanism and wavelet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076009305&doi=10.1145%2f3357254.3358600&partnerID=40&md5=d9950831e29011996c436064dead3789","In this paper, we consider the problem of super-resolution reconstruction. This is a hot topic because super-resolution reconstruction has a wide range of applications in the medical field, remote sensing monitoring, and criminal investigation. Compared with traditional algorithms, the current super-resolution reconstruction algorithm based on deep learning greatly improves the clarity of reconstructed pictures. Existing work like Super-Resolution Using a Generative Adversarial Network (SRGAN) can effectively restore the texture details of the image. However, experimentally verified that the texture details of the image recovered by the SRGAN are not robust. In order to get super-resolution reconstructed images with richer high-frequency details, we improve the network structure and propose a super-resolution reconstruction algorithm combining wavelet transform and Generative Adversarial Network. The proposed algorithm can efficiently reconstruct high-resolution images with rich global information and local texture details. We have trained our model by Py Torch framework and VOC2012 dataset, and tested it by Set5, Set14, BSD100 and Urban100 test datasets. © 2019 Association for Computing Machinery.","Deep learning; Generative Adversarial Network; Super-resolution reconstruction; Wavelet transform","Artificial intelligence; Image enhancement; Image reconstruction; Image texture; Learning algorithms; Optical resolving power; Pattern recognition; Remote sensing; Statistical tests; Textures; Wavelet transforms; Adversarial networks; Criminal investigation; Global informations; High resolution image; Learning mechanism; Reconstructed image; Remote sensing monitoring; Super resolution reconstruction; Deep learning"
"Automatic Extraction and Classification of Road Markings Based on Deep Learning [基于深度学习的道路标线自动提取与分类方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071938081&doi=10.3788%2fCJL201946.0804002&partnerID=40&md5=6fc3773108c9ebfd1fd9a3261e776ce6","Extraction and classification of road markings are two key technologies to be solved in the construction of an intelligent city and urgent technical problems that must be solved for intelligent driving. Therefore, herein, we propose a method of automatic extraction and classification for road markings based on deep learning. First, the ground point clouds are extracted through the moving-window method combined with the topological relations of adjacent scanning lines, and then the intensity images are generated. Automatic road-marking extraction and classification are realized based on the deep learning method. Road-marking vectorization is performed using the KD tree clustering algorithm and the vectorization scheme. The proposed method is analyzed based on the obtained experimental data. Results show that the precision and Fscore of the automatic road-marking extraction and classification reach 92.59% and 90.15%, respectively, proving the feasibility and accuracy of this method. Thus, the proposed method provides a new idea for automatic road-marking extraction and improves its accuracy, efficiency, and intelligent degree of road-marking acquisition and classification. © 2019, Chinese Lasers Press. All right reserved.","Automatic extraction; Deep learning; Mobile measurement system; Remote sensing; Road marking","Clustering algorithms; Deep learning; Extraction; Highway markings; Remote sensing; Roads and streets; Trees (mathematics); Automatic extraction; Intensity images; Key technologies; Learning methods; Mobile measurement systems; Moving window methods; Road marking; Topological relations; Road and street markings"
"An object-based and heterogeneous segment filter convolutional neural network for high-resolution remote sensing image classiﬁcation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062362438&doi=10.1080%2f01431161.2019.1584687&partnerID=40&md5=23c6737fb8d6f8966bf0a0f06d75e483","In recent years, object-based segmentation methods and shallow-model classification algorithms have been widely integrated for remote sensing image supervised classification. However, as the image resolution increases, remote sensing images contain increasingly complex characteristics, leading to higher intraclass heterogeneity and interclass homogeneity and thus posing substantial challenges for the application of segmentation methods and shallow-model classification algorithms. As important methods of deep learning technology, convolutional neural networks (CNNs) can hierarchically extract higher-level spatial features from images, providing CNNs with a more powerful recognition ability for target detection and scene classification in high-resolution remote sensing images. However, the input of the traditional CNN is an image patch, the shape of which is scarcely consistent with a given segment. This inconsistency may lead to errors when directly using CNNs in object-based remote sensing classification: jagged errors may appear along the land cover boundaries, and some land cover areas may overexpand or shrink, leading to many obvious classification errors in the resulting image. To address the above problem, this paper proposes an object-based and heterogeneous segment filter convolutional neural network (OHSF-CNN) for high-resolution remote sensing image classiﬁcation. Before the CNN processes an image patch, the OHSF-CNN includes a heterogeneous segment filter (HSF) to process the input image. For the segments in the image patch that are obviously different from the segment to be classified, the HSF can differentiate them and reduce their negative influence on the CNN training and decision-making processes. Experimental results show that the OHSF-CNN not only can take full advantage of the recognition capabilities of deep learning methods but also can effectively avoid the jagged errors along land cover boundaries and the expansion/shrinkage of land cover areas originating from traditional CNN structures. Moreover, compared with the traditional methods, the proposed OHSF-CNN can achieve higher classification accuracy. Furthermore, the OHSF-CNN algorithm can serve as a bridge between deep learning technology and object-based segmentation algorithms thereby enabling the application of object-based segmentation methods to more complex high-resolution remote sensing images. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",,"Complex networks; Convolution; Data mining; Decision making; Deep learning; Errors; Image resolution; Neural networks; Object recognition; Positive ions; Remote sensing; Classification accuracy; Complex characteristics; Convolutional neural network; Decision making process; High resolution remote sensing images; Remote sensing classification; Segmentation algorithms; Supervised classification; Image segmentation; accuracy assessment; algorithm; artificial neural network; image classification; image processing; image resolution; remote sensing; segmentation"
"Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071496532&doi=10.3390%2fs19163595&partnerID=40&md5=22422ccd528d8fbb394039e3b69dc30e","Detection and classification of tree species from remote sensing data were performed using mainly multispectral and hyperspectral images and Light Detection And Ranging (LiDAR) data. Despite the comparatively lower cost and higher spatial resolution, few studies focused on images captured by Red-Green-Blue (RGB) sensors. Besides, the recent years have witnessed an impressive progress of deep learning methods for object detection. Motivated by this scenario, we proposed and evaluated the usage of Convolutional Neural Network (CNN)-based methods combined with Unmanned Aerial Vehicle (UAV) high spatial resolution RGB imagery for the detection of law protected tree species. Three state-of-the-art object detection methods were evaluated: Faster Region-based Convolutional Neural Network (Faster R-CNN), YOLOv3 and RetinaNet. A dataset was built to assess the selected methods, comprising 392 RBG images captured from August 2018 to February 2019, over a forested urban area in midwest Brazil. The target object is an important tree species threatened by extinction known as Dipteryx alata Vogel (Fabaceae). The experimental analysis delivered average precision around 92% with an associated processing times below 30 miliseconds. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Object-detection; Remote sensing","Antennas; Convolution; Deep learning; Forestry; Image resolution; Neural networks; Object detection; Object recognition; Optical radar; Remote sensing; Spectroscopy; Unmanned aerial vehicles (UAV); Convolutional neural network; Experimental analysis; High spatial resolution; Individual tree detections; Light detection and ranging; Object detection method; Remote sensing data; Spatial resolution; Aircraft detection; chemistry; discriminant analysis; Fabaceae; photography; physiology; remote sensing; statistical model; Deep Learning; Discriminant Analysis; Fabaceae; Likelihood Functions; Neural Networks, Computer; Photography; Remote Sensing Technology"
"Remote Sensing Image Aircraft Detection Technology Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078309449&doi=10.1109%2fIHMSC.2019.00048&partnerID=40&md5=56675c31c443676865dadfb540fc3537","How to use high-resolution remote sensing images to quickly and accurately obtain high-value target information such as aircraft and ships has become a hot topic in the research of automatic target detection. This paper uses Google Earth image to make aircraft data set, then detect the aircraft targets respectively based on the two deep learning models YOLOv3 and Faster-R-CNN. At the same time, in order to significantly improve the aircraft detection accuracy, this paper proposes a shadow processing algorithm with double threshold random sampling to conduct data preprocessing for aircraft targets in remote sensing images. The experimental results show that both deep learning models can effectively detect aircraft targets and have great application potential in automatic detection of remote sensing image targets. The shadow processing algorithm can effectively eliminate the projected shadow of the aircraft, restore the texture characteristics of the shaded area, greatly improve the overall quality of the remote sensing image, and lay a good data foundation for subsequent aircraft detection. © 2019 IEEE.","deep learning; Remote sensing image; shadow processing; target detection","Aircraft; Aircraft detection; Image enhancement; Man machine systems; Object recognition; Remote sensing; Target tracking; Textures; Automatic Detection; Automatic target detection; Data preprocessing; Detection technology; High resolution remote sensing images; Processing algorithms; Remote sensing images; Texture characteristics; Deep learning"
"Automatic building change image quality assessment in high resolution remote sensing based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069882014&doi=10.1016%2fj.jvcir.2019.102585&partnerID=40&md5=2c3951c02f84ffbb09446427a8a8c0e6","The multi-temporal high-resolution remote sensing (HRRS) images are usually acquired at different imaging angles, with serious noise interferences and obvious building shadows, so that detecting the changes of urban buildings is a problem. In order to address this challenge, a deep learning-based algorithm called ABCDHIDL is proposed to automatically detect the building changes from multi-temporal HRRS images. Firstly, an automatic selection method of labeled samples of building changes based on morphology (ASLSBCM) is proposed. Secondly, a deep learning model (DBN-ELM) for building changes detection based on deep belief network (DBN) and extreme learning machine (ELM) is proposed. A convolution operation is employed to extract the spectral, texture and spatial features and generate a combined low-level features vector for each pixel in the multi-temporal HRRS images. The unlabeled samples are introduced to pre-train the DBN, and the parameters of DBN-ELM are globally optimized by jointly using the ELM classifier and the labeled samples are offered by ASLSBCM to further improve the detection accuracy. In order to evaluate the performance of ABCDHIDL, four groups of double-temporal WorldView2 HRRS images in four different experimental regions are selected respectively as the test datasets, and five other representative methods are used and compared with ABCDHIDL in the experiments of buildings change detection. The results show that ABCDHIDL has higher accuracy and automation level than the other five methods despite its relatively higher time consumption. © 2019","Building change detection; Deep belief network; Extreme learning machine; Morphological building index; Morphological shadow index; Quality Assessment","Buildings; Knowledge acquisition; Machine learning; Remote sensing; Textures; Building change detection; Deep belief networks; Extreme learning machine; Morphological building index; Quality assessment; Shadow indices; Deep learning"
"Crop classification based on deep learning in northeast China using sar and optical imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074154612&doi=10.1109%2fBIGSARDATA.2019.8858437&partnerID=40&md5=a2780970a8d9270272bddbe9f9dda808","Crop classification is a significant requirement to estimate crop area, structure, and spatial distribution, as well as provide important input parameters for crop yield models. Compared with optical remote sensing, Synthetic Aperture Radar (SAR) can be applied in all-time and all-weather condition without clouds interference. This study aims to develop a deep learning based crop classification for multi-source and multi-temporal remote sensing imageries, including C-band GF-3, Sentinel-1 and Sentinel-2 data. The experiment was carried out in Northeast of China. Convolutional neural network (CNN) and visual geometry group (VGG) were used for classify crops based on the different numbers of input bands composed by optical and SAR data. The overall accuracy of crop classification reached 91.6% , and the kappa coefficient was 0.88. The classification results proved that combination of multi-source and multi-temporal remote sensing imagery can effectively improve the classification accuracy of crops. © 2019 IEEE.","CNN; Crop Classification; Optical; SAR; VGG","Big data; Crops; Image classification; Image enhancement; Neural networks; Radar imaging; Remote sensing; Synthetic aperture radar; Classification accuracy; Classification results; Convolutional neural network; Crop classification; Multi-temporal remote sensing; Optical; Optical remote sensing; Overall accuracies; Deep learning"
"A Data Augmentation-Assisted Deep Learning Model for High Dimensional and Highly Imbalanced Hyperspectral Imaging Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073234397&doi=10.1109%2fICIST.2019.8836913&partnerID=40&md5=62185fd1e03c98c526fd5cbd8e3b6a38","Recent advances in remote sensing technologies have led to the fast proliferation of massive and often imbalanced datasets. Direct classification in these datasets becomes difficult, because of the high dimensionality, and the fact that minority classes are overlapped and dwarfed by majority classes. Deep learning is the state-of-The-Art in image classification, with applications in face-and text detection, text recognition, as well as voice classification. However, deep learning requires a favorable ratio between dimensionality and sample size. To address high dimensional yet imbalanced datasets, in this paper, we propose the integration of data augmentation, to a deep learning classifier of a high dimensional and highly imbalanced photo-Thermal infrared hyperspectral dataset of chemical substances. First, we apply a basic deep machine learning approach using a convolutional neural network (CNN) on the original dataset. Second, we apply principal component analysis (PCA) to reduce dimensionality before applying CNN. Third, we prepend an offline data augmentation step to increase dataset size before applying CNN. After that, we evaluate the performance by calculating the probability of detection (POD), and recall based on true positive (TP), false negative (FN), false positive (FP), and true negative (TN). © 2019 IEEE.","Convolutional neural networks; Data augmentation; Deep learning; Hyperspectral classification; Principal component analysis","Character recognition; Convolution; Deep learning; Deep neural networks; Hyperspectral imaging; Neural networks; Principal component analysis; Remote sensing; Spectroscopy; Text processing; Convolutional neural network; Data augmentation; Hyper-spectral classification; Hyperspectral imaging datum; Imbalanced Data-sets; Machine learning approaches; Probability of detection; Remote sensing technology; Classification (of information)"
"Deep Learning Based Single Image Super-resolution: A Survey","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069183006&doi=10.1007%2fs11633-019-1183-x&partnerID=40&md5=c52bfaba5585ee6e3009605124ee0e6d","Single image super-resolution has attracted increasing attention and has a wide range of applications in satellite imaging, medical imaging, computer vision, security surveillance imaging, remote sensing, objection detection, and recognition. Recently, deep learning techniques have emerged and blossomed, producing “the state-of-the-art” in many domains. Due to their capability in feature extraction and mapping, it is very helpful to predict high-frequency details lost in low-resolution images. In this paper, we give an overview of recent advances in deep learning-based models and methods that have been applied to single image super-resolution tasks. We also summarize, compare and discuss various models from the past and present for comprehensive understanding and finally provide open problems and possible directions for future research. © 2019, Institute of Automation, Chinese Academy of Sciences and Springer-Verlag GmbH Germany, part of Springer Nature.","convolutional neural network; deep learning; high-resolution image; Image super-resolution; low-resolution image","Deep learning; Deep neural networks; Neural networks; Optical resolving power; Remote sensing; Convolutional neural network; High frequency HF; High resolution image; Image super resolutions; Learning Based Models; Learning techniques; Low resolution images; Security surveillance; Medical imaging"
"Application of deep learning techniques for determining the spatial extent and classification of seagrass beds, Trang, Thailand","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067418338&doi=10.1515%2fbot-2018-0017&partnerID=40&md5=a4e014071bb3b85000790baef7faf071","Few studies have investigated the long-term temporal dynamics of seagrass beds, especially in Southeast Asia. Remote sensing is one of the best methods for observing these dynamic patterns, and the advent of deep learning technology has led to recent advances in this method. This study examined the feasibility of applying image classification methods to supervised classification and deep learning methods for monitoring seagrass beds. The study site was a relatively natural seagrass bed in Hat Chao Mai National Park, Trang Province, Thailand, for which aerial photographs from the 1970s were available. Although we achieved low accuracy in differentiating among various densities of vegetation coverage, classification related to the presence of seagrass was possible with an accuracy of 80% or more using both classification methods. Automatic classification of benthic cover using deep learning provided similar or better accuracy than that of the other methods even when grayscale images were used. The results also demonstrate that it is possible to monitor the temporal dynamics of an entire seagrass area, as well as variations within sub-regions, located in close proximity to a river mouth. © 2019 Walter de Gruyter GmbH, Berlin/Boston.","Andaman Sea; deep learning; land cover classification; long-term dynamics; remote sensing","algorithm; detection method; image classification; long-term change; remote sensing; seagrass; supervised classification; vegetation cover; Andaman Sea; Indian Ocean; Southern Region; Thailand; Trang"
"Winter Wheat Yield Estimation from Multitemporal Remote Sensing Images based on Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074281995&doi=10.1109%2fMulti-Temp.2019.8866918&partnerID=40&md5=111d336a8f86e1b7b7133a923a1e1d57","The development of deep learning and big data technology has introduced information and intelligent techniques to agricultural remote sensing estimation. The deep learning methods represented by Convolutional Neural Network (CNN) have abilities to extract the depth-dependent features of crop growth. In the field of crop yield estimation, the core challenge is to utilize CNN to extract the related information from remote sensing image. In this paper, we apply histogram dimensionality reduction and time series fusion to generate the input layer of CNN. In view of the data characteristics, the CNN network structure was designed to extract the features of winter wheat growth from multitemporal MODIS images for yield estimation in North China. The results showed that the estimated yield of winter wheat based on time-series remote sensing images is highly correlated with statistical data, with Pearson's r of 0.82, RMSE of 724.72 kg.hm-2. In the case of sufficient statistical data, the provincial model performs better. CNN is able to mine more relevant information and has higher robustness. It also provides a technical reference for estimating large-scale crop yield. © 2019 IEEE.","CNN; North China; Winter Wheat; Yield Estimation","Convolution; Crops; Deep learning; Extraction; Image analysis; Neural networks; Time series; Agricultural remote sensing; Convolutional neural network; Dimensionality reduction; Intelligent techniques; Multi-temporal remote sensing; North China; Winter wheat; Yield estimation; Remote sensing"
"Quality assessment on remote sensing image based on neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068890685&doi=10.1016%2fj.jvcir.2019.102580&partnerID=40&md5=7d01245ea0af3eeda43af4de11fd59f8","Image quality assessment is of great significance for the designment and application of remote sensing systems. CNN based method is proposed for image quality assessment on remote sensing image in this paper. Specifically, we first introduce the convolutional neural network and deep learning method. Then a deep CNN architecture is constructed to automatically extract image features to evaluate image quality. Afterward, the information entropy threshold is used to remove the image blocks with less information content. Finally, a deep network model with two convolutional layers is used to achieve feature extraction and image quality scoring. The experimental results show that the quality score of this method has good subjective and objective consistency for multi-distortion remote sensing images and common multi-distortion images. Evaluation of distorted images does not depend on a specific database and has database independence. In addition, our proposed method is simple to achievement. © 2019","Deep learning; Image quality assessment; Information entropy; Remote sensing image","Convolution; Deep learning; Neural networks; Quality control; Remote sensing; Convolutional neural network; Image quality assessment; Information contents; Information entropy; Learning methods; Quality assessment; Remote sensing images; Remote sensing system; Image quality"
"Hierarchical and Robust Convolutional Neural Network for Very High-Resolution Remote Sensing Object Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069769380&doi=10.1109%2fTGRS.2019.2900302&partnerID=40&md5=6c64de95eb52654c49f924c6070b0aa1","Object detection is a basic issue of very high-resolution remote sensing images (RSIs) for automatically labeling objects. At present, deep learning has gradually gained the competitive advantage for remote sensing object detection, especially based on convolutional neural networks (CNNs). Most of the existing methods use the global information in the fully connected feature vector and ignore the local information in the convolutional feature cubes. However, the local information can provide spatial information, which is helpful for accurate localization. In addition, there are variable factors, such as rotation and scaling, which affect the object detection accuracy in RSIs. In order to solve these problems, this paper presents a hierarchical robust CNN. First, multiscale convolutional features are extracted to represent the hierarchical spatial semantic information. Second, multiple fully connected layer features are stacked together so as to improve the rotation and scaling robustness. Experiments on two data sets have shown the effectiveness of our method. In addition, a large-scale high-resolution remote sensing object detection data set is established to make up for the current situation that the existing data set is insufficient or too small. The data set is available at https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset. © 1980-2012 IEEE.","Convolutional neural networks (CNNs); hierarchical robust CNN (HRCNN); hierarchical spatial semantic (HSS); object detection; remote sensing images (RSIs); rotation and scaling robust enhancement (RSRE)","Competition; Convolution; Deep learning; Image enhancement; Neural networks; Object recognition; Remote sensing; Semantics; Competitive advantage; Convolutional neural network; hierarchical robust CNN (HRCNN); High resolution remote sensing; Remote sensing images; Spatial informations; Spatial semantics; Very high resolution; Object detection; artificial neural network; data set; detection method; hierarchical system; machine learning; remote sensing"
"A cloud detection algorithm for satellite imagery based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065902613&doi=10.1016%2fj.rse.2019.03.039&partnerID=40&md5=308a6a9530730d055b358b472aec01fb","Reliable detection of clouds is a critical pre-processing step in optical satellite based remote sensing. Currently, most methods are based on classifying invidual pixels from their spectral signatures, therefore they do not incorporate the spatial patterns. This often leads to misclassifications of highly reflective surfaces, such as human made structures or snow/ice. Multi-temporal methods can be used to alleviate this problem, but these methods introduce new problems, such as the need of a cloud-free image of the scene. In this paper, we introduce the Remote Sensing Network (RS-Net), a deep learning model for detection of clouds in optical satellite imagery, based on the U-net architecture. The model is trained and evaluated using the Landsat 8 Biome and SPARCS datasets, and it shows state-of-the-art performance, especially over biomes with hardly distinguishable scenery, such as clouds over snowy and icy regions. In particular, the performance of the model that uses only the RGB bands is significantly improved, showing promising results for cloud detection with smaller satellites with limited multi-spectral capabilities. Furthermore, we show how training the RS-Net models on data from an existing cloud masking method, which are treated as noisy data, leads to increased performance compared to the original method. This is validated by using the Fmask algorithm to annotate the Landsat 8 datasets, and then use these annotations as training data for regularized RS-Net models, which then show improved performance compared to the Fmask algorithm. Finally, the classification time of a full Landsat 8 product is 18.0 ± 2.4 s for the largest RS-Net model, thereby making it suitable for production environments. © 2019","Cloud detection; Deep learning; Open data; Optical satellite imagery","Open Data; Remote sensing; Satellite imagery; Signal detection; Classification time; Cloud detection; Cloud detection algorithms; Optical satellite imagery; Pre-processing step; Production environments; Reliable detection; State-of-the-art performance; Deep learning; algorithm; detection method; Landsat; pixel; satellite data; satellite imagery"
"Unsupervised deep learning based change detection in Sentinel-2 images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074261635&doi=10.1109%2fMulti-Temp.2019.8866899&partnerID=40&md5=25ba3a12a30dfec24e683bb4a0a26b96","Change Detection (CD) is an important application of remote sensing. Recent technological evolution resulted in the availability of optical multispectral sensors that provide High spatial Resolution (HR) images with many spectral bands. Such characteristics allow for new applications of CD, however present new challenges on the proper exploitation of the information. HR multitemporal data processing is challenging due to spatial correlation of pixels and spatial context information needs to be exploited to benefit from multitemporal HR images. Moreover most of the state-of-The-Art CD methods exploit single or couple of spectral channels from the optical sensors to derive CD map. To overcome these challenges, this paper presents a novel unsupervised deep-learning based method that can effectively model contextual information and handle all the bands in multispectral images. In particular, we focus on the Sentinel-2 images provided by the European Space Agency (ESA) that provides both higher spatial and temporal resolution optical images with 13 spectral bands with respect to previous generation sensors. Experimental results on the urban Onera satellite CD (OSCD) dataset and on agricultural multitemporal images from Barrax, Spain confirms the effectiveness of the proposed method. © 2019 IEEE.","Change detection; Deep learning; Generative Adversarial Network; High Resolution; Sentinel-2","Data handling; Geometrical optics; Image analysis; Remote sensing; Space optics; Adversarial networks; Change detection; High resolution; High spatial resolution; Learning-based methods; Sentinel-2; Spatial and temporal resolutions; Technological evolution; Deep learning"
"Edge-Enhanced GAN for Remote Sensing Image Superresolution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069773369&doi=10.1109%2fTGRS.2019.2902431&partnerID=40&md5=4ce590788aca7fbb4534236883443046","The current superresolution (SR) methods based on deep learning have shown remarkable comparative advantages but remain unsatisfactory in recovering the high-frequency edge details of the images in noise-contaminated imaging conditions, e.g., remote sensing satellite imaging. In this paper, we propose a generative adversarial network (GAN)-based edge-enhancement network (EEGAN) for robust satellite image SR reconstruction along with the adversarial learning strategy that is insensitive to noise. In particular, EEGAN consists of two main subnetworks: an ultradense subnetwork (UDSN) and an edge-enhancement subnetwork (EESN). In UDSN, a group of 2-D dense blocks is assembled for feature extraction and to obtain an intermediate high-resolution result that looks sharp but is eroded with artifacts and noises as previous GAN-based methods do. Then, EESN is constructed to extract and enhance the image contours by purifying the noise-contaminated components with mask processing. The recovered intermediate image and enhanced edges can be combined to generate the result that enjoys high credibility and clear contents. Extensive experiments on Kaggle Open Source Data set, Jilin-1 video satellite images, and Digitalglobe show superior reconstruction performance compared to the state-of-the-art SR approaches. © 1980-2012 IEEE.","Adversarial learning; dense connection; edge enhancement; remote sensing imagery; superresolution","Deep learning; Image reconstruction; Optical resolving power; Remote sensing; Satellites; Adversarial learning; dense connection; Edge enhancements; Remote sensing imagery; Super resolution; Image enhancement; algorithm; experimental design; image processing; reconstruction; remote sensing; satellite data; satellite imagery"
"Cloud removal with fusion of SAR and Optical Images by Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074253572&doi=10.1109%2fMulti-Temp.2019.8866939&partnerID=40&md5=9a2c84444d558efceb089852f44c5eb9","Due to the different imaging methods of SAR image and optical image, it is difficult to establish the corresponding relationship between them by traditional methods. However, with the development of deep learning, there are many researches on the transformation from SAR image to optical image based on GAN, which prove that the mapping between SAR image and optical image can be achieved. Based on this, this work will transform the SAR image into optical image and fuse to fill the cloud area of the optical image. This work will provide a method of heterogeneous image fusion to remove cloud, and get a good effect. © 2019 IEEE.","Cloud removal; Deep Learning; GAN","Deep learning; Geometrical optics; Image analysis; Image fusion; Remote sensing; Synthetic aperture radar; Cloud removal; Imaging method; Optical image; SAR Images; Radar imaging"
"Remote sensing estimation on yield of winter wheat in North China based on convolutional neural network [基于卷积神经网络的中国北方冬小麦遥感估产]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074646685&doi=10.11975%2fj.issn.1002-6819.2019.15.016&partnerID=40&md5=7f5a4fb9fc6f408b1d083dd66ad833f6","Accurate and timely winter wheat yield estimation has significant effect to grain markets and policy. Most crop estimation methods can be divided into two categories, one is based on the crop model and the other is the statistical learning method. For statistical learning methods with recent advances in deep learning, convolutional neural network (CNN) have become state-of-the-art algorithms. can extract the depth-dependent features of crop growth. However, the pivotal challenge is to combine remote sensing images with CNN. In this paper, we employ the method of histogram dimensionality reduction and time series fusion to generate the input layer. The experiment firstly performed projection transformation, splicing, mask, fusion, and clipping for 6 different MODIS images in the research area from 2006 to 2016, and then generated 21 600 fusion images of 12 bands (surface reflectance data of 7 different wavelengths in MOD09A1, surface temperature of day and night in MYD11A2, NDVI and EVI in MOD13A1, and FPAR in MOD15A2H). Then, the sensitivity range of winter wheat growth in each band is divided into 36 sections, and the histogram statistics are used to reduce the dimension to generate a vector of length 36, so the remote sensing image generates a matrix of 36×36×12 in the 228-day growing season. The corresponding time and regional statistics are applied as the output layer to construct a complete sample. The yield estimation sample database of 12 indices in the winter wheat region of north China (60 prefecture-level cities) from 2006 to 2016 was constructed, and the training set and verification set were divided into 10:1 for the training and evaluation of yield estimation model. Finally, the neural network structure is designed according to the sample, which consists of the input layer, 7 convolution layers (c1-c7), 7 activation layers, 7 batch normalization layers, 3 dropout layers, 2 full connection layers, and output layer. The number of c1-c7 convolution kernels is 64, 64, 128, 128, 256, 256, 256, the convolution kernel size is 3×3 dpi, and the sliding step length is 2, 1, 2, 2, 2, 1 and 2 respectively, 1 zero paddings per convolutional layer. At the same time, batch normalization and Relu function activation are performed on each convolutional layer, and the Dropout layer is used in the fully connected layer. The results show that: 1) The root-mean-square error (RMSE) and coefficient of determination (R2) of the convolutional neural network model on the training set are 183.82 kg/hm2 and 0.98 respectively. In the validation set, RMSE and R2 are 689.72 kg/hm2 and 0.71. 2) With the same neural network structure, the average RMSE of the estimated samples from 2006 to 2016 trained as validation sets for 11 models was 772.03 kg/hm2. The error of the yield estimation model was the largest in 2007 and the smallest in 2012, and the RMSE was 920.45 kg/hm2 and 632.08 kg/hm2 respectively. Crop estimation algorithm based on CNN has high robustness and precision; 3) The accuracy analysis of prediction yield at the municipal level of different provinces in three temporal points of 2007, 2012 and 2016 indicates that the model has higher accuracy in most areas of the northern winter wheat region, especially, RMSE of Hebei and Shandong provinces is approximately 500 kg/hm2. The result shows that CNN is well applied to the estimation of winter wheat production. This is a great thought of remote sensing combined with the deep learning algorithm. This method can be used to estimate yield by remote sensing in different scales and regions. Compared with the traditional method, this ""start-to-end"" learning method has the advantage of synergy and can obtain the optimal estimation model relative to the whole area. Meanwhile, As data accumulates, the estimation accuracy will be continuously improved, and it has a good application prospect in the national agricultural production forecast. © 2019, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Convolutional neural network; Crop yield estimation; Crops; Deep learning; Remote sensing; Winter wheat; Yield","Chemical activation; Convolution; Crops; Deep learning; Deep neural networks; Graphic methods; Learning algorithms; Mean square error; Metadata; Remote sensing; Coefficient of determination; Convolutional neural network; Crop yield; Remote sensing estimations; State-of-the-art algorithms; Statistical learning methods; Winter wheat; Yield; Multilayer neural networks"
"A Semisupervised Deep Learning Framework for Tropical Cyclone Intensity Estimation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074261688&doi=10.1109%2fMulti-Temp.2019.8866970&partnerID=40&md5=3b68e11c613512f52e8018d21321c997","Tropical cyclone intensity estimation is important to catastrophic weather forecast. In this paper, it is treated as a classification task, with the intensity categories as class labels. Normally, traditional supervised methods require a large amount of prior knowledge for training. However, in reality, only a small amount of labeled samples can be available. Therefore, this paper proposes a novel semisupervised deep learning framework based on convolutional neural networks (CNNs) for FY-4 multispectral images (MSI). The new model only needs a small set of samples labeled a priori to accurately classify the images and estimate cyclone intensity. Moreover, the model involves an iterative training set update process with a hybrid similarity measurement especially designed for the task. The experiments show that the classification performance of the network is improved during the iterations. Evaluation on the estimated intensity categories indicate that the proposed method is significantly better than several existing methods, including the state-of-The-Art cyclone intensity estimation model based on CNN, while small training sets are used. © 2019 IEEE.","classification; convolutional neural network (CNN); intensity estimation; semisupervised; tropical cyclone","Classification (of information); Convolution; Hurricanes; Image analysis; Iterative methods; Neural networks; Remote sensing; Storms; Tropics; Weather forecasting; Classification performance; Convolutional neural network; Intensity estimation; Multispectral images; Semi-supervised; Similarity measurements; Tropical cyclone; Tropical cyclone intensity; Deep learning"
"Siamese Convolutional Neural Networks for Remote Sensing Scene Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066608270&doi=10.1109%2fLGRS.2019.2894399&partnerID=40&md5=68478a6850c444d175d306f5152d8a6f","The convolutional neural networks (CNNs) have shown powerful feature representation capability, which provides novel avenues to improve scene classification of remote sensing imagery. Although we can acquire large collections of satellite images, the lack of rich label information is still a major concern in the remote sensing field. In addition, remote sensing data sets have their own limitations, such as the small scale of scene classes and lack of image diversity. To mitigate the impact of the existing problems, a Siamese CNN, which combines the identification and verification models of CNNs, is proposed in this letter. A metric learning regularization term is explicitly imposed on the features learned through CNNs, which enforce the Siamese networks to be more robust. We carried out experiments on three widely used remote sensing data sets for performance evaluation. Experimental results show that our proposed method outperforms the existing methods. © 2019 IEEE.","Classification; convolutional neural networks (CNNs); deep learning; remote sensing","Classification (of information); Convolution; Deep learning; Deep neural networks; Image enhancement; Neural networks; Convolutional neural network; Feature representation; Label information; Regularization terms; Remote sensing data; Remote sensing imagery; Scene classification; Verification model; Remote sensing; algorithm; artificial neural network; image classification; numerical method; remote sensing; satellite imagery"
"Multi-Modal Object Tracking and Image Fusion with Unsupervised Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072642667&doi=10.1109%2fJSTARS.2019.2920234&partnerID=40&md5=e693a177a38e7e4ef8f55cda87b165c2","The number of different modalities for remote sensors continues to grow, bringing with it an increase in the volume and complexity of the data being collected. Although these datasets individually provide valuable information, in aggregate they provide additional opportunities to discover meaningful patterns on a large scale. However, the ability to combine and analyze disparate datasets is challenged by the potentially vast parameter space that results from aggregation. Each dataset in itself requires instrument-specific and dataset-specific knowledge. If the intention is to use multiple, diverse datasets, one needs an understanding of how to translate and combine these parameters in an efficient and effective manner. While there are established techniques for combining datasets from specific domains or platforms, there is no generic, automated method that can address the problem in general. Here, we discuss the application of deep learning to track objects across different image-like data-modalities, given data in a similar spatiooral range, and automatically co-register these images. Using deep belief networks combined with unsupervised learning methods, we are able to recognize and separate different objects within image-like data in a structured manner, thus making progress toward the ultimate goal of a generic tracking and fusion pipeline requiring minimal human intervention. © 2008-2012 IEEE.","Big data applications; clustering; computer vision; deep belief networks (DBNs); deep learning","Computer vision; Image fusion; Image processing; Large dataset; Remote sensing; Tracking (position); Unsupervised learning; Automated methods; Big data applications; clustering; Deep belief networks; Human intervention; Parameter spaces; Specific knowledge; Unsupervised learning method; Deep learning; algorithm; cluster analysis; computer simulation; image analysis; machine learning; remote sensing; satellite imagery; tracking"
"Query point-oriented Hashing retrieval of remote sensing images [面向查询点的遥感影像哈希检索方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073264305&doi=10.3969%2fj.issn.1001-506X.2019.08.06&partnerID=40&md5=88949fc5eb0c16916f61a1cdb41a0ca1","Due to the low storage cost and fast query speed, the Hashing retrieval algorithm has been widely used for large-scale image retrieval. Aiming at the inefficiency of large-scale remote sensing image dataset trai-ning, a remote sensing image retrieval method based on query points for feature learning is proposed. First, the image features are extracted from the remote sensing image data training set with multiple semantic tags by using the deep convolutional network. Then, the Hashing function is learned from the query points and the Hashing codes of the query points are generated by using the learned Hashing function, and finally the binary Hashing codes of the whole image database are obtained through iterative learning, which is helpful to improve the retrieval accuracy. The feature extraction of the entire database is avoided in the process of image retrieval, and thus the supervised information in the large-scale database is more effectively utilized for image retrieval. Extensive experimental results conducted on three different datasets demonstrate that the performance of the proposed method is better than those of several other state-of-the-art approaches. © 2019, Editorial Office of Systems Engineering and Electronics. All right reserved.","Deep learning; Hashing; Image retrieval; Remote sensing image","Database systems; Deep learning; Digital storage; Image enhancement; Iterative methods; Large dataset; Query processing; Remote sensing; Semantics; Convolutional networks; Hashing; Iterative learning; Large-scale database; Remote sensing image retrieval; Remote sensing images; Retrieval algorithms; State-of-the-art approach; Image retrieval"
"Aircraft detection in remote sensing images using cascade convolutional neural networks [级联卷积神经网络的遥感影像飞机目标检测]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072536682&doi=10.11947%2fj.AGCS.2019.20180471&partnerID=40&md5=6dc191d9ad29e55ea5ed312e907f6c44","Traditional aircraft detection algorithms which adopt handcraft features have poor performance in complex scene images and recognizing multi-scale objects. Methods using deep convolutional neural networks still face difficulty in dim small target search and recognition in large images with complex background. Aiming at these problems, a coarse-to-fine algorithm for aircraft detection in remote sensing images using cascade convolutional neural networks is proposed. To quickly and effectively acquire suspicious regions of interest (ROI), the whole image is searched by a small and shallow fully convolutional neural network which could deal with images of any size. Then deeper convolutional neural networks are used to refine the classification and location of the ROIs. A multilayer perceptron is introduced to the convolutional layer to improve identification capability of the convolutional neural networks and the strategies of multi-task learning and offline hard example mining are adopted in the process of training. At the detecting stage, the image pyramid is constructed and the redundant windows could be eliminated by the non-maximal suppression. Multiple datasets are tested and the results show that the proposed method has higher accuracy and stronger robustness and provides a fast and efficient solution for object detection in large remote sensing images. © 2019, Surveying and Mapping Press. All right reserved.","Aircraft detection; Cascade convolutional neural networks; Deep learning; Hard example mining; Remote sensing image","Aircraft detection; Complex networks; Convolution; Deep learning; Deep neural networks; Large dataset; Object detection; Remote sensing; Training aircraft; Complex background; Convolutional neural network; Detection algorithm; Multiple data sets; Multitask learning; Non-maximal suppressions; Regions of interest; Remote sensing images; Multilayer neural networks; airborne survey; algorithm; artificial neural network; detection method; image analysis; image classification; remote sensing"
"Small Scale Crater Detection based on Deep Learning with Multi-Temporal Samples of High-Resolution Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074271533&doi=10.1109%2fMulti-Temp.2019.8866941&partnerID=40&md5=1f468eeab9233caf7426d2f0ea7e2ed1","This paper presents an automated method for small-scale crater detection from high resolution images based on deep neural network. And the network performance is improved by the use of a multi-Temporal samples extraction technique. Obtaining topographical features of planet surface, such as craters, is of great importance. It contributes to the understanding of planetary geology. And the detected craters serve as important research objects during the whole aerospace engineering process like landing and navigation of rovers. The large scale craters detection have been extensively studied. However, with the development of high resolution sensors of Moon and Mars, methods specified on extracting smaller craters from high resolution image (HRI) are needed. But dealing with HRI is tricky because: (1) small ground targets can be varied largely in images of different illuminations; (2) A small crater's shape could easily Erode over time to blend into the surface; (3) The number of the small craters are much larger than that of the large carters. These problems are properly addressed in this paper using the proposed method. Experiments with the images of the Chang' e-4 landing area proved that this method is practical and has an accuracy of 92.9% which is higher than other methods. © 2019 IEEE.","crater detection; deep learning; multi-Temporal; samples extraction","Deep learning; Deep neural networks; Image analysis; Remote sensing; Automated methods; Crater detections; Extraction techniques; High resolution image; High resolution sensors; Multi-temporal; Planetary geology; Topographical features; Extraction"
"Synthesis of Multispectral Optical Images from SAR/Optical Multitemporal Data Using Conditional Generative Adversarial Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069501813&doi=10.1109%2fLGRS.2019.2894734&partnerID=40&md5=273712603dde1a4c2477a5546fab9439","The synthesis of realistic data using deep learning techniques has greatly improved the performance of classifiers in handling incomplete data. Remote sensing applications that have profited from those techniques include translating images of different sensors, improving the image resolution and completing missing temporal or spatial data such as in cloudy optical images. In this context, this letter proposes a new deep-learning-based framework to synthesize missing or corrupted multispectral optical images using multimodal/multitemporal data. Specifically, we use conditional generative adversarial networks (cGANs) to generate the missing optical image by exploiting the correspondent synthetic aperture radar (SAR) data with a SAR-optical data from the same area at a different acquisition date. The proposed framework was evaluated in two land-cover applications over tropical regions, where cloud coverage is a major problem: crop recognition and wildfire detection. In both applications, our proposal was superior to alternative approaches tested in our experiments. In particular, our approach outperformed recent cGAN-based proposals for cloud removal, on average, by 7.7% and 8.6% in terms of overall accuracy and F1-score, respectively. © 2019 IEEE.","Conditional generative adversarial networks (cGANs); crop recognition; deep learning; remote sensing; wildfire detection","Crops; Data handling; Deep learning; Fires; Geometrical optics; Image resolution; Radar imaging; Remote sensing; Synthetic aperture radar; Adversarial networks; Crop recognition; Land cover applications; Learning techniques; Multi-temporal data; Performance of classifier; Remote sensing applications; Wildfire detection; Image enhancement; artificial neural network; data acquisition; experimental study; image analysis; image resolution; optical method; pattern recognition; remote sensing; satellite data; synthetic aperture radar; wildfire"
"BoSR: A CNN-based aurora image retrieval method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065800461&doi=10.1016%2fj.neunet.2019.04.012&partnerID=40&md5=88f9df4d93fb4f94e5ff41e33384f1af","The deep learning models especially the CNN have achieved amazing performance on natural image retrieval. However, remote sensing images captured with anamorphic lens are still retrieved via manual selection or traditional SIFT-based methods. How to leverage the advanced CNN models for remote sensing image retrieval is a new task of significance. This paper focuses on the aurora images captured with all-sky-imagers (ASI). By analyzing the imaging principle of ASI and characteristics of aurora, a salient region determination (SRD) scheme is proposed and embedded into the Mask R-CNN framework. Thus, we can regard an image as a “bag” of salient regions (BoSR). In practice, each salient region is represented with a CNN feature extracted from the SRD embedded Mask R-CNN. After clustered to generate a visual vocabulary, each CNN feature is quantized to its nearest center for indexing. In the stage of online retrieval, by computing the similarity scores between query image and all images in the dataset, ranking results can be obtained and image with the highest value is exported as the top rank. Extensive experiments are conducted on the big aurora data, and the results demonstrate that the proposed method improves the retrieval accuracy and efficiency. © 2019 Elsevier Ltd","Aurora image retrieval; Bag of salient regions; Circular fisheye lens","Deep learning; Remote sensing; Fish-eye lens; Remote sensing image retrieval; Remote sensing images; Retrieval accuracy; Retrieval methods; Salient regions; Similarity scores; Visual vocabularies; Image retrieval; Article; artificial neural network; computer model; deep learning; image analysis; image processing; image retrieval; priority journal; remote sensing; automated pattern recognition; human; information retrieval; procedures; standards; Deep Learning; Humans; Information Storage and Retrieval; Neural Networks (Computer); Pattern Recognition, Automated"
"Study of LSTM model in sea surface temperature prediction of the yellow sea cold water mass area","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083571158&doi=10.1109%2fSmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00106&partnerID=40&md5=f00910fac515833e0a042e0f19528be7","Marine science is an observational science and observational data is a prerequisite for understanding the ocean. Satellite remote sensing technology provides a new means for observing the important parameters of Sea Surface Temperature (SST). This work innovatively explores the Long Short-Term Memory network (LSTM), a deep learning model, for the analysis and prediction of the SST in the specific research area of the Yellow Sea cold water mass. The research results show that LSTM can effectively analyze and predict SST. The accuracy of area-averaged SST prediction depends on the length of training observation time sequence. The increase in sequence length enhances prediction accuracy. Given training data of a dozen years, the accuracy of a 30-day length prediction can reach up to 95.87%, which meets the requirements of ocean forecasting. The study also revealed that specifying an observation length, the area-averaged prediction accuracy of a 30-day length prediction for different locations of the cold water mass make no difference. © 2019 IEEE.","Long short-term memory network (lstm); prediction; Remote sensing data; Sea surface temperature (sst); Yellow sea cold water mass","Atmospheric temperature; Deep learning; Forecasting; Long short-term memory; Remote sensing; Smart city; Submarine geophysics; Surface properties; Surface waters; Trusted computing; Ubiquitous computing; Observational data; Ocean forecasting; Prediction accuracy; Research results; Satellite remote sensing; Sea surface temperature (SST); Sequence lengths; Short term memory; Oceanography"
"Extraction of rice-planted area based on mobileunet model and radarsat-2 data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074216668&doi=10.1109%2fBIGSARDATA.2019.8858473&partnerID=40&md5=c9fb240e3613d6c488873fd4970378e1","Extracting rice-planted area based on Synthetic aperture radar (SAR) image can effectively solve the cloud pollution problems of optical images. Compared with the traditional image classification method, deep learning method based on semantic segmentation algorithm can effectively utilize the information that can represent the image features and backscatter difference between different ground objects in remote sensing data, which satisfies the need of rapid and accurate extraction of ground information in the field. In this paper, the extraction of rice-planted area was implemented using MobileUnet model and rice label datasets were created using Radarsat-2 quad-polarized backscattering coefficient images and Freeman-Durden decomposition images obtained during the rice growth period in 2016. The results of our experiment showed that the Precision, Recall and MIoU are 0.964, 0.962 and 0.826 respectively. We analyzed the results and proposed future research prospects finally. © 2019 IEEE.","Deep learning; MobileUnet; Radarsat-2; Rice-planted area; SAR","Backscattering; Big data; Classification (of information); Data mining; Deep learning; Extraction; Geometrical optics; Remote sensing; Semantics; Synthetic aperture radar; Backscattering coefficients; Classification methods; MobileUnet; Planted areas; Radarsat-2; Remote sensing data; Semantic segmentation; Synthetic aperture radar (SAR) images; Image segmentation"
"Advances in scene classification of remotely sensed high resolution images and the existing datasets","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071222634&doi=10.35940%2fijitee.J8841.0881019&partnerID=40&md5=08fe3564a1fd43390e1e3209fed68150","Research on Scene classification of remotely sensed images has shown a significant improvement in the recent years as it is used in various applications such as urban planning, urban mapping, management of natural resources, precision agriculture, detecting targets etc. The recent advancement of intelligent earth observation system has led to the generation of high resolution remote sensing images in terms of spatial, spectral and temporal resolutions which in turn helped the researchers to improve the performance of Land Use Land Class (LULC) Classification Techniques to a higher level. With the usage of different deep learning architecture and the availability of various high resolution image datasets, the field of Remote Sensing Scene Classification of high resolution (RSSCHR) images has shown tremendous improvement in the past decade. In this paper we present the different publicly available datasets, various scene classification methods and the future research scope of remotely sensed high resolution images. © BEIESP.","Convolutional neural networks; Datasets; Deep learning; Remote sensing; Scene classification",
"A hyperspectral image classification method based on multi-discriminator generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070723724&doi=10.3390%2fs19153269&partnerID=40&md5=83e8385b86999a1eeb0fd6d8295e133d","Hyperspectral remote sensing images (HSIs) have great research and application value. At present, deep learning has become an important method for studying image processing. The Generative Adversarial Network (GAN) model is a typical network of deep learning developed in recent years and the GAN model can also be used to classify HSIs. However, there are still some problems in the classification of HSIs. On the one hand, due to the existence of different objects with the same spectrum phenomenon, if only according to the original GAN model to generate samples from spectral samples, it will produce the wrong detailed characteristic information. On the other hand, the gradient disappears in the original GAN model and the scoring ability of a single discriminator limits the quality of the generated samples. In order to solve the above problems, we introduce the scoring mechanism of multi-discriminator collaboration and complete semi-supervised classification on three hyperspectral data sets. Compared with the original GAN model with a single discriminator, the adjusted criterion is more rigorous and accurate and the generated samples can show more accurate characteristics. Aiming at the pattern collapse and diversity deficiency of the original GAN generated by single discriminator, this paper proposes a multi-discriminator generative adversarial networks (MDGANs) and studies the influence of the number of discriminators on the classification results. The experimental results show that the introduction of multi-discriminator improves the judgment ability of the model, ensures the effect of generating samples, solves the problem of noise in generating spectral samples and can improve the classification effect of HSIs. At the same time, the number of discriminators has different effects on different data sets. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Generative adversarial networks; Hyperspectral image classification; Multi-discriminator generative adversarial network; Semi-supervised classification","Classification (of information); Deep learning; Hyperspectral imaging; Image classification; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Classification results; Different effects; Generating samples; Hyperspectral Data; Hyperspectral Remote Sensing Image; Research and application; Semi-supervised classification; Discriminators"
"Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne lidar and image data using active contours","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066730376&doi=10.1016%2fj.isprsjprs.2019.05.013&partnerID=40&md5=52d19dfff7dc4ebfacad229debb7002f","Robust and reliable automatic building detection and segmentation from aerial images/point clouds has been a prominent field of research in remote sensing, computer vision and point cloud processing for a number of decades. One of the largest issues associated with deep learning methods is the high quantity of data required for training. To help address this we present a method to improve public GIS building footprint labels by using Morphological Geodesic Active Contours (MorphGACs). We demonstrate by improving the quality of building footprint labels for detection and semantic segmentation, more robust and reliable models can be obtained. We evaluate these methods over a large UK-based dataset of 24556 images containing 169835 building instances. This is achieved by training several Mask/Faster R-CNN and RetinaNet deep convolutional neural networks. Networks are supplied with both RGB and fused RGB-lidar data. We offer quantitative analysis on the benefits of the inclusion of depth data for building segmentation. By employing both methods we achieve a detection accuracy of 0.92 (mAP@0.5) and segmentation f1 scores of 0.94 over a 4911 test images ranging from urban to rural scenes. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Aerial; Convolutional neural networks; Deep learning; Image processing; Lidar; Segmentation","Antennas; Buildings; Convolution; Deep learning; Deep neural networks; Image enhancement; Image processing; Large dataset; Neural networks; Optical radar; Remote sensing; Semantics; Automatic building detection; Building footprint; Convolutional neural network; Detection accuracy; Geodesic Active Contour; Learning methods; Reliable models; Semantic segmentation; Image segmentation; accuracy assessment; artificial neural network; automation; computer vision; data set; detection method; GIS; image processing; lidar; machine learning; quantitative analysis; remote sensing"
"Development of a human metabolic rate prediction model based on the use of Kinect-camera generated visual data-driven approaches","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067643960&doi=10.1016%2fj.buildenv.2019.106216&partnerID=40&md5=149cd2c8c4afc4ea9c372273846f88c4","Predicting thermal comfort is one of the primary building research domains due to its technical and environmental significance. A metabolic rate, one of the significant variables for predicting an individual's thermal comfort, is primarily based on the human body's activity level. While other human and environmental factors, such as air temperature and relative humidity are easily measured and collected, with the help of sensory devices, a metabolic rate varies with time, and is not easy to measure to determine an accurate thermal comfort estimation in reality. Therefore, this study investigated the potential use of Deep Learning algorithm to accurately estimate the metabolic rate for a better thermal comfort estimation. A series of chamber tests were conducted with 23 test participants. The Kinect sensor was adopted to detect a user's physical motion, by capturing the motion images. With the help of a wearable sensor, a user's heart rate was also measured to estimate a metabolic rate. This study found that males showed higher MET than females, and the high BMI group generated higher MET than the low BMI group. The result also indicated that an estimated accurate range of 77%–89% was reasonably acceptable in the self-MET prediction modeling, while it was 65% in the third-party MET prediction. Therefore, the outcome of this research confirms that it is possible to use the Kinect sensor as a remote sensing device to estimate a user's metabolic rate, based on the use of a Deep Learning algorithm developed per individual. © 2019 Elsevier Ltd","Computer vision; Data-driven modeling; Machine learning; Physiological signals; Predictive mean vote; Thermal comfort","Computer vision; Deep learning; Forecasting; Learning systems; Metabolism; Remote sensing; Thermal comfort; Wearable sensors; Data-driven model; Environmental factors; Environmental significance; Physiological signals; Prediction model; Predictive mean vote; Research domains; Significant variables; Learning algorithms; algorithm; computer vision; machine learning; metabolism; numerical model; physiology; prediction; sensor"
"Validating Hyperspectral Image Segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069529343&doi=10.1109%2fLGRS.2019.2895697&partnerID=40&md5=678ba3154c538120a59d578adf5b79fd","Hyperspectral satellite imaging attracts enormous research attention in the remote sensing community, and hence, automated approaches for precise segmentation of such imagery are being rapidly developed. In this letter, we share our observations on the strategy for validating hyperspectral image segmentation algorithms currently followed in the literature, and show that it can lead to overoptimistic experimental insights. We introduce a new routine for generating segmentation benchmarks and use it to elaborate ready-to-use hyperspectral training-test data partitions. They can be utilized for fair validation of new and existing algorithms without any training-test data leakage. © 2019 IEEE.","Classification; deep learning (DL); hyperspectral imaging; segmentation; validation","Classification (of information); Deep learning; Hyperspectral imaging; Remote sensing; Satellite imagery; Spectroscopy; Automated approach; HyperSpectral; Hyperspectral satellite; Ready to use; Test data; validation; Image segmentation; algorithm; image analysis; imaging method; observational method; remote sensing; satellite imagery; segmentation"
"Pyramid scene parsing network in 3D: Improving semantic segmentation of point clouds with multi-scale contextual information","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068136296&doi=10.1016%2fj.isprsjprs.2019.06.010&partnerID=40&md5=3dabd4d669a4f5c06dd7cd919ca55cea","Analyzing and extracting geometric features from 3D data is a fundamental step in 3D scene understanding. Recent works demonstrated that deep learning architectures can operate directly on raw point clouds, i.e. without the use of intermediate grid-like structures. These architectures are however not designed to encode contextual information in-between objects efficiently. Inspired by a global feature aggregation algorithm designed for images (Zhao et al., 2017), we propose a 3D pyramid module to enrich pointwise features with multi-scale contextual information. Our module can be easily coupled with 3D semantic segmentation methods operating on 3D point clouds. We evaluated our method on three large scale datasets with four baseline models. Experimental results show that the use of enriched features brings significant improvements to the semantic segmentation of indoor and outdoor scenes. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Multi-scale contextual information; Point cloud; Semantic segmentation","Large dataset; Network architecture; Semantic Web; Semantics; 3D point cloud; Baseline models; Contextual information; Geometric feature; Large-scale datasets; Learning architectures; Point cloud; Semantic segmentation; Deep learning; algorithm; data set; image analysis; information; machine learning; segmentation"
"Multitemporal Hyperspectral Image Change Detection by Joint Affinity and Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074278273&doi=10.1109%2fMulti-Temp.2019.8866928&partnerID=40&md5=077a122dbe443f5c4d887c18ae51299e","To improve performance of change detection (CD) using multitemporal hyperspectral images (HSI), this paper designs a novel spectral-spatial feature descriptor based on joint affinity (JA) and proposes a deep learning framework embedded with it. JA preserves context information that can increase accuracy and robustness of the CD model. It completes the task of CD in three steps, dimensionality reduction which is optional, JA tensor construction and binary classification by convolutional neural networks (CNN). Thus, the proposed method is denoted by JA-CNN. Experiments on one set of real-world data and two sets of synthetic data show that JA-CNN outperforms several state-of-The-Art methods for CD. In addition, it is more robust than its counterpart which does not consider spatial information when applied to severely corrupted HSIs. © 2019 IEEE.","change detection; CNN; joint affinity; multitemporal hyperspectral images; spectral-spatial","Convolution; Deep learning; Image analysis; Image registration; Neural networks; Remote sensing; Spectroscopy; Binary classification; Change detection; Convolutional neural network; Dimensionality reduction; Image change detection; Multi-temporal; spectral-spatial; State-of-the-art methods; Image enhancement"
"Mask R-CNN for Object Detection in Multitemporal SAR Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074250479&doi=10.1109%2fMulti-Temp.2019.8866949&partnerID=40&md5=293f235ff80cd84218ff51eece705e94","Synthetic aperture radar (SAR) has unique advantages in ocean monitoring. Ship object detection in multitemporal SAR images has great potentials in various applications. In this study, we aim to improve the accuracy of deep learning method for ship detection in SAR images. We have proposed a framework based on the Mask R-CNN. The training process utilizes SAR dataset SSDD, and the model incorporates ImageNet weights which are adequately trained for transfer learning. Experiments based on both Single-Mask and Multiple-Masks have been designed and conducted with data preparing methods such as denoising and data augmentation. The results showed that Mask R-CNN model outperformed other start-of-Art models for SAR image-based ship detection. Specifically, the accuracy has been improved to 96.8% within 116ms. © 2019 IEEE.","Mask R-CNN; SAR; Ship Detection","Deep learning; Image enhancement; Object detection; Object recognition; Remote sensing; Ships; Synthetic aperture radar; Data augmentation; Learning methods; Multi-temporal SAR images; Ocean monitoring; Ship detection; Single-mask; Training process; Transfer learning; Radar imaging"
"Pan-Sharpening Using an Efficient Bidirectional Pyramid Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069762106&doi=10.1109%2fTGRS.2019.2900419&partnerID=40&md5=3a7ed28000227b453c01ace44bd47575","Pan-sharpening is an important preprocessing step for remote sensing image processing tasks; it fuses a low-resolution multispectral image and a high-resolution (HR) panchromatic (PAN) image to reconstruct a HR multispectral (MS) image. This paper introduces a new end-to-end bidirectional pyramid network for pan-sharpening. The overall structure of the proposed network is a bidirectional pyramid, which permits the network to process MS and PAN images in two separate branches level by level. At each level of the network, spatial details extracted from the PAN image are injected into the upsampled MS image to reconstruct the pan-sharpened image from coarse resolution to fine resolution. Subpixel convolutional layers and the enhanced residual blocks are used to make the network efficient. Comparison of the results obtained with our proposed method and the results using other widely used state-of-the-art approaches confirms that our proposed method outperforms the others in visual appearance and objective indexes. © 1980-2012 IEEE.","Bidirectional pyramid network (BDPN); deep learning; image fusion; multilevel; pan-sharpening; remote sensing","Deep learning; Image fusion; Remote sensing; Low resolution multispectral images; multilevel; Multispectral images; Pan-sharpening; Panchromatic (Pan) image; Pyramid network; Remote sensing image processing; State-of-the-art approach; Image reconstruction; algorithm; comparative study; image processing; image resolution; index method; multispectral image; network analysis; pixel; remote sensing"
"Crop yield prediction with deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068157908&doi=10.1016%2fj.compag.2019.104859&partnerID=40&md5=fdad47bf7ffd34b4da05a2c3bb3907e4","Using remote sensing and UAVs in smart farming is gaining momentum worldwide. The main objectives are crop and weed detection, biomass evaluation and yield prediction. Evaluating machine learning methods for remote sensing based yield prediction requires availability of yield mapping devices, which are still not very common among farmers. In this study Convolutional Neural Networks (CNNs) – a deep learning methodology showing outstanding performance in image classification tasks – are applied to build a model for crop yield prediction based on NDVI and RGB data acquired from UAVs. The effect of various aspects of the CNN such as selection of the training algorithm, depth of the network, regularization strategy, and tuning of the hyperparameters on the prediction efficiency are tested. Using the Adadelta training algorithm, L2 regularization with early stopping and a CNN with 6 convolutional layers, mean absolute error (MAE) in yield prediction of 484.3 kg/ha and mean absolute percentage error (MAPE) of 8.8% was achieved for data acquired during the early period of the growth season (i.e., in June of 2017, growth phase &lt;25%) with RGB data. When using data acquired later in July and August of 2017 (growth phase &gt;25%), MAE of 624.3 kg/ha (MAPE: 12.6%) was obtained. Significantly, the CNN architecture performed better with RGB data than the NDVI data. © 2019","Barley; Convolutional neural network; Crop yield prediction; Growth phase; Multispectral; NDVI; UAV; Wheat","Convolution; Crops; Forecasting; Forestry; Learning algorithms; Neural networks; Remote sensing; Unmanned aerial vehicles (UAV); Weed control; Barley; Convolutional neural network; Crop yield; Growth phase; Multi-spectral; NDVI; Wheat; Deep neural networks; algorithm; artificial neural network; barley; crop yield; data acquisition; image classification; machine learning; multispectral image; NDVI; prediction; remote sensing; unmanned vehicle; wheat; Hordeum; Triticum aestivum"
"Sensor-specific Transfer Learning for Hyperspectral Image Processing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074278209&doi=10.1109%2fMulti-Temp.2019.8866896&partnerID=40&md5=038876656f1ad187bbe731e2c9292c8c","Transfer learning (TL) has shown its great advantage to solve small-Training-sample problems using knowledge learned from existing large data with deep learning techniques, which can be used for hyperspectral image intelligent processing in which labeled data is very difficult and even impossible to be obtained. However, the mismatch of hyperspectral sensors results in lots of difficulty for transfer learning to be used in hyperspectral image (HSI) processing. In this paper, sensor-specific based transfer learning is proposed for hyperspectral images acquired from same sensors, in which knowledge learn from hyperspectral images, e.g., the network structure and parameters of a deep neural network, are limited to transfer to images of the same sensor only. Specifically, the validity of sensor-specific transfer learning is evaluated using three deep learning based tasks, including feature learning, super-resolution, and image denoising. Experimental results from two benchmark datasets from the well-known ROSIS sensor, i.e., Pavia Centre and Pavia University, have demonstrated that sensor-specific based transfer learning can achieve satisfying performance even without fine-Tune by small-Training-samples on the target scene. © 2019 IEEE.","convolutional neural network (CNN); feature learning; hyperspectral image processing; image denoising; super-resolution; transfer learning","Benchmarking; Deep neural networks; Hyperspectral imaging; Image analysis; Machine learning; Neural networks; Optical resolving power; Remote sensing; Sampling; Spectroscopy; Convolutional neural network; Feature learning; Hyperspectral sensors; Intelligent processing; Learning techniques; Network structures; Super resolution; Transfer learning; Image denoising"
"Multi-Temporal Change Detection based on Deep Semantic Segmentation Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074292519&doi=10.1109%2fMulti-Temp.2019.8866913&partnerID=40&md5=20872c155a9407a9b6c1b5f629b8456c","The task of change detection (CD) can be considered as a pixel-level classification problem and can be fulfilled by a semantic segmentation task. In this paper, we propose an end-To-end change detection framework based on a deep semantic segmentation network, in particular, fully convolutional networks in terms of the difference image or the concatenated one of multi-Temporal images. Experimental results illustrate that our framework can obtain a state-of-The-Art result on the real-world high resolution CD dataset. © 2019 IEEE.","change detection; deep learning; Remote sensing; semantic segmentation","Deep learning; Image analysis; Image segmentation; Semantic Web; Semantics; Change detection; Convolutional networks; Difference images; High resolution; Multi-temporal; Multi-temporal image; Semantic segmentation; State of the art; Remote sensing"
"Uncovering Ecological Patterns with Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065237151&doi=10.1016%2fj.tree.2019.03.006&partnerID=40&md5=0621b107f10dc1b3f5dd65f91ab372d1","Using remotely sensed imagery to identify biophysical components across landscapes is an important avenue of investigation for ecologists studying ecosystem dynamics. With high-resolution remotely sensed imagery, algorithmic utilization of image context is crucial for accurate identification of biophysical components at large scales. In recent years, convolutional neural networks (CNNs) have become ubiquitous in image processing, and are rapidly becoming more common in ecology. Because the quantity of high-resolution remotely sensed imagery continues to rise, CNNs are increasingly essential tools for large-scale ecosystem analysis. We discuss here the conceptual advantages of CNNs, demonstrate how they can be used by ecologists through distinct examples of their application, and provide a walkthrough of how to use them for ecological applications. © 2019 Elsevier Ltd","convolutional neural network; deep learning; image segmentation; machine learning; object detection; remote sensing","algorithm; artificial neural network; ecological approach; ecosystem dynamics; image analysis; image processing; machine learning; remote sensing; satellite imagery; segmentation; ecology; ecosystem; image processing; Ecology; Ecosystem; Image Processing, Computer-Assisted; Neural Networks, Computer"
"Monitoring ecosystem service change in the City of Shenzhen by the use of high-resolution remotely sensed imagery and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068990323&doi=10.1002%2fldr.3337&partnerID=40&md5=364185e925c168fa38ec399edae7b8ad","Information concerning land-use change is imperative for improving conservation policies that promote sustainable land development. However, to date, most of the previous studies have largely focused on the use of coarse- or moderate-resolution data, with which it may not be possible to identify the land-use classes in urban environments. Due to the improved spatial details, high-resolution (HR) remote sensing imagery provides us with an opportunity for the semantic interpretation of urban landscapes. Therefore, in this study, we took the City of Shenzhen (1997 km2) in China as an example to assess the detailed land-use change and its effect on ecosystem services (ESs), based on HR satellite data from 2005 to 2017. In particular, deep learning was used to obtain accurate land-use maps, because this technique is able to model the hierarchical representations of features and can thus effectively characterize urban scenes. The results revealed the following findings: (a) The overall accuracy of the proposed approach was 96.9% and 97.1% for 2005 and 2017, respectively, outperforming state-of-the-art semantic classification models; (b) residential and commercial areas in Shenzhen increased dramatically over the study period by 10,416 and 9,168 ha, at the expense of ecological land; (c) supply capacity of the ecosystem decreased by 13.7%, but demand for ESs showed an increase of 23.5%. By courtesy of HR images, detailed land-use changes and the associated ESs can be monitored, which facilitates the in-depth understanding of urban environmental systems. © 2019 John Wiley & Sons, Ltd.","ecosystem service; land degradation; land-use development; urban environment; urbanization","Ecosystems; Image enhancement; Land use; Remote sensing; Semantics; Urban planning; Ecosystem services; Hierarchical representation; High-resolution remotely sensed imageries; Land degradation; Semantic classification; Semantic interpretation; Urban environments; urbanization; Deep learning; accuracy assessment; ecosystem service; environmental monitoring; land degradation; land use change; learning; remote sensing; satellite data; satellite imagery; urbanization; China; Guangdong; Shenzhen"
"Research progress on methods of automatic coastline extraction based on remote sensing images [遥感影像的海岸线自动提取方法研究进展]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070722502&doi=10.11834%2fjrs.20197410&partnerID=40&md5=bfa94f36e1d9bcb2234ead7386c4bef9","The coastline refers to the boundary line between land and sea. The coastline detection is an important part of studying the effects of land-sea interaction and human activities on coastal zones. It is also significant for the effective development, integrated management and sustainable exploitation of coastal resources and protection of coastal ecosystems. Therefore, it is critical to achieve the extraction of coastline quickly and accurately. Remote sensing technology has gradually become a way to detect the location of the coastline and monitor its dynamic changes with its wide range, high temporal resolution, high spatial resolution, multispectral, low cost and other prominent advantages, overcoming the shortcomings of traditional coastline detection methods of long time period, high intensity of labor and so on. This paper reviews the research progress on methods of automatic coastline extraction based on remote sensing images proposed in recent years. First, the definition and classification of the coastline are given. The two stages of extraction of the instantaneous waterline and the real coastline and the specific extraction process are clarified. Then, these methods are divided into eight categories: threshold segmentation, edge detection operator, active contour model, data mining, multiresolution analysis, object-oriented, polarization and other methods. And the basic ideas of the main methods of coastline extraction are elaborated. Finally, the advantages and disadvantages of all kinds of methods are analyzed and compared. The principle of tidal correction and the methods of accuracy evaluation are expounded, and the research work of the next step is prospected. The detailed feature description of main methods are as follows. Threshold segmentation method is simple and easy to implement. While the selection of appropriate threshold is a certain degree of difficulty, and the extraction accuracy needs to be improved. The edge detection operator method has a good effect on the extraction of edge. Simultaneously, this method is susceptible to noise and prone to detect pseudo edges, even the continuity of extracted coastline is poor. The extraction result of the active contour model is accurate. However, the model has high complexity and large computational burden. Data mining method utilizes intelligent means to achieve automatic extraction of coastline. But a variety of methods need to be combined to make the extraction accuracy higher. Although the multiresolution analysis method can obtain rich edge information, the mostly applied wavelets have a limitation of dealing with directional information. Object-oriented approach can achieve image segmentation of a higher level and reduce the influence of texture and other characteristics inside the cell. It cannot make full use of the implicit information of the image when the amount of data is large. And the polarization method is essentially a threshold segmentation method. The only difference is the selection method of threshold. The polarization method achieves threshold selection by usage of different polarization methods of SAR images. In the end, in view of the deficiency of currently existing methods, the following feasible research approaches and prominent research prospects of automatic coastline extraction based on remote sensing images are put forward: constructing dataset, utilizing hyperspectral data, employing deep learning method, adopting swarm intelligence algorithm to optimize the parameters in the method of active contour model, using multiscale transform which contains directional information to improve the extraction accuracy of multiresolution analysis method, combining various kinds of methods to synthesize their advantages and achieving the automatic extraction of coastline with subpixel precision. © 2019, Science Press. All right reserved.","Accuracy evaluation; Automatic extraction; Coastline; Remote sensing; Remote sensing image; Waterline","Coastal zones; Deep learning; Ecosystems; Edge detection; Extraction; Image enhancement; Image segmentation; Landforms; Multiresolution analysis; Polarization; Remote sensing; Swarm intelligence; Synthetic aperture radar; Textures; Accuracy evaluation; Automatic extraction; Coastline; Remote sensing images; Waterline; Data mining"
"Predicting economic development using geolocated wikipedia articles","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071178000&doi=10.1145%2f3292500.3330784&partnerID=40&md5=381c5b80e4bff458207459d79c5f4d44","Progress on the UN Sustainable Development Goals (SDGs) is hampered by a persistent lack of data regarding key social, environmental, and economic indicators, particularly in developing countries. For example, data on poverty - the first of seventeen SDGs - is both spatially sparse and infrequently collected in Sub-Saharan Africa due to the high cost of surveys. Here we propose a novel method for estimating socioeconomic indicators using open-source, geolocated textual information from Wikipedia articles. We demonstrate that modern NLP techniques can be used to predict community-level asset wealth and education outcomes using nearby geolocated Wikipedia articles. When paired with nightlights satellite imagery, our method outperforms all previously published benchmarks for this prediction task, indicating the potential of Wikipedia to inform both research in the social sciences and future policy decisions. © 2019 Association for Computing Machinery.","Computational sustainability; Deep learning; Remote sensing","Deep learning; Developing countries; Forecasting; Remote sensing; Satellite imagery; Sustainable development; Computational sustainability; Economic development; Economic indicators; Policy decisions; Socio-economic indicators; Sub-saharan africa; Textual information; Wikipedia articles; Data mining"
"River Extraction from High-Resolution Satellite Images Combining Deep Learning and Multiple Chessboard Segmentation [基于深度学习和多次棋盘分割法的高分辨率影像河流提取]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070492320&doi=10.13209%2fj.0479-8023.2019.045&partnerID=40&md5=1a319ee9bd16b1046033227c87b39776","Using existing methods to extract rivers, especially the small river from remote sensing images, is liable to be interrupted. The combination of deep learning and multiple chessboard segmentation is applied to river extraction from high resolution remote sensing images. Three GF-2 satellite remote sensing images in mountain area, plain and city are used for experiment. The results show that compared with the existing methods, extracted river by proposed method is more continuous. The small rivers accounts for two pixel widths can also be extracted in GF-2 satellite remote sensing images. © 2019 Peking University.","Convolution neural network (CNN); Deep learning; High resolution satellite images; Multiple chessboard segmentation; River extraction","Deep learning; Deep neural networks; Extraction; Image segmentation; Rivers; Small satellites; Convolution neural network; High resolution remote sensing images; High resolution satellite images; Remote sensing images; Satellite remote sensing; Small rivers; Remote sensing"
"Object detection in remote sensing images based on GaN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073071197&doi=10.1145%2f3349341.3349458&partnerID=40&md5=2b3227d478d57bb6b154fd096096c072","Remote sensing image recognition has been widely used in civil and military fields. In view of plenty of interference factors in remote-sensing aircraft such as shade, noise, the changing of perspective, etc. An improved target recognition algorithm in remote sensing image based on generative adversarial network is proposed. Before target recognition, Image enhancement based on generative adversarial network is carried out, and noise is removed through attention cycle sub-network to realize the reconstruction of high-resolution images from low-resolution images that retain high-frequency details of the target and improve the accuracy of target detection. Simulation results show that the feasibility of aircraft target recognition algorithm in removing sensing image and the scale and posture changes of target can be overcome. © 2019 Association for Computing Machinery.","Aircraft recognition rule; Deep learning; Enhancement image; Generative adversarial Networks","Aircraft; Artificial intelligence; Deep learning; Gallium nitride; III-V semiconductors; Image recognition; Military photography; Object detection; Object recognition; Optical character recognition; Remote sensing; Adversarial networks; High resolution image; Interference factor; Low resolution images; Recognition rules; Remote sensing aircrafts; Remote sensing images; Target recognition algorithms; Image enhancement"
"2018 4th International Conference on Environmental Science and Material Application - Energy Science and Power Engineering","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070456548&partnerID=40&md5=818a0782d87c31c0b614ec1def88cecb","The proceedings contain 659 papers. The topics discussed include: assessment of the phytotoxicity of gold nanoclusters on soybean; detection of belt longitudinal rip based on canny operator; current research state of irregular objects simulation; research on landform classification of Beijing-Tianjin-Hebei region based on SRTM remote sensing image data; research on face image encryption based on deep learning; research on the captcha recognition method based on neural network; master-slave collaboration system of unmanned surface vehicle and robotic fish; the servo system drive and control of pneumatic actuator; the study of the variation law of TRMM rainfall measurement precision on time scale; and severe weather in Zhoushan Sea area - analysis and prediction of typhoon.",,
"2018 4th International Conference on Environmental Science and Material Application - Resources and Geological Engineering","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070369127&partnerID=40&md5=01f2970681a8cdd24228a1425be44098","The proceedings contain 659 papers. The topics discussed include: assessment of the phytotoxicity of gold nanoclusters on soybean; detection of belt longitudinal rip based on canny operator; current research state of irregular objects simulation; research on landform classification of Beijing-Tianjin-Hebei region based on SRTM remote sensing image data; research on face image encryption based on deep learning; research on the captcha recognition method based on neural network; master-slave collaboration system of unmanned surface vehicle and robotic fish; the servo system drive and control of pneumatic actuator; the study of the variation law of TRMM rainfall measurement precision on time scale; and severe weather in Zhoushan Sea area - analysis and prediction of typhoon.",,
"2018 4th International Conference on Environmental Science and Material Application - Preface","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070368778&partnerID=40&md5=5bd18ce4992f3803a7a065285a11a9b2","The proceedings contain 659 papers. The topics discussed include: assessment of the phytotoxicity of gold nanoclusters on soybean; detection of belt longitudinal rip based on canny operator; current research state of irregular objects simulation; research on landform classification of Beijing-Tianjin-Hebei region based on SRTM remote sensing image data; research on face image encryption based on deep learning; research on the captcha recognition method based on neural network; master-slave collaboration system of unmanned surface vehicle and robotic fish; the servo system drive and control of pneumatic actuator; the study of the variation law of TRMM rainfall measurement precision on time scale; and severe weather in Zhoushan Sea area - analysis and prediction of typhoon.",,
"2018 4th International Conference on Environmental Science and Material Application - Material Application and Chemical Engineering","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070246980&partnerID=40&md5=0c08bf61e29f89668bc19b3e6ce10d4e","The proceedings contain 659 papers. The topics discussed include: assessment of the phytotoxicity of gold nanoclusters on soybean; detection of belt longitudinal rip based on canny operator; current research state of irregular objects simulation; research on landform classification of Beijing-Tianjin-Hebei region based on SRTM remote sensing image data; research on face image encryption based on deep learning; research on the captcha recognition method based on neural network; master-slave collaboration system of unmanned surface vehicle and robotic fish; the servo system drive and control of pneumatic actuator; the study of the variation law of TRMM rainfall measurement precision on time scale; and severe weather in Zhoushan Sea area - analysis and prediction of typhoon.",,
"2018 4th International Conference on Environmental Science and Material Application - Environmental Engineering and Technology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070203043&partnerID=40&md5=621359d0fe3ef7305dc266f699ccb41a","The proceedings contain 659 papers. The topics discussed include: assessment of the phytotoxicity of gold nanoclusters on soybean; detection of belt longitudinal rip based on canny operator; current research state of irregular objects simulation; research on landform classification of Beijing-Tianjin-Hebei region based on SRTM remote sensing image data; research on face image encryption based on deep learning; research on the captcha recognition method based on neural network; master-slave collaboration system of unmanned surface vehicle and robotic fish; the servo system drive and control of pneumatic actuator; the study of the variation law of TRMM rainfall measurement precision on time scale; and severe weather in Zhoushan Sea area - analysis and prediction of typhoon.",,
"Semantic segmentation of high spatial resolution images with deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059890501&doi=10.1080%2f15481603.2018.1564499&partnerID=40&md5=7d834226d7c60e10936b028a11bfdd7c","Availability of reliable delineation of urban lands is fundamental to applications such as infrastructure management and urban planning. An accurate semantic segmentation approach can assign each pixel of remotely sensed imagery a reliable ground object class. In this paper, we propose an end-to-end deep learning architecture to perform the pixel-level understanding of high spatial resolution remote sensing images. Both local and global contextual information are considered. The local contexts are learned by the deep residual net, and the multi-scale global contexts are extracted by a pyramid pooling module. These contextual features are concatenated to predict labels for each pixel. In addition, multiple additional losses are proposed to enhance our deep learning network to optimize multi-level features from different resolution images simultaneously. Two public datasets, including Vaihingen and Potsdam datasets, are used to assess the performance of the proposed deep neural network. Comparison with the results from the published state-of-the-art algorithms demonstrates the effectiveness of our approach. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.","deep learning; global context information; high-resolution image segmentation; pyramid pooling; residual network","algorithm; artificial neural network; data set; image resolution; pixel; remote sensing; segmentation; spatial resolution; urban area; Baden-Wurttemberg; Brandenburg [Germany]; Germany; Potsdam; Vaihingen an der Enz"
"Robust Real-Time Object Detection Based on Deep Learning for Very High Resolution Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077678881&doi=10.1109%2fIGARSS.2019.8897976&partnerID=40&md5=efba9dbd1cc11988493d5d01206a7fcd","Recently, the development of deep learning boosts the object detection for remote sensing images. The existing deep learning methods can be divided into two types. The region-based methods represented by Faster R-CNN have progressive performance in accuracy. However, their computational cost is massive due to the deep Convolutional Neural Network (CNN) backbones, which limits the efficiency. The regression-based methods such as YOLO and Single Shot MultiBox Detector (SSD) are advantageous in speed while the accuracy is not satisfactory. To meet the increasing demand in both speed and accuracy for object detection of remote sensing images, we employ the Reception Field Block Net (RFBNet) detector. It embeds the Receptive Field Block (RFB) module into SSD to obtain better feature representation. The experimental results on NWPU VHR-10 dataset demonstrate that the mAP of RFBNet-512 reaches 91.56%, which outperforms other state-of-the-art networks. Meanwhile, the speed is also competitive. © 2019 IEEE.","Convolutional Neural Network; Deep Learning; Object Detection; Remote Sensing",
"An Extensible and Easy-to-use Toolbox for Deep Learning Based Analysis of Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077706367&doi=10.1109%2fIGARSS.2019.8898823&partnerID=40&md5=cdf05d50de0fc243f747fb2a240abf71","Deep Learning (DL) methods are currently the state-of-the-art in Machine Learning and Pattern Recognition. In recent years, DL has been successfully applied to Remote Sensing (RS) image processing for several tasks, from pre-processing to classification. This paper presents DeepGeo, a toolbox that provides state-of-the-art DL algorithms for RS image classification and analysis. DeepGeo focuses on providing easy-to-use and extensible methods, making it easier to those RS analysts without strong programming skills. It is distributed as free and open source package and is available at https: //github.com/rvmaretto/deepgeo. © 2019 IEEE.","Convolutional Neural Networks; Deep Learning; Remote Sensing; Semantic Segmentation",
"Construction of a drought monitoring model using deep learning based on multi-source remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062843586&doi=10.1016%2fj.jag.2019.03.006&partnerID=40&md5=0756ec91b49fca9a5665451cc9c4b30b","Drought is a popular scientific issue in global climate change research. Accurate monitoring of drought has important implications for the sustainable development of regional agriculture in the context of increasingly complex global climate change. Deep learning is a widely used technique in the field of artificial intelligence. However, ongoing on drought monitoring using deep learning is relatively scarce. In this paper, the various hazard factors in drought development were comprehensively considered based on satellite data including Moderate Resolution Imaging Spectroradiometer (MODIS) and tropical rainfall measuring mission (TRMM) as multi-source remote sensing data. By using the deep learning technique, a comprehensive drought monitoring model was constructed and tested in Henan Province of China as an example. The results showed that the comprehensive drought model has good applicability in the monitoring of meteorological drought and agricultural drought. There was a significant positive correlation between the drought indicators of the model output and the comprehensive meteorological drought index (CI) measured at the site scale. The consistency rate of the drought grade of the two models was 85.6% and 79.8% for the training set and the test set, respectively. The correlation coefficient between the drought index of the model and the standard precipitation evapotranspiration index (SPEI) was between 0.772 and 0.910 (P < 0.01), which indicated a strong level of significance. The correlation coefficient between the drought index of the model and the soil relative moisture at a 10 cm depth was greater than 0.550 (P < 0.01), and there was a good correlation between them. This study provides a new method for the comprehensive assessment of regional drought. © 2019 Elsevier B.V.","Deep learning; Drought; Remote sensing","climate change; drought; global climate; learning; MODIS; monitoring; remote sensing; satellite data; China; Henan"
"Deep Learning Model for Target Detection in Remote Sensing Images Fusing Multilevel Features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077681228&doi=10.1109%2fIGARSS.2019.8898759&partnerID=40&md5=728ed1ac355b57906195b1932918a71a","Target detection in remote sensing image has long been one of the research focuses in related areas. This paper proposed a deep learning model for target detection in remote sensing image fusing multilevel features and applied to detect aircrafts in remote sensing images. Because the model is small in size and applies fusion of multilevel features, the detection accuracy of aircraft targets with different scales and denser in remote sensing images has been improved, without compromising the detection speed. A packet fusion reject detection bounding boxes (PFR-DBB) algorithm was also proposed, which is able to better remove redundant detection boxes and further improve detection accuracy. With the experiment results of two remote sensing aircraft data sets detection based on the model, it is proved that small-scale deep networks can also achieve high performance for multi-scale aircraft target detection on small sample data sets. © 2019 IEEE.","Convolutional Neural Network; Feature Fusion; Post Processing; Remote Sensing Image; Target Detection",
"Rapid Earthquake Damage Detection Using Deep Learning from VHR Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077712435&doi=10.1109%2fIGARSS.2019.8898147&partnerID=40&md5=a58c8cc52be888d3b0b53813ffc1f110","Very High Resolution (VHR) remote sensing optical imagery is a huge source of information that can be utilized for earthquake damage detection and assessment. Time critical task such as performing the damage assessment, providing immediate delivery of relief assistance require immediate response; however, processing voluminous VHR imagery using highly accurate, but computationally expensive deep learning algorithms demands the High Performance Computing (HPC) power.To maximize the accuracy, deep convolution neural network (CNN) model is designed especially for the earthquake damage detection using remote sensing data and implemented using high performance GPU without compromising with the execution time. Geoeye1 VHR disaster images of the Haiti earthquake occurred in year 2010 is used for analysis. Proposed model provides good accuracy for damage detection; also significant execution speed is observed on GPU K80 High Performance Computing (HPC) platform. © 2019 IEEE.","damage detection; Deep CNN; Deep learning; GPU; HPC",
"MSPPF-Nets: A Deep Learning Architecture for Remote Sensing Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077706784&doi=10.1109%2fIGARSS.2019.8899068&partnerID=40&md5=94348d4605f264a2a1cd643493fdfaaa","Nowadays, deep learning has got a major success in computer vision, especially in image recognition. In this paper, a new architecture based on DenseNets which is referred to as Multi-Scale Input Spatial Pyramid Pooling Fusion Networks (MSPPF-nets) is proposed for the work of classification of local climate zones (LCZs). Multi-scale remote sensing images can be inputs of the networks by the benefit of Spatial Pyramid Pooling (SPP) layer, multi-scale features from different channels were extracted and fused by our multi-branch-input framework. The final classification results have illustrated the feasibility of this presented classification method. © 2019 IEEE.","Classification; Feature fusion; Local Climate Zones (LCZs); MSPPF-nets; Remote sensing image",
"Identify Urban Area from Remote Sensing Image Using Deep Learning Method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077716473&doi=10.1109%2fIGARSS.2019.8898874&partnerID=40&md5=b76f50c30c94137915f86ee655935989","Urban area is the main and important space of human activities with a large number of population. Compared with rural and other natural areas, the dense buildings and high-intensity land use are the most different features of urban areas. Therefore, the urban area has obvious texture in remote sensing images. Effective and accurate identification of urban area can play an important role in urban study, urban planning and other urban-related fields. In this paper, a new method based on urban and non-urban scene classification using Convolutional Neural Network (CNN) technique is developed to identify the boundary of urban areas and is applied in Beijing as an example. An acceptable result of the urban area identification was obtained, indicating a great potential of deep learning method in urban related studies. © 2019 IEEE.","Convolutional Neural Network (CNN); remote sensing; urban area identification; urban scene",
"Scene Classification of High Resolution Remote Sensing Images Via Self-Paced Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077720406&doi=10.1109%2fIGARSS.2019.8898387&partnerID=40&md5=6cf495bba7a9539d92afa34c40e0ee68","Scene classification of high resolution remote sensing (HRRS) images is a fundamental yet challenging problem for remote sensing image analysis. In this paper, we focus on tackling the problem of HRSS scene classification using a small pool of unlabeled images and only a few labeled images per category, namely, few-shot scene classification (FSSC), which is more challenging than common scene classification task. The key challenge arises from selecting trustworthy samples from the pool of unlabeled images that have high confidence. To address this challenge, a novel local manifold constrained self-paced deep learning method is proposed. Specifically, the model is learned by gradually selecting easy samples from the pool of unlabeled images, assigning them with pseudo-labels and further adopting them with labeled images as the new training set. In addition, a local manifold constraint is introduced to enforce that the pseudo-labels assigned by the initial model should be consistent with the local manifold of the labeled samples. In such way, the confidence of the selecting samples is increased and is beneficial to train more robust classifier. Experimental results on a publicly available large scale NWPU-RESISC45 data set demonstrated the effectiveness of our method in achieving competitive performance while significantly reducing manually labeled cost. © 2019 IEEE.","Few-shot scene classification; remote sensing images; self-paced learning",
"PM2.5 prediction based on random forest, XGBoost, and deep learning using multisource remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068938395&doi=10.3390%2fatmos10070373&partnerID=40&md5=7e406b259a761a8d01531f3bd9112c06","In recent years, air pollution has become an important public health concern. The high concentration of fine particulate matter with diameter less than 2.5 μm (PM2.5) is known to be associated with lung cancer, cardiovascular disease, respiratory disease, and metabolic disease. Predicting PM2.5 concentrations can help governments warn people at high risk, thus mitigating the complications. Although attempts have been made to predict PM2.5 concentrations, the factors influencing PM2.5 prediction have not been investigated. In this work, we study feature importance for PM2.5 prediction in Tehran's urban area, implementing random forest, extreme gradient boosting, and deep learning machine learning (ML) approaches. We use 23 features, including satellite and meteorological data, ground-measured PM2.5, and geographical data, in the modeling. The best model performance obtained was R2 = 0.81 (R = 0.9), MAE = 9.93 μg/m3, and RMSE = 13.58 μg/m3 using the XGBoost approach, incorporating elimination of unimportant features. However, all three ML methods performed similarly and R2 varied from 0.63 to 0.67, when Aerosol Optical Depth (AOD) at 3 km resolution was included, and 0.77 to 0.81, when AOD at 3 km resolution was excluded. Contrary to the PM2.5 lag data, satellite-derived AODs did not improve model performance. © 2019 by the authors.","Deep leaning; Feature importance; PM2.5; Prediction; Random forest; XGBoost","Adaptive boosting; Decision trees; Diseases; Forecasting; Machine learning; Meteorology; Remote sensing; Deep leaning; Feature importance; PM2.5; Random forests; XGBoost; Deep learning; aerosol; algorithm; concentration (composition); machine learning; particulate matter; prediction; remote sensing; Iran; Tehran [Iran]; Tehran [Tehran (PRV)]"
"Buildings Extraction from Remote Sensing Data Using Deep Learning Method Based on Improved U-Net Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077713473&doi=10.1109%2fIGARSS.2019.8899798&partnerID=40&md5=8a9218d8004e05bd3cff9a5f26e7d6a7","Due to the different shapes of buildings and the cross-distribution with various surface types around them, it is difficult to extract buildings in high precision using traditional classification methods. The deep learning method based on neural network can mine useful information of remote sensing image in depth and improve the accuracy of building recognition. However, the application of neural network in building extraction is limited because of the large number of parameters involved and the large demand for training samples. In order to improve the accuracy of building extraction in remote sensing images by using deep learning method, identity skip connection is inserted into U-net network for samples training, which effectively reduces the number of parameters, significantly reduces the size of the model, and avoids the gradient explosion caused by the deepening of the number of layers, and obviously improves the accuracy of segmentation. By comparing the results of different layers, it is shown that with the deepening of layers, the accuracy increases. © 2019 IEEE.","building; Identity skip connection; semantic segmentation; U-net network",
"Building Extraction of Multi-source Data Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084324623&doi=10.1109%2fICIVC47709.2019.8980990&partnerID=40&md5=89d3c6c5e575189b98e4b7bb3a6edb34","Aiming at the poor robustness of high-resolution remote sensing image building extraction methods based on machine learning and it is difficult to fully exploit the deep features in images, several methods based on deep learning semantic segmentation have been compared and fully analyzed with average precision, category accuracy, F1 score and IoU as measures. The results show that DeepLabv3plus model can obtain more accurate pixel-level building extraction results, the overall accuracy can be improved by 2%, IoU can be increased by 3%, and the boundary details can be fully expressed. A method of automatic labeling is designed based on multi-source data. Experiments were carried out and proved that the sample sets produced by our method is close to the training results of true value labels. © 2019 IEEE.","automatic labeling; building extraction; deep learning; DeepLabv3; remote sensing image","Extraction; Image segmentation; Remote sensing; Semantics; Automatic labeling; Building extraction; High resolution remote sensing images; Learning semantics; Multisource data; On-machines; Overall accuracies; Pixel level; Deep learning"
"Woodland Detection Using Most-Sure Strategy to Fuse Segmentation Results of Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077675301&doi=10.1109%2fIGARSS.2019.8897705&partnerID=40&md5=d394b25bdcdb60ea0e6e750948da74cd","For obtaining information about ecosystem resource, GF-1 satellite was launched on April 26, 2013, which is the first satellite of the China's High-Resolution Earth Observation System. After obtaining some of the remote sensing images from GF-1, we selected WFV(wide field vision) images and detected the woodland to separate it from other geography types in the images. First, WFV images were clipped and labeled, then two deep learning models, POI-Net and Deep-UNet were used for training. We fused the prediction matrixes of deep learning networks using proposed ""Most-sure strategy"". The results show that our method can effectively improve the accuracy of woodland detection and segmentation results are outstanding. In addition, the proposed framework can also detect woodland in images returned by GF-6 satellite. © 2019 IEEE.","Deep Learning; Most-sure Fusion Strategy; Remote Sensing Image; Woodland Detect",
"Classification of remote sensing scenes based on neural architecture search network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074387274&doi=10.1109%2fSIPROCESS.2019.8868439&partnerID=40&md5=2266485e841f8f54500cc9bb7010e70f","Remote sensing image scene classification plays an important role in remote sensing image retrieval, land-use identification and urban planning. Deep learning brings great opportunity to the research in this field, but it transfers the difficulty of traditional characteristic engineering to the design of network structure. In this paper, we focus on the automatic design of the network model and propose a remote sensing scene classification method based on Neural Architecture Search Network (NASNet). We further use the transfer learning technology to make the designed network well migrated to the remote sensing scene classification data set. This method can automatically build the appropriate network structure according to the application. We compare the proposed method on a publicly large-scale dataset with several convolutional neural network (CNN) models. The experimental results demonstrate that the proposed method provides state-of-the-art performance compared with the traditional artificial neural network. © 2019 IEEE.","Automatic design; Neural architecture search network; Remote sensing scene classification; Transfer learning","Classification (of information); Deep learning; Image processing; Image retrieval; Land use; Large dataset; Neural networks; Remote sensing; Automatic design; Convolutional neural network; Neural architectures; Remote sensing image retrieval; Remote sensing images; Scene classification; State-of-the-art performance; Transfer learning; Network architecture"
"Change Detection in Vegetation Cover Using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081158703&doi=10.1109%2fICCES45898.2019.9002581&partnerID=40&md5=05d225607324cab40fb333e642399b2b","Because of man-made occasions and regular causes, numerous areas on the land are experiencing quick and wide-running changes in vegetation spread. Vegetation is the significant piece of land spread and its progressions have a significant effect on the vitality and mass biochemical cycles. It is also a key marker of territorial natural condition change. Change discovery alludes to the way toward recognizing contrasts in the condition of a state by watching it at various occasions. The Multi Spectral Remote Sensing pictures are proficient for acquiring a superior comprehension of the earth condition and its changes. A Convolution Neural Network (CNN) plays a prominent role in classifying the images more accurately, compared to other Machine Learning algorithms. This paper concentrates on identifying the vegetation change in Bapatla region of Andhra Pradesh state, India between the years 2015 to 2017 using multi spectral Landsat images and deep learning techniques and the experimental results shown a decrease in vegetation land in approximately 240 square kilo meters. © 2019 IEEE.","Change Detection; Convolution Neural Network; Multi spectral; Vegetation Cover","Convolution; Learning algorithms; Learning systems; Remote sensing; Vegetation; Andhra Pradesh; Change detection; Convolution neural network; Learning techniques; Multi-spectral; Natural conditions; Vegetation change; Vegetation cover; Deep learning"
"Deep Learning for Multilabel Land Cover Scene Categorization Using Data Augmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068237732&doi=10.1109%2fLGRS.2019.2893306&partnerID=40&md5=192eb8d379680520385fdb7f70fe0b2a","Land cover classification is a flourishing research topic in the field of remote sensing. Conventional methodologies mainly focus either on the simplified single-label case or on the pixel-based approaches that cannot efficiently handle high-resolution images. On the other hand, the problem of multilabel land cover scene categorization remains, to this day, fairly unexplored. While deep learning and convolutional neural networks have demonstrated an astounding capacity at handling challenging machine learning tasks, such as image classification, they exhibit an underwhelming performance when trained with a limited amount of annotated examples. To overcome this issue, this paper proposes a data augmentation technique that can drastically increase the size of a smaller data set to copious amounts. Our experiments on a multilabel variation of the UC Merced Land Use data set demonstrate the potential of the proposed methodology, which outperforms the current state of the art by more than 6% in terms of the F-score metric. © 2004-2012 IEEE.","Convolutional neural networks (CNNs); data augmentation; land cover; multilabel classification; remote sensing; scene categorization","Convolution; Land use; Neural networks; Remote sensing; Convolutional neural network; Data augmentation; Land cover; Multi-label classifications; Scene categorization; Deep learning; artificial neural network; data processing; image classification; land cover; machine learning; pixel; remote sensing"
"Cloud Detection Based on Deep Learning Combining Muti-Feature for Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077697742&doi=10.1109%2fIGARSS.2019.8898838&partnerID=40&md5=52843240e4ade8ea37cdb9ad4f3632d2","The accurate detection of clouds in images is prerequisite for remote sensing image processing and applications. Traditional cloud detection methods rely on particular sensors, and the artificial neural network method only uses spectral or spatial information. In this paper, a novel method combining multiple features based on deep learning (MDL) for cloud detection is proposed. Deep neural network (DNN) and fully convolutional neural (FCN) network are applied to extract the spectral and spatial features of the images respectively, and the features are used for the input of another DNN for re-learning while the image data also serves as an input to the DNN. Finally, joint feature obtained by relearning is classified by Support Vector Machine (SVM). The method makes full use of the spectral-spatial information of the images to detect cloud comprehensively. A comparative experiment was carried on Landsat 8 images containing different types of clouds over various underlying surfaces. The results show that the MDL method performs favorably, which is significantly improved compared to the single neural network algorithm and the function of mask (FMask) algorithm. © 2019 IEEE.","Cloud detection; DNN; FCN; Landsat 8",
"Visual Question Answering from Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077717968&doi=10.1109%2fIGARSS.2019.8898891&partnerID=40&md5=dcfbcfa591ea2d8ec216d061f438772c","Remote sensing images carry wide amounts of information beyond land cover or land use. Images contain visual and structural information that can be queried to obtain high level information about specific image content or relational dependencies between the objects sensed. This paper explores the possibility to use questions formulated in natural language as a generic and accessible way to extract this type of information from remote sensing images, i.e. visual question answering. We introduce an automatic way to create a dataset using OpenStreetMap1 data and present some preliminary results. Our proposed approach is based on deep learning, and is trained using our new dataset. © 2019 IEEE.","Deep Learning; Natural Language; OpenStreetMap; Remote Sensing; Visual Question Answering",
"A method for road surface anomaly detection from remote sensing data by deep convolutional networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074944114&doi=10.1109%2fICTIS.2019.8883584&partnerID=40&md5=eaa36087fe5f284b0a45cc423ff03de1","In the area of transportation construction and road safety, road structures need to be frequently inspected in order to discover damage threatens and potential risks. Therefore, the automatic technique of detecting road surface anomaly is very important. The paper aims to extract road surface anomaly from remote sensing data. The anomalies can be regarded as possible road damage candidates. This is a very challenging task, and we focus on the targets whose sizes are not too tiny to be seen in remote sensing image, like landslide, pothole, ponding, and so on. The basic idea of the proposed approach is to firstly identify the road pavement materials, and then analyze their characteristics to find abnormal conditions. We treat the task of road material identification as an image object segmentation/classification process, for which the methods based on deep learning framework can be applied. Then, an approach is developed to detect if there exits any abnormal subjects in the road region, and these subjects are the candidates of desired anomalies. These candidates are further analyzed based on the shape and spectral features, to output the final anomaly results. © 2019 IEEE.","Deep convolutional network; Remote sensing imagery; Road region extraction; Road surface anomaly detection; Transportation maintenance","Convolution; Deep learning; Image segmentation; Motor transportation; Remote sensing; Roads and streets; Convolutional networks; Region extraction; Remote sensing imagery; Road surfaces; Transportation maintenance; Anomaly detection"
"An End-to-End Joint Unsupervised Learning of Deep Model and Pseudo-Classes for Remote Sensing Scene Representation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073200293&doi=10.1109%2fIJCNN.2019.8852335&partnerID=40&md5=e452bad26b569b57606f919e77303dea","This work develops a novel end-to-end deep unsupervised learning method based on convolutional neural network (CNN) with pseudo-classes for remote sensing scene representation. First, we introduce center points as the centers of the pseudo classes and the training samples can be allocated with pseudo labels based on the center points. Therefore, the CNN model, which is used to extract features from the scenes, can be trained supervised with the pseudo labels. Moreover, a pseudo-center loss is developed to decrease the variance between the samples and the corresponding pseudo center point. The pseudo-center loss is important since it can update both the center points with the training samples and the CNN model with the center points in the training process simultaneously. Finally, joint learning of the pseudo-center loss and the pseudo softmax loss which is formulated with the samples and the pseudo labels is developed for unsupervised remote sensing scene representation to obtain discriminative representations from the scenes. Experiments are conducted over two commonly used remote sensing scene datasets to validate the effectiveness of the proposed method and the experimental results show the superiority of the proposed method when compared with other state-of-the-art methods. © 2019 IEEE.","Convolutional Neural Network (CNN); End-to-End Learning; Pseudo-Class; Remote Sensing Scene Representation; Unsupervised Learning","Convolution; Deep learning; Machine learning; Neural networks; Sampling; Unsupervised learning; Convolutional neural network; End to end; Pseudo-Class; Scene representation; State-of-the-art methods; Training process; Training sample; Unsupervised learning method; Remote sensing"
"SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline In Vitro","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068202699&doi=10.1109%2fLGRS.2018.2890413&partnerID=40&md5=74c339d4987181eb9923db41fba40bc6","Lack of annotated samples greatly restrains the direct application of deep learning in remote sensing image scene classification. Although research studies have been done to tackle this issue by data augmentation with various image transformation operations, they are still limited in quantity and diversity. Recently, the advent of the unsupervised learning-based generative adversarial networks (GANs) brings us a new way to generate augmented samples. However, such GAN-generated samples are currently only served for training GANs model itself and for improving the performance of the discriminator in GANs internally (in vivo). It becomes a question of serious doubt whether the GAN-generated samples can help better improve the scene classification performance of other deep learning networks (in vitro), compared with the widely used transformed samples. To answer this question, this letter proposes a SiftingGAN approach to generate more numerous, more diverse, and more authentic labeled samples for data augmentation. SiftingGAN extends traditional GAN framework with an Online-Output method for sample generation, a Generative-Model-Sifting method for model sifting, and a Labeled-Sample-Discriminating method for sample sifting. Experiments on the well-known aerial image data set demonstrate that the proposed SiftingGAN method can not only effectively improve the performance of the scene classification baseline that is achieved without data augmentation but also significantly excels the comparison methods based on traditional geometric/radiometric transformation operations. © 2004-2012 IEEE.","Data augmentation; deep learning; generative adversarial networks (GANs); scene classification","Antennas; Classification (of information); Deep learning; Image enhancement; Mathematical transformations; Metadata; Remote sensing; Adversarial networks; Aerial image data; Comparison methods; Data augmentation; Image transformations; Remote sensing images; Sample generations; Scene classification; Image classification; data quality; data set; image analysis; image classification; remote sensing; research work"
"Hyperspectral Image Features Classification Using Deep Learning Recurrent Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066637421&doi=10.1007%2fs10916-019-1347-9&partnerID=40&md5=17e55c1f43585be4b70ec7c6f4633de1","The implementation of Deep learning (DL) techniques, Object detection and classification has achieved remarkable results in remote sensing application. Deep learning with Recurrent Neural Network (RNN) technique on hyper-spectral data has been presented here. The only model which can analyze the hyper-spectral pixels as the sequence of information and also to identify the additional information categories through network reasoning is RNN model. This is first time that the framework of RNN has been introduced for the classification of hyper spectral Image. An activation function is proposed by the DL-RNN and also the parameter rectified functions for analyzing the sequence of data in the hyper-spectral images. Throughout the training procedure, the higher learning rates are fairly used by the activation function which has been proposed by avoiding the risk of divergence. In the proposed system the pixels of hyper–spectral images through the sequential perspective has been processed for capturing the sequence based data. The experimental result also shows that the proposed RNN has produced the improved F- score than the traditional deep learning methods. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Activation functions; Deep learning; Features vectors; Hyperspectral imaging; Recurrent neural network","article; deep learning; spectroscopy; image processing; procedures; Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer"
"Deep Learning Road Extraction Model Based on Similarity Mapping Relationship","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077701979&doi=10.1109%2fIGARSS.2019.8898155&partnerID=40&md5=e179f678215356d0e1f935642123fd24","This paper proposed a deep learning road extraction model based on similarity mapping relationship for extracting roads from high resolution remote sensing images. The model adopted a full convolutional network structure, and used the similarity between each pixel in the input image and the typical label sample to perform mapping relationship search, thereby, the classification of each pixel in the image to be classified is marked to the classification category of the typical sample. The model breaks through the traditional deep learning mindset and directly stores knowledge in the network, instead of just learning a set of feature extraction and integrated network parameters. Under the training data of 10, 000 small samples, the corresponding image results are obtained after 100, 000 iterations. The experimental results show that the accuracy of the method is 0.87, which proves that this method is more accurate and efficient than traditional algorithms. © 2019 IEEE.","Deep learning; Road extraction; Similarity mapping",
"An end-to-end conditional random fields and skip-connected generative adversarial segmentation network for remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068508238&doi=10.3390%2frs11131604&partnerID=40&md5=4a2a08fbd7a721fe262938d69c2cbac6","Semantic segmentation is an important process of scene recognition with deep learning frameworks achieving state of the art results, thus gaining much attention from the remote sensing community. In this paper, an end-to-end conditional random fields generative adversarial segmentation network is proposed. Three key factors of this algorithm are as follows. First, the network combines generative adversarial network and Bayesian framework to realize the estimation from the prior probability to the posterior probability. Second, the skip connected encoder-decoder network is combined with CRF layer to implement end-to-end network training. Finally, the adversarial loss and the cross-entropy loss guide the training of the segmentation network through back propagation. The experimental results show that our proposed method outperformed FCN in terms of mIoU for 0.0342 and 0.11 on two data sets, respectively. © 2019 by the authors.","Conditional random fields; Generative adversarial network; Loss function; Semantic segmentation","Backpropagation; Deep learning; Random processes; Remote sensing; Semantics; Adversarial networks; Bayesian frameworks; Conditional random field; Learning frameworks; Loss functions; Posterior probability; Remote sensing images; Semantic segmentation; Image segmentation"
"Multisource Labeled Data: An Opportunity for Training Deep Learning Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077710106&doi=10.1109%2fIGARSS.2019.8898311&partnerID=40&md5=67adeed76b7f437b31a684cbae8094ea","This paper addresses the opportunities and the challenges offered by multisource labeled data in the framework of deep learning techniques. After a review of the types of multisource labeled data, the focus is devoted to their use for the training of deep learning classification architectures. The need to generate training sets containing a very large number of labeled samples pushes toward the exploitation of all the possible available sources of labeled data. This crucial topic is addressed by categorizing the approaches to the collection of labeled data and presenting a framework for characterizing and modeling their information content and uncertainties to be used in the training of processing algorithms. The framework defines the main expected properties of large multisource training sets and relates them to both the characteristics of different data sources and the possible learning paradigms for the training of a deep architecture. © 2019 IEEE.","deep learning; Labeled data; land-cover maps; multisource data; remote sensing",
"Learning rotation-invariant binary codes for efficient object detection from remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072406328&doi=10.1117%2f1.JRS.13.036504&partnerID=40&md5=5ec305249d71724238a594f80ebdd073","Object detection is one of the most important research topics for remote sensing image (RSI) analysis. Because of the rapid development of satellite imaging technology, the resolution of RSI has increased dramatically, which brings a great challenge to the efficiency of RSI-based object detection.We present an object detection model, which greatly improves the detection speed by using learning-based hashing techniques. Specifically, first, a selective search method is used to generate many high quality object proposals that may contain objects. Then for each proposal, we learn rotation-invariant binary codes, which can handle the objects with arbitrary orientations in RSI, to quickly eliminate most nonobject proposals in Hamming space. And finally, the object detection task can be achieved by classifying the left (very limited amount of) proposals with more discriminating classification model. Experimental evaluations on a public very high-resolution remote sensing dataset show the superiority of the proposed method. Specifically, the rotation-invariant learning metric has been combined with three popular hashing methods, and performance improvements have been obtained for all the cases. Furthermore, comparisons with deep learning-based detectors show that hash-based methods fail to achieve state-of-the-art detection performance, but when the graphics processing unit is inaccessible, it achieves a good compromise between performance and runtime. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Learning to hash; Object classification; Object detection; Remote sensing; Rotationinvariant","Binary codes; Computer graphics; Deep learning; Graphics processing unit; Imaging techniques; Object recognition; Program processors; Remote sensing; Space optics; Arbitrary orientation; Detection performance; Efficient object detections; Experimental evaluation; Learning to hash; Object classification; Remote sensing images; Rotation invariant; Object detection"
"Haze removal from a single remote sensing image based on a fully convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072387084&doi=10.1117%2f1.JRS.13.036505&partnerID=40&md5=97731e15cfae5ab453ad150ee87df48f","In many remote sensing (RS) applications, haze greatly affects the quality of optical RS images, but we do not always have the conditions to acquire multiple images in the same area for haze removal tasks. Therefore, the research on haze removal from a single RS image is necessary. Previous haze-removal methods introduce various prior knowledge to solve this problem, and thus, the quality of these methods largely depends on the reliability and validity of prior knowledge, which brings various limitations. We propose and validate a deep-learning-based model for haze removal, named haze removal fully convolutional network, to estimate transmission maps and generate corresponding haze-removed images via an atmospheric scattering model. Moreover, we propose an approximate method to produce hazy-and-clear image pairs as a dataset for training and validation. Experiments using this dataset demonstrated that the proposed model achieved the desired results in both visual effect and quantitative measurement. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","deep learning; haze removal; haze removal fully convolutional network; remote sensing","Convolution; Deep learning; Neural networks; Atmospheric scattering models; Convolutional networks; Convolutional neural network; Haze removal; Learning Based Models; Quantitative measurement; Reliability and validity; Remote sensing images; Remote sensing"
"A Fast and Precise Method for Large-Scale Land-Use Mapping Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077679225&doi=10.1109%2fIGARSS.2019.8898705&partnerID=40&md5=0e112dcc06b4847811e9c25b4590fca2","The land-use map is an important data that can reflect the use and transformation of human land, and can provide valuable reference for land-use planning. For the traditional image classification method, producing a high spatial resolution (HSR), land-use map in large-scale is a big project that requires a lot of human labor, time, and financial expenditure. The rise of the deep learning technique provides a new solution to the problems above. This paper proposes a fast and precise method that can achieve large-scale land-use classification based on deep convolutional neural network (DCNN). In this paper, we optimize the data tiling method and the structure of DCNN for the multi-channel data and the splicing edge effect, which are unique to remote sensing deep learning, and improve the accuracy of land-use classification. We apply our improved methods in the Guangdong Province of China using GF-1 images, and achieve the land-use classification accuracy of 81.52%. It takes only 13 hours to complete the work, which will take several months for human labor. © 2019 IEEE.","Big Data; Deep Learning; Land-use Mapping; Semantic Segmentation",
"Deep Learning for SAR-Optical Image Matching","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077692111&doi=10.1109%2fIGARSS.2019.8898635&partnerID=40&md5=c382fe0d785719e2bb15b46747bf7656","The automatic matching of corresponding regions in remote sensing imagery acquired by synthetic aperture radar (SAR) and optical sensors is a crucial pre-requesite for many data fusion endeavours such as target recognition, image registration, or 3D-reconstruction by stereogrammetry. Driven by the success of deep learning in conventional optical image matching, we have carried out extensive research with regard to deep matching for SAR-optical multi-sensor image pairs in the recent past. In this paper, we summarize the achieved findings, including different concepts based on (pseudo-)siamese convolutional neural network architectures, hard negative mining, alternative formulations of the underlying loss function, and creation of artificial images by generative adversarial networks. Based on data from state-of-the-art remote sensing missions such as TerraSAR-X, Prism, Worldview-2, and Sentinel-1/2, we show what is already possible today, while highlighting challenges to be tackled by future research endeavors. © 2019 IEEE.","Data Fusion; Deep Learning; Image Matching; Optical Images; SAR Images",
"Arbitrary-Oriented Ship Detection Based on Rotation Region Locating Networks in Large Scale Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084293548&doi=10.1109%2fICIVC47709.2019.8981006&partnerID=40&md5=84fa1c7ec852ea442a9fdf1e880ccd71","Although deep learning has dominated the ship detection domain, it still has two challenges: arbitrary-oriented densely arranged ships cause detection omissions and large scale image contains redundant areas. This paper proposes an effective convolutional neural network framework for arbitrary-oriented ship detection in large scale and complex scenes. In this framework, we propose Cumulative Feature Pyramid Networks for multi-receptive-field feature fusion, which enhances high-level semantic information at all scales. Based on the outputs of Region Proposal Networks, Rotation Region Locating Network predicts rotation bounding box of arbitrary-oriented ships and adopts rotation intersection over union to avoid the effect of ship dense arrangement. For large scale scenes, No-Ship Area Suppression uses OTSU algorithm to generate binary mask and then filter out non-ship regions to reduce redundant computations. Additionally, we firstly build a remote sensing image dataset for ship detection, which contains 4237 images and over 21200 ships. Experimental comparisons with the state-of-the-art approaches validate the effectiveness, accuracy and robustness of the proposed method. © 2019 IEEE.","deep learning; feature fushion; remote sensing; rotation region proposal; ship detection","Convolutional neural networks; Deep learning; Remote sensing; Rotation; Semantics; Experimental comparison; Feature pyramid; High level semantics; Otsu algorithm; Receptive fields; Redundant computation; Remote sensing images; State-of-the-art approach; Ships"
"Evaluation of deep learning convolutional neural network for crop classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071514880&doi=10.35940%2fijrte.B2872.078219&partnerID=40&md5=ba00bb52a7f74e4832fd8c1f2cdfb2d5","In this paper, we have done exploratory experiments using deep learning convolutional neural network framework to classify crops into cotton, sugarcane and mulberry. In this contribution we have used Earth Observing-1 hyperion hyperspectral remote sensing data as the input. Structured data has been extracted from hyperspectral data using a remote sensing tool. An analytical assessment shows that convolutional neural network (CNN) gives more accuracy over classical support vector machine (SVM) and random forest methods. It has been observed that accuracy of SVM is 75 %, accuracy of random forest classification is 78 % and accuracy of CNN using Adam optimizer is 99.3 % and loss is 2.74 %. CNN using RMSProp also gives the same accuracy 99.3 % and the loss is 4.43 %. This identified crop information will be used for finding crop production and for deciding market strategies. ©BEIESP.","Convolutional neural network; Hyperspectral remote sensing data; Random forest classifier; Support vector machine",
"A Multi-Task Architecture for Remote Sensing by Joint Scene Classification and Image Quality Assessment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077705053&doi=10.1109%2fIGARSS.2019.8898659&partnerID=40&md5=d711b4899eecd6cc2ba2203f73eb09c6","In this work, we propose a compact multi-task architecture based on deep learning for remote sensing scene classification and image quality assessment (IQA) simultaneously. The model can be trained in an end-to-end manner, and the robustness of classification is improved in our method. More importantly, by exploiting IQA and super-resolution, the accurate classification results can be obtained even if the images are distorted or with low quality. To the best of our knowledge, it is the first successful attempt to associate IQA with scene classification in a unified multi-task architecture. Our method is evaluated on the expanded UC Merced Land-Use dataset after data augmentation. In comparison with some other methods, the experimental results show that the proposed structure makes a great improvement on both classification and IQA. © 2019 IEEE.","deep learning; image quality assessment; image super-resolution; multi-task learning; Remote sensing; scene classification",
"Aircraft segmentation from remote sensing image by transferring natual image trained forground extraction CNN model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074437596&doi=10.1109%2fSIPROCESS.2019.8868727&partnerID=40&md5=2291a691b6d79a12d298aa88e4b3c1c6","With the rapid development of remote sensing technology, remote sensing image processing has been widely used in the field of national defense and the people's livelihood. However, due to the high cost of manually collecting and annotating data, the number of currently public remote sensing datasets are limited, and the datasets of remote sensing images for foreground segmentation and detection is also confined. In this paper, because of the difficulty of small-sized datasets, the foreground segmentation of aircraft remote sensing image is realized by using the idea of transfer learning. We implemented three kinds of different aircraft segmentation networks for small-sized aircraft remote sensing images by transferring three kinds of different foreground segmentation networks trained on large-scaled datasets. The best aircraft segmentation accuracy is up to 79.201% under custom evaluation criteria. Experiment results showed that transferring a foreground extraction CNN model trained on public datasets to cope with the task of aircraft segmentation on small aircraft remote sensing images is significantly effective. © 2019 IEEE.","Deep learning; Foreground segmentation; Remote sensing images; Transfer learning","Aircraft; Deep learning; Extraction; Image segmentation; Large dataset; Evaluation criteria; Foreground extraction; Foreground segmentation; Remote sensing image processing; Remote sensing images; Remote sensing technology; Segmentation accuracy; Transfer learning; Remote sensing"
"Sar-Image Based Urban Change Detection in Bangkok, Thailand Using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077702817&doi=10.1109%2fIGARSS.2019.8899341&partnerID=40&md5=de9bb1bb382457af61797bc3cae48f58","The building change detection, which is important for monitoring human activity in urban and sub-urban area, can be done by the using of remote sensing data. To overcome the cloud cover problem that cause the limited utilizing in optical images which is repeatedly occurred in tropical areas, we decided to use synthetic aperture radar (SAR) data, a kind of remote sensing data that does not get affected by weather condition, to fulfill this change detection purpose. In this study, Bangkok, Thailand were selected as our study area in order to demonstrate that the SAR data can be used in the area that optical data cannot be used such as tropical areas like in Thailand. As our target is to detect changes of buildings, so we have to consider separating the change that cause by seasonal and other non-target objects. To complete this purpose, we decided to use one of the deep learning techniques called U-net which is created for the image segmentation task. We have created the ground truth and used one portion for training the network and another portion for validate the result. Finally, the model that trained by our training data was able to provide promising results. © 2019 IEEE.","deep learning; SAR; Satellite imagery; U-net; urban change",
"High-Resolution Remote Sensing Image Scene Understanding: A Review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077724564&doi=10.1109%2fIGARSS.2019.8899293&partnerID=40&md5=8f97d4d78b06c24dca2a1b9771ab3e26","High-resolution remote sensing (HRS) image analysis is a fundamental but challenging problem. To bridge the semantic gap, scene understanding has been proposed to achieve higher-level interpretation, through classifying the HRS scene through spatial relationship cognition and semantic induction between the land-cover objects. As a new research field, however, there has not yet been a study expatiating and summarizing the current situation of scene understanding. This paper first defines the concept of scene understanding for HRS imagery, which is different from natural image scene classification. The theory of scene understanding for HRS imagery is investigated, and is classified into four main categories: 1) scene classification based on semantic objects; 2) scene classification based on mid-level features; 3) scene classification based on deep learning; and 4) scene understanding applications based on geographic data mining. © 2019 IEEE.","deep learning; high spatial resolution; remote sensing; Scene understanding; semantic objects",
"Land Cover Classification Using Remote Sensing Images and Lidar Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077721729&doi=10.1109%2fIGARSS.2019.8899840&partnerID=40&md5=386d4e18c203fb7707ec676c6530f579","This study proposes a land cover classification method using remote sensing images and LiDAR data. In this method, deep learning and conditional random fields (CRF) optimization is applied for accurately classifying land covers. To address the issue that pixel-based deep learning methods are difficult to capture the precise outline of ground objects, we combine deep feature learning strategy with image objects for accurately interpreting remote sensing images. Context information revealing relationships between image objects are explored for optimizing the classification result by using object-based CRF, where height information derived from LiDAR data is considered. Vaihingen dataset is used to validate the proposed method and an overall classification accuracy of 92.4% is achieved. © 2019 IEEE.","CNN; CRF model; deep learning; LiDAR; OBIA; VHR images",
"A Multi-Task Deep Learning Framework Coupling Semantic Segmentation and Image Reconstruction for Very High Resolution Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077684966&doi=10.1109%2fIGARSS.2019.8898133&partnerID=40&md5=8dd52697606c71d9fa54571998ca8b46","Semantic segmentation, especially for very high-resolution satellite data, is one of the pillar problems in the remote sensing community. Lately, deep learning techniques are the ones that set the state-of-the-art for a number of benchmark datasets, however, there are still a lot of challenges that need to be addressed, especially in the case of limited annotations. To this end, in this paper, we propose a novel framework based on deep neural networks that is able to address concurrently semantic segmentation and image reconstruction in an end to end training. Under the proposed formulation, the image reconstruction acts as a regularization, constraining efficiently the solution in the entire image domain. This self-supervised component helps significantly the generalization of the network for the semantic segmentation, especially in cases of a low number of annotations. Experimental results and the performed quantitative evaluation on the publicly available ISPRS (WGIII/4) dataset indicate the great potential of the developed approach. © 2019 IEEE.","Autoencoders; Deep learning; Feature representations; Fully-convolutional networks; Limited annotations",
"A resurgence in neuromorphic architectures enabling remote sensing computation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074173257&doi=10.1109%2fSpaceComp.2019.00009&partnerID=40&md5=f25f526992896c635bb411db9c36c61f","Technological advances have enabled exponential growth in both sensor data collection, as well as computational processing. However, as a limiting factor, the transmission bandwidth in between a space-based sensor and a ground station processing center has not seen the same growth. A resolution to this bandwidth limitation is to move the processing to the sensor, but doing so faces size, weight, and power operational constraints. Different physical constraints on processor manufacturing are spurring a resurgence in neuromorphic approaches amenable to the space-based operational environment. Here we describe historical trends in computer architecture and the implications for neuromorphic computing, as well as give an overview of how remote sensing applications may be impacted by this emerging direction for computing. © 2019 IEEE.","Computer-Architectures; Deep-Learning; Neural-Networks; Neuromorphic-computing; Remote-Sensing","Bandwidth; Deep learning; Deep neural networks; Network architecture; Neural networks; Remote sensing; Space optics; Computational processing; Neuromorphic Architectures; Neuromorphic computing; Operational constraints; Operational environments; Remote sensing applications; Remote sensing computations; Sensor data collections; Computer architecture"
"A Deep Learning based architecture for rainfall estimation integrating heterogeneous data sources","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073186975&doi=10.1109%2fIJCNN.2019.8852229&partnerID=40&md5=550bc91fbc55279f61cd3ce15c22b3f7","Rain gauges are sensors providing direct measurement of precipitation intensity at individual point sites, and, usually, spatial interpolation methods are used to obtain an estimate of the precipitation field over the entire area of interest. Among them, Kriging with External Drift (KED) is a largely used and well-recognized method in this field. However, interpolation methods need to work with real-time data, and therefore can be hardly used in real-time scenarios. To overcome this issue, we propose a general machine learning framework, which can be trained offline, based on a deep learning architecture, also integrating information derived from remote sensing measurements such as weather radars and satellites. The framework allows to provide accurate estimations of the rainfall in the areas where no rain gauge data is available. Experimental results, conducted on real data concerning a southern region in Italy, provided by the Department of Civil Protection (DCP), show significant improvement in comparison with KED and other machine learning techniques. © 2019 IEEE.","classification; deep neural networks; hydrology; rainfall estimation","Classification (of information); Hydrology; Interpolation; Machine learning; Meteorological radar; Network architecture; Rain; Rain gages; Remote sensing; Weather satellites; Heterogeneous data sources; Integrating information; Kriging with external drifts; Learning architectures; Machine learning techniques; Precipitation intensity; Rainfall estimations; Spatial interpolation method; Deep neural networks"
"A Deep Architecture Based on a Two-Stage Learning for Semantic Segmentation of Large-Size Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077675993&doi=10.1109%2fIGARSS.2019.8899204&partnerID=40&md5=d816367b8c7682d9cd5acc9e9eb5322a","Remote sensing images (RSIs) usually have much larger size compared to typical natural images used in computer vision applications. This makes the computational cost of training convolutional neural networks with full-size images unaffordable. Commonly used methodologies for semantic segmentation of RSIs perform training and prediction on cropped local image patches. Thus they fail to model the potential dependencies between ground objects at a higher level of abstraction. In order to better exploit global context information in RSIs, a deep architecture based on a two-stage training approach that is specially tailored to training large-size RSIs is proposed. In the first training stage, down-scaled images are used as input to learn high-level features from a large image area. In the second training stage, a local feature extraction network is designed to extract low-level information from cropped image patches. The complementary information learned from different levels is fused to make the prediction. As a result, the proposed two-stage training approach is able to exploit the context information of RSIs from a larger perspective without losing spatial details. Experimental results on a benchmark remote sensing dataset demonstrate the effectiveness of the proposed approach. © 2019 IEEE.","Convolutional Neural Network; Deep Learning; Remote Sensing; Semantic Segmentation",
"Classification Performance Evaluation of Deep Learning Architectures for Complex Object Based Facility Recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077691835&doi=10.1109%2fIGARSS.2019.8898653&partnerID=40&md5=213e3825f7196b6533832c631e0f8877","Nuclear proliferation monitoring using remote sensing imagery is an essential area of research from a national security perspective. Studies have shown that traditional per-pixel and small patch based classification schemes prove to be insufficient in detecting complex facilities such as coal, gas and nuclear. In this paper, we evaluate the state of the art deep learning architectures for detecting the aforementioned complex facilities. We use the principles of transfer learning to fine-tune the state of the art deep learning architectures using models pre-trained on 1000 classes of Imagenet data. We evaluated the classification performance of these models on a benchmark imagery database consisting of two hundred complex facilities collected over the United States. © 2019 IEEE.",,
"An Introspective Learning Strategy for Remote Sensing Scene Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077676652&doi=10.1109%2fIGARSS.2019.8898925&partnerID=40&md5=5f893458a7cbdb5038f87270da5a9a44","In this paper, a novel introspective learning strategy for remote sensing scene classification is proposed. Through this strategy, the neural network used for classification can introspectively generate negative samples. In most training deep neural networks, negative samples are rarely noticed. We are the first to actively introduce negative samples into the remote sensing scene classification tasks. The goal of this paper is to analyze the effect of introspective negative samples on remote sensing scene classification tasks. Experiments demonstrate that the introduction of negative samples in training can effectively improve the classification accuracy and robustness. In addition, we found that our method can effectively against invalid remote sensing images. © 2019 IEEE.","Deep learning; Introspective strategie; Negative samples; Remote sensing; Scene classification",
"Light encoder-decoder network for road extraction of remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072510217&doi=10.1117%2f1.JRS.13.034510&partnerID=40&md5=0f2a22b3aa3c285b82ed081a87925ae1","Extracting roads from remote sensing images is an important task in the remote sensing field. We propose an approach of designing a light encoder-decoder network for road extraction. We analyze the relationship between road features and the receptive field of encoder-decoder networks and point out that the light encoder-decoder network can be achieved by controlling its receptive field. Based on this, we design road extraction networks on the architecture of a general encoder-decoder network, according to data specifications. In addition, we propose an adaptive weighted binary cross-entropy loss function to solve the problem of data imbalance in the training process. We validate our approach on the Massachusetts roads dataset and the DeepGlobe road extraction dataset. The experimental results show that our method reduces 98% and 94% of the parameters, respectively, compared with general encoder-decoder networks, whereas the performance of road extraction keeps well. Our approach has fewer parameters and good performance, so it is easier to deploy on mobile platforms. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","deep learning; encoder-decoder network; receptive field; remote sensing; road extraction","Decoding; Deep learning; Extraction; Feature extraction; Highway planning; Image processing; Remote sensing; Roads and streets; Data specifications; Encoder-decoder; General encoder decoders; Mobile platform; Receptive fields; Remote sensing images; Road extraction; Training process; Network coding"
"Segmentation of building footprints with xception and iouloss","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071417348&doi=10.1109%2fICMEW.2019.00078&partnerID=40&md5=6db56b8f7d15d5847692d144e527b50b","Advanced sensors are increasingly used in aeronautical facility. Broader acquisition of images improves the quality and quantity of remote sensing images and makes a large number of fine remote sensing images available. Although semantic segmentation models based on deep learning has been a great success in computer vision, there are still a lot of difficulties compared to regular visual senses. In this paper, we propose a novel end-to-end model for remote sensing image segmentation of building footprints and a loss function based on IoU to improve the performance. Semantic labels for building structures are being accurately allocated by our method. We make control experiments on Map Challenge dataset with other semantic segmentation model based on recent deep learning advances. Our proposed method generates 94.73% of IoU on the Map Challenge Dataset's validation set, which outperforms all previous models in literature. © 2019 IEEE.","Atrous convolution; Building extraction; Convolution network; Satellite imagery; Semantic segmentation","Convolution; Deep learning; Image enhancement; Remote sensing; Satellite imagery; Semantics; Advanced sensors; Building extraction; Building footprint; Building structure; Control experiments; End-to-end models; Remote sensing images; Semantic segmentation; Image segmentation"
"Feature Sparsity in Convolutional Neural Networks for Scene Classification of Remote Sensing Image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077713489&doi=10.1109%2fIGARSS.2019.8898875&partnerID=40&md5=0cb2924008f67dbca7827aad04c49a10","Recently, the analysis of remote sensing images has attracted a lot of attention. In the domain of scene classification, deep learning methods, especially convolutional networks (CNNs), currently achieve the best results. Although the classification performance has reached a high level, there are still some factors limiting the improvement of classification accuracy. Based on obeservation of remote sensing scene images, we fing that some scenes are quite similar though they belong to different classes. To improve the classification performance between different scenes with similar characteristics, we propose a significant Feature Sparsity Layer that can be esaily embedded into various convolutional network architectures. The proposed layer can inhibit the confusing features meanwhile stress the discriminative features, and it is used to sparse the multi-layer feature map, which is extracted by the convolutional layers. The proposed method achieves the state-of-the-art results on three datasets UC Merced Land Use, Aerial Image Data and OPTIMAL-31, and competitive result on dataset WHU-RS19. © 2019 IEEE.","CNNs; feature sparsity; Remote sensing image; scence classification",
"Forest Monitoring in Guatemala Using Satellite Imagery and Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077710458&doi=10.1109%2fIGARSS.2019.8899782&partnerID=40&md5=4051cfa40c46d398714bf377e8a03de7","Forests cover 34% of Guatemala. The Guatemalan government have taken proficient actions in past decades to reduce deforestation and are looking toward new space technologies to improve forestry monitoring. This paper demonstrates the ability to automatically detect pixel-level changes in satellite images of forested areas that can be used to assist Guatemalan agencies, using satellite imagery from the Copernicus program and specially-developed deep learning algorithms for image segmentation. © 2019 IEEE.","change detection; convolutional neural network; deforestation; Guatemala; machine learning; remote sensing",
"Learning Region Response Ranking Features for Remote Sensing Image Scene Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077714560&doi=10.1109%2fIGARSS.2019.8897886&partnerID=40&md5=0df200028b32fe9de5a7efab6fc91186","Recently, deep learning especially convolutional neural networks (CNNs) has huge great success for remote sensing image scene classification. However, global CNN features still lack geometric invariance for addressing the problem of large intra-class variations and so are not optimal for scene classification. In this paper, we introduce a new feature representation for scene classification, named region response ranking (3R) feature representations by using off-the-shelf CNN models. Specifically, by considering each cube pixel of a certain convolutional feature map as one image region, we jointly train a class-specific support vector machine (SVM) base classifier and a decision function for each scene class. The base classifier is used to generate 3R feature by reordering the SVM responses of all image regions in descending order and the decision function is used for classification with 3R feature representations. Comprehensive evaluations on the publicly available NWPU-RESISC45 data set and comparisons with state-of-the-art methods demonstrate that the proposed 3R feature is effective for remote sensing image scene classification.1 © 2019 IEEE.","convolutional neural network (CNN); region response ranking (3R) feature; Remote sensing image; scene classification",
"Contour-oriented Cropland Extraction from High Resolution Remote Sensing Imagery Using Richer Convolution Features Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072942426&doi=10.1109%2fAgro-Geoinformatics.2019.8820430&partnerID=40&md5=84964a15a25a02ecd2ed9be736fee8b5","Cropland extraction has great significance in many agricultural applications and has always been an important research focus. In this study, we proposed a contour-oriented approach that used the RCF network to extract cropland from high resolution remote sensing imagery. Weining County, Guizhou Province in China was selected to be the research area and Google Earth images were used as the data source. Compared with the canny algorithm, the RCF network detected the cropland contour much more accurately and completely, showing substantial improvement both numerically and visually. At last, we successfully employed this method to produce a cropland thematic map of a part of Weining County with 5 times increase in productivity comparing with complete manual production, suggesting the application value of such contour-oriented method. © 2019 IEEE.","Cropland extraction; Deep learning; High resolution remote sensing imagery; RCF","Deep learning; Extraction; Maps; Canny algorithm; Data-source; Google earths; Guizhou Province; High resolution remote sensing imagery; Research focus; Thematic maps; Remote sensing"
"Spatial Enhanced-SSD for Multiclass Object Detection in Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077712317&doi=10.1109%2fIGARSS.2019.8898526&partnerID=40&md5=3d9916b9c05074ecb945dbb9bd15e9aa","Accurate multiclass object detection in remote sensing images is a challenging task, especially for small objects. since the scales of objects in remote sensing images have a great variance, almost all of the advanced detection methods have shortcomings. Consequently, improving the accuracy of multiclass objects detection has always been the direction of researchers' efforts. In this paper, a spatial enhanced-Single Shot MultiBox Detector (SE-SSD) is proposed. First, to enhance the spatial information, we enlarge the input image channels with embedding oriented-gradients feature maps. Second, the multiple output layers in the backbone network are changed to reduce one pooling operation. Finally, we design a context module to enhance the receptive field for feature layer description in SE-SSD framework. Experimental results on DOTA dataset demonstrate that Spatial Enhanced-SSD method reaches a much higher mean average precision (mAP) than Faster R-CNN, SSD and other classic detection network. © 2019 IEEE.","Context Module; Deep Learning; Object Detection; Oriented Gradients; Receptive Field; Remote Sensing",
"Classification and identification of plant species based on multi-source remote sensing data: Research progress and prospect","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078911033&doi=10.17520%2fbiods.2019197&partnerID=40&md5=a92c6c8faa9c1a0e4cecc289933d0794","Species classification and identification is the basis of biodiversity monitoring, and is critical to deal with almost all ecological questions. In this paper, we aim to understand the current status and existing problems in plant species classification and identification using multi-source remote sensing data. We summarized the studies in this field since the year 2000, and found that most of these studies focus on temperate or boreal forests in Europe and North America, or African savanna. Airborne hyperspectral data is the most widely used remote sensing data source, and the LiDAR, as a supplementary data, significantly improves the classification accuracy through the information of single tree segmentation and three-dimensional vertical structure. Support vector machine and random forest are the most widely used non-parametric classification algorithms with an average classification accuracy of 80%. With the development of computer technology and machine learning, artificial neural network has developed rapidly in species identification. Based on the literature-based analysis, we propose that the current research in this field is still facing some challenges, including the complexity of classification objects, the effective integration of multi-source remote sensing data, the integration of plant phenology and texture characteristics, and the improvement in plant classification algorithm. The accuracy of plant classification and identification could be greatly improved by using the high-frequency data collection over time, the integration of hyperspectral and LiDAR data, the use of specific spectral information such as short-wave infrared imagery, and the development of novel deep learning techniques. © 2019, Chinese Academy of Sciences. All rights reserved.","Biodiversity; Remote sensing monitoring; Species classification; Species identification; Supervised classification",
"Maximum Membership Fraction Based Pure Pixel Assessment Approach for Hyperspectral Data Analysis Using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077694914&doi=10.1109%2fIGARSS.2019.8898389&partnerID=40&md5=ebc7b8e85716607c3534d721bd86352d","Land cover classification in the remote sensing has been done using various deep learning algorithms; and higher classification accuracies have been achieved. Such classification is based on the maximum membership fraction (MMF), when we use Convolutional Neural Network (CNN). MMF is basically the maximum probability fraction. A pixel under prediction has been assigned to that class which has maximum fraction out of the corresponding fractions for all land cover classes. Various methodologies exist for pure pixel extraction and used for hyperspectral unmixing. An assumption has been taken that MMF and abundance used in the case of unmixing are similar. Both MMF and abundance follow the rule of sum to one. In this paper, a classification method has been implemented using CNN to achieve better classification accuracy. Thereafter number of pure pixels extracted based on the various MMF thresholds. © 2019 IEEE.","Abundance; Convolutional Neural Network (CNN); Maximum Membership Fraction (MMF)",
"Multilabel annotation of multispectral remote sensing images using error-correcting output codes and most ambiguous examples","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070547356&doi=10.1109%2fJSTARS.2019.2916838&partnerID=40&md5=d8908bb21b353729126cde7ff204e5d3","This paper presents a novel framework for multilabel classification of multispectral remote sensing images using error-correcting output codes. Starting with a set of primary class labels, the proposed framework consists in transforming the multiclass problem into multiple binary learning subtasks. The distributed output representations of these binary learners are then transformed into primary class labels. In order to train robust binary classifiers on a reduced annotated dataset, the learning process is iterative and involves determining most ambiguous examples, which are included in the training set at each iteration. As part of the semantic image recognition process, two categories of high-level image representations are proposed for the feature extraction part. First, deep convolutional neural networks are used to form high-level representations of the images. Second, we test our classification framework with a bag-of-visual words model based on the scale invariant feature transform, used in combination with color descriptors. In the first case, we propose the usage of pretrained state-of-the-art deep learning models that cancel the need to estimate model parameters of complex architectures, whereas, in the second case, a dictionary of visual words must be determined from the training set. Experiments are conducted on GeoEye-1 and Sentinel-2 images and the results show the effectiveness of the proposed approach toward a multilabel classification, when compared to other methods. © 2008-2012 IEEE.","Error-correcting output codes (ECOCs); multilabel image classification; pretrained convolutional neural networks; support vector machines (SVMs)","Classification (of information); Codes (symbols); Convolution; Deep neural networks; Errors; Image recognition; Iterative methods; Neural networks; Remote sensing; Semantics; Support vector machines; Classification framework; Convolutional neural network; Error correcting output code; Multi-label; Multi-label classifications; Multispectral remote sensing image; Scale invariant feature transforms; Support vector machine (SVMs); Image annotation; artificial neural network; error correction; GeoEye; image classification; remote sensing; Sentinel; support vector machine"
"Domain Adaptation with Discriminative Distribution and Manifold Embedding for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066851522&doi=10.1109%2fLGRS.2018.2889967&partnerID=40&md5=e82549a522a865c749c5fe1cbd7b08fc","Hyperspectral remote sensing image classification has drawn a great attention in recent years due to the development of remote sensing technology. To build a high confident classifier, the large number of labeled data is very important, e.g., the success of deep learning technique. Indeed, the acquisition of labeled data is usually very expensive, especially for the remote sensing images, which usually needs to survey outside. To address this problem, in this letter, we propose a domain adaptation method by learning the manifold embedding and matching the discriminative distribution in source domain with neural networks for hyperspectral image classification. Specifically, we use the discriminative information of source image to train the classifier for the source and target images. To make the classifier can work well on both domains, we minimize the distribution shift between the two domains in an embedding space with prior class distribution in the source domain. Meanwhile, to avoid the distortion mapping of the target domain in the embedding space, we try to keep the manifold relation of the samples in the embedding space. Then, we learn the embedding on source domain and target domain by minimizing the three criteria simultaneously based on a neural network. The experimental results on two hyperspectral remote sensing images have shown that our proposed method can outperform several baseline methods. © 2004-2012 IEEE.","Domain adaptation; hyperspectral image classification; manifold embedding; maximum mean discrepancy (MMD); neural network; remote sensing","Classification (of information); Deep learning; Embeddings; Hyperspectral imaging; Neural networks; Remote sensing; Space optics; Spectroscopy; Distortion mappings; Domain adaptation; Hyperspectral Remote Sensing Image; Learning techniques; manifold embedding; maximum mean discrepancy (MMD); Remote sensing images; Remote sensing technology; Image classification; adaptive management; artificial neural network; computer vision; image analysis; image classification; multispectral image; remote sensing; technological development"
"Semantic segmentation of satellite images using a U-shaped fully connected network with dense residual blocks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071455788&doi=10.1109%2fICMEW.2019.00037&partnerID=40&md5=fc91ea1ed06db54a3dfd0a18001278b5","Semantic segmentation is the task of clustering pixels into an object class. In the field of remote sensing semantic segmentation has wide applications ranging from scene cover classification to change detection for scene understanding. With the success of deep learning algorithms for classification tasks, there has been much work to apply convolutional neural networks in remote sensing with much success. However, feature extraction of high resolution remote sensing imagery poses a challenge when applying such networks. In particular, there is a need to extract high level features while maintaining an objects resolution in the networks feature space. This work proposes an efficient deep fully convolution architecture that obtains high level features without loss of spatial resolution by replacing the standard convolutional layers in U-Net with dense residual blocks. By stacking identity blocks, we allow the input to flow through the network at every proceeding layer. Our network is termed DRU-Net, and is shown to outperform standard U-Net. © 2019 IEEE.","Dense skip connection; Satellite image; Semantic segmentation; U-Net","Convolution; Deep learning; Image segmentation; Neural networks; Semantic Web; Semantics; Space optics; Classification tasks; Convolutional neural network; Dense skip connection; Fully connected networks; High resolution remote sensing imagery; Satellite images; Scene understanding; Semantic segmentation; Remote sensing"
"Rotation-Based Deep Forest for Hyperspectral Imagery Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068238864&doi=10.1109%2fLGRS.2019.2892117&partnerID=40&md5=3fcd8b9c131e6a2fd093ded1cb2383f8","In recent years, deep learning methods have been widely used for the classification of hyperspectral images (HSIs). However, the training of deep models is very time-consuming. In addition, the rare labeled samples of remote sensing images also limit the classification performance of deep models. In this letter, a simple deep learning model, a rotation-based deep forest (RBDF), is proposed for the classification of HSIs. Specifically, the output probability of each layer is used as the supplement feature of the next layer. The rotation forest is used to increase the discriminative power of spectral features and neighboring pixels are used to introduce spatial information. The RBDF consumes much less training time than traditional deep models. Experimental results based on three HSIs demonstrate that the proposed method achieves the state-of-the-art classification performance. In addition, the RBDF obtains satisfied classification results with very few training samples. © 2004-2012 IEEE.","Classification; deep forest; hyperspectral imagery; rotation forest (ROF)","Classification (of information); Deep learning; Forestry; Remote sensing; Rotation; Spectroscopy; Classification performance; Classification results; deep forest; Hyper-spectral imageries; Hyperspectral imagery classifications; Remote sensing images; Rotation forests; Spatial informations; Image classification; image classification; performance assessment; pixel; remote sensing; satellite imagery"
"An Automatic Extraction Architecture of Urban Green Space Based on DeepLabv3plus Semantic Segmentation Model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084318274&doi=10.1109%2fICIVC47709.2019.8981007&partnerID=40&md5=f386b1fd5ff4ab665661dc6db59e7458","Urban green space plays an important role in maintaining the balance of ecological environment and the sustainable development of the city. Extracting urban green space from remote sensing imagery can provide fast and accurate reference for urban planning and management. Deep learning semantic segmentation is a new exploration for image processing including remote sensing (RS) images these years. This paper describes a multilevel architecture which targets urban green space extraction from GF-2 imagery. The pillar of the architecture is semantic segmentation model (DeepLabv3plus) that is used for satellite imagery classification. In this paper, we take 19687 256 pixels × 256 pixels slices from Gaofen-2(GF-2) satellite images of three different cities and ground truth as training samples, and data of another city in Hebei province is taken for model verification. Then, five comparative methods are carried out for monitoring urban green space distribution, including ML (Maximum likelihood), SVM (Support Vector Machine), Object-oriented method, FCN and U-Net. The accuracy indexes of each method are obtained by calculating the difference between the extraction result and the ground truth. The results shows that the architecture with DeepLabv3plus outperforms the other five methods allowing us to better extract urban green space, in particular eliminating interference from farmland pixels. The architecture allows us to reach the target extraction accuracy of 89.46%. This paper is also an exploration of applying artificial intelligence technology to remote sensing image processing. © 2019 IEEE.","deep learning; DeepLab; remote sensing; semantic segmentation; urban green space","Architecture; Deep learning; Extraction; Image segmentation; Maximum likelihood; Pixels; Remote sensing; Satellite imagery; Semantics; Support vector machines; Sustainable development; Urban planning; Vector spaces; Artificial intelligence technologies; Ecological environments; Object oriented method; Remote sensing image processing; Remote sensing imagery; Remote sensing images; Semantic segmentation; SVM(support vector machine); Space optics"
"Comparison of Deep Learning Model Performance between Meta-Dataset Training Versus Deep Neural Ensembles","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077716649&doi=10.1109%2fIGARSS.2019.8898596&partnerID=40&md5=1221edfb7f9ef61340de90fcd0057fc9","Recently, many high-resolution remote sensing imagery (HR-RSI) datasets have been released that have diverse characteristics, such as high inter-class and low intra-class variation. Additionally, a benchmark meta-dataset (MDS) was created by agglomerating object classes from multiple HR-RSI datasets. Previous work has shown that deep convolutional neural networks (DCNN) trained on the MDS perform on par with DCNN trained on constituent benchmark datasets in cross-validation experiments. Here we train a model ensemble on four datasets and compare it with a single robust DCNN trained on the MDS. The goal is to better understand under what conditions an ensemble of models, each trained with distinct datasets, is advantaged or disadvantaged compared to a single model trained with the agglomerated MDS for classification performance. © 2019 IEEE.","Deep Learning; Ensemble; Fusion",
"Deep learning based remote sensing using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073468359&doi=10.35940%2fijeat.E1026.0785S319&partnerID=40&md5=a11ab71c1ee0b6a767d4c6c45d802063","We describe our achievements in collecting alternating convergence points with a thickness of 7 µm and focal lengths of 200 and 350 mm, combined with shadow correction, deconvolution and significant neural frame training for transmission close to photography. Visual quality image. Although images taken using diffractive optics have been shown in previous papers, important neural structures have been used in the recovery phase. We use the imagery component of our imaging structure to activate the rise of ultralight cameras with remote identification for Nano and pico satellites, as well as small drones and solar-guided aircraft for aeronautical remote identification systems.. We extend the customizability of the liquid center focus on non-circular surfaces, forcing movement at the liquid convergence point of the surface. We study their trends and whether we can use them in optical structures. © BEIESP.","Deconvolution; Neural framework; Picosatellites; Ultra-lightweight remote",
"Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077709430&doi=10.1109%2fIGARSS.2019.8897897&partnerID=40&md5=c6ef0ce8072d069c684914636afe91dc","Hyperspectral imaging which is also known as imaging spectroscopy, detects radiation of earth surface features in narrow contiguous spectral regions of the electromagnetic spectrum. The Airborne Visible Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) is an airborne hyperspectral sensor of NASA's Jet Propulsion Laboratory (JPL) with 425 spectral bands ranging from 380 nm to 2510 nm with a bandwidth of 5 nm and spatial resolution of 4-6 m. This study aims at pixel-wise identification and discrimination of crop types using AVIRIS-NG hyperspectral images, with novel Parallel Convolutional Neural Networks architecture. To tackle the challenge posed by a large number of correlated bands, we compare two band selection techniques using Principal Component Analysis (PCA) and back traversal of pre-trained Artificial Neural Network (ANN). We also propose an automated technique for augmentation of training dataset with a large number of pixels from unlabelled parts of an image, based on Euclidian distance. Experiments show that bands selected by ANN achieve higher accuracy compare to PCA selected bands with automated data augmentation. © 2019 IEEE.","convolutional neural networks; data augmentation; deep learning; Hyperspectral imaging; Remote sensing",
"Deep Learning for Agricultural Land Detection in Insular Areas","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075883615&doi=10.1109%2fIISA.2019.8900670&partnerID=40&md5=6b3b0a653c7576f24012ecc47461494a","Nowadays, governmental programs like ESA's Copernicus provide freely available data that can be easily utilized for earth observation. In the present work, the problem of detecting agricultural and non-agricultural land cover is addressed. The methodology is based on classification with convolutional neural networks (CNNs) and transfer learning using AlexNet. The study area is located at the Ionian Islands, which include several land cover classes according to Copernicus CORINE Land Cover 2018 (CLC 2018). Furthermore, the dataset consists of natural color images acquired by Sentinel-2A multi-spectral instrument. Experimentation proves that extra addition of training data from foreign grounds, unfamiliar to the Greek data, serves much as a confusing agent regarding network performance. © 2019 IEEE.","Agricultural land; CORINE database; Deep Convolutional Neural Networks; Deep learning; Ionian Sea; Remote Sensing; Satellite Image Classification; Transfer learning","Agriculture; Classification (of information); Convolution; Deep learning; Neural networks; Remote sensing; Agricultural land; Convolutional neural network; Ionian sea; Satellite image classification; Transfer learning; Deep neural networks"
"Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070509108&doi=10.1109%2fJSTARS.2019.2918242&partnerID=40&md5=00f37e6ba334676d0f79e3cde09b8b7b","In this paper, we present a patch-based land use and land cover classification approach using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible, and are provided in the earth observation program Copernicus. We present a novel dataset, based on these images that covers 13 spectral bands and is comprised of ten classes with a total of 27 000 labeled and geo-referenced images. Benchmarks are provided for this novel dataset with its spectral bands using state-of-the-art deep convolutional neural networks. An overall classification accuracy of 98.57% was achieved with the proposed novel dataset. The resulting classification system opens a gate toward a number of earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes, and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat. © 2008-2012 IEEE.","Dataset; deep convolutional neural network; deep learning; earth observation; land cover classification; land use classification; machine learning; remote sensing; satellite image classification; satellite images","Convolution; Deep learning; Deep neural networks; Land use; Learning systems; Neural networks; Observatories; Remote sensing; Satellites; Convolutional neural network; Dataset; Earth observations; Land cover classification; Landuse classifications; Satellite image classification; Satellite images; Classification (of information); accuracy assessment; artificial neural network; data set; image classification; land cover; satellite data; satellite imagery; Sentinel"
"Drbox Family: A Group of Object Detection Techniques for Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077695587&doi=10.1109%2fIGARSS.2019.8898821&partnerID=40&md5=84128cc26765882c9137f75964e89458","Objects in remote sensing images are difficult to detect due to arbitrarily rotated angles and the wide variance of scales. As bounding box plays an important role in object detection, we proposes a new bounding box type named rotated bounding box (rBox). With the application of rBox, we have proposed a series of detection techniques (DrBox, DrBoxLight, DrBoxSemi, DrBoxPro) to effectively handle the situation where the orientation angles of the objects are arbitrary. This article is a brief overview of these techniques. The original DrBox detector applies VGG-net as its main network framework, with image pyramid input to address multi-scale problem. DrBoxLight is a mini version of DrBox, which applies MobileNet and knowledge distillation to be deployed on embedded devices. DrBoxSemi is a semi-supervised version of DrBox, so annotation of all training samples is no longer necessary. DrBoxPro is the most important update for DrBox with professional designing of abundant prior-rBoxes on feature pyramid networks. In our experiments, we demonstrated how rBox helps to improve the performance of object detection compared with traditional bounding boxes. Besides, we evaluated the performance of our DrBox family on a series of object detection tasks. © 2019 IEEE.","deep learning; Object detection; remote sensing images; rotated object",
"The Unified Object Detection Framework with Arbitrary Angle","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071943119&doi=10.1109%2fBigDIA.2019.8802710&partnerID=40&md5=de0b60856d32f8893bcab717369f2772","Traditional horizontal bounding boxes are hard to locate targets with various directions. In the complex background, this method also cannot distinguish object from background very well and it is difficult to separate dense objects. Therefore, the traditional deep learning model is far from meeting the need for object detection in arbitrary orientation scenarios. In order to solve these problems, the unified object detection framework with arbitrary orientation is proposed in this paper. The angle is used as a variable for regression, which can accurately detect the rotating object, its position and direction information in complex background. The rotation region of interest (RRoI) align is proposed to replace region of interest(RoI) pooling, which can improve the precision of the detection model. Besides, a skew intersection over union (IoU) calculation method is adopted to solve the problem that densely placed objects are hard to detect and improve the recall of the detection model, which can reduce redundant detection regions. Experiments on remote sensing data set show that our method has better robustness and performance than other algorithms in detecting small targets. Compared with other algorithms, our algorithm is not only applicable to ship detection, but also to remote sensing images, document detection and other arbitrary-oriented object detection scenarios. © 2019 IEEE.","arbitrary orientation; object detection; ship detection","Big data; Deep learning; Image segmentation; Object recognition; Remote sensing; Ships; Arbitrary orientation; Complex background; Detection framework; Region of interest; Remote sensing data; Remote sensing images; Rotating objects; Ship detection; Object detection"
"A Training-Free, One-Shot Detection Framework for Geospatial Objects in Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077326711&doi=10.1109%2fIGARSS.2019.8898679&partnerID=40&md5=aa112c61ae5679f17b8db47c40df9eb0","Deep learning based object detection has achieved great success. However, these supervised learning methods are data-hungry and time-consuming. This restriction makes them unsuitable for limited data and urgent tasks, especially in the applications of remote sensing. Inspired by the ability of humans to quickly learn new visual concepts from very few examples, we propose a training-free, one-shot geospatial object detection framework for remote sensing images. It consists of (1) a feature extractor with remote sensing domain knowledge, (2) a multi-level feature fusion method, (3) a novel similarity metric method, and (4) a 2-stage object detection pipeline. Experiments on sewage treatment plant and airport detections show that proposed method has achieved a certain effect. Our method can serve as a baseline for training-free, one-shot geospatial object detection. © 2019 IEEE.","object detection; one-shot; Training-free",
"Thin cloud removal with residual symmetrical concatenation network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065761555&doi=10.1016%2fj.isprsjprs.2019.05.003&partnerID=40&md5=ce9cff9d448e0917239154d99779967f","Thin cloud removal is important for enhancing the utilization of optical remote sensing imagery. Different from thick cloud removal, the pixels contaminated by thin clouds still preserve some surface information. Therefore, thin cloud removal methods usually focus on suppressing the cloud influence instead of replacing the cloudy pixels. In this paper, we proposed a deep residual symmetrical concatenation network (RSC-Net)to make end-to-end thin cloud removal. The RSC-Net is based on the encoding-decoding framework consisting of multiple residual convolutional layers and residual deconvolutional layers. The feature maps of each convolutional layer are copied and concatenated to the symmetrical deconvolutional layer. We used real cloud-contaminated and cloud-free Landsat-8 data very close in time for both training and testing. The RSC-Net is trained to take cloudy images as input and directly produce corresponding cloud-free images as output with all the bands together except the cirrus band and the panchromatic band. Compared with other traditional and state-of-the-art deep learning based methods, the experimental results show that our method has significant advantages in removing thin cloud contaminations in different bands. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural network; Residual blocks; Symmetrical concatenation; Thin cloud removal","Convolution; Deep learning; Neural networks; Pixels; Remote sensing; Cloud removal; Convolutional neural network; Learning-based methods; Optical remote-sensing imagery; Residual blocks; Surface information; Symmetrical concatenation; Training and testing; Image enhancement; artificial neural network; cloud; deconvolution; numerical method; panchromatic image; pixel; remote sensing; satellite data; satellite imagery"
"A Weakly-Supervised Deep Network for DSM-Aided Vehicle Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077687568&doi=10.1109%2fIGARSS.2019.8897989&partnerID=40&md5=3aa844b84cdcc84db391ca6082c26eff","With the breakthrough of the spatial resolution of optical remote sensing images at the sub-meter level and the explosive development of deep learning, geospatial object detection has achieved a growing interest in remote sensing community. However, labeling large training datasets in object level is still an expensive and tedious procedure. This might lead to the poor model generalization and degraded network learning ability. To this end, a weakly-supervised deep network (WSDN) is developed for geospatial object detection by applying a digital surface model (DSM)-aided auto-labeling and a pre-trained network learned from the task-independent dataset. Experimental results conducted on the stereo aerial imagery of a large camping site are performed to demonstrate that the proposed WSDN yields better detection results, with 62.78% precision and 55.13% recall. © 2019 IEEE.","Deep learning; digital surface model; geospatial object detection; optical remote sensing imagery; vehicle; weakly-supervised",
"Pixelwise Remote Sensing Image Classification Based on Recurrence Plot Deep Features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077706165&doi=10.1109%2fIGARSS.2019.8898128&partnerID=40&md5=55af85f33ccf6b8a3fc57f0ffb925c68","Pixelwise remote sensing image classification has benefited from temporal contextual information encoded in time series. In this paper, we investigate the use of data-driven features extracted from time series representations based on recurrence plots, with the goal of improving the effectiveness of classification systems. Performed experiments considered the classification of eucalyptus plantations based on time series profiles. Achieved results demonstrate that the combination of recurrence plot representations with deep-learning features are a promising research venue for addressing pixelwise classification problems. © 2019 IEEE.","deep features; eucalyptus; Modis; NDVI index; pixelwise classification; recurrence plot; time series",
"Land Cover Satellite Image Classification Using NDVI and SimpleCNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078184065&doi=10.1109%2fICCCNT45670.2019.8944840&partnerID=40&md5=e398a0612fe1b1aa43aa5b05501057ba","Image classification and prediction is a task which is embedded with quite a lot of challenges. Introduction of deep learning gave a rapid rise in this area of research. The efficient and the simplest deep learning algorithm that has helped researchers to make immense contributions in the field of image classification is Convolutional Neural Network (CNN). One of the important applications of image classification is in remote sensing, where it is used for land cover classification. In this paper we developed a SimpleCNN architecture for the classification of multi-spectral images from SAT-4 and SAT-6 airborne datasets. Two sets of experiments are conducted using the model by feeding it with different features. First level of experiment is done by providing the model with Near-Infrared (NIR) band information as it can sense vegetation health. The domain knowledge of Normalized Difference Vegetation Index (NDVI) motivated us to utilize Red and NIR spectral bands together in the second level of experimentation for the classification. It is observed from the experiment that the two band information gave better results for land cover classification. © 2019 IEEE.","Convolutional Neural Network; Near- Infrared; Normalized Difference Vegetation Index; Trainable Parameters","Classification (of information); Convolution; Deep learning; Infrared devices; Learning algorithms; Neural networks; Remote sensing; Spectroscopy; Vegetation; Convolutional neural network; Land cover classification; Multispectral images; Near Infrared; Near-infrared bands; Normalized difference vegetation index; Satellite image classification; Trainable Parameters; Image classification"
"Multisource Region Attention Network for Fine-Grained Object Recognition in Remote Sensing Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068265976&doi=10.1109%2fTGRS.2019.2894425&partnerID=40&md5=c6e18700c9bbeb9d4cbfe6e9974dcf6b","Fine-grained object recognition concerns the identification of the type of an object among a large number of closely related subcategories. Multisource data analysis that aims to leverage the complementary spectral, spatial, and structural information embedded in different sources is a promising direction toward solving the fine-grained recognition problem that involves low between-class variance, small training set sizes for rare classes, and class imbalance. However, the common assumption of coregistered sources may not hold at the pixel level for small objects of interest. We present a novel methodology that aims to simultaneously learn the alignment of multisource data and the classification model in a unified framework. The proposed method involves a multisource region attention network that computes per-source feature representations, assigns attention scores to candidate regions sampled around the expected object locations by using these representations, and classifies the objects by using an attention-driven multisource representation that combines the feature representations and the attention scores from all sources. All components of the model are realized using deep neural networks and are learned in an end-to-end fashion. Experiments using RGB, multispectral, and LiDAR elevation data for classification of street trees showed that our approach achieved 64.2% and 47.3% accuracies for the 18-class and 40-class settings, respectively, which correspond to 13% and 14.3% improvement relative to the commonly used feature concatenation approach from multiple sources. © 1980-2012 IEEE.","Deep learning; fine-grained classification; image alignment; multisource classification; object recognition","Deep learning; Deep neural networks; Image classification; Object recognition; Remote sensing; Trees (mathematics); Between-class variances; Classification models; Feature representation; Fine grained; Image alignment; Multisources; Remote sensing imagery; Structural information; Classification (of information); artificial neural network; data interpretation; image classification; lidar; machine learning; pattern recognition; pixel; remote sensing"
"Change Detection from Unlabeled Remote Sensing Images Using SIAMESE ANN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077724965&doi=10.1109%2fIGARSS.2019.8898672&partnerID=40&md5=54c117909aca2877d50eec61bd401134","In this article, we propose a new semi-supervised method to detect changes occurring in a geographical area after a major event such as war, an earthquake or flood. The detection is made by processing a pair of bi-temporal remotely sensed images of the area under consideration. The proposed method adopts a patch-based approach, where successive pairs of patches from the input images are compared using a deep machine learning method trained with augmented data. Our main contribution consists of proposing an approach for generating a training dataset from unlabeled pair of input images. The genuine training patch-pairs are directly generated from the transformed maps of the image taken before the event, while the impostor patch-pairs are generated by pairing the image taken before the event with any images, from the Internet, with textures that resemble the change shown in the image taken after the event. Several experiments were conducted on pairs of images related to five major events. The obtained subjective results demonstrate the effectiveness of the proposed method. © 2019 IEEE.","change detection; deep learning; remote sensing; Siamese neural networks; unlabeled data",
"Attention Networks for Band Weighting and Selection in Hyperspectral Remote Sensing Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077680802&doi=10.1109%2fIGARSS.2019.8898004&partnerID=40&md5=b120988b8e6c3f189bd66af8e7a25fd4","Hyperspectral imaging is widely used in remote sensing because of its capability to capture the detailed spectral reflection of the ground object. The acquired rich band information brings significant benefits to better discriminate the target pixels. However, this imaging method also introduces redundant and noisy bands which may lower the classification accuracy. In addition, the contribution of different bands towards the final classification task are not necessarily the same. Therefore, band weighting and band selection are often adopted to model the relationship among the bands and remove the irrelevant ones. Attention mechanism is a method in neural networks to guide the algorithm to focus on the important information. In this paper, we propose an attention based deep learning framework to achieve band weighting and selection. The experimental results on two hyperspectral image datasets show the effectiveness of the proposed framework. © 2019 IEEE.","attention; band selection; band weighting; convolutional neural networks; Hyperspectral image",
"Multi-Scale Enhanced Deep Network for Road Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077711899&doi=10.1109%2fIGARSS.2019.8899115&partnerID=40&md5=311255f5a251a833e6505481c8780cf2","Road detection is a hot research topic in the very high resolution (VHR) remote sensing field and has been applied in various practical applications. Many deep-learning based methods have been used to detect roads and achieved good performance. In this paper, a multi-scale enhanced road detection framework (DenseUNet) is proposed which based on the densely connected convolutional networks (DenseNet) and U-Net. The U-Net has strong capabilities of preserving spatial details due to its skip connections, and the DenseNet can better optimize the deep network. Meanwhile, atrous spatial pyramid pooling (ASPP) is employed to effectively capture multi-scale features for road detection. Finally, a public road dataset was used to verify the proposed approach, compared with other state-of-the-art methods. The proposed method achieve the best performance, which illustrates its superiority. © 2019 IEEE.","atrous spatial pyramid pooling; deep learning; DenseUNet; remote sensing; road detection",
"Different Modality Based Remote Sensing Data Fusion Approach for Efficient Classification of Agriculture and Urban Subclasses","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077717154&doi=10.1109%2fIGARSS.2019.8899201&partnerID=40&md5=9edda5c7ff3b4f0713c89792b73379ff","Subclasses classification is one of the major challenges in remote sensing (RS) scene classification. The area under observation, in order to classify agriculture and urban subclasses, requires efficient classification algorithms. Among such algorithms, deep learning algorithm based on Convolutional Neural Network (CNN) architecture is one such promising candidate to obtain the classified map. In this work, performance of a CNN network has been demonstrated on the data obtained from National Ecological Observatory Network (NEON) field site Domain 17 by considering different modality data and its subsequent fusion using the proposed model of CNN as applied on (i) the Hyperspectral, (ii) the Light Detection and Ranging (LiDAR) and then (iii) fused data respectively. Both the Hyperspectral and the LiDAR data have been fused at pixel level. Using the proposed methodology, a classified map is obtained with an overall accuracy of 96 percent for fused data. © 2019 IEEE.","Convolutional Neural Network (CNN); Data Fusion; Subclass classification",
"Use of Remote Sensing Radar Images for Offshore Oil Slick Detection in Oil and Gas Domain: Manual and Automatic Interpretation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077722854&doi=10.1109%2fIGARSS.2019.8898962&partnerID=40&md5=28b16f1be9d72329c5b1ca28a9111556","Natural and anthropogenic hydrocarbon oil slicks can be present anywhere on the surface of the world's oceans. Oil companies are interested in detecting natural hydrocarbon slicks (Seeps) for Exploration as well as anthropogenic hydrocarbons slicks (Spills) for Environmental purposes. To meet these needs, hydrocarbon detection studies in the offshore domain are essentially carried out using Synthetic aperture radar (SAR) data on which oil slicks appear as 'black regions'. Today, segmentation of oil slicks is done manually by photo interpreters who manually or semi-automatically draw the contours of the potential oil slicks.Nowadays, multiplicity of SAR sensors considerably increases the amount of available SAR data, which is an advantage as the availability of images is no longer a problem, but also an issue as the photo interpreter gets more and more data to analyze in a shorter period of time. The development of Deep Learning methods is a very promising approach in the context of image processing and pattern recognition to cope with this increasing flow of data. Several solutions are currently being studied at Total and this paper presents the first encouraging results obtained. © 2019 IEEE.","Automatic detection; Deep Learning; Look alike; Manual detection; Oil Seeps; Oil Slicks; Oil Spill; SAR Images",
"Deep learning convolutional neural network for the retrieval of land surface temperature from AMSR2 data in China","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069326799&doi=10.3390%2fs19132987&partnerID=40&md5=7601f30f50efb135579026142d276090","A convolutional neural network (CNN) algorithm was developed to retrieve the land surface temperature (LST) from Advanced Microwave Scanning Radiometer 2 (AMSR2) data in China. Reference data were selected using the Moderate Resolution Imaging Spectroradiometer (MODIS) LST product to overcome the problem related to the need for synchronous ground observation data. The AMSR2 brightness temperature (TB) data and MODIS surface temperature data were randomly divided into training and test datasets, and a CNN was constructed to simulate passive microwave radiation transmission to invert the surface temperature. The twelve V/H channel combinations (7.3, 10.65, 18.7, 23.8, 36.5, 89 GHz) resulted in the most stable and accurate CNN retrieval model. Vertical polarizations performed better than horizontal polarizations; however, because CNNs rely heavily on large amounts of data, the combination of vertical and horizontal polarizations performed better than a single polarization. The retrievals in different regions indicated that the CNN accuracy was highest over large bare land areas. A comparison of the retrieval results with ground measurement data from meteorological stations yielded R2 = 0.987, RMSE = 2.69 K, and an average relative error of 2.57 K, which indicated that the accuracy of the CNN LST retrieval algorithm was high and the retrieval results can be applied to long-term LST sequence analysis in China. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","CNN; LST retrieval; Passive microwave remote sensing; Soil moisture","Atmospheric temperature; Convolution; Deep neural networks; Neural networks; Polarization; Radiometers; Remote sensing; Soil moisture; Surface measurement; Surface properties; Advanced microwave scanning radiometer; Brightness temperatures; Convolutional neural network; Horizontal polarization; LST retrieval; Meteorological station; Moderate resolution imaging spectroradiometer; Passive microwave remote sensing; Land surface temperature"
"Fine-Grained Road Mining from Satellite Images with Bilateral Xception and DeepLab","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073235179&doi=10.1109%2fIJCNN.2019.8852009&partnerID=40&md5=00c4dc2cc1ca54e2c0edd424b5b66d1a","With the recent development of remote sensing and deep learning techniques, automatic and robust road extraction from satellite imaging data has become one of the most popular topics in both fields of Geographic Information System (GIS) and Computer Vision. Despite of the superior performance of Convolutional Neural Networks (DCNNs), a common problem of choosing between the classification and segmentation DCNNs still remains. By comparing two state-of-the-art baseline classification/segmentation DCNNs in several industrial application scenarios, we illustrate that their relative performance may vary, leading to different choices. Based on that observation, we propose a general fusion strategy that conveniently combines the strength of both classification and segmentation DCNNs using an end-to-end network architecture; this paradigm only requires pre-train segmentation/classification DCNNs once, which then can be reused in different road feature mining tasks. The task-specific experiments show that our fusion strategy guarantees superior results in all tested industrial scenarios. © 2019 IEEE.","classification; convolutional neural network; fusion strategy; road extraction; satellite images; segmentation","Classification (of information); Convolution; Deep learning; Extraction; Feature extraction; Network architecture; Neural networks; Remote sensing; Roads and streets; Satellite imagery; Convolutional neural network; Fusion strategies; Industrial scenarios; Learning techniques; Relative performance; Road extraction; Satellite images; Satellite imaging; Image segmentation"
"How can Big Data and machine learning benefit environment and water management: A survey of methods, applications, and future directions","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072021256&doi=10.1088%2f1748-9326%2fab1b7d&partnerID=40&md5=d7608a3132ff2256c500a9d17792ca1e","Big Data and machine learning (ML) technologies have the potential to impact many facets of environment and water management (EWM). Big Data are information assets characterized by high volume, velocity, variety, and veracity. Fast advances in high-resolution remote sensing techniques, smart information and communication technologies, and social media have contributed to the proliferation of Big Data in many EWM fields, such as weather forecasting, disaster management, smart water and energy management systems, and remote sensing. Big Data brings about new opportunities for data-driven discovery in EWM, but it also requires new forms of information processing, storage, retrieval, as well as analytics. ML, a subdomain of artificial intelligence (AI), refers broadly to computer algorithms that can automatically learn from data. ML may help unlock the power of Big Data if properly integrated with data analytics. Recent breakthroughs in AI and computing infrastructure have led to the fast development of powerful deep learning (DL) algorithms that can extract hierarchical features from data, with better predictive performance and less human intervention. Collectively Big Data and ML techniques have shown great potential for data-driven decision making, scientific discovery, and process optimization. These technological advances may greatly benefit EWM, especially because (1) many EWM applications (e.g. early flood warning) require the capability to extract useful information from a large amount of data in autonomous manner and in real time, (2) EWM researches have become highly multidisciplinary, and handling the ever increasing data volume/types using the traditional workflow is simply not an option, and last but not least, (3) the current theoretical knowledge about many EWM processes is still incomplete, but which may now be complemented through data-driven discovery. A large number of applications on Big Data and ML have already appeared in the EWM literature in recent years. The purposes of this survey are to (1) examine the potential and benefits of data-driven research in EWM, (2) give a synopsis of key concepts and approaches in Big Data and ML, (3) provide a systematic review of current applications, and finally (4) discuss major issues and challenges, and recommend future research directions. EWM includes a broad range of research topics. Instead of attempting to survey each individual area, this review focuses on areas of nexus in EWM, with an emphasis on elucidating the potential benefits of increased data availability and predictive analytics to improving the EWM research. © 2019 The Author(s). Published by IOP Publishing Ltd.","artificial intelligence; big Data; deep learning; environmental management; Machine learning; predictive analytics; remote sensing","Artificial intelligence; Big data; Data Analytics; Data mining; Decision making; Deep learning; Digital storage; Disaster prevention; Disasters; Discovery wells; Energy management systems; Environmental management; Learning systems; Machine learning; Optimization; Predictive analytics; Remote sensing; Surveys; Water management; Weather forecasting; Computing infrastructures; Future research directions; Hierarchical features; High resolution remote sensing; Information and Communication Technologies; Issues and challenges; Predictive performance; Technological advances; Information management"
"LW-ODF: A Light-Weight Object Detection Framework for Optical Remote Sensing Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077693868&doi=10.1109%2fIGARSS.2019.8898673&partnerID=40&md5=5a8948008cfa773e59dc23520428b93f","In this paper, we propose to extract the multi-scaled and rotation-insensitive deep features to address the issues of object multi-solutions and rotations in geospatial object detection. To this end, we develop a novel object detection framework where a rotation-insensitive convolution neural network is applied for extracting multi-scaled and direction-insensitive feature representation and then the learned features can be fed into the ensemble classifier learning with fast feature pyramid. Such a non-end-to-end learning strategy intuitively reduces the computational cost without the additional performance loss, yielding an effective and efficient light-weight object detection framework. Experimental results conducted on the NWPU VHR-10 dataset demonstrate that the proposed framework outperforms several state-of-the-art baselines. © 2019 IEEE.","Deep learning; direction-insensitive; geospatial object detection; light-weight; multi-scaled; optical remote sensing imagery",
"Large-Scale Oil Palm Tree Detection from High-Resolution Remote Sensing Images Using Faster-RCNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077682323&doi=10.1109%2fIGARSS.2019.8898360&partnerID=40&md5=0b0fcb6870b3bd6c9e4b5321bf93136a","Oil palm is of great importance in agricultural productivity for many tropic developing countries and accordingly investigating as well as counting oil palms is a meaningful and valuable research. In this paper, we firstly apply Faster-RCNN, one of the most popular object detection algorithms, to detect tree crowns from satellite images. Although Faster-RCNN has an excellent performance in well-known datasets of general object detection, it does not have obvious advantages in oil palm tree detection in this study compared with other classical machine learning based methods. We argue two reasons accounting for the drawbacks of Faster-RCNN: (1) the size of each oil palm tree is too small (only 17 × 17 pixels on average) in 0.6m-resolution QuickBird satellite images; (2) there are lots of other similar trees around the oil palm trees that make it difficult to detect them correctly. In order to reach a satisfying accuracy, we tailored the Region Proposal Network (RPN) and proposed a simple but practical post-processing strategy based on empirical planting rules, filtering out the wrongly detected trees (False Positives) effectively. Eventually we achieved a higher average F1-score of 94.99% (using IOU based evaluation matrices) in our six study regions compared wtih state-of-the-art oil palm detection methods. In addition, we proposed a workflow of large-scale oil palm tree detection using high-resolution remotely sensed images based deep learning methods. © 2019 IEEE.","Faster-RCNN; high-resolution remote sensing images; large-scale object detection; Oil palm trees",
"SAR object classification using the DAE with a modified triplet restriction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067655000&doi=10.1049%2fiet-rsn.2018.5413&partnerID=40&md5=f72077a80d093ba3dcfdf04eb564dbc7","Although deep learning methods have made great progress in synthetic aperture radar (SAR)-based remote sensing, lack of training data has often been the major obstacle while they are adopted for SAR automatic target recognition. In this study, a new deep network in the form of a restricted three-branch denoising auto-encoder (DAE) is proposed to take the full advantage of limited training samples. In this model, a modified triplet restriction that combines the semi-hard triplet loss with the intra-class distance penalty is devised to learn discriminative features with a small intra-class divergence and a large inter-class divergence. Besides, the reconstruction distortion is measured between the model outputs and the images filtered by the improved Lee Sigma filter rather than the original inputs to suppress clutter in the background. Furthermore, a batch-based triplet loss, which calculates the modified triplet loss in a batch-based manner, is proposed to tackle the difficulties in implementation and reduce its computation complexity. The simplified version of the three-branch Triplet-DAE is subsequently devised as a one-branch DAE restricted by the batch-based triplet loss. Experimental results with the MSTAR data demonstrate the effectiveness of the proposed method on real SAR images. © The Institution of Engineering and Technology 2019.",,"Automatic target recognition; Deep learning; Image enhancement; Radar imaging; Remote sensing; Synthetic aperture radar; Auto encoders; Computation complexity; Discriminative features; Learning methods; Model outputs; Object classification; Reconstruction distortion; Training sample; Radar target recognition"
"In-Season Prediction of Crop Types in the us Great Plains Using Sequence Based Stochastic Models and Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077697201&doi=10.1109%2fIGARSS.2019.8898888&partnerID=40&md5=e1ef052744028b5d537f842dc93223f2","In this study, a convolutional neural network (CNN) is developed that predicts crop type in the US great plains using optical and active microwave remote sensing, historical crop type data and economic commodity crop price information. It includes a Markov chain model that predicts the prior probability of crop types from historical crop type sequences for each pixel. This prior and the remotely sensed imagery is fed through a CNN to produce the final crop type. The CNN is trained using imagery and crop type from the previous year. The field-scale accuracy of this method is found to be tilde 85% using imagery from just the first 90 growing days. © 2019 IEEE.","Crop Mapping; Land Cover; Machine Learning",
"Can a Deep Network Understand the Land Cover Across Sensors?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077695547&doi=10.1109%2fIGARSS.2019.8899080&partnerID=40&md5=e9d104c05634bd6ca04a46f6528d080c","Deep learning algorithms are widely used in remote sensing image scene understanding. Generally, a large-scale annotated dataset is essential to train a deep neural network for classification. In practical terms, however, a large amount of unknown remote sensing images obtained from different sensors need to be understood which may vary from resolution, geolocation and imaging conditions compared with annotated datasets. In this paper, an unsupervised domain adaptation framework based on ResNet-18 is presented to transfer the knowledge of an existing annotated land cover dataset to other remote sensing data, decreasing the discrepancy among images across sensors. The results show a significant improvement in scene understanding of new remote sensing images. © 2019 IEEE.","domain adaptation; land use classification; remote sensing images; transfer learning",
"Deep Learning for Semantic Segmentation of UAV Videos","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077705885&doi=10.1109%2fIGARSS.2019.8899786&partnerID=40&md5=a6d06a66beebbebb80163753966fcd29","As one of the key problems in both remote sensing and computer vision, video semantic segmentation has been attracting increasing amounts of attention. Using video segmentation technique for Unmanned Aerial Vehicle (UAV) data processing is also a popular application. Previous methods extended single image segmentation approaches to multiple frames. The temporal dependencies are ignored in these methods. This paper proposes a novel segmentation method to solve this problem. Combining the fully convolutional networks (FCN) and the Convolution Long Short Term Memory (Conv-LSTM) together, we segment the sequence of the video frames instead of segmenting each individual frame separately. FCN serves as the frame-based segmentation method. Conv-LSTM makes use of the temporal information between consecutive frames. Experimental results show the superiority of this method especially in some classes compared to the single image segmentation model using video dataset from UAV. © 2019 IEEE.","Conv-LSTM; FCN; UAV; video semantic segmentation",
"Classification research based on residual network for hyperspectral image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074412775&doi=10.1109%2fSIPROCESS.2019.8868838&partnerID=40&md5=b0bf03cb2f1d640aaa50f3d3e625e815","Classification is one of the main ways to mining the rich information in hyperspectral image (HSIs). At present, the deep learning methods have been applied to image processing and achieved good classification results. But overfitting is still a problem in the image classification by the deep neural networks. Residual network (ResNet) can help to overcome the overfitting phenomenon. It can optimize parameters and extract characteristic information conveniently by adding identity mapping between input and output. In this paper, ResNet is introduced to the HSI classification. In order to reduce the dependence of network on parameter initialization and to improve the generalization ability of the model, batch normalization method is adopted to optimize the network. Then, some virtual samples are generated to reduce the impact of limited number of training samples on the classification accuracy. Two different HSIs have been classified by the proposed method and the experimental results show that this method has promising prospects in the HSI classification. © 2019 IEEE.","Deep learning; Feature extraction; Remote sensing image; ResNet; Virtual sample","Deep learning; Deep neural networks; Feature extraction; Image classification; Remote sensing; Spectroscopy; Classification accuracy; Classification results; Generalization ability; Identity mappings; Normalization methods; Remote sensing images; ResNet; Virtual sample; Classification (of information)"
"Image Registration of Satellite Imagery with Deep Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077677867&doi=10.1109%2fIGARSS.2019.8898220&partnerID=40&md5=27edbb02278536fdcf83a297c9d62c6b","Image registration in multimodal, multitemporal satellite imagery is one of the most important problems in remote sensing and essential for a number of other tasks such as change detection and image fusion. In this paper, inspired by the recent success of deep learning approaches we propose a novel convolutional neural network architecture that couples linear and deformable approaches for accurate alignment of remote sensing imagery. The proposed method is completely unsupervised, ensures smooth displacement fields and provides real time registration on a pair of images. We evaluate the performance of our method using a challenging multitemporal dataset of very high resolution satellite images and compare its performance with a state of the art elastic registration method based on graphical models. Both quantitative and qualitative results prove the high potentials of our method. © 2019 IEEE.","Convolutional Neural Networks (CNN); Deep Learning; Deformable and Linear Registration; Very High Resolution Satellite Images",
"KORE Application: Potatoes Yield Assessment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077673883&doi=10.1109%2fIGARSS.2019.8898996&partnerID=40&md5=6fab62b0e05f347ddab595b7c4adf558","KORE is a cloud-based precision agriculture platform, based on subscriptions, that combines satellites, drones and ground sensor data. This paper focuses on one of its applications: yield assessment in potato fields using UAV data and deep learning. © 2019 IEEE.","classification; CNN; deep learning; image segmentation; machine learning; remote sensing; UAV",
"Where do labels come from?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077701575&doi=10.1109%2fIGARSS.2019.8899033&partnerID=40&md5=5ec59eadb25c5c0d31af92926494d717","The use of machine learning in remote sensing often depends on the use of labels to guide an algorithm as to what matters in the data. This supervised learning can effectively transfer human understanding to algorithms such as those used in Deep Learning to analyze large complex data sets. However, this approach can be brittle and lead to poor performance and confusion in decision making. Here we provide an analysis of labels to reveal their limitations in learning algorithms. We frame the supervised learning problem as a simplification of an interaction between unsupervised and reinforcement learning. From this perspective, we develop a formalism of specific value to remote sensing that uses information-theoretic techniques to determine the forward model that preserves the information content between two supervised learning problems using different data but sharing the same labels. © 2019 IEEE.","Deep Learning; Experimental Design; Information Theory; Labels; Machine Learning; Retrievals",
"Social sensing from street-level imagery: A case study in learning spatio-temporal urban mobility patterns","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065124685&doi=10.1016%2fj.isprsjprs.2019.04.017&partnerID=40&md5=668f8be4ea57bbeabc8bd78205c0487e","Street-level imagery has covered the comprehensive landscape of urban areas. Compared to satellite imagery, this new source of image data has the advantage in fine-grained observations of not only physical environment but also social sensing. Prior studies using street-level imagery focus primarily on urban physical environment auditing. In this study, we demonstrate the potential usage of street-level imagery in uncovering spatio-temporal urban mobility patterns. Our method assumes that the streetscape depicted in street-level imagery reflects urban functions and that urban streets of similar functions exhibit similar temporal mobility patterns. We present how a deep convolutional neural network (DCNN)can be trained to identify high-level scene features from street view images that can explain up to 66.5% of the hourly variation of taxi trips along with the urban road network. The study shows that street-level imagery, as the counterpart of remote sensing imagery, provides an opportunity to infer fine-scale human activity information of an urban region and bridge gaps between the physical space and human space. This approach can therefore facilitate urban environment observation and smart urban planning. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Social sensing; Street-level imagery; Urban mobility; Urban physical environment","Deep learning; Deep neural networks; Neural networks; Remote sensing; Taxicabs; Urban planning; Convolutional neural network; Physical environments; Remote sensing imagery; Social sensing; Urban environments; Urban mobility; Urban mobility patterns; Urban road networks; Satellite imagery; artificial neural network; landscape ecology; learning; remote sensing; satellite data; satellite imagery; social mobility; spatiotemporal analysis; urban area; urban region"
"GAN-based multi-level mapping network for satellite imagery super-resolution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071032705&doi=10.1109%2fICME.2019.00097&partnerID=40&md5=50841be70ffa72c8423d321c3f30c9cc","Although many deep-learning-based image super-resolution (SR) methods have been proposed, most of them assume that all hierarchical features share the unified mapping equations. They ignore the differences between mapping equations at different feature levels, and create an average effect of mapping prediction, thus poorly building the mapping relations between low resolution (LR) and high resolution (HR) spaces. In this paper, we propose a multi-level mapping framework along with the adversarial learning strategy, namely MMGAN, for satellite imageries SR reconstruction. We also construct a feature extraction and tuning block (FETB) for fine feature expression. In particular, a novel two-dimension dense unit (DU) and a mapping attention unit (MAU) are constructed for building multi-level mappings in different stages. With our strategies, an HR image is reconstructed directly from the input image using multi-level mappings. Extensive experiments on Kaggle Open Source Dataset and Jilin-1 video satellite images exhibit superior reconstruction performance when compared with the state-of-the-art SR approaches. © 2019 IEEE.","Adversarial learning; Attention unit; Multi-level mapping; Remote sensing imagery; Super resolution","Deep learning; Mapping; Optical resolving power; Remote sensing; Satellite imagery; Adversarial learning; Attention unit; Multi-level mapping; Remote sensing imagery; Super resolution; Image reconstruction"
"A Class Activation Mapping Guided Adversarial Training Method for Land-Use Classification and Object Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077689297&doi=10.1109%2fIGARSS.2019.8897938&partnerID=40&md5=1822d8f3c1b58c26f6d40181fba10fa4","Interpretation of convolutional neural networks (CNNs) critically influence our understanding of deep learning models' internal dynamics. In this paper, we demonstrate an interpretable training method, namely class activation mapping guided adversarial training (CAMAT), for two typical remote sensing tasks, land-use classification and object detection. We first generate class activation maps of the current batch training samples. Class activation map is a kind of class-specific saliency map that quantifies the contributions of a particular region in the image to the CNN prediction result. Then, high contribution regions in the training samples are occluded, and we leverage the partial masked images as the inputs for network training. Following this paradigm, the key areas for network learning and decision making are purposefully disturbed in the training phase, thus the trained model could have better performance in robustness and generalization. Experiments conducted on classic remote sensing datasets verified the outperforming effectiveness and efficiency of the proposed CAMAT. © 2019 IEEE.","Adversarial training; class activation mapping; land-use classification; object detection; remote sensing imagery",
"Convolution Based Spectral Partitioning Architecture for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077705368&doi=10.1109%2fIGARSS.2019.8898038&partnerID=40&md5=b38ce5f5cf1511e6db3e8bd6f133099a","Hyperspectral images (HSIs) can distinguish materials with high number of spectral bands, which is widely adopted in remote sensing applications and benefits in high accuracy land cover classifications. However, HSIs processing are tangled with the problem of high dimensionality and limited amount of labelled data. To address these challenges, this paper proposes a deep learning architecture using three dimensional convolutional neural networks with spectral partitioning to perform effective feature extraction. We conduct experiments using Indian Pines and Salinas scenes acquired by NASA Airborne Visible/Infra-Red Imaging Spectrometer. In comparison to prior results, our architecture shows competitive performance for classification results over current methods. © 2019 IEEE.","Convolutional Neural Network; Hyperspectral Imagery; Landcover Classification; Pattern Recognition; Remote Sensing",
"Cropland Mapping in Fragmented Agricultural Landscape Using Modified Pyramid Scene Parsing Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077680920&doi=10.1109%2fIGARSS.2019.8897867&partnerID=40&md5=ae620452d0654f37879578c4ea56e0f5","It is significant to extract cropland mapping accurately and rapidly in many fields. In southern China, restrictions on complex planting structure and fragmented patches, long rainy season all pose challenges to remote sensing. Deep Learning has advantages in such complex classification problem. The combination of medium and high resolution data is an effective way to solve the problem of data acquisition and scale refinement in fragmented agricultural landscape area. In this paper, a modified Pyramid Scene Parsing Network which could integrate the medium-resolution data and the high-resolution data (HM PSP) was proposed focusing on different scales features. The experimental results showed the applicability of the HM PSP, which also had better visual results, the 3.42 % and 1.58% improvement of OA for the test sets. By comparing the results of HM PSP and M PSP (only trained by the medium resolution data), the enhanced effect after adding high-resolution data, as well as the generalization ability of spatial and temporal generalization ability of the model had been verified. © 2019 IEEE.","cropland mapping; Deep Learning; detailed feature; fragmented landscape; PSPNet; southern China",
"Fire hazard research of forest areas based on the use of convolutional and capsule neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074953654&doi=10.1109%2fUKRCON.2019.8879867&partnerID=40&md5=e18256003c7b1c6192bd7112f2ed3bc8","The scientific and practical problem of detecting fire hazardous forest areas by using deep learning artificial neural networks applied to Camp Fire (California, USA) is considered in the paper. The theory of deep learning neural networks, the theory of recognition multispectral images and mathematical statistics methods are used. A novel solution of the multispectral images recognition method by using capsule and convolutional neural networks applied to Camp Fire area is presented. A comparative analysis of convolutional and capsule neural networks is conducted. © 2019 IEEE.","Capsule neural networks; Convolutional neural networks; Earth remote sensing; Multispectral images; Wildfires","Convolution; Fires; Forestry; Hazards; Neural networks; Remote sensing; Statistics; Comparative analysis; Convolutional neural network; Earth remote sensing; Learning neural networks; Mathematical statistics methods; Multispectral images; Recognition methods; Wildfires; Deep neural networks"
"R3-Net: A Deep Network for Multioriented Vehicle Detection in Aerial Images and Videos","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064001060&doi=10.1109%2fTGRS.2019.2895362&partnerID=40&md5=cc7246a7fce48cab1a4170311a0d4bea","Vehicle detection is a significant and challenging task in aerial remote sensing applications. Most existing methods detect vehicles with regular rectangle boxes and fail to offer the orientation of vehicles. However, the orientation information is crucial for several practical applications, such as the trajectory and motion estimation of vehicles. In this paper, we propose a novel deep network, called a rotatable region-based residual network (R3-Net), to detect multioriented vehicles in aerial images and videos. More specially, R3-Net is utilized to generate rotatable rectangular target boxes in a half coordinate system. First, we use a rotatable region proposal network (R-RPN) to generate rotatable region of interests (R-RoIs) from feature maps produced by a deep convolutional neural network. Here, a proposed batch averaging rotatable anchor strategy is applied to initialize the shape of vehicle candidates. Next, we propose a rotatable detection network (R-DN) for the final classification and regression of the R-RoIs. In R-DN, a novel rotatable position-sensitive pooling is designed to keep the position and orientation information simultaneously while downsampling the feature maps of R-RoIs. In our model, R-RPN and R-DN can be trained jointly. We test our network on two open vehicle detection image data sets, namely, DLR 3K Munich Data set and VEDAI Data set, demonstrating the high precision and robustness of our method. In addition, further experiments on aerial videos show the good generalization capability of the proposed method and its potential for vehicle tracking in aerial videos. The demo video is available at https://youtu.be/xCYD-tYudN0. © 1980-2012 IEEE.","Aerial images and videos; deep learning; multioriented detection; remote sensing; vehicle detection","Antennas; Deep learning; Deep neural networks; Image segmentation; Motion estimation; Neural networks; Remote sensing; Statistical tests; Aerial images; Aerial remote sensing; Convolutional neural network; Estimation of vehicles; Generalization capability; Orientation information; Position and orientations; Vehicle detection; Vehicles; aerial photograph; algorithm; artificial neural network; data set; detection method; experimental study; precision; remote sensing; tracking; videography"
"Spotlight SAR image recognition based on dual-channel feature map convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074393648&doi=10.1109%2fSIPROCESS.2019.8868672&partnerID=40&md5=d4364187257bb50943c7ea5b45d0d834","Synthetic Aperture Radar (SAR) is widely used in agriculture, remote sensing and many other fields due to its allweather working mode and its excellent penetration. However, the decipherment of synthetic aperture radar imaging is very difficult compared to optical images. This problem is even worse in the SAR target recognition. Although the traditional feature engineering method is helpful for SAR image information content extraction, the effect is not satisfied with the requirements in practice. Convolutional neural network is an effective method to extract synthetic aperture radar imaging features and recognize targets. In this paper, a dualchannel feature map convolutional neural network (DCFM-CNN) is proposed, using two different down sampling methods, - pooling and convolution, to extract features for SAR image automatic target recognition (SAR-ATR). An average recognition accuracy of 99.45% was achieved on MSTAR public data set. Preprocessing of synthetic aperture radar imaging is not needed here, and the target recognition is completed by the CNN model. The proposed object recognition approach is effective with low overhead. © 2019 IEEE.","Convolutional neural network; Deep learning; Dual-channel feature map; Object recognition; Synthetic aperture radar imaging","Automatic target recognition; Convolution; Deep learning; Deep neural networks; Geometrical optics; Image recognition; Neural networks; Object recognition; Optical data processing; Radar target recognition; Remote sensing; Synthetic aperture radar; Convolutional neural network; Down sampling; Feature engineerings; Feature map; Optical image; Recognition accuracy; Target recognition; Working mode; Radar imaging"
"Use of deep neural networks for crop yield prediction: A case study of soybean yield in lauderdale county, Alabama, USA","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072937932&doi=10.1109%2fAgro-Geoinformatics.2019.8820257&partnerID=40&md5=c69fae33211860282af5d044bd7b3c4e","World population is constantly increasing and it is necessary to have sufficient crop production. Monitoring crop growth and yield estimation are very important for the economic development of a nation. The prediction of crop yield has direct impact on national and international economies and play important role in the food management and food security. Deep learning gains importance on crop monitoring, crop type classification and crop yield estimation applications with the recent advances in image classification using deep Convolutional Neural Networks. Traditional crop yield prediction approaches based on remote sensing consist of classical Machine Learning methods such as Support Vector Machines and Decision Trees. Convolutional Neural Network CNN] and Long-Short Term Memory Network (LSTM] are deep neural network models that are proposed for crop yield prediction recently. This study focused on soybean yield prediction of Lauderdale County, Alabama, USA using 3D CNN model that leverages the spatiotemporal features. The yield is provided from USDA NASS Quick Stat tool for years 2003-2016. The satellite data used is collected from NASA's MODIS land products surface reflectance, land surface temperature and land surface temperature via Google Earth Engine. The root mean squared error (RMSE] is used as the evaluation metric in order to be able to compare the results with other methods that generally uses RMSE as the evaluation metric. © 2019 IEEE.","Convolutional neural networks; Crop yield prediction; Deep neural networks","3D modeling; Atmospheric temperature; Convolution; Crops; Cultivation; Decision trees; Food supply; Forecasting; Land surface temperature; Learning algorithms; Long short-term memory; Mean square error; NASA; Remote sensing; Support vector machines; Surface measurement; Surface properties; Convolutional neural network; Crop growth and yields; Crop type classification; Crop yield; International economy; Machine learning methods; Root mean squared errors; Spatio temporal features; Deep neural networks"
"Automatic Land Cover Reconstruction from Historical Aerial Images: An Evaluation of Features Extraction and Classification Algorithms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066413462&doi=10.1109%2fTIP.2019.2896492&partnerID=40&md5=e55187dc4a34a7cf10fb9ab82f1104fa","The land cover reconstruction from monochromatic historical aerial images is a challenging task that has recently attracted an increasing interest from the scientific community with the proliferation of large-scale epidemiological studies involving retrospective analysis of spatial patterns. However, the efforts made by the computer vision community in remote-sensing applications are mostly focused on prospective approaches through the analysis of high-resolution multi-spectral data acquired by the advanced spatial programs. Hence, four contributions are proposed in this paper. They aim at providing a comparison basis for the future development of computer vision algorithms applied to the automation of the land cover reconstruction from monochromatic historical aerial images. First, a new multi-scale multi-date dataset composed of 4.9 million non-overlapping annotated patches of the France territory between 1970 and 1990 has been created with the help of geography experts. This dataset has been named HistAerial. Second, an extensive comparison study of the state-of-the-art texture features extraction and classification algorithms, including deep convolutional neural networks (DCNNs), has been performed. It is presented in the form of an evaluation. Third, a novel low-dimensional local texture filter named rotated-corner local binary pattern (R-CRLBP) is presented as a simplification of the binary gradient contours filter through the use of an orthogonal combination representation. Finally, a novel combination of low-dimensional texture descriptors, including the R-CRLBP filter, is introduced as a light combination of local binary patterns (LCoLBPs). The LCoLBP filter achieved state-of-the-art results on the HistAerial dataset while conserving a relatively low-dimensional feature vector space compared with the DCNN approaches (17 times shorter). © 1992-2012 IEEE.","deep convolutional neural networks; deep learning; Features extraction; historical aerial images; land cover; machine learning; texture filters","Antennas; Application programs; Classification (of information); Computer vision; Convolution; Data mining; Deep learning; Deep neural networks; Extraction; Image classification; Image segmentation; Learning systems; Neural networks; Remote sensing; Textures; Vector spaces; Aerial images; Convolutional neural network; Features extraction; Land cover; Texture filters; Image reconstruction"
"SPNet: A Spectral Patching Network for End-To-End Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077679370&doi=10.1109%2fIGARSS.2019.8897843&partnerID=40&md5=16efebab8b9d882f85537e5b23a537b6","Deep learning (DL)-based hyperspectral classification primarily use ""spatial patching"" as preprocessing for incorporating local spatial information. This operation can help to promote classification accuracy but faces the following problems. First, it is difficult to determine the optimal size of spatial patches for different hyperspectral images (HSIs). Second, this operation only exploits spatial features locally but not globally. In this paper, we propose a novel spectral patching network (SPNet) with an end-to-end deep learning architecture for HSI classification. SPNet uses ""spectral patching"" and Atrous Spatial Pyramid Pooling (ASPP) module to fully preserve the local and global spatial contextual information of original HSIs. The experimental results with UAV-borne hyperspectral dataset demonstrate that the SPNet achieved state-of-the-art accuracy and visualization performance in. © 2019 IEEE.","hyperspectral image classification; spectral patching; UAV-borne remote sensing",
"Road Network Extraction from Satellite Images Using CNN Based Segmentation and Tracing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077673434&doi=10.1109%2fIGARSS.2019.8898565&partnerID=40&md5=824831a78605ebf2e2e2954c6e73ef04","Drawing and updating road networks are both time-consuming and labor-intensive. Deep learning technology and high-resolution remote sensing images have provided opportunities for automatic road extraction. However, recent convolutional neural network (CNN) based segmentation methods have shown serious problems on connectivity; road tracing methods with single starting point perform well in connectivity but often result in part areas unreached. We propose a multiple starting points tracer which benefits from both segmentation and tracing methods. We compare our approach with most recent tracing methods on satellite images of global cities and find that our method achieves 8% improvement on IoU. © 2019 IEEE.","convolutional neural network; corner detection.; Road network extraction; segmentation; tracing",
"Evaluating Deep Contextual Description of Superpixels for Detection in Aerial Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077691590&doi=10.1109%2fIGARSS.2019.8899023&partnerID=40&md5=89f32ad0ec7820ae36add7495283eba1","In several applications, the use of pixel contextual information has led to significant effectiveness results in remote sensing image classification tasks. In this paper, we investigate the use of deep superpixel context representation for detection in aerial images. Experimental results considering the widely used ISPRS Postdam and Munich Vehicle datasets demonstrate that max pooling and standard deviation are the most promising pooling methods, while the concatenation of contextual features from different pooling approaches leads to an effective characterization of superpixel contextual information in remote sensing images. © 2019 IEEE.","Contextual description; deep learning; detection; su-perpixel",
"Estimating Sea Ice Concentration from SAR: Training Convolutional Neural Networks with Passive Microwave Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068268973&doi=10.1109%2fTGRS.2019.2892723&partnerID=40&md5=22cc6c2f0de36cac7ea633a58041df9e","Historically, sea ice concentration (SIC) has been measured through the use of passive microwave sensors, as well as human interpretation of synthetic aperture radar (SAR). Although passive microwave data are processed automatically, it suffers from poor spatial resolution and the higher frequency channels are sensitive to weather conditions. Deep learning has demonstrated its ability to perform complex and accurate analysis of images; here, we apply deep learning to estimate ice concentration from SAR scenes. We developed a deep convolutional neural network (CNN) that predicts SIC from SAR, trained upon passive microwave data. The model achieves a 5.24%/7.87% error on its train and test set, respectively. To assess the real-world applicability, we performed an independent validation on 18 SAR scenes (from two distinct geographical regions), not previously seen during training or test. Comparing against human-generated ice analysis charts, we achieved an L1 error of 0.2059, competitive with passive microwave ( E-L1 = 0.1863 ) for the Canadian Arctic Archipelago. For the Gulf of Saint Lawrence region, we achieved an L1 error of 0.2653, significantly better than the passive microwave result ( E-L1 = 0.3593 ). By using novel techniques for model training, as well as training entirely upon passive microwave data, we present an accessible and robust method of developing similar systems for processing SAR.1 Our results suggest that with further postprocessing, CNNs are accurate and robust enough to be used for operational tasks.1Code available: https://github.com/clvcooke/Estimating-SIC-from-SAR-github.com/clvcooke/Estimating-SIC-from-SAR © 1980-2012 IEEE.","Image processing; neural networks; remote sensing; sea ice; synthetic aperture radar (SAR)","Convolution; Deep neural networks; Errors; Geographical regions; Image processing; Microwaves; Neural networks; Radar measurement; Remote sensing; Sea ice; Synthetic aperture radar; Canadian Arctic Archipelago; Convolutional neural network; Higher frequencies; Passive microwave data; Passive microwave sensors; Passive microwaves; Sea ice concentration; Spatial resolution; Microwave sensors; artificial neural network; estimation method; image processing; remote sensing; satellite data; satellite sensor; sea ice; spatial resolution; synthetic aperture radar; Atlantic Ocean; Canada; Canadian Arctic; Gulf of Saint Lawrence"
"Cloud-Net: An End-To-End Cloud Detection Algorithm for Landsat 8 Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077685992&doi=10.1109%2fIGARSS.2019.8898776&partnerID=40&md5=ecf3d310c7303240a5ab40d4172b3ad5","Cloud detection in satellite images is an important first-step in many remote sensing applications. This problem is more challenging when only a limited number of spectral bands are available. To address this problem, a deep learning-based algorithm is proposed in this paper. This algorithm consists of a fully convolutional network (FCN) that is trained by multiple patches of Landsat 8 images. This network, which is called Cloud-Net, is capable of capturing global and local cloud features in an image using its convolutional blocks. since the proposed method is an end-to-end solution no complicated pre-processing step is required. Our experimental results prove that the proposed method outperforms the state-of-the-art method over a benchmark dataset by 8.7% in Jaccard Index. © 2019 IEEE.","Cloud detection; image segmentation; Landsat; satellite",
"A simple rotational equivariance loss for generic convolutional segmentation networks: Preliminary results","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077678377&doi=10.1109%2fIGARSS.2019.8898722&partnerID=40&md5=b859d71c7c36d8c5caa2036b4aed030c","Segmentation convolutional neural networks (SCNNs) are now popular for the semantic segmentation (i.e., dense pixel-wise labeling) of remote sensing imagery, such as color or hyperspectral satellite imagery. One desirable property of SCNNs when applied to remote sensing problems is rotational equivariance. This property implies that the class label assigned to a particular pixel (building, road, etc.) does not change if the input imagery is rotated by an arbitrary angle. We argue that recently proposed methods to make rotational equivariant SCNNs fall into two broad categories: easily employed methods that are somewhat ineffective, and highly effective methods that are complicated and potentially incompatible with state-of-the-art SCNN techniques. We propose a simple addition to the standard SCNN loss function that encourages the SCNN to be rotationally equivariant, and is easily added to modern SCNNs. We test the method on the Inria building labeling dataset and compare it to the popular simple approach of adding random rotational augmentations of the input imagery during training. We show that the proposed approach (i) achieves improved equivariance and (ii) yields performance improvements on average. © 2019 IEEE.","aerial imagery; building detection; convolutional neural networks; deep learning; semantic segmentation",
"Residual Unet for Urban Building Change Detection with Sentinel-1 SAR Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077699712&doi=10.1109%2fIGARSS.2019.8898146&partnerID=40&md5=d74b644c5a0043f6d4ce8249a2e35fd7","Urban building change detection is one of the most important parts of remote sensing applications. Researching change detection method based on deep learning is an effective solution to monitor the urban expansion and recognize the specific change classes. In this paper, we propose a novel urban building change detection method based on the revised residual Unet with Sentinel-1 SAR intensity images. Firstly, we present a new difference image by combing both the original intensity image and the enhanced log-ratio difference image using a non-linear function. Then, the combined difference image is sent to a revised residual Unet network to detect the building changes. By the proposed combined difference image and the revised network, our method is able to focus on the building's change while ignoring other land type changes in a large area. A pair of real bitemporal SAR images is used to test the proposed approach and the obtained experimental results confirm its effectiveness. © 2019 IEEE.","change detection; feature combination; residual Unet; stacked convolutional autoencoder; urban building",
"Artificial intelligence and agriculture 5. 0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071074483&doi=10.35940%2fijrte.B1510.078219&partnerID=40&md5=9c965130c3f766e0f0b63f798db88f0a","Use of Artificial Intelligence and Robotics in agriculture is called as Agriculture 5.0 Disruptive technology should help in solving the social needs. Rogers suggest to develop human centric “ Ubiquitous Computing ” solution, for specific domain (agriculture production). Crop yield prediction (CYP) is vital to address the ever growing demand of food requirements of burgeoning world population and to prevent starvation. Artificial Intelligence can offer effective and practical solution for the problem. Machine Learning (ML) and Deep learning (DL) have been evaluated. Machine Learning models (using python, R, Seaborn) have been experimented in this paper. Data of crop yield is used for model evaluation, which includes horticultural product (Banana), cash crop (Sugarcane), food crop (Rice), for kharif, rabi season (Dataset of Tamil Nadu and US region); Future research could combine remote sensing data and machine learning to predict the yield (using google earth engine). Better accuracy in crop prediction is possible when vital data like soil moisture content (ground level, root level and extreme ends of the field), 14 micro nutrients of soil is made available, for many seasons. ©BEIESP.","AI 2.0; Cyber Physical System (CPS); CYP; Drones (UAV); Floccinaucinihilipilification; LSTM; Machine Learning (Python, R, Seaborn Plotting (Python visualization package)); STARMAC Quadrator helicopter UAV; SVM; Trans Disciplinary Approach",
"Semantic Vehicle Segmentation in Very High Resolution Multispectral Aerial Images Using Deep Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077713130&doi=10.1109%2fIGARSS.2019.8898513&partnerID=40&md5=efad9f5320d446a169cd43d4d73f5aa8","The fusion of complementary information from co-registered multi-modal image data enables a more detailed and more robust understanding of an image scene or specific objects, and is important for several applications in the field of remote sensing. In this paper, the benefits of combining RGB, near infrared (NIR) and thermal infrared (TIR) aerial images for the task of semantic vehicle segmentation through deep neural networks are investigated. Therefore, RGB, NIR and TIR image triplets acquired by the Modular Aerial Camera System (MACS) are precisely co-registered through the application of a virtual camera system and subsequently used for the training of different neural network architectures. Various experiments were conducted to investigate the influence of the different sensor characteristics and an early or late fusion within the network on the quality of the segmentation results. © 2019 IEEE.","Aerial Imagery; Data Fusion; Deep Learning; Multispectral Imagery; Vehicle Segmentation",
"Towards Automated Delineation of Smallholder Farm Fields from VHR Images Using Convolutional Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077722319&doi=10.1109%2fIGARSS.2019.8897979&partnerID=40&md5=161e242168ae39335e66d0bebaf72433","Automated delineation of smallholder farm fields is difficult because of their small size, irregular shape and the use of mixed-cropping systems. Edges between smallholder plots are often indistinct in satellite imagery and contours have to be identified by considering the transition of the complex textural patterns of the fields. We introduce a strategy to delineate field boundaries using a fully convolutional network in combination with a globalization and grouping algorithm to produce a hierarchical segmentation of the fields. We carry out an experimental analysis in a study area in Kofa, Nigeria, using a WorldView-3 image, comparing several state-of-the-art contour detection algorithms. The proposed strategy outperforms state-of-the-art computer vision methods and shows promising results by automatically delineating field boundaries with an accuracy close to human level photo-interpretation. © 2019 IEEE.","agriculture and food security; convolutional neural networks; deep learning; Field boundary extraction; remote sensing; smallholder farming",
"Towards a Sentinel-2 Based Human Settlement Layer","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077676565&doi=10.1109%2fIGARSS.2019.8898172&partnerID=40&md5=fbca8b3a55e5ea66c3bc1110e9bddacc","In this paper, we present how multi-spectral Sentinel-2 satellite images can be used in a machine learning approach based on an encoder-decoder semantic segmentation network to map human settlements. We show the effectiveness of the proposed CNN approach for the mapping of settlements in experiments with 785 European cities. The proposed approach to learn a settlement mapping with noisy ground truth data results in an effective settlement segmentation network with a mean intersection over union of 80.55% and a pixel accuracy of 87.40%. © 2019 IEEE.","Computer Vision; Convolutional Neural Network; Deep Learning; Human Settlements; Machine Learning; Remote Sensing; Satellite Images; Sentinel-2",
"Building Type Classification from Social Media Texts via Geo-Spatial Textmining","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077673453&doi=10.1109%2fIGARSS.2019.8898836&partnerID=40&md5=bbc1ce5dfc0766acd627a4e14feab09c","In this work, we present a model for building type classification from Twitter text messages (tweets) by employing geo-spatial textmining methods. First, we apply standard text pre-processing methods and convert the tweets into sentence vectors using fastText. For classification, we apply a feedforward network with two fully connected hidden layers and feed the generated sentence vectors as linguistic features. Classification results suggest that the classes are distinguishable to a certain extent with pure text even with unbalanced class distributions and a very small sample size. However, these findings also undermine, that building type classification with pure text data is a challenging task. © 2019 IEEE.","Building Settlement Type; Classification; Data Mining; Deep Learning; Language; Natural Language Processing; Social Media; Urban Remote Sensing; Word Embedding",
"DAEN: Deep Autoencoder Networks for Hyperspectral Unmixing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068254141&doi=10.1109%2fTGRS.2018.2890633&partnerID=40&md5=3a0e7868d05f34590a5563e28618c299","Spectral unmixing is a technique for remotely sensed image interpretation that expresses each (possibly mixed) pixel as a combination of pure spectral signatures (endmembers) and their fractional abundances. In this paper, we develop a new technique for unsupervised unmixing which is based on a deep autoencoder network (DAEN). Our newly developed DAEN consists of two parts. The first part of the network adopts stacked autoencoders (SAEs) to learn spectral signatures, so as to generate a good initialization for the unmixing process. In the second part of the network, a variational autoencoder (VAE) is employed to perform blind source separation, aimed at obtaining the endmember signatures and abundance fractions simultaneously. By taking advantage from the SAEs, the robustness of the proposed approach is remarkable as it can unmix data sets with outliers and low signal-to-noise ratio. Moreover, the multihidden layers of the VAE ensure the required constraints (nonnegativity and sum-to-one) when estimating the abundances. The effectiveness of the proposed method is evaluated using both synthetic and real hyperspectral data. When compared with other unmixing methods, the proposed approach demonstrates very competitive performance. © 1980-2012 IEEE.","Deep autoencoder network (DAEN); deep learning; endmember identification; hyperspectral unmixing; variational autoencoder (VAE)","Blind source separation; Signal to noise ratio; Auto encoders; Competitive performance; Endmembers; Hyperspectral Data; Hyperspectral unmixing; Low signal-to-noise ratio; Remotely sensed images; Spectral signature; Deep learning; artificial neural network; identification method; image analysis; machine learning; numerical method; pixel; remote sensing; signal-to-noise ratio; spectral analysis"
"Training a single multi-class convolutional segmentation network using multiple datasets with heterogeneous labels: Preliminary results","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077704204&doi=10.1109%2fIGARSS.2019.8898617&partnerID=40&md5=b60253d7a1a945088c547e893c3ce040","Segmentation convolutional neural networks (CNNs) are now popular for the semantic segmentation (i.e., dense pixel-wise labeling) of remote sensing imagery, such as color or hyperspectral satellite imagery. In recent years a large number of hand-labeled datasets of overhead imagery have emerged, leading to breakthrough performance for CNNs. However, these datasets are typically used in isolation of one another because they are either (i) annotated with heterogeneous object type labels, or (ii) they are collected over different geographic areas. This imposes a major bottleneck on the value of these datasets. In this work we present what we call a class-asymmetric loss function that makes it possible to train a single multi-class network using multiple datasets that are heterogeneously-labeled. We show, for example, that it is possible to train a segmentation algorithm for Buildings, roads, and background using two datasets: one annotated with buildings and one annotated with buildings. We propose a class asymmetric loss that under certain common conditions, allows for one to train models on datasets in which the target class is unlabeled. © 2019 IEEE.","aerial imagery; building detection; convolutional neural networks; deep learning; semantic segmentation",
"Automatically delineating the calving front of Jakobshavn Isbræ from multitemporal TerraSAR-X images: A deep learning approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068220350&doi=10.5194%2ftc-13-1729-2019&partnerID=40&md5=e135c63843f61a7a6b59f2f2a0f65428","The calving fronts of many tidewater glaciers in Greenland have been undergoing strong seasonal and interannual fluctuations. Conventionally, calving front positions have been manually delineated from remote sensing images. But manual practices can be labor-intensive and time-consuming, particularly when processing a large number of images taken over decades and covering large areas with many glaciers, such as Greenland. Applying U-Net, a deep learning architecture, to multitemporal synthetic aperture radar images taken by the TerraSAR-X satellite, we here automatically delineate the calving front positions of Jakobshavn Isbræ from 2009 to 2015. Our results are consistent with the manually delineated products generated by the Greenland Ice Sheet Climate Change Initiative project. We show that the calving fronts of Jakobshavn's two main branches retreated at mean rates of -117±1 and -157± 1myr-1, respectively, during the years 2009 to 2015. The interannual calving front variations can be roughly divided into three phases for both branches. The retreat rates of the two branches tripled and doubled, respectively, from phase 1 (April 2009-January 2011) to phase 2 (January 2011- January 2013) and then stabilized to nearly zero in phase 3 (January 2013-December 2015). We suggest that the retreat of the calving front into an overdeepened basin whose bed is retrograde may have accelerated the retreat after 2011, while the inland-uphill bed slope behind the bottom of the overdeepened basin has prevented the glacier from retreating further after 2012. Demonstrating through this successful case study on Jakobshavn Isbræ and due to the transferable nature of deep learning, our methodology can be applied to many other tidewater glaciers both in Greenland and elsewhere in the world, using multitemporal and multisensor remote sensing imagery. © 2019 Copernicus. All rights reserved.",,"climate change; glacier; ice sheet; machine learning; multispectral image; remote sensing; satellite imagery; synthetic aperture radar; TerraSAR-X; Arctic; Greenland; Greenland Ice Sheet"
"A deep learning approach for robust head pose independent eye movements recognition from videos","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069475315&doi=10.1145%2f3314111.3319844&partnerID=40&md5=9518db12f9618cf737194d30b3d78a0b","Recognizing eye movements is important for gaze behavior understanding like in human communication analysis (human-human or robot interactions) or for diagnosis (medical, reading impairments). In this paper, we address this task using remote RGB-D sensors to analyze people behaving in natural conditions. This is very challenging given that such sensors have a normal sampling rate of 30 Hz and provide low-resolution eye images (typically 36x60 pixels), and natural scenarios introduce many variabilities in illumination, shadows, head pose, and dynamics. Hence gaze signals one can extract in these conditions have lower precision compared to dedicated IR eye trackers, rendering previous methods less appropriate for the task. To tackle these challenges, we propose a deep learning method that directly processes the eye image video streams to classify them into fixation, saccade, and blink classes, and allows to distinguish irrelevant noise (illumination, low-resolution artifact, inaccurate eye alignment, difficult eye shapes) from true eye motion signals. Experiments on natural 4-party interactions demonstrate the benefit of our approach compared to previous methods, including deep learning models applied to gaze outputs. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Blink; Convolutional neural network; Eye movements; Remote sensors; Saccade; Video processing","Deep learning; Diagnosis; Eye tracking; Human robot interaction; Motion estimation; Neural networks; Remote sensing; Video signal processing; Blink; Convolutional neural network; Human communications; Learning approach; Natural conditions; Remote sensors; Robot interactions; Video processing; Eye movements"
"A Comparison: Different DCNN Models for Intelligent Object Detection in Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049170178&doi=10.1007%2fs11063-018-9878-5&partnerID=40&md5=e3a24cd25638e65ac02cdcf2a1dfd83e","In recent years, deep learning especially deep convolutional neural networks (DCNN) has made great progress. Many researchers take advantage of different DCNN models to do object detection in remote sensing. Different DCNN models have different advantages and disadvantages. But in the field of remote sensing, many scholars usually do comparison between DCNN models and traditional machine learning. In this paper, we compare different state-of-the-art DCNN models mainly over two publicly available remote sensing datasets—airplane dataset and car dataset. Such comparison can provide guidance for related researchers. Besides,we provide suggestions for fine-tuning different DCNN models. Moreover, for DCNN models including fully connected layers, we provide a method to save storage space. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Deep convolution neural networks; Deep learning; Object detection; Remote sensing images","Computer vision; Convolution; Deep learning; Deep neural networks; Neural networks; Object detection; Object recognition; Space optics; Convolution neural network; Deep convolutional neural networks; Fully-connected layers; Intelligent object; Provide guidances; Remote sensing images; State of the art; Storage spaces; Remote sensing"
"FusionCNN: a remote sensing image fusion algorithm based on deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056629372&doi=10.1007%2fs11042-018-6850-3&partnerID=40&md5=1ccefca854fc213fa8b02deeaba42294","In remote sensing image fusion field, traditional algorithms based on the human-made fusion rules are severely sensitive to the source images. In this paper, we proposed an image fusion algorithm using convolutional neural networks (FusionCNN). The fusion model implicitly represents a fusion rule whose inputs are a pair of source images and the output is a fused image with end-to-end property. As no datasets can be used to train FusionCNN in remote sensing field, we constructed a new dataset from a natural image set to approximate MS and Pan images. In order to obtain higher fusion quality, low frequency information of MS is used to enhance the Pan image in the pre-processing step. The method proposed in this paper overcomes the shortcomings of the traditional fusion methods in which the fusion rules are artificially formulated, because it learns an adaptive strong robust fusion function through a large amount of training data. In this paper, Landsat and Quickbird satellite data are used to verify the effectiveness of the proposed method. Experimental results show that the proposed fusion algorithm is superior to the comparative algorithms in terms of both subjective and objective evaluation. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Deep learning; Image enhancement; Remote sensing image fusion","Convolution; Deep learning; Deep neural networks; Image enhancement; Neural networks; Remote sensing; Convolutional neural network; Deep convolutional neural networks; Fusion algorithms; Image fusion algorithms; Pre-processing step; QuickBird satellite data; Remote sensing images; Subjective and objective evaluations; Image fusion"
"ACM International Conference Proceeding Series","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072069562&partnerID=40&md5=7e786f8fb1ec68585c62d6d01d322387","The proceedings contain 26 papers. The topics discussed include: railway signal interlocking logic simulation system; using machine vision to command a 6-axis robot arm to act on a randomly placed zinc die cast product; a robust and effective tracking method in remote sensing video sequences; an approach to real time parking management using computer vision; recognizing faces in shades of gray; multi-focus color image fusion using Laplacian filter and discrete Fourier transformation with qualitative error image metrics; Chinese rubbing image binarization-based on deep learning for image denoising; controlling software evolution process using code smell visualization; modified watershed transform for automated brain segmentation from magnetic resonance images; and research on the fast image motion compensation technology of aerial digital camera.",,
"Improved SSD Algorithm and Its Performance Analysis of Small Target Detection in Remote Sensing Images [改进的SSD算法及其对遥感影像小目标检测性能的分析]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069883843&doi=10.3788%2fAOS201939.0628005&partnerID=40&md5=c89d6ab11be8875bcb6c424504aaa2b9","An improved single shot multibox detector (SSD) algorithm is proposed aiming at the problems of slow detection speed of the target proposal based remote sensing image target detection method represented by faster regions with convolutional neural network (R-CNN) and the low performance in small target detection by the SSD algorithm. The algorithm can combine the advantages of the existing detection methods based on target proposal and one-stage target detection to improve the target detection performance. Furthermore, the algorithm replaces the original visual geometry group net with a densely connected network as the backbone network and constructs a feature pyramid between the densely connected modules instead of the original multi-scale feature map. A sample data online acquisition system is designed to verify the accuracy and performance of the proposed algorithm. A sample set of aircraft and playground target is collected as the experimental sample. The network structure stability is verified by training the improved SSD algorithm. Consequently, good results can be achieved without the support of transfer learning. Moreover, the training process is not easy to diverge. By comparing the Faster R-CNN algorithm using ResNet101 as the backbone network and the R-FCN (region-based fully convolutional networks) algorithm, we find that the mean average precision (MAP) of the improved SSD algorithm is 9.13% and 8.48% higher than that of the faster R-CNN and R-FCN algorithms in the test set, respectively. The proposed SSD algorithm improves the MAP in the small target detection by 14.46% and 13.92% compared to the faster R-CNN and R-FCN algorithms, respectively. Detecting a single image takes 71.8 ms, which is 45.7 ms and 7.5 ms less than that of the faster R-CNN and R-FCN algorithms, respectively. © 2019, Chinese Lasers Press. All right reserved.","Deep learning; Feature pyramid; Mean average precision; Multi-scale prediction; Remote sensing; Small target detection","Convolution; Deep learning; Neural networks; Remote sensing; Training aircraft; Convolutional networks; Convolutional neural network; Densely connected networks; Feature pyramid; Improved SSD algorithm; Mean average precision; Multi scale prediction; Small target detection; Image enhancement"
"V-rsir: A web-based tool and benchmark dataset for remote sensing image retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067492567&doi=10.5194%2fisprs-archives-XLII-2-W13-1545-2019&partnerID=40&md5=15d91d4930dedeca24d4e74a35bd614d","Benchmark datasets play an important role in evaluating remote sensing image retrieval methods. Current benchmark datasets are mostly collected through the Google Map API or other desktop tools. However, the Google Map API requires the users to have programming skills and other collection tools are not publicly available, which may hinder the development of new benchmark datasets. This paper develops an open access web-based tool V-RSIR to help users generating new benchmark datasets with volunteers for remote sensing image retrieval. Using this tool, a new benchmark dataset V-RSIR that contains 38 classes with at least 1500 images per class is created by 32 volunteers. A handcrafted low-level feature method and a deep learning high-level feature method are used to test the dataset. The evaluation results are consistent with our perception. This shows that the tool can help users effectively creating benchmark datasets for RSIR. © Authors 2019.","Benchmark dataset; Remote sensing image retrieval; Volunteer; Web-based tool","Application programming interfaces (API); Deep learning; Open access; Remote sensing; Statistical tests; Websites; Benchmark datasets; Evaluation results; High-level features; Low-level features; Programming skills; Remote sensing image retrieval; Volunteer; Web-based tools; Image retrieval"
"A machine learning dataset for large-scope high resolution remote sensing image interpretation considering landscape spatial heterogeneity","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067500515&doi=10.5194%2fisprs-archives-XLII-2-W13-731-2019&partnerID=40&md5=39da9f396c5420171cdb7e2d3475bbd2","The demand for timely information about earth's surface such as land cover and land use (LC/LU), is consistently increasing. Machine learning method shows its advantage on collecting such information from remotely sensed images while requiring sufficient training sample. For satellite remote sensing image, however, sample datasets covering large scope are still limited. Most existing sample datasets for satellite remote sensing image built based on a few frames of image located on a local area. For large scope (national level) view, choosing a sufficient unbiased sampling method is crucial for constructing balanced training sample dataset. Dependable spatial sample locations considering spatial heterogeneity of land cover are needed for choosing sample images. This paper introduces an ongoing work on establishing a national scope sample dataset for high spatial-resolution satellite remote sensing image processing. Sample sites been chosen sufficiently using spatial sampling method, and divided sample patches been grouped using clustering method for further uses. The neural network model for road detection trained our dataset subset shows an increased performance on both completeness and accuracy, comparing to two widely used public dataset. © Authors 2019.","Deep Learning; Land cover and Land use; Landscape spatial Heterogeneity; Sample Dataset; Sample distribution","Deep learning; Distributed computer systems; Image processing; Land use; Large dataset; Machine learning; Sampling; Satellites; High resolution remote sensing images; High spatial resolution; Land cover; Machine learning methods; Sample dataset; Sample distributions; Satellite remote sensing; Spatial heterogeneity; Remote sensing"
"Superpixel classification of high spatial resolution remote sensing image based on multi-scale cnn and scale parameter estimation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067520442&doi=10.5194%2fisprs-archives-XLII-2-W13-681-2019&partnerID=40&md5=1e4fcbbcf37efd649ffcdf0a1d3f55dc","In recent years, considerable attention has been paid to integrate convolutional neural network (CNN) with land cover classification of high spatial resolution remote sensing image. Per-pixel classification method based on CNN (Per-pixel CNN) achieved higher accuracy with the help of high-level features, however, this method still has limitations. Even though per-superpixel classification method based on CNN (Per-superpixel CNN) overcome the limitations of per-pixel CNN, classification accuracy of complex urban is easily influenced by scale effect. To solve this issue, superpixel classification method combining multi-scale CNN (Per-superpixel MCNN) method is proposed. Besides, this paper proposes a novel spatial statistics based method to estimate applicable scale parameter of per-superpixel CNN. Experiments using proposed method were performed on Digital Orthophoto Quarer Quad (DOQQ) images in urban and suburban area. Classification results show that per-superpixel MCNN can effectively avoid misclassification in complex urban area compared with per-superpixel classification method combining single-scale CNN (Per-superpixel SCNN). Series of classification results also show that using the pre-estimated scale parameter can guarantee high classification accuracy, thus arbitrary nature of scale estimation can be avoided to some extent. © Authors 2019.","Deep Learning; High Spatial Resolution Remote Sensing Image; Image Segmentation; OBIA; Spatial Statistics","Complex networks; Deep learning; Image classification; Image resolution; Image segmentation; Neural networks; Parameter estimation; Remote sensing; Superpixels; Classification accuracy; Convolutional neural network; High spatial resolution; Land cover classification; OBIA; Remote sensing images; Spatial statistics; Urban and suburban areas; Pixels"
"Comparative analysis of SVM, ann and cnn for classifying vegetation species using hyperspectral thermal infrared data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067521327&doi=10.5194%2fisprs-archives-XLII-2-W13-1861-2019&partnerID=40&md5=a57f4d6b50eec6f9e55de9a34ba29713","Vegetation includes a significant class of terrestrial ecosystem. Information on tree species categorization is important for environmentalists, foresters, agriculturist, urban managers, landscape architects and biodiversity conservationist. The traditional methods of measuring and identifying tree species (i.e., through field-based survey) are time taking, laborious and costly. Remote sensing data provides an opportunity to identify and classify vegetation species over a large spatial extent. Hyperspectral remote sensing can detect the sublet spectral details among species classes and thus make it possible to differentiate vegetation species based on these subtle variations. This research examines the thermal infrared (2.5 to 14.0 &mu;m) hyperspectral emissivity spectra (comprised of 3456 spectral bands) for the classification of thirteen different plant species. The use of thermal infrared hyperspectral emissivity spectra for the identification of vegetation species is very rare. Three different machine learning methods including support vector machine (SVM), artificial neural network (ANN) and convolutional neural network (CNN) are used to classify thirteen vegetation species and their performance is assessed based on their overall accuracy. The accuracy obtained by CNN, ANN and SVM is 99%, 94% and 91%, respectively. Each classifier was also tested for the advantage associated with increase in training samples or object segmentation size. Increase in the training samples improved the performance of SVM. In a nutshell, all comparative machine learning methods provide very high classification accuracy and CNN outperformed the comparative methods. This study concludes that thermal infrared hyperspectral emissivity data has the potential to discern vegetation species using state of the art machine learning and deep learning methods. © Authors 2019.","classification; Hyperspectral imaging; machine learning; thermal infrared data; vegetation species","Biodiversity; Classification (of information); Deep learning; Electromagnetic wave emission; Infrared radiation; Learning systems; Machine learning; Neural networks; Personnel training; Remote sensing; Sampling; Spectroscopy; Support vector machines; Vegetation; Classification accuracy; Comparative analysis; Convolutional neural network; Hyperspectral remote sensing; Machine learning methods; Terrestrial ecosystems; Thermal infrared data; Vegetation species; Hyperspectral imaging"
"Semantic labeling of als point clouds for tree species mapping using the deep neural network pointnet++","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067508166&doi=10.5194%2fisprs-archives-XLII-2-W13-951-2019&partnerID=40&md5=3864ed478882ff30663409d6b84bf592","Most methods for the mapping of tree species are based on the segmentation of single trees that are subsequently classified using a set of hand-crafted features and an appropriate classifier. The classification accuracy for coniferous and deciduous trees just using airborne laser scanning (ALS) data is only around 90% in case the geometric information of the point cloud is used. As deep neural networks (DNNs) have the ability to adaptively learn features from the underlying data, they have outperformed classic machine learning (ML) approaches on well-known benchmark datasets provided by the robotics, computer vision and remote sensing community. Though, tree species classification using deep learning (DL) procedures has been of minor research interest so far. Some studies have been conducted based on an extensive prior generation of images or voxels from the 3D raw data. Since innovative DNNs directly operate on irregular and unordered 3D point clouds on a large scale, the objective of this study is to exemplarily use PointNet++ for the semantic labeling of ALS point clouds to map deciduous and coniferous trees. The dataset for our experiments consists of ALS data from the Bavarian Forest National Park (366 trees/ha), only including spruces (coniferous) and beeches (deciduous). First, the training data were generated automatically using a classic feature-based Random Forest (RF) approach classifying coniferous trees (precision Combining double low line 93%, recall Combining double low line 80%) and deciduous trees (precision Combining double low line 82%, recall Combining double low line 92%). Second, PointNet++ was trained and subsequently evaluated using 80 randomly chosen test batches à 400 m2. The achieved per-point classification results after 163 training epochs for coniferous trees (precision Combining double low line 90%, recall Combining double low line 79%) and deciduous trees (precision Combining double low line 81%, recall Combining double low line 91%) are fairly high considering that only the geometry was included. Nevertheless, the classification results using PointNet++ are slightly lower than those of the baseline method using a RF classifier. Errors in the training data and occurring edge effects limited a better performance. Our first results demonstrate that the architecture of the 3D DNN PointNet++ can successfully be adapted to the semantic labeling of large ALS point clouds to map deciduous and coniferous trees. Future work will focus on the integration of additional features like i.e. the laser intensity, the surface normals and multispectral features into the DNN. Thus, a further improvement of the accuracy of the proposed approach is to be expected. Furthermore, the classification of numerous individual tree species based on pre-segmented single trees should be investigated. © Authors 2019.","ALS point clouds; deep neural network; PointNet++; semantic labeling; tree species mapping","Decision trees; Deep neural networks; Forestry; Mapping; Remote sensing; Semantic Web; Semantics; Airborne Laser scanning; Bavarian Forest National Park; Classification accuracy; Classification results; Point cloud; PointNet; Semantic labeling; Tree species; Classification (of information)"
"Super-resolution land cover mapping by deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065895043&doi=10.1080%2f2150704X.2019.1587196&partnerID=40&md5=a02836ffde3fe9147cc7ea059e74a1b6","Super-resolution mapping (SRM) is a technique to estimate a fine spatial resolution land cover map from coarse spatial resolution fractional proportion images. SRM is often based explicitly on the use of a spatial pattern model that represents the land cover mosaic at the fine spatial resolution. Recently developed deep learning methods have considerable potential as an alternative approach for SRM, based on learning the spatial pattern of land cover from existing fine resolution data such as land cover maps. This letter proposes a deep learning-based SRM algorithm (DeepSRM). A deep convolutional neural network was first trained to estimate a fine resolution indicator image for each class from the coarse resolution fractional image, and all indicator maps were then combined to create the final fine resolution land cover map based on the maximal value strategy. The results of an experiment undertaken with simulated images show that DeepSRM was superior to conventional hard classification and a suite of popular SRM algorithms, yielding the most accurate land cover representation. Consequently, methods such as DeepSRM may help exploit the potential of remote sensing as a source of accurate land cover information. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",,"Deep neural networks; Image resolution; Neural networks; Optical resolving power; Remote sensing; Convolutional neural network; Land cover informations; Land cover mapping; Learning methods; Proportion Image; Spatial resolution; Super resolution; Super-resolution mappings; Mapping; algorithm; artificial neural network; classification; image analysis; land cover; machine learning; mapping method; remote sensing; spatial resolution; spectral resolution"
"Progress and challenges in deep learning analysis of geoscience images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084021398&partnerID=40&md5=80a5c6f016668ca556e6e4d6d85838fa","Deep learning and deep convolutional neural network (CNN) models have shown promising results and are gaining popularity in the geoscientific community. In contrast to traditional machine learning methodologies based on a suite of carefully selected attributes, deep learning is based on the raw images themselves. Deep CNNs are currently the tools of choice for computer vision tasks such as self-driving cars. Unfortunately, deep learning is encumbered by jargon that is unfamiliar to most geoscientists, providing black box applications resulting in two common reactions: deep learning models are the solution for everything or deep learning models are a modern fad that discards the interpreter's insight or experience with a given problem. In this presentation, we show that CNN models are based on attributes similar to those we use in seismic interpretation and remote sensing. We also show that through a process called transfer learning based on the analysis of 2D colour images, we can exploit much of the previous work developed for image recognition applications to rocks. We illustrate the successful use of transfer learning to microfossil classification, core description, petrographic analysis, and hand specimen identification. We also discuss some of the challenges in CNN analysis of 3D seismic data volumes. © 81st EAGE Conference and Exhibition 2019. All rights reserved.",,
"Deep learning in remote sensing applications: A meta-analysis and review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064709843&doi=10.1016%2fj.isprsjprs.2019.04.015&partnerID=40&md5=2476aa6c3040edc5047fbabc62472050","Deep learning (DL)algorithms have seen a massive rise in popularity for remote-sensing image analysis over the past few years. In this study, the major DL concepts pertinent to remote-sensing are introduced, and more than 200 publications in this field, most of which were published during the last two years, are reviewed and analyzed. Initially, a meta-analysis was conducted to analyze the status of remote sensing DL studies in terms of the study targets, DL model(s)used, image spatial resolution(s), type of study area, and level of classification accuracy achieved. Subsequently, a detailed review is conducted to describe/discuss how DL has been applied for remote sensing image analysis tasks including image fusion, image registration, scene classification, object detection, land use and land cover (LULC)classification, segmentation, and object-based image analysis (OBIA). This review covers nearly every application and technology in the field of remote sensing, ranging from preprocessing to mapping. Finally, a conclusion regarding the current state-of-the art methods, a critical conclusion on open challenges, and directions for future research are presented. © 2019 The Authors","Deep learning (DL); LULC classification; Object detection; Remote sensing; Scene classification","Deep learning; Image fusion; Image segmentation; Land use; Object detection; Object recognition; Classification accuracy; Image spatial resolution; Land-use and land cover classifications; Object based image analysis (OBIA); Remote sensing applications; Remote sensing images; Scene classification; State-of-the-art methods; Remote sensing; accuracy assessment; algorithm; detection method; image analysis; land cover; land use change; literature review; machine learning; meta-analysis; numerical model; remote sensing"
"Deep metric learning method for high resolution remote sensing image scene classification [高分辨率光学遥感场景分类的深度度量学习方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073899606&doi=10.11947%2fj.AGCS.2019.20180434&partnerID=40&md5=c1d903b0c9b1165b81b7d0376f57ca41","Due to the similarity of intra-class and dissimilarity of inter-class of high-resolution remote sensing image scene, it is difficult to identify some image scene class. In this paper, a new classification approach for high-resolution remote sensing image scene is proposed based on deep learning and metric learning. Firstly, a clustering center of each class is preset on the output features of deep learning model. Secondly, the Euclidean distance method is used to calculate the average central metric loss. Finally, the final loss function consists of a central metric loss term, a cross entropy loss term, and a weight and bias term. The goal of this method is to improve the classification accuracy by forcing intra-class compactness and inter-class separability. The experimental results show that the proposed method significantly improves the classification accuracy. Compared with state-of-the-art results, the classification accuracy ratios on RSSCN7, UC Merced and NWPU-RESISC45 datasets are increased by 1.46%, 1.09% and 2.51%, respectively. ©2019, Surveying and Mapping Press. All right reserved.","Average center metric loss; Deep learning; Metric learning; Remote sensing image; Scene classification","Classification (of information); Image classification; Remote sensing; Classification accuracy; Classification approach; Clustering centers; Euclidean distance methods; High resolution remote sensing images; Metric learning; Remote sensing images; Scene classification; Deep learning; accuracy assessment; cluster analysis; image analysis; image classification; machine learning; numerical method; numerical model; remote sensing; spectral resolution"
"The comparison of fusion methods for HSRRSI considering the effectiveness of land cover (Features) object recognition based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068112552&doi=10.3390%2frs11121435&partnerID=40&md5=59d29727cba223cbbe401014f19aa8c9","The efficient and accurate application of deep learning in the remote sensing field largely depends on the pre-processing technology of remote sensing images. Particularly, image fusion is the essential way to achieve the complementarity of the panchromatic band and multispectral bands in high spatial resolution remote sensing images. In this paper, we not only pay attention to the visual effect of fused images, but also focus on the subsequent application effectiveness of information extraction and feature recognition based on fused images. Based on the WorldView-3 images of Tongzhou District of Beijing, we apply the fusion results to conduct the experiments of object recognition of typical urban features based on deep learning. Furthermore, we perform a quantitative analysis for the existing pixel-based mainstream fusion methods of IHS (Intensity-Hue Saturation), PCS (Principal Component Substitution), GS (Gram Schmidt), ELS (Ehlers), HPF (High-Pass Filtering), and HCS (Hyper spherical Color Space) from the perspectives of spectrum, geometric features, and recognition accuracy. The results show that there are apparent differences in visual effect and quantitative index among different fusion methods, and the PCS fusion method has the most satisfying comprehensive effectiveness in the object recognition of land cover (features) based on deep learning. © 2019 by the authors. All right reserved.","Deep learning; High spatial resolution remotely sensed imagery; Image fusion; Method comparison; Object recognition","High pass filters; Image fusion; Image resolution; Object recognition; Principal component analysis; Remote sensing; Space optics; High spatial resolution; Intensity hue saturations; Method comparison; Pre-processing technology; Principal Components; Quantitative indices; Remote sensing images; Remotely sensed imagery; Deep learning"
"Automatic segmentation of river and land in SAR images: A deep learning approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071443420&doi=10.1109%2fAIKE.2019.00011&partnerID=40&md5=b939aa53c04e034c302718af4f2325e4","The ubiquitousness of satellite imagery and powerful, computationally efficient Deep Learning frameworks have found profound use in the field of remote sensing. Augmented with easy access to abundant image data made available by different satellites such as LANDSAT and European Space Agency's Copernicus missions, deep learning has opened various avenues of research in monitoring the world's oceans, land, rivers, etc. One significant problem in this direction is the accurate identification and subsequent segmentation of surface-water in images in the microwave spectrum. Typically, standard image processing tools are used to segment the images which are time inefficient. However, in recent years, deep learning methods for semantic segmentation is the preferred choice given its high accuracy and ease of use. This paper proposes the use of deep-learning approaches such as U-Net to perform an efficient segmentation of river and land. Experimental results show that our approach achieves vastly superior performance on SAR images with pixel accuracy of 0.98 and F1 score of 0.99. © 2019 IEEE.","Deep Learning; SAR image; Semantic Segmentation; U-Net","Image segmentation; Knowledge engineering; Microwave spectroscopy; Radar imaging; Remote sensing; Rivers; Satellite imagery; Semantics; Space-based radar; Synthetic aperture radar; Automatic segmentations; Computationally efficient; European Space Agency; Learning approach; Learning frameworks; Microwave spectrum; SAR Images; Semantic segmentation; Deep learning"
"Individual tree-crown detection in rgb imagery using semi-supervised deep learning neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067384089&doi=10.3390%2frs11111309&partnerID=40&md5=e59179dfb1f474fc98640b73ae699252","Remote sensing can transform the speed, scale, and cost of biodiversity and forestry surveys. Data acquisition currently outpaces the ability to identify individual organisms in high resolution imagery. We outline an approach for identifying tree-crowns in RGB imagery while using a semi-supervised deep learning detection network. Individual crown delineation has been a long-standing challenge in remote sensing and available algorithms produce mixed results. We show that deep learning models can leverage existing Light Detection and Ranging (LIDAR)-based unsupervised delineation to generate trees that are used for training an initial RGB crown detection model. Despite limitations in the original unsupervised detection approach, this noisy training data may contain information from which the neural network can learn initial tree features. We then refine the initial model using a small number of higher-quality hand-annotated RGB images. We validate our proposed approach while using an open-canopy site in the National Ecological Observation Network. Our results show that a model using 434,551 self-generated trees with the addition of 2848 hand-annotated trees yields accurate predictions in natural landscapes. Using an intersection-over-union threshold of 0.5, the full model had an average tree crown recall of 0.69, with a precision of 0.61 for the visually-annotated data. The model had an average tree detection rate of 0.82 for the field collected stems. The addition of a small number of hand-annotated trees improved the performance over the initial self-supervised model. This semi-supervised deep learning approach demonstrates that remote sensing can overcome a lack of labeled training data by generating noisy data for initial training using unsupervised methods and retraining the resulting models with high quality labeled data. © 2019 by the authors.","Deep learning; Detection; LIDAR; NEON; Remote sensing; RGB; Trees","Biodiversity; Data acquisition; Data visualization; Deep learning; Deep neural networks; Error detection; Forestry; Neon; Optical radar; High resolution imagery; Individual tree crown; Labeled training data; Learning neural networks; Light detection and ranging; Observation networks; Trees; Unsupervised detection; Remote sensing"
"A Review on deep learning techniques for 3D sensed data classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068178990&doi=10.3390%2frs11121499&partnerID=40&md5=a855733383ef6c44a94bdecae23cb9d9","Over the past decade deep learning has driven progress in 2D image understanding. Despite these advancements, techniques for automatic 3D sensed data understanding, such as point clouds, is comparatively immature. However, with a range of important applications from indoor robotics navigation to national scale remote sensing there is a high demand for algorithms that can learn to automatically understand and classify 3D sensed data. In this paper we review the current state-of-the-art deep learning architectures for processing unstructured Euclidean data. We begin by addressing the background concepts and traditional methodologies. We review the current main approaches, including RGB-D, multi-view, volumetric and fully end-to-end architecture designs. Datasets for each category are documented and explained. Finally, we give a detailed discussion about the future of deep learning for 3D sensed data, using literature to justify the areas where future research would be most valuable. © 2019 by the authors.","Classification; Deep learning; Machine learning; Point cloud; Segmentation; Semantics","Classification (of information); Data handling; Image segmentation; Learning systems; Remote sensing; Robots; Semantics; Architecture designs; Data classification; Data understanding; Learning architectures; Learning techniques; Multi-views; Point cloud; State of the art; Deep learning"
"MU-net: Deep learning-based thermal IR image estimation from RGB image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083300222&doi=10.1109%2fCVPRW.2019.00134&partnerID=40&md5=9f751b72a6d9a74b2abb3b2325a49f66","Terrain imagery collected by satellite remote sensing or by rover on-board sensors is the primary source for terrain classification used in determining terrain traversibility and mission plans for planetary rovers. Mapping models between RGB and IR for terrain classes are learned from real RGB and IR data examples in the same or similar terrain. This paper adds a new class of deep learning architectures called MU-Net (Multiple U-Net) and shows its efficiency in deriving better RGB-to-IR mapping models, improving over past work the estimation of thermal IR images from incoming RGB images and learned RGB-IR mappings. © 2019 IEEE.",,"Computer vision; Image enhancement; Infrared imaging; Landforms; Mapping; Remote sensing; Satellite imagery; Its efficiencies; Learning architectures; Mapping model; On-board sensors; Planetary rovers; Primary sources; Satellite remote sensing; Terrain classification; Deep learning"
"CNN-based dense image matching for aerial remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068981479&doi=10.14358%2fPERS.85.6.415&partnerID=40&md5=a8bba0f9f6443642fdd91d2ef5c6f9af","Dense stereo matching plays a key role in 3D reconstruction. The capability of using deep learning in the stereo matching of remote sensing data is currently uncertain. This article investigated the application of deep learning–based stereo methods in aerial image series and proposed a deep learning– based multi-view dense matching framework. First, we applied three typical convolutional neural network models, MC-CNN, GC-Net, and DispNet, to aerial stereo pairs and compared the results with those of the SGM and a commercial software, SURE. Second, on different data sets, the generalization ability of each network is evaluated by using direct transfer learning with models pretrained on other data sets and by fine-tuning with a small number of target training data. Third, we present a deep learning–based multi-view dense matching framework where the multi-view geometry is introduced to further refine matching results. Three sets of aerial images as the main data sets and two open-source sets of street images as auxiliary data sets are used for testing. Experiments show that, first, the performance of deep learning–based stereo methods is slightly better than traditional methods. Second, both the GC-Net and the MC-CNN have demonstrated good generalization ability and can obtain satisfactory results on aerial images using a pretrained model on several available stereo benchmarks. Third, multi-view geometry constraints can further improve the performance of deep learning–based methods, which is better than that of the multi-view–based SGMand…SURE. © 2019 American Society for Photogrammetry and Remote Sensing.",,"Antennas; Deep learning; Neural networks; Open source software; Remote sensing; 3D reconstruction; Aerial remote sensing; Commercial software; Convolutional neural network; Dense stereo matching; Generalization ability; Multi-view geometry; Remote sensing data; Stereo image processing; artificial neural network; image analysis; numerical model; performance assessment; remote sensing; satellite data; satellite imagery; software; stereo image"
"Multimodal remote sensing image classification with small sample size based on high-level feature fusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067800272&doi=10.3788%2fLOP56.111001&partnerID=40&md5=02a7dcd07cd39e6ffc4ea3581806315f","The training sample size for some objects on the ground is quite small when applying a deep learning model to study the classification of remote sensing images. Meanwhile, diversified remote sensing image acquisition methods generate numerous multimodal remote sensing images with different spatial resolutions. Fusing these multi-modal remote sensing images to remedy the small sample size defect and achieve a highly precise classification of remote sensing images is an urgent problem to be solved. To this end, the present study proposes a fusion method for image classification based on the correlation of two spatial resolutions. A deep learning network is utilized to extract the high-level features of the remote sensing images in two spatial resolutions. Two types of high-level features are integrated via the proposed fusion strategy and further used as the input to train the whole network model. The experimental results demonstrate that the proposed fusion algorithm can achieve high classification accuracy. Further, because different fusion rules have different classification accuracies, a suitable selection can improve the classification accuracy. © 2019 Universitat zu Koln. All rights reserved.","Deep learning; High-level feature fusion; Image processing; Multimodal remote sensing image; Small sample size",
"Deep learning for classification of hyperspectral data: A comparative review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067996944&doi=10.1109%2fMGRS.2019.2912563&partnerID=40&md5=9c6e189ea3ca9e59536230c7ae1dbd23","In recent years, deep-learning techniques revolutionized the way remote sensing data are processed. The classification of hyperspectral data is no exception to the rule, but it has intrinsic specificities that make the application of deep learning less straightforward than with other optical data. This article presents the state of the art of previous machine-learning approaches, reviews the various deeplearning approaches currently proposed for hyperspectral classification, and identifies the problems and difficulties that arise in the implementation of deep neural networks for this task. In particular, the issues of spatial and spectral resolution, data volume, and transfer of models from multimedia images to hyperspectral data are addressed. Additionally, a comparative study of various families of network architectures is provided, and a software toolbox is publicly released to allow experimenting with these methods (https://github.com/nshaud/DeepHyperX). This article is intended for both data scientists with interest in hyperspectral data and remote sensing experts eager to apply deeplearning techniques to their own data set. © 2019 IEEE.",,"Deep neural networks; Network architecture; Remote sensing; Comparative studies; Hyper-spectral classification; Hyperspectral Data; Learning techniques; Machine learning approaches; Multimedia images; Remote sensing data; State of the art; Classification (of information); artificial neural network; comparative study; image analysis; image classification; multimedia; remote sensing; software"
"Deep learning based retrieval of forest aboveground biomass from combined LiDAR and landsat 8 data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068172326&doi=10.3390%2frs11121459&partnerID=40&md5=e8e3a11ce143f9412ee9ce7b328b85ed","Estimation of forest aboveground biomass (AGB) is crucial for various technical and scientific applications, ranging from regional carbon and bioenergy policies to sustainable forest management. However, passive optical remote sensing, which is the most widely used remote sensing data for retrieving vegetation parameters, is constrained by spectral saturation problems and cloud cover. On the other hand, LiDAR data, which have been extensively used to estimate forest structure attributes, cannot provide suffcient spectral information of vegetation canopies. Thus, this study aimed to develop a novel synergistic approach to estimating biomass by integrating LiDAR data with Landsat 8 imagery through a deep learning-based workflow. First the relationships between biomass and spectral vegetation indices (SVIs) and LiDAR metrics were separately investigated. Next, two groups of combined optical and LiDAR indices (i.e., COLI1 and COLI2) were designed and explored to identify their performances in biomass estimation. Finally, five prediction models, including K-nearest Neighbor, Random Forest, Support Vector Regression, the deep learning model, i.e., Stacked Sparse Autoencoder network (SSAE), and multiple stepwise linear regressions, were individually used to estimate biomass with input variables of different scenarios, i.e., (i) all the COLI1 (ACOLI1), (ii) all the COLI2 (ACOLI2), (iii) ACOLI1 and all the optical (AO) and LiDAR variables (AL), and (iv) ACOLI2, AO and AL. Results showed that univariate models with the combined optical and LiDAR indices as explanatory variables presented better modeling performance than those with either optical or LiDAR data alone, regardless of the combination mode. The SSAE model obtained the best performance compared to the other tested prediction algorithms for the forest biomass estimation. The best predictive accuracy was achieved by the SSAE model with inputs of combined optical and LiDAR variables (i.e., ACOLI1, AO and AL) that yielded an R2 of 0.935, root mean squared error (RMSE) of 15.67 Mg/ha, and relative root mean squared error (RMSEr) of 11.407%. It was concluded that the presented combined indices were simple and effective by integrating LiDAR-derived structure information with Landsat 8 spectral data for estimating forest biomass. Overall, the SSAE model with inputs of Landsat 8 and LiDAR integrated information resulted in accurate estimation of forest biomass. The presented modeling workflow will greatly facilitate future forest biomass estimation and carbon stock assessments. © 2019 by the authors.","Combined optical and LiDAR indices; Forest aboveground biomass (AGB); Landsat 8 OLI; LiDAR; Stacked sparse autoencoder network (SSAE); Synergy","Biomass; Carbon; Data integration; Decision trees; Deep learning; Forecasting; Forestry; Mean square error; Nearest neighbor search; Remote sensing; Vegetation; Aboveground biomass; Auto encoders; Combined optical and LiDAR indices; LANDSAT; Synergy; Optical radar"
"Newly built construction detection in SAR images using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068151259&doi=10.3390%2frs11121444&partnerID=40&md5=bbaad140edbb2455c24decdc8c4aa05c","Remote sensing data can be utilized to help developing countries monitor the use of land. However, the problem of constant cloud coverage prevents us from taking full advantage of satellite optical images. Therefore, we instead opt to use data from synthetic-aperture radar (SAR), which can capture images of the Earth's surface regardless of the weather conditions. In this study, we use SAR data to identify newly built constructions. Most studies on change detection tend to detect all of the changes that have a similar temporal change characteristic occurring on two occasions, while we want to identify only the constructions and avoid detecting other changes such as the seasonal change of vegetation. To do so, we study various deep learning network techniques and have decided to propose the fully convolutional network with a skip connection. We train this network with pairs of SAR data acquired on two different occasions from Bangkok and the ground truth, which we manually create from optical images available from Google Earth for all of the SAR pairs. Experiments to assign the most suitable patch size, loss weighting, and epoch number to the network are discussed in this paper. The trained model can be used to generate a binary map that indicates the position of these newly built constructions precisely with the Bangkok dataset, as well as with the Hanoi and Xiamen datasets with acceptable results. The proposed model can even be used with SAR images of the same specific satellite from another orbit direction and still give promising results. © 2019 by the authors. All right reserved.","Deep learning; SAR; Satellite imagery; U-net; Urban change","Deep learning; Developing countries; Geometrical optics; Orbits; Remote sensing; Satellite imagery; Space-based radar; Synthetic aperture radar; Change detection; Convolutional networks; Earth's surface; Learning network; Remote sensing data; Seasonal changes; Temporal change; Urban changes; Radar imaging"
"Meta-analysis of deep neural networks in remote sensing: A comparative study of mono-temporal classification to support vector machines","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064940848&doi=10.1016%2fj.isprsjprs.2019.04.016&partnerID=40&md5=64e8090672eb270f6e96a324a49a44ee","Deep learning methods have recently found widespread adoption for remote sensing tasks, particularly in image or pixel classification. Their flexibility and versatility has enabled researchers to propose many different designs to process remote sensing data in all spectral, spatial, and temporal dimensions. In most of the reported cases they surpass their non-deep rivals in overall classification accuracy. However, there is considerable diversity in implementation details in each case and a systematic quantitative comparison to non-deep classifiers does not exist. In this paper, we look at the major research papers that have studied deep learning image classifiers in recent years and undertake a meta-analysis on their performance compared to the most used non-deep rival, Support Vector Machine (SVM) classifiers. We focus on mono-temporal classification as the time-series image classification did not offer sufficient samples. Our work covered 103 manuscripts and included 92 cases that supported direct accuracy comparisons between deep learners and SVMs. Our general findings are the following: (i) Deep networks have better performance than non-deep spectral SVM implementations, with Convolutional Neural Networks (CNNs) performing better than other deep learners. This advantage, however, diminishes when feeding SVM with richer features extracted from data (e.g. spatial filters). (ii) Transfer learning and fine-tuning on pre-trained CNNs are offering promising results over spectral or enhanced SVM, however these pre-trained networks are currently limited to RGB input data, therefore currently lack applicability in multi/hyperspectral data. (iii) There is no strong relationship between network complexity and accuracy gains over SVM; small to medium networks perform similarly to more complex networks. (iv) Contrary to the popular belief, there are numerous cases of high deep networks performance with training proportions of 10% or less. Our study also indicates that the new generation of classifiers is often overperforming existing benchmark datasets, with accuracies surpassing 99%. There is a clear need for new benchmark dataset collections with diverse spectral, spatial and temporal resolutions and coverage that will enable us to study the design generalizations, challenge these new classifiers, and further advance remote sensing science. Our community could also benefit from a coordinated effort to create a large pre-trained network specifically designed for remote sensing images that users could later fine-tune and adjust to their study specifics. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Classification; Convolutional neural network; Deep belief network; Deep learning; Stacked auto encoder; Support vector machine","Classification (of information); Complex networks; Convolution; Deep learning; Image classification; Neural networks; Remote sensing; Support vector machines; Auto encoders; Classification accuracy; Convolutional neural network; Deep belief networks; Quantitative comparison; Remote sensing images; Spatial and temporal resolutions; Temporal classification; Deep neural networks; accuracy assessment; artificial neural network; benchmarking; comparative study; image analysis; image classification; meta-analysis; performance assessment; remote sensing; software; spectral analysis; support vector machine"
"A template matching method of multimodal remote sensing images based on deep convolutional feature representation [深度卷积特征表达的多模态遥感影像模板匹配方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073894326&doi=10.11947%2fj.AGCS.2019.20180432&partnerID=40&md5=4e72bc197b7786805b4254ee35e651c0","Due to significant non-linear radiometric differences between multimodal remote sensing images (e.g., optical, infrared, and SAR), traditional methods cannot efficiently extract common features between such images, and are vulnerable for image matching. To address that, the deep learning technique is introduced into the present study to design a matching method based on Siamese network, which aims to extract common features between multimodal images. The network is first optimized by removing the pooling layer and extracting the feature layer from Siamese network to maintain the integrity and positional accuracy of the feature information, making it possible the effective extraction of common features between multimodal images. Then, the template matching strategy is adopted to achieve high-precision matching of multimodal images. The proposed method is evaluated by using multiple multimodal remote sensing images. The results show that the proposed method outperforms traditional template-matching methods in both the matching correct ratio and matching accuracy. ©2019, Surveying and Mapping Press. All right reserved.","Deep learning; Image matching; Multimodal image; Siamese network","Deep learning; Image matching; Image processing; Network layers; Remote sensing; Feature information; Feature representation; Learning techniques; Matching methods; Multi-modal image; Positional accuracy; Remote sensing images; Template matching method; Template matching; accuracy assessment; artificial neural network; numerical method; numerical model; radiometric method; remote sensing; satellite imagery"
"RoofN3D: A database for 3d building reconstruction with deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069962419&doi=10.14358%2fPERS.85.6.435&partnerID=40&md5=1232a616325793faf535573bd423c446","Machine learning methods, in particular those based on deep learning, have gained in importance through the latest development of artificial intelligence and computer hardware. However, the direct application of deep learning methods to improve the results of 3D building reconstruction is often not possible due, for example, to the lack of suitable training data. To address this issue, we present RoofN3D which provides a three-dimensional (3D) point cloud training dataset that can be used to train machine learning models for different tasks in the context of 3D building reconstruction. The details about RoofN3D and the developed framework to automatically derive such training data are described in this paper. Furthermore, we provide an overview of other available 3D point cloud training data and approaches from current literature in which solutions for the application of deep learning to 3D point cloud data are presented. Finally, we exemplarily demonstrate how the provided data can be used to classify building roofs with the PointNet framework. © 2019 American Society for Photogrammetry and Remote Sensing.",,"Computer hardware; Image reconstruction; Machine learning; Three dimensional computer graphics; 3-d building reconstruction; 3D point cloud; Latest development; Learning methods; Machine learning methods; Machine learning models; Threedimensional (3-d); Training dataset; Deep learning; algorithm; artificial intelligence; database; hardware; machine learning; numerical method; reconstruction"
"Object-Oriented Method Combined with Deep Convolutional Neural Networks for Land-Use-Type Classification of Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060151933&doi=10.1007%2fs12524-019-00945-3&partnerID=40&md5=2b9f1419be19bec0f31073cd95f3d2df","Land-use information provides a direct representation of the effect of human activities on the environment, and an accurate and efficient land-use classification of remote sensing images is an important element of land-use and land-cover change research. To solve the problems associated with traditional land-use classification methods (e.g., rapid increase in dimensionality of data, inadequate feature extraction, and low running efficiency), a method that combines object-oriented approach with deep convolutional neural network (COCNN) is presented. First, a multi-scale segmentation algorithm is used to segment images to generate image segmentation regions with high homogeneity. Second, a typical rule set of feature objects is constructed on the basis of the object-oriented segmentation results, and the segmentation objects are classified and extracted to form a training sample set. Third, a convolutional neural network (CNN) model structure is modified to improve classification performance, and the training algorithm is optimized to avoid the overfitting phenomenon that occurs during training using small datasets. Ten land-use types are classified by using the remote sensing images covering the area around Fuxian Lake as an example. By comparing the COCNN method with the method based solely on CNN, precision and kappa index were selected to evaluate the classification accuracy of the two methods. For the COCNN method, on the basis of the classification statistics, precision and kappa index coefficients are 96.2% and 0.96, respectively, which are 8.98% and 0.1 higher than those of the method based solely on CNN. Experimental results show that the COCNN method reasonably and efficiently combines object-oriented and deep learning approaches, thereby effectively solving the problem of the inaccurate classification of typical features with better classification accuracy than the simple use of CNN. © 2019, The Author(s).","Convolutional neural networks; Deep learning; Land-use-type classification; Multi-scale segmentation; Object-oriented","algorithm; artificial neural network; image classification; land cover; land use; numerical method; remote sensing; satellite imagery; segmentation; China; Fuxian Lake; Yunnan"
"Dual learning-based siamese framework for change detection using bi-temporal VHR optical remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067387493&doi=10.3390%2frs11111292&partnerID=40&md5=923ae197f95ead9e45aa093b8d0a5e24","Asafundamentalandprofoundtaskinremotesensing, changedetectionfromvery-high-resolution (VHR) images plays a vital role in a wide range of applications and attracts considerable attention. Current methods generally focus on the research of simultaneouslymodeling and discriminating the changed and unchanged features. In practice, for bi-temporal VHR optical remote sensing images, the temporal spectral variability tends to exist in all bands throughout the entire paired images,making it difficult to distinguish none-changes and changes with a singlemodel. In this paper,motivated by this observation, we propose a novel hybrid end-to-end framework named dual learning-based Siamese framework (DLSF) for change detection. The framework comprises two parallel streams which are dual learning-based domain transfer and Siamese-based change decision. The former stream is aimed at reducing the domain differences of two paired images and retaining the intrinsic information by translating theminto each other's domain. While the latter stream is aimed at learning a decision strategy to decide the changes in two domains, respectively. By training our proposed framework with certain changemap references, thismethod learns a cross-domain translation in order to suppress the differences of unchanged regions and highlight the differences of changed regions in two domains, respectively, then focus on the detection of changed regions. To the best of our knowledge, the idea of incorporating dual learning framework and Siamese network for change detection is novel. The experimental results on two datasets and the comparison with other state-of-the-artmethods verify the efficiency and superiority of our proposed DLSF. © 2019 by the authors.","Change detection; Deep learning technology; Dual learning framework; Siamese network; VHR optical remote sensing images","Deep learning; Change detection; Decision strategy; Domain differences; Domain transfers; Learning frameworks; Learning technology; Optical remote sensing; Spectral variability; Remote sensing"
"Sea surface temperature inversion model for infrared remote sensing images based on deep neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065121805&doi=10.1016%2fj.infrared.2019.04.022&partnerID=40&md5=ae2b7c74d0b49a812f4533a497d4638f","The traditional sea surface temperature (SST)inversion model has a complicated parameter fitting process and poor adaptability in different sea areas. This paper presents an infrared remote sensing inversion model of SST based on deep neural network to refine the situation. The training data are the moderate-resolution imaging spectroradiometer (MODIS)infrared remote sensing data on sunny days and measured data from buoy in Bohai. The accuracy of inversion results is analyzed, the determination coefficient of inversion and measured values is 0.98, the standard error is 0.71 °C and the mean absolute deviation is 0.85 °C, the results show good accuracy of the model. The accuracy of Bohai SST inversion results is compared with SST products from the MODIS sensors and the inversion model is applied to other sea areas, demonstrating the credibility and portability of the model. The data experiments in this paper prove the feasibility of the model, which provides ideas for global SST inversion. © 2019 Elsevier B.V.","Deep learning; Infrared remote sensing; Inversion; Sea surface temperature","Atmospheric temperature; Deep learning; Deep neural networks; Radiometers; Remote sensing; Submarine geophysics; Surface properties; Surface waters; Determination coefficients; Infrared remote sensing; Inversion; Inversion results; Mean absolute deviations; Moderate resolution imaging spectroradiometer; Parameter fitting; Sea surface temperature (SST); Oceanography"
"Achieving Super-Resolution Remote Sensing Images via the Wavelet Transform Combined with the Recursive Res-Net","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066629658&doi=10.1109%2fTGRS.2018.2885506&partnerID=40&md5=4928766409b577671c9659a3e71eb480","Deep learning (DL) has been successfully applied to single image super-resolution (SISR), which aims at reconstructing a high-resolution (HR) image from its low-resolution (LR) counterpart. Different from most current DL-based methods, which perform reconstruction in the spatial domain, we use a scheme based in the frequency domain to reconstruct the HR image at various frequency bands. Further, we propose a method that incorporates the wavelet transform (WT) and the recursive Res-Net. The WT is applied to the LR image to divide it into various frequency components. Then, an elaborately designed network with recursive residual blocks is used to predict high-frequency components. Finally, the reconstructed image is obtained via the inverse WT. This paper has three main contributions: 1) an SISR scheme based on the frequency domain is proposed under a DL framework to fully exploit the potential to depict images at different frequency bands; 2) recursive block and residual learning in global and local manners are adopted to ease the training of the deep network, and the batch normalization layer is removed to increase the flexibility of the network, save memory, and promote speed; and 3) the low-frequency wavelet component is replaced by an LR image with more details to further improve performance. To validate the effectiveness of the proposed method, extensive experiments are performed using the NWPU-RESISC45 data set, and the results demonstrate that the proposed method outperforms several state-of-the-art methods in terms of both objective evaluation and subjective perspective. © 2019 IEEE.","Recursive network; remote sensing image; residual learning; super resolution; wavelet transform (WT)","Deep learning; Frequency domain analysis; Image compression; Image reconstruction; Inverse problems; Optical resolving power; Remote sensing; Wavelet transforms; Frequency components; High frequency components; High resolution image; Objective evaluation; Remote sensing images; residual learning; State-of-the-art methods; Super resolution; Image enhancement; frequency analysis; image analysis; learning; reconstruction; remote sensing; satellite imagery; wavelet analysis"
"PolSAR Image Semantic Segmentation Based on Deep Transfer Learning - Realizing Smooth Classification With Small Training Sets","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063492369&doi=10.1109%2fLGRS.2018.2886559&partnerID=40&md5=810001366f25352229475d93a71d7a68","Suffering from speckle noise and complex scattering phenomena, classification results of SAR images are usually noisy and shattered, which makes them difficult to use in practical applications. Deep-learning-based semantic segmentation realizes segmentation and categorization at the same time, and thus can obtain smooth and fine-grained classification maps. However, this kind of methods require large data sets with pixel-wise categorical annotations, which are time consuming and tedious to retrieve. Compared with photographs and optical remote sensing images, manually annotating SAR data is even harder, which results in a delay of using relevant techniques in this field. In this letter, a new data set is proposed to support semantic segmentation for high-resolution PolSAR images. Limited by the aforementioned problems, the data set is only a small one with 50 image patches. Therefore, two transfer learning strategies are proposed, which adopt the fully convolutional network (FCN) and U-net architecture, respectively, and use distinct pretraining data sets to adapt to different situations. The experiments demonstrate the good performance of both methods and a promising applicability of using small training sets. Moreover, although trained with small patches, both networks can perfectly apply on large images. The new data set and methods are hopeful to support various PolSAR applications as baselines. © 2018 IEEE.","Deep learning; image classification; image segmentation; polarimetry; SAR","Classification (of information); Deep learning; Image classification; Polarimeters; Radar imaging; Remote sensing; Semantics; Synthetic aperture radar; Classification maps; Classification results; Convolutional networks; NET architecture; Optical remote sensing; Scattering phenomenon; Semantic segmentation; Transfer learning; Image segmentation; data set; image analysis; image classification; machine learning; noise; polarization; remote sensing; segmentation; speckle; spectral resolution; synthetic aperture radar"
"Cloud detection in remote sensing images based on multiscale features-convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066607253&doi=10.1109%2fTGRS.2018.2889677&partnerID=40&md5=9eed7183d1e3a92cddcd3c8f7932eb50","Cloud detection in remote sensing images is a challenging but significant task. Due to the variety and complexity of underlying surfaces, most of the current cloud detection methods have difficulty in detecting thin cloud regions. In fact, it is quite meaningful to distinguish thin clouds from thick clouds, especially in cloud removal and target detection tasks. Therefore, we propose a method based on multiscale features-convolutional neural network (MF-CNN) to detect thin cloud, thick cloud, and noncloud pixels of remote sensing images simultaneously. Landsat 8 satellite imagery with various levels of cloud coverage is used to demonstrate the effectiveness of our proposed MF-CNN model. We first stack visible, near-infrared, short-wave, cirrus, and thermal infrared bands of Landsat 8 imagery to obtain the combined spectral information. The MF-CNN model is then used to learn the multiscale global features of input images. The high-level semantic information obtained in the process of feature learning is integrated with low-level spatial information to classify the imagery into thick, thin and noncloud regions. The performance of our proposed model is compared to that of various commonly used cloud detection methods in both qualitative and quantitative aspects. Compared to other cloud detection methods, the experimental results show that our proposed method has a better performance not only in thick and thin clouds but also in the entire cloud regions. © 1980-2012 IEEE.","Cloud detection; convolutional neural network (CNN); deep learning; multiscale features (MF); remote sensing images","Classification (of information); Convolution; Deep learning; Infrared devices; Neural networks; Remote sensing; Satellite imagery; Semantics; Cloud detection; Cloud detection method; Convolutional neural network; High level semantics; Multi-scale features; Remote sensing images; Spectral information; Thermal infrared bands; Feature extraction; artificial neural network; image analysis; Landsat; learning; pixel; remote sensing; satellite imagery; spatial analysis"
"Advanced multi-sensor optical remote sensing for urban land use and land cover classification: Outcome of the 2018 ieee grss data fusion contest","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069443081&doi=10.1109%2fJSTARS.2019.2911113&partnerID=40&md5=9bcdff23d6ea3058185d5b494988d1bc","This paper presents the scientific outcomes of the 2018 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2018 Contest addressed the problem of urban observation and monitoring with advanced multi-source optical remote sensing (multispectral LiDAR, hyperspectral imaging, and very high-resolution imagery). The competition was based on urban land use and land cover classification, aiming to distinguish between very diverse and detailed classes of urban objects, materials, and vegetation. Besides data fusion, it also quantified the respective assets of the novel sensors used to collect the data. Participants proposed elaborate approaches rooted in remote-sensing, and also in machine learning and computer vision, to make the most of the available data. Winning approaches combine convolutional neural networks with subtle earth-observation data scientist expertise. © 2008-2012 IEEE.","Convolutional neural networks (CNN); deep learning; hyperspectral (HS) imaging (HSI); image analysis and data fusion; multimodal; multiresolution; multisource; multispectral light detection and ranging (LiDAR)","Convolution; Deep learning; Deep neural networks; Hyperspectral imaging; Image analysis; Image fusion; Land use; Neural networks; Optical radar; Remote sensing; Spectroscopy; Convolutional neural network; HyperSpectral; Light detection and ranging; Multi-modal; Multiresolution; Multisources; Sensor data fusion; artificial neural network; data processing; image analysis; imaging method; land classification; land cover; land use; lidar; observational method; remote sensing; sensor"
"A deep learning approach for rooftop geocoding","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067346755&doi=10.1111%2ftgis.12536&partnerID=40&md5=d130c8e37e6f359726b25049a65d2ad4","Geocoding has become a routine task for many research investigations to conduct spatial analysis. However, the output quality of geocoding systems is found to impact the conclusions of subsequent studies that employ this workflow. The published development of geocoding systems has been limited to the same set of interpolation methods and reference data sets for quite some time. We introduce a novel geocoding approach utilizing object detection on remotely sensed imagery based on a deep learning framework to generate rooftop geocoding output. This allows geocoding systems to use and output exact building locations without employing typical geocoding interpolation methods or being completely limited by the availability of reference data sets. The utility of the proposed approach is demonstrated over a sample of 22,481 addresses resulting in significant spatial error reduction and match rates comparable to typical geocoding methods. For different land-use types, our approach performs better on low-density residential and commercial addresses than on high-density residential addresses. With appropriate model setup and training, the proposed approach can be extended to search different object locations and to generate new address and point-of-interest reference data sets. © 2019 John Wiley & Sons Ltd",,"data set; error analysis; GIS; interpolation; land use; remote sensing; residential location; satellite imagery; spatial analysis"
"Vegetation analysis and land cover and crop types classification of granite quarry area of Dharmapuri and Krishna Giri districts of Tamil Nadu","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070714846&partnerID=40&md5=2d955d3efe98c83a56f1b300e1d8195a","Deep Learning (DL) constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture and their allied services. The study mentioned that the aspect and altitude influenced the forest types and vegetation pattern. Deep learning (DL) is a powerful state-of-the-art technique for image processing including Remote Sensing (RS)images. This letter describes a multilevel deep learning (DL) architecture that targets land cover and crop type classification and detection from multitemporal multisource satellite imagery. The ubiquitous and wide applications like scene image understanding, video surveillance, robotics, and self-driving systems triggered vast research in the domain of computer vision in the most recent decade. Being the core of all these applications, visual recognition systems which encompasses image classification, localization and detection have achieved great research now. Due to significant development in neural networks especially deep learning, these visual recognition systems have reached remarkable performance. Object detection is one of these domains witnessing great success in computer vision. This research paper demystifies the role of deep learning techniques based on Convolutional Neural Network(CNN) for object detection. Deep learning frameworks and services available for object detection are also enunciated. Deep learning techniques for state-of-the-art object detection systems are assessed in this research paper. Experiments are carried out for the joint experiment of crop assessment and monitoring test site in Ukraine for classification of crops in a heterogeneous environment using nineteen multitemporal scenes acquired by LANDSAT-8 and SENTINEL-1A RS satellites. The architecture with an ensemble of CNNs outperforms the one with MLPs allowing us to better discriminate certain summer crop types, in particular Teak and Sugarcane, and yielding the target accuracies more than 85% for all major crops in Tamilnadu(Banana Tree, Teak, Paddy, and Sugarcane). © 2019, Blue Eyes Intelligence Engineering and Sciences Publication. All rights reserved.","ArcGIS; Deep Learning(DL); Geographic Information System (GIS); Images Classification; Land Use Crop Type; Remote Sensing (RS); Vegetation",
"A new remote sensing dataset for heliport detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073903820&doi=10.1109%2fRAST.2019.8767833&partnerID=40&md5=c2b0bba176cb8bbc10aef80f629b5686","Labelled training image datasets are the main driving forces for many research areas. Especially object detection and recognition research require quite a few labelled data in order to come up with a successful model. In this work a new dataset for H-shaped heliport detection is presented. The presence of a heliport on an image usually implies an important facility such as governmental buildings or military facilities and detection of heliports can reveal critical information about the content of an image. The dataset directory and labelling structure is in Pascal Visual Object Classes (VOC) format. Hence, it is possible to use most of the well-known neural network architectures for creating a model. The dataset contains Google Maps images with heliport samples from all over the world. Therefore, the dataset represents a large variety of image context and different heliport samples. © 2019 IEEE.","deep learning; heliport; remote sensing image dataset","Deep learning; Heliports; Large dataset; Military photography; Network architecture; Neural networks; Object detection; Object recognition; Space optics; Driving forces; Google maps; H-shaped; Military facilities; Object detection and recognition; Remote sensing images; Training image; Visual objects; Remote sensing"
"Multi-task cGAN for simultaneous spaceborne DSM refinement and roof-type classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067397435&doi=10.3390%2frs11111262&partnerID=40&md5=154191cfa925592748eb801a5f60e663","Various deep learning applications benefit from multi-task learning with multiple regression and classification objectives by taking advantage of the similarities between individual tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models compared to separately trained models. In this paper, we make an observation of such influences for important remote sensing applications like elevation model generation and semantic segmentation tasks from the stereo half-meter resolution satellite digital surface models (DSMs). Mainly, we aim to generate good-quality DSMs with complete, as well as accurate level of detail (LoD)2-like building forms and to assign an object class label to each pixel in the DSMs. For the label assignment task, we select the roof type classification problem to distinguish between flat, non-flat, and background pixels. To realize those tasks, we train a conditional generative adversarial network (cGAN) with an objective function based on least squares residuals and an auxiliary term based on normal vectors for further roof surface refinement. Besides, we investigate recently published deep learning architectures for both tasks and develop the final end-to-end network, which combines different models, as using them first separately, they provide the best results for their individual tasks. © 2019 by the authors.","3D scene refinement; Conditional generative adversarial networks; Digital surface model; Multi-task learning; Roof type classification; Satellite imagery; Semantic segmentation; Urban region","Pixels; Remote sensing; Roofs; Satellite imagery; Semantics; Stereo image processing; 3D scenes; Adversarial networks; Digital surface models; Multitask learning; Semantic segmentation; Type classifications; Urban regions; Deep learning"
"End-to-end change detection for high resolution satellite images using improved UNet++","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067384493&doi=10.3390%2frs11111382&partnerID=40&md5=80211961a9fb64312090c68fb37b5f36","Change detection (CD) is essential to the accurate understanding of land surface changes using available Earth observation data. Due to the great advantages in deep feature representation and nonlinear problem modeling, deep learning is becoming increasingly popular to solve CD tasks in remote-sensing community. However, most existing deep learning-based CD methods are implemented by either generating difference images using deep features or learning change relations between pixel patches, which leads to error accumulation problems since many intermediate processing steps are needed to obtain final change maps. To address the above-mentioned issues, a novel end-to-end CD method is proposed based on an effective encoder-decoder architecture for semantic segmentation named UNet++, where change maps could be learned from scratch using available annotated datasets. Firstly, co-registered image pairs are concatenated as an input for the improved UNet++ network, where both global and fine-grained information can be utilized to generate feature maps with high spatial accuracy. Then, the fusion strategy of multiple side outputs is adopted to combine change maps from different semantic levels, thereby generating a final change map with high accuracy. The effectiveness and reliability of our proposed CD method are verified on very-high-resolution (VHR) satellite image datasets. Extensive experimental results have shown that our proposed approach outperforms the other state-of-the-art CD methods. © 2019 by the authors.","Change detection; Deep learning; Encoder-decoder architecture; End-to-end; Feature maps; Multiple side-outputs fusion","Decoding; Deep learning; Network architecture; Remote sensing; Semantics; Signal encoding; Change detection; Earth observation data; Encoder-decoder architecture; End to end; Feature map; Feature representation; High resolution satellite images; Semantic segmentation; Image enhancement"
"Land Cover Change Detection Using Convolution Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072832928&doi=10.1109%2fICECA.2019.8821840&partnerID=40&md5=26b17fd5ea2c9491249dcb5dd5cd3726","The experimental results prove that only few landscapes are remaining on the earth due to anthropogenic activities and the human activities, which has a great impact on the globe. As the population is getting increased day by day, there are many changes occurring in the land use and land cover which results in mitigating the agriculture land that leads to drought and other natural calamities. To reduce such impact on the globe, change detection techniques are very helpful for finding the areas where the change is occurring. Deep learning methods are being used efficiently in remote sensing projects these days. In this paper, Land cover classification and change detection is made using high resolution satellite images of Guntur region taken over the years 2013 and 2016 with Landsat-7. The change detection process is performed by using VGG 19 technique in Convolution neural network algorithm. © 2019 IEEE.","Change detection; CNN; Deep Learning; Landsat-7; Relu; VGG 19.","Convolution; Land use; Remote sensing; Change detection; Convolution neural network; High resolution satellite images; Land cover classification; Land use and land cover; LandSat 7; Relu; VGG 19; Deep learning"
"A convolutional neural network with Fletcher-Reeves algorithm for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067411666&doi=10.3390%2frs11111325&partnerID=40&md5=4108830a0f3f5c23e189e9fd3037c548","Deep learning models, especially the convolutional neural networks (CNNs), are very active in hyperspectral remote sensing image classification. In order to better apply the CNN model to hyperspectral classification, we propose a CNN model based on Fletcher-Reeves algorithm (F-R CNN), which uses the Fletcher-Reeves (F-R) algorithm for gradient updating to optimize the convergence performance of the model in classification. In view of the fact that there are fewer optional training samples in practical applications, we further propose a method of increasing the number of samples by adding a certain degree of perturbed samples, which can also test the anti-interference ability of classification methods. Furthermore, we analyze the anti-interference and convergence performance of the proposed model in terms of different training sample data sets, different batch training sample numbers and iteration time. In this paper, we describe the experimental process in detail and comprehensively evaluate the proposed model based on the classification of CHRIS hyperspectral imagery covering coastal wetlands, and further evaluate it on a commonly used hyperspectral image benchmark dataset. The experimental results show that the accuracy of the two models after increasing training samples and adjusting the number of batch training samples is improved. When the number of batch training samples is continuously increased to 350, the classification accuracy of the proposed method can still be maintained above 80.7%, which is 2.9% higher than the traditional one. And its time consumption is less than that of the traditional one while ensuring classification accuracy. It can be concluded that the proposed method has anti-interference ability and outperforms the traditional CNN in terms of batch computing adaptability and convergence speed. © 2019 by the authors.","Coastal wetland classification; Conjugate gradient; Convolutional neural network (CNN); Fletcher-Reeves algorithm (F-R); Hyperspectral imagery","Classification (of information); Conjugate gradient method; Convolution; Data mining; Deep learning; Iterative methods; Neural networks; Remote sensing; Sampling; Spectroscopy; Wetlands; Classification accuracy; Coastal wetlands; Convergence performance; Convolutional neural network; Fletcher-Reeves algorithm; Hyper-spectral classification; Hyper-spectral imageries; Hyperspectral Remote Sensing Image; Image classification"
"Estimating maize-leaf coverage in field conditions by applying a machine learning algorithm to UAV remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067257881&doi=10.3390%2fapp9112389&partnerID=40&md5=9c9afc054a7db962afde0f79bbe2593b","Leaf coverage is an indicator of plant growth rate and predicted yield, and thus it is crucial to plant-breeding research. Robust image segmentation of leaf coverage from remote-sensing images acquired by unmanned aerial vehicles (UAVs) in varying environments can be directly used for large-scale coverage estimation, and is a key component of high-throughput field phenotyping. We thus propose an image-segmentation method based on machine learning to extract relatively accurate coverage information from the orthophoto generated after preprocessing. The image analysis pipeline, including dataset augmenting, removing background, classifier training and noise reduction, generates a set of binary masks to obtain leaf coverage from the image. We compare the proposed method with three conventional methods (Hue-Saturation-Value, edge-detection-based algorithm, random forest) and a frontier deep-learning method called DeepLabv3+. The proposed method improves indicators such as Qseg, Sr, Es and mIOU by 15% to 30%. The experimental results show that this approach is less limited by radiation conditions, and that the protocol can easily be implemented for extensive sampling at low cost. As a result, with the proposed method, we recommend using red-green-blue (RGB)-based technology in addition to conventional equipment for acquiring the leaf coverage of agricultural crops. © 2019 by the authors.","Image segmentation; Machine learning; Maize-leaf coverage; UAV remoting images",
"Automatic detection of track and fields in China from high-resolution satellite images using multi-scale-fused single shot multibox detector","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067392959&doi=10.3390%2frs11111377&partnerID=40&md5=80405e0454666ef4020884ffbd169215","Object detection is facing various challenges as an important aspect in the field of remote sensing-especially in large scenes due to the increase of satellite image resolution and the complexity of land covers. Because of the diversity of the appearance of track and fields, the complexity of the background and the variety between satellite images, even superior deep learning methods have difficulty extracting accurate characteristics of track and field from large complex scenes, such as the whole of China. Taking track and field as a study case, we propose a stable and accurate method for target detection. Firstly, we add the ""deconvolution"" and ""concat"" module to the structure of the original Single Shot MultiBox Detector (SSD), where Visual Geometry Group 16 (VGG16) is served as a basic network, followed by multiple convolution layers. The two modules are used to sample the high-level feature map and connect it with the low-level feature map to form a new network structure multi-scale-fused SSD (abbreviated as MSF_SSD). MSF-SSD can enrich the semantic information of the low-level feature, which is especially effective for small targets in large scenes. In addition, a large number of track and fields are collected as samples for the whole China and a series of parameters are designed to optimize the MSF_SSD network through the deep analysis of sample characteristics. Finally, by using MSF_SSD network, we achieve the rapid and automatic detection of meter-level track and fields in the country for the first time. The proposed MSF_SSD model achieves 97.9% mean average precision (mAP) on validation set which is superior to the 88.4% mAP of the original SSD. Apart from this, the model can achieve an accuracy of 94.3% while keeping the recall rate in a high level (98.8%) in the nationally distributed test set, outperforming the original SSD method. © 2019 by the authors.","China; Convolutional neural network; High-resolution satellite images; Multi-scale-fused SSD; Object detection; Track and field","Convolution; Deep learning; Image resolution; Neural networks; Object detection; Object recognition; Remote sensing; Satellites; Semantics; Sports; China; Convolutional neural network; High resolution satellite images; Multi-scale-fused SSD; Track and fields; Complex networks"
"Land cover classification for synthetic aperture radar imagery by using unsupervised methods","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073915256&doi=10.1109%2fRAST.2019.8767877&partnerID=40&md5=dee44267a45192a8c312ea7e5a70645e","Land cover classification is an important application in remote sensing and it plays a critical role in urban planning, land cover change monitoring and agricultural monitoring. Synthetic aperture radar (SAR)has long been recognized as an effective sensing tool for land cover monitoring, because of its ability of capturing images day and night without affected by weather conditions. On the other hand, the interpretation of SAR imagery and to get many labeled SAR images are still a challenging problem for remote sensing. Therefore, the aim of this paper is the implementation of unsupervised classification methods which require unlabeled data and comparison of their performances. In order to achieve this goal, Vertical-Vertical (VV)and Vertical-Horizontal (VH)polarization Sentinel-l SAR images are used. The acquisition year of these images is 2018. Principal Component Analysis (PCA), Kernel PCA, Eigenface and Autoencoder feature extraction methods and also user defined features are implemented for unsupervised classification and the results have been reported and discussed. The performances of these methods are compared by using the cluster validity indices which is the criterion for unsupervised classification, and also the optimum cluster number is determined. Results demonstrate that, KPCA and Autoencoder methods are better for VH polarization. Also, user defined features and Autoencoder are better for VV polarization. © 2019 IEEE.","Autoencoder; Cluster validity index; Clustering; Deep Learning; Eigenfaces; K-means; Kernel PCA; Land Cover; PCA; Remote sensing; SAR; Unsupervised Classification","Classification (of information); Deep learning; Image classification; K-means clustering; Polarization; Principal component analysis; Remote sensing; Space optics; Space-based radar; Synthetic aperture radar; Tracking radar; Auto encoders; Cluster validity indices; Clustering; Eigenfaces; K-means; Kernel PCA; Land cover; Unsupervised classification; Radar imaging"
"Land cover classification from fused DSM and UAV images using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068175705&doi=10.3390%2frs11121461&partnerID=40&md5=5d8307b9b6bd9cea3736bbe40817040d","In recent years, remote sensing researchers have investigated the use of different modalities (or combinations of modalities) for classification tasks. Such modalities can be extracted via a diverse range of sensors and images. Currently, there are no (or only a few) studies that have been done to increase the land cover classification accuracy via unmanned aerial vehicle (UAV)-digital surface model (DSM) fused datasets. Therefore, this study looks at improving the accuracy of these datasets by exploiting convolutional neural networks (CNNs). In this work, we focus on the fusion of DSM and UAV images for land use/land cover mapping via classification into seven classes: bare land, buildings, dense vegetation/trees, grassland, paved roads, shadows, and water bodies. Specifically, we investigated the effectiveness of the two datasets with the aim of inspecting whether the fused DSM yields remarkable outcomes for land cover classification. The datasets were: (i) only orthomosaic image data (Red, Green and Blue channel data), and (ii) a fusion of the orthomosaic image and DSM data, where the final classification was performed using a CNN. CNN, as a classification method, is promising due to hierarchical learning structure, regulating and weight sharing with respect to training data, generalization, optimization and parameters reduction, automatic feature extraction and robust discrimination ability with high performance. The experimental results show that a CNN trained on the fused dataset obtains better results with Kappa index of ~0.98, an average accuracy of 0.97 and final overall accuracy of 0.98. Comparing accuracies between the CNN with DSM result and the CNN without DSM result for the overall accuracy, average accuracy and Kappa index revealed an improvement of 1.2%, 1.8% and 1.5%, respectively. Accordingly, adding the heights of features such as buildings and trees improved the differentiation between vegetation specifically where plants were dense. © 2019 by the authors.","Deep-Learning; Fusion; GIS; Land cover classification; Remote sensing; UAV","Antennas; Classification (of information); Convolution; Deep learning; Fusion reactions; Geographic information systems; Image classification; Image fusion; Land use; Neural networks; Remote sensing; Unmanned aerial vehicles (UAV); Vegetation; Automatic feature extraction; Classification methods; Convolutional neural network; Digital surface models; Discrimination ability; Hierarchical learning; Land cover classification; Red , green and blues; Mapping"
"Capsulenet-Based Spatial-Spectral Classifier for Hyperspectral Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068174076&doi=10.1109%2fJSTARS.2019.2913097&partnerID=40&md5=06fdc089a47ce2370eb167b781b547c6","In this paper, a Capsulenet-based framework is proposed for extracting spectral and spatial features for improving hyperspectral image classification. Unlike conventional strategies, the proposed framework simultaneously optimizes both feature extraction and classification. The spectral features/patterns derived at different levels of hierarchies are remodeled as spectral-feature capsules. Consequently, unlike conventional convolutional neural network-based approaches, the relative locations as well as other properties such as depth, width, and position of the spectral patterns are taken into consideration. In addition to learning spectral features/patterns, a convolutional long short-Term memory (conv-LSTM) is employed for sequentially integrating the spatial features learned from each band. The integrated spatial-feature representation, thus obtained from the final hidden state of conv-LSTM, forms spatial-feature capsules. The capsule-level integration of spatial and spectral features/patterns yields better convergence and accuracy as compared to both ensemble-based and kernel-level integrations. Along with the margin loss, a spectral-Angle-based reconstruction loss is also minimized to regularize the learning of network weights. Experiments over different standard datasets indicate that the proposed approach performs better than other prominent hyperspectral classifiers. Furthermore, in comparison with the recent deep learning models, our approach is found to be less sensitive to the network parameters and achieves better accuracy even with lesser network depth. © 2008-2012 IEEE.","Capsulenet; classification; convolutional neural network (CNN); hyperspectral","Convolution; Deep learning; Image classification; Image enhancement; Long short-term memory; Spectroscopy; Capsulenet; Convolutional neural network; Feature extraction and classification; HyperSpectral; Network parameters; Relative location; Spectral classifier; Spectral patterns; Classification (of information); accuracy assessment; artificial neural network; data set; ensemble forecasting; image classification; learning; remote sensing; spatial analysis; spectral resolution"
"Transfer learning approach to map urban slums using high and medium resolution satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065592637&doi=10.1016%2fj.habitatint.2019.04.008&partnerID=40&md5=1d162c54f3bac08dd17c6388682ced05","Slums provide cheaper workforce and informal services which contribute substantially towards GDP. However, such areas, due to the high population density, sub-standard housing and lack of essential services are urban risks. The socio-physical development of such settlements has often been neglected due to poor laws and provisions in urban management and policies. One of the primary reasons for negligence has been the unavailability of slum maps to study the evolution of slums and to actively manage and contain them. Various remote sensing techniques have been utilized to answer the problem but have not produced universal solutions. In recent years, Deep Learning (DL)techniques with remote sensing have been found beneficial in comprehending the underlying structure of physical features present in the satellite imageries. This study deals with one of the Deep Learning techniques which use pre-trained convolutional networks for slum detection in Very High Resolution (VHR)and Medium Resolution (MR)satellite imagery. We created a training dataset which comprises of four classes including slums, built, green and water. We further trained the model to detect these classes in the entire city. Classification performance was evaluated for Very high and Medium Resolution imagery with the help of manually delineated slum boundaries gathered from urban local authorities of Mumbai. The Overall accuracy of 94.2 and 90.2 and kappa of 0.70 and 0.55 is obtained from VHR and MR imagery respectively. We provide a comprehensive technique for the detection of informal settlements which can be tailored and applied to any city to detect various landforms. © 2019 Elsevier Ltd",,"artificial neural network; housing management; image classification; informal settlement; map; satellite imagery; urban housing; India; Maharashtra; Mumbai"
"Smartwatch Performance for the Detection and Quantification of Atrial Fibrillation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066497927&doi=10.1161%2fCIRCEP.118.006834&partnerID=40&md5=2c8fd6613776bb4bf989db56f3b56df1","Background: Atrial fibrillation (AF) burden and duration appear to be related to stroke risk. A wearable consumer electronic device could provide long-term assessment of these measures inexpensively and noninvasively. This study compares the accuracy of an AF-sensing watch (AFSW; Apple Watch with KardiaBand) with simultaneous recordings from an insertable cardiac monitor (ICM; Reveal LINQ). Methods: SmartRhythm 2.0, a convolutional neural network, was trained on anonymized data of heart rate, activity level, and ECGs from 7500 AliveCor users. The network was validated on data collected in 24 patients with ICMs and a history of paroxysmal AF who simultaneously wore the AFSW with SmartRhythm 0.1 software. The primary outcome was sensitivity of the AFSW for AF episodes ≥1 hour. Secondary end points included sensitivity of the AFSW for detection of AF by subject and sensitivity for total AF duration across all subjects. Subjects with >50% false-positive AF episodes on ICM were excluded. Results: We analyzed 31 348.9 hours (mean (SD), 11.3 (4.4) hours/day) of simultaneous AFSW and ICM recordings in 24 patients. The ICM detected 82 episodes of AF ≥1 hour while the AFSW was worn, with a total duration of 1127.1 hours. Of these, the SmartRhythm 2.0 neural network detected 80 episodes (episode sensitivity, 97.5%) with a total duration of 1101.1 hours (duration sensitivity, 97.7%). Three of the 18 subjects with AF ≥1 hour had AF only when the watch was not being worn (patient sensitivity, 83.3%; or 100% during time worn). Positive predictive value for AF episodes was 39.9%. Conclusions: An AFSW is highly sensitive for detection of AF and assessment of AF duration in an ambulatory population when compared with an ICM. Such devices may represent an inexpensive, noninvasive approach to long-term AF surveillance and management. © 2019 American Heart Association, Inc.","atrial fibrillation; electrophysiology; heart rate; stroke","aged; Article; atrial fibrillation; clinical article; diagnostic accuracy; diagnostic test accuracy study; female; human; male; predictive value; priority journal; sensitivity and specificity; action potential; ambulatory electrocardiography; atrial fibrillation; comparative study; devices; electronic device; heart atrium; heart rate; mobile application; pathophysiology; remote sensing; reproducibility; signal processing; Action Potentials; Aged; Atrial Fibrillation; Deep Learning; Electrocardiography, Ambulatory; Female; Heart Atria; Heart Rate; Humans; Male; Mobile Applications; Predictive Value of Tests; Remote Sensing Technology; Reproducibility of Results; Signal Processing, Computer-Assisted; Wearable Electronic Devices"
"A scalable model of vegetation transitions using deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063132305&doi=10.1111%2f2041-210X.13171&partnerID=40&md5=259990b1e9790a60eea911e8f0415cff","In times of rapid global change, anticipating vegetation changes and assessing their impacts is of key relevance to managers and policy makers. Yet, predicting vegetation dynamics often suffers from an inherent scale mismatch, with abundant data and process understanding being available at a fine spatial grain, but the relevance for decision-making is increasing with spatial extent. We present a novel approach for scaling vegetation dynamics (SVD), using deep learning to predict vegetation transitions. Vegetation is discretized into a large number (103–106) of potential states based on its structure, composition and functioning. Transition probabilities between states are estimated via a deep neural network (DNN) trained on observed or simulated vegetation transitions in combination with environmental variables. The impact of vegetation transitions on important ecological indicators is quantified by probabilistically linking attributes such as carbon storage and biodiversity to vegetation states. Here, we describe the SVD approach and present results of applying the framework in a meta-modelling context. We trained a DNN using simulations of a process-based forest landscape model for a complex mountain forest landscape under different climate scenarios. Subsequently, we evaluated the ability of SVD to project long-term vegetation dynamics and the resulting changes in forest carbon storage and biodiversity. SVD captured spatial (e.g. elevational gradients) and temporal (e.g. species succession) patterns of vegetation dynamics well, and responded realistically to changing environmental conditions. In addition, we tested the computational efficiency of the approach, highlighting the utility of SVD for country- to continental scale applications. SVD is the—to our knowledge—first vegetation model harnessing deep neural networks. The approach has high predictive accuracy and is able to generalize well beyond training data. SVD was designed to run on widely available input data (e.g. vegetation states defined from remote sensing, gridded global climate datasets) and exceeds the computational performance of currently available highly optimized landscape models by three to four orders of magnitude. We conclude that SVD is a promising approach for combining detailed process knowledge on fine-grained ecosystem processes with the increasingly available big ecological datasets for improved large-scale projections of vegetation dynamics. © 2019 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society.","deep neural networks; ecological forecasting; simulation modelling; state and transition modelling; upscaling; vegetation dynamics; vegetation transitions",
"DAMAGE DETECTION on BUILDING FAÇADES USING MULTI-TEMPORAL AERIAL OBLIQUE IMAGERY","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067422250&doi=10.5194%2fisprs-annals-IV-2-W5-29-2019&partnerID=40&md5=700f901a65fd56158fae01efadbad4ae","Over the past decades, a special interest has been given to remote-sensing imagery to automate the detection of damaged buildings. Given the large areas it may cover and the possibility of automation of the damage detection process, when comparing with lengthy and costly ground observations. Currently, most image-based damage detection approaches rely on Convolutional Neural Networks (CNN). These are used to determine if a given image patch shows damage or not in a binary classification approach. However, such approaches are often trained using image samples containing only debris and rubble piles. Since such approaches often aim at detecting partial or totally collapsed buildings from remote-sensing imagery. Hence, such approaches might not be applicable when the aim is to detect façade damages. This is due to the fact that façade damages also include spalling, cracks and other small signs of damage. Only a few studies focus their damage analysis on the façade and a multi-temporal approach is still missing. In this paper, a multi-temporal approach specifically designed for the image classification of façade damages is presented. To this end, three multi-temporal approaches are compared with two mono-temporal approaches. Regarding the multi-temporal approaches the objective is to understand the optimal fusion between the two imagery epochs within a CNN. The results show that the multi-temporal approaches outperform the mono-temporal ones by up to 22% in accuracy. © Authors 2019.","Change Detection; Convolutional Neural Networks; Deep Learning; Manned-platforms; Remote Sensing","Antennas; Convolution; Deep learning; Neural networks; Remote sensing; Binary Classification Approach; Change detection; Collapsed buildings; Convolutional neural network; Detection approach; Ground observations; Manned-platforms; Remote sensing imagery; Damage detection"
"ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067432441&partnerID=40&md5=7baeac265c4817e515407f10da09d86d","The proceedings contain 83 papers. The topics discussed include: flood-water level estimation from social media images; LGS: local geometrical structure-based interest point matching for wide-baseline imagery in urban areas; precise vehicle reconstruction for autonomous driving applications; damage detection on building façades using multi-temporal aerial oblique imagery; facade reconstruction for textured LOD2 CITYGML models based on deep learning and mixed integer linear programming; unsupervised window extraction from photogrammetric point clouds with thermal attributes; confidence-aware pedestrian tracking using a stereo camera; using 3D models to generate labels for panoptic segmentation of industrial scenes; and reduction of the fronto-parallel bias for wide-baseline semi-global matching.",,
"Road segmentation of cross-modal remote sensing images using deep segmentation network and transfer learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059454121&doi=10.1108%2fIR-05-2018-0112&partnerID=40&md5=7960fc9baaaf3bd97c3249890a836c3c","Purpose: The purpose of this paper is to study the road segmentation problem of cross-modal remote sensing images. Design/methodology/approach: First, the baseline network based on the U-net is trained under a large-scale dataset of remote sensing imagery. Then, the cross-modal training data are used to fine-tune the first two convolutional layers of the pre-trained network to achieve the adaptation to the local features of the cross-modal data. For the cross-modal data of different band, an autoencoder is designed to achieve data conversion and local feature extraction. Findings: The experimental results show the effectiveness and practicability of the proposed method. Compared with the ordinary method, the proposed method gets much better metrics. Originality/value: The originality is the transfer learning strategy that fine-tunes the low-level layers for the cross-modal data application. The proposed method can achieve satisfied road segmentation with a small amount of cross-modal training data, so that is has a good application value. Still, for the similar application of cross-modal data, the idea provided by this paper is helpful. © 2019, Emerald Publishing Limited.","Autoencoder; Cross-modal; Deep learning; Remote sensing; Road segmentation; Transfer learning","Data handling; Deep learning; Image segmentation; Modal analysis; Roads and streets; Auto encoders; Cross-modal; Design/methodology/approach; Local feature extraction; Remote sensing imagery; Remote sensing images; Road segmentation; Transfer learning; Remote sensing"
"Multi-instance neural network architecture for scene classification in remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067068115&doi=10.1109%2fICCISci.2019.8716411&partnerID=40&md5=74de924a5e6d6794e5b0423381f55e9b","Scene classification is important problem in remote sensing since it is prerequisite to other more intelligent analysis operations. Often times, for a given scene only one part of it indicates which class it belongs to, whereas the other parts are either irrelevant or they actually tend to another class. To address this problem, we propose to divide the RS scene into multiple sub-images, consider each one as an instance of the original scene. In that case, we can view the scene as a bag of instances having the same label and we proposed a deep multi-instance learning (MIL) architecture for the classification problem. Our proposed deep MIL architecture extracts CNN features from each instance, then learns how to fuse them into one feature using weighted average layer in the network. The weights used in this layer are automatically learned by the network for each scene. We test the proposed multitask network on three popular scene datasets, namely UC Merced, KSA, and AID datasets. Preliminary results show the promising capabilities of this solution at improving the classification accuracy by giving more weight to the feature extracted from the more informative scene instance. © 2019 IEEE.","Convolutional Neural Network; Multi-instance deep learning; Remote sensing; Scene classification","Deep learning; Network layers; Neural networks; Remote sensing; Classification accuracy; Convolutional neural network; Intelligent analysis; Multi-instance learning; One parts; Scene classification; Subimages; Weighted averages; Network architecture"
"Remote Sensing Image Building Extraction Based on Deep Convolutional Neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067674612&doi=10.1088%2f1742-6596%2f1187%2f4%2f042030&partnerID=40&md5=f5950b35a42aaa77c9614e503d7e57dc","Segmentation Building extraction in high resolution remote sensing image is difficult due to different object has the same spectral feature. In this paper, we build a convolutional neural network RSBD4 base on theory of deep learning. We also proposes the overlap split method to solve the problem when we split image to speed up compute process and it causes the loss of edge information. We conduct experiments on large area Quick Bird image. Result shows that the proposed method extracts building well and has great practical value. © Published under licence by IOP Publishing Ltd.",,"Control theory; Convolution; Extraction; Image segmentation; Neural networks; Power control; Power electronics; Remote sensing; Building extraction; Convolutional neural network; Edge information; High resolution remote sensing images; Quick bird image; Remote sensing images; Spectral feature; Speed up; Deep neural networks"
"Application of Deep Convolution Neural Network in Automatic Classification of Land Use","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067676964&doi=10.1088%2f1742-6596%2f1187%2f4%2f042104&partnerID=40&md5=4e11bedbdb43d387c44f6476ac3c578c","Automatic classification of land use has always been a topic of concern for remote sensing and land science. It plays an important role in the field of land survey and land management and is the basis for the country to carry out land use planning. In last few years, with more and more high resolution remote sensing platforms is becoming usable, it is possible to update and evaluate land use classification quickly with the advantage of huge volume of data and more frequent of the image data updating. At the same time, we are facing more and more challenges of the big data in practice. With the rapid development and achievements of deep learning in the field of image recognition, this paper introduces a deep convolutional neural network to classify and evaluate the existing land use information, and conduct experiments and demonstrations through the self-constructed convolutional neural network. The test results show that the method has a good effect in the determination of houses, factories, greenhouses, waters and woodlands. Due to the small number of samples and the inconspicuous features, the site is confused with other land features, resulting in lower classification accuracy. The method of this paper can realize the automatic classification of land use types and the evaluation of classification effects.[1] © Published under licence by IOP Publishing Ltd.",,"Classification (of information); Convolution; Deep neural networks; Image recognition; Neural networks; Power control; Power electronics; Remote sensing; Automatic classification; Classification accuracy; Convolution neural network; Convolutional neural network; High resolution remote sensing; Land Use Planning; Landuse classifications; Number of samples; Land use"
"Detection of building roofs and facades from aerial laser scanning data using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065655424&doi=10.5194%2fisprs-Archives-XLII-2-W11-975-2019&partnerID=40&md5=c4853b0e60e450fda4ae82886bcf63da","In this work we test the power of prediction of deep learning for detection of buildings from aerial laser scanner point cloud information. Automatic extraction of built features from remote sensing data is of extreme interest for many applications. In particular latest paradigms of 3D mapping of buildings, such as CityGML and BIM, can benefit from an initial determination of building geometries. In this work we used a LiDAR dataset of urban environment from the ISPRS benchmark on urban object detection. The dataset is labelled with eight classes, two were used for this investigation: roof and facades. The objective is to test how TensorFlow neural network for deep learning can predict these two classes. Results show that for ""roof"" and ""facades"" semantic classes respectively, recall is 84% and 76% and precision is 72% and 63%. The number and distribution of correct points well represent the geometry, thus allowing to use them as support for CityGML and BIM modelling. Further tuning of the hidden layers of the DL model will likely improve results and will be tested in future investigations. © Authors 2019.","Deep learning; Laser scanning; Lidar; Semantic classification; Tensorflow","Antennas; Architectural design; Facades; Laser applications; Object detection; Optical radar; Remote sensing; Roofs; Semantics; Automatic extraction; Building geometry; Laser scanning; Laser scanning data; Remote sensing data; Semantic classification; Tensorflow; Urban environments; Deep learning"
"A scale robust convolutional neural network for automatic building extraction from aerial and satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055059004&doi=10.1080%2f01431161.2018.1528024&partnerID=40&md5=ed7b69ddd90cb0091c3826d20b80d519","Identifying buildings from remote sensing imagery has been a challenge due to uncertainties from remote sensing imagery and variations in building structure and texture. In this study, we develop a scale robust CNN structure to improve the segmentation accuracy of building data from high-resolution aerial and satellite images. Based on a fully convolutional network, we introduce two Atrous convolutions on the first two lowest-scale layers, respectively, in the decoding step, aiming at enlarging the sight-of-view and integrate semantic information of large buildings. Then, a multi-scale aggregation strategy is applied. The last feature maps of each scale are used to predict the corresponding building labels, and further up-sampled to the original scale and concatenated for the final prediction. In addition, we introduce a combined data augmentation and relative radiometric calibration method for multi-source building extraction. The method enlarges sample spaces and hence the generalization ability of the deep learning models. We validate our developed methods with an aerial dataset of more than 180, 000 buildings with various architectural types, and a satellite image dataset consists of more than 29,000 buildings. The results are compared with several most recent studies. The comparison result shows our neural network outperformed other studies, especially in segmenting scenes of large buildings. The test on transfer learning from aerial dataset to satellite dataset showed our augmentation strategy significantly improved the prediction accuracy; however, further studies are needed to improve the generalization ability of the CNN model. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Antennas; Buildings; Convolution; Deep learning; Extraction; Forecasting; Image enhancement; Image segmentation; Neural networks; Remote sensing; Semantics; Statistical tests; Automatic building extraction; Convolutional networks; Convolutional neural network; Generalization ability; Radiometric calibrations; Remote sensing imagery; Segmentation accuracy; Semantic information; Satellite imagery; artificial neural network; building; calibration; prediction; radiometric method; remote sensing; satellite data; satellite imagery"
"A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and scene-guided object detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066481564&doi=10.3390%2fapp9102110&partnerID=40&md5=1ee70bdb913972531d5a3ded6a8fad68","As a fundamental and important task in remote sensing, remote sensing image scene understanding (RSISU) has attracted tremendous research interest in recent years. RSISU includes the following sub-tasks: remote sensing image scene classification, remote sensing image scene retrieval, and scene-driven remote sensing image object detection. Although these sub-tasks have different goals, they share some communal hints. Hence, this paper tries to discuss them as a whole. Similar to other domains (e.g., speech recognition and natural image recognition), deep learning has also become the state-of-the-art technique in RSISU. To facilitate the sustainable progress of RSISU, this paper presents a comprehensive review of deep-learning-based RSISU methods, and points out some future research directions and potential applications of RSISU. © 2019 by the authors.","Deep learning; Remote sensing image object detection; Remote sensing image scene classification; Remote sensing image scene retrieval; Remote sensing image scene understanding (RSISU)",
"Deep learning based fossil-fuel power plant monitoring in high resolution remote sensing images: A comparative study","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065734363&doi=10.3390%2frs11091117&partnerID=40&md5=b35e36dc39f92ede06122e832dccf096","The frequent hazyweatherwith air pollution inNorth China has arousedwide attention in the past fewyears. One of themost important pollution resource is the anthropogenic emission by fossil-fuel power plants. To relieve the pollution and assist urban environment monitoring, it is necessary to continuously monitor the working status of power plants. Satellite or airborne remote sensing provides high quality data for such tasks. In this paper, we design a power plantmonitoring framework based on deep learning to automatically detect the power plants and determine their working status in high resolution remote sensing images (RSIs). To this end, we collected a dataset named BUAA-FFPP60 containing RSIs of over 60 fossil-fuel power plants in the Beijing-Tianjin-Hebei region in North China, which covers about 123 km2 of an urban area. We compared eight state-of-the-art deep learning models and comprehensively analyzed their performance on accuracy, speed, and hardware cost. Experimental results illustrate that our deep learning based framework can effectively detect the fossil-fuel power plants and determine their working status with mean average precision up to 0.8273, showing good potential for urban environment monitoring. © 2019 by the authors.","Comparison; Deep learning; Power plant detection; Remote sensing image","Deep learning; Fossil fuel deposits; Fossil fuels; Pollution; Remote sensing; Urban planning; Airborne remote sensing; Anthropogenic emissions; Beijing-tianjin-hebei regions; Comparative studies; Comparison; High resolution remote sensing images; Plant detections; Remote sensing images; Fossil fuel power plants"
"Object detection on remote sensing images using deep learning: An improved single shot multibox detector method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069508485&doi=10.1117%2f1.JEI.28.3.033026&partnerID=40&md5=f34c4edefb2299f3453cfce1e8e55008","Remote sensing images recognition technology has great significance in many aspects, such as military navigation and environmental monitoring. We propose an improved single shot multibox detector approach by combining some strategies, including upsampling, focal loss, and proper calibration of key parameters. Comprehensive experiments on three remote sensing images datasets have demonstrated the effectiveness of the proposed approach in benchmarking with several state-of-the-art object detection methods. © 2019 SPIE and IS&T.","focal loss; object detection; remote sensing images; single shot multibox detector; upsampling","Deep learning; Environmental technology; Military photography; Object detection; Object recognition; Remote sensing; Signal sampling; Environmental Monitoring; Object detection method; Remote sensing images; Single shots; State of the art; Up sampling; Image enhancement"
"Deep learning models to count buildings in high-resolution overhead images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072034228&doi=10.1109%2fJURSE.2019.8809058&partnerID=40&md5=50ebf4dc639e68242dcdb5f4ccc27100","This paper addresses the problem of counting buildings in very high-resolution overhead true color imagery. We study and discuss the relevance of deep-learning based methods to this task. Two architectures and two loss functions are proposed and compared. We show that a model enforcing equivariance to rotations is beneficial for the task of counting in remotely sensed images. We also highlight the importance of robustness to outliers of the loss function when considering remote sensing applications. © 2019 IEEE.","counting; Deep learning; equivariance; loss functions; regression; remote sensing","Remote sensing; counting; Equivariance; Learning-based methods; Loss functions; regression; Remote sensing applications; Remotely sensed images; Very high resolution; Deep learning"
"Low-High-Power Consumption Architectures for Deep-Learning Models Applied to Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057874619&doi=10.1109%2fLGRS.2018.2881045&partnerID=40&md5=6c0cdb908dbe733414f9f2923d2890e9","Convolutional neural networks have emerged as an excellent tool for remotely sensed hyperspectral image (HSI) classification. Nonetheless, the high computational complexity and energy requirements of these models typically limit their application in on-board remote sensing scenarios. In this context, low-power consumption architectures are promising platforms that may provide acceptable on-board computing capabilities to achieve satisfactory classification results with reduced energy demand. For instance, the new NVIDIA Jetson Tegra TX2 device is an efficient solution for on-board processing applications using deep-learning (DL) approaches. So far, very few efforts have been devoted to exploiting this or other similar computing platforms in on-board remote sensing procedures. This letter explores the use of low-power consumption architectures and DL algorithms for HSI classification. The conducted experimental study reveals that the NVIDIA Jetson Tegra TX2 device offers a good choice in terms of performance, cost, and energy consumption for on-board HSI classification tasks. © 2004-2012 IEEE.","Deep learning (DL); embedded computing; hyperspectral image (HSI) classification; low-power consumption architectures","Computer architecture; Computer graphics; Computer hardware; Electric power utilization; Energy utilization; Graphics processing unit; Green computing; Image classification; Network architecture; Neural networks; Program processors; Remote sensing; Spectroscopy; Classification results; Classification tasks; Computing capability; Convolutional neural network; Embedded computing; Hyperspectral sensors; Low-power consumption; Performance evaluations; Deep learning; complexity; computer; cost analysis; experimental study; image classification; multispectral image; remote sensing"
"Urban object classification with 3D deep-learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072033303&doi=10.1109%2fJURSE.2019.8809043&partnerID=40&md5=d5f2c883527d06f5eda2ac6f247918c6","Automatic urban object detection remains a challenge for city management. Existing approaches in remote sensing include the use of aerial images or LiDAR to map a scene. This is, for example, the case for patch-based detection methods. However, these methods do not fully exploit the 3D information given by a LiDAR acquisition because they are similar to depth map. 3D Deep-Learning methods are promising to tackle the issue of the urban objects detection inside a LiDAR cloud. In this paper, we present the results of several experiments on urban object classification with the PointNet network trained with public data and tested on our data-set. We show that such a methodology delivers encouraging results, and also identify the limits and the possible improvements. © 2019 IEEE.","3D points cloud; classification; deep-learning; LiDAR; remote sensing; urban objects","Antennas; Classification (of information); Object detection; Optical radar; Remote sensing; 3D information; Aerial images; City management; Detection methods; Learning methods; Patch based; Public data; Urban objects; Deep learning"
"Automatic post-disaster damage mapping using deep-learning techniques for change detection: Case study of the Tohoku tsunami","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065706047&doi=10.3390%2frs11091123&partnerID=40&md5=adacebc46f5c0c5d1c54bda33c5a678b","Post-disaster damage mapping is an essential task following tragic events such as hurricanes, earthquakes, and tsunamis. It is also a time-consuming and risky task that still often requires the sending of experts on the ground to meticulously map and assess the damages. Presently, the increasing number of remote-sensing satellites taking pictures of Earth on a regular basis with programs such as Sentinel, ASTER, or Landsat makes it easy to acquire almost in real time images from areas struck by a disaster before and after it hits. While the manual study of such images is also a tedious task, progress in artificial intelligence and in particular deep-learning techniques makes it possible to analyze such images to quickly detect areas that have been flooded or destroyed. From there, it is possible to evaluate both the extent and the severity of the damages. In this paper, we present a state-of-the-art deep-learning approach for change detection applied to satellite images taken before and after the Tohoku tsunami of 2011. We compare our approach with other machine-learning methods and show that our approach is superior to existing techniques due to its unsupervised nature, good performance, and relative speed of analysis. © 2019 by the authors.","Change detection; Damage mapping; Deep learning; Remote sensing; Tsunami","Artificial intelligence; Damage detection; Disasters; Learning algorithms; Mapping; Remote sensing; Tsunamis; Change detection; Damage mapping; Learning approach; Learning techniques; Machine learning methods; Real time images; Remote sensing satellites; Satellite images; Deep learning"
"Remote Sensing Image Classification Method Based on Deep Convolution Neural Network and Multi-kernel Learning [基于深度卷积神经网络和多核学习的遥感图像分类方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068962950&doi=10.11999%2fJEIT180628&partnerID=40&md5=a540c829de4341b1cfa7865923b1bded","To solve the problems of complex feature extraction process and low characteristic expressiveness of traditional remote sensing image classification methods, a high resolution remote sensing image classification method based on deep convolution neural network and multi-kernel learning is proposed. Firstly, the deep convolution neural network is constructed to train the remote sensing image data set to learn the outputs of two fully connected layers, which are taken as two high-level features of remote sensing images. Then, the multi-kernel learning is used to train the kernel functions for these two high-level features, so that they can be mapped to the high dimensional space, where these two features are fused adaptively. Finally, with the combined features, a remote sensing image classifier based on Multi-Kernel Learning-Support Vector Machine (MKL-SVM) is designed for remote sensing image classification. Experimental results show that compared with the existing deep learning based remote sensing classification methods, the proposed algorithm achieves improved results in terms of classification accuracy, error, and Kappa coefficient. On the experimental test set, the above three indicators reach 96.43%, 3.57%, and 96.25% respectively, and satisfactory results are obtained. © 2019, Science Press. All right reserved.","Classification; Convolution neural network; High resolution remote sensing image; Multi-Kernel Learning(MKL)","Classification (of information); Convolution; Deep neural networks; Image classification; Multilayer neural networks; Space optics; Support vector machines; Classification accuracy; Convolution neural network; High dimensional spaces; High resolution remote sensing images; Multi-kernel learning; Remote sensing classification; Remote sensing image classification; Remote sensing images; Remote sensing"
"Fault Detection and Isolation in Industrial Processes Using Deep Learning Approaches","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065388432&doi=10.1109%2fTII.2019.2902274&partnerID=40&md5=ea92e04ef3f05ae01a0dfc69aa423b45","Automated fault detection is an important part of a quality control system. It has the potential to increase the overall quality of monitored products and processes. The fault detection of automotive instrument cluster systems in computer-based manufacturing assembly lines is currently limited to simple boundary checking. The analysis of more complex nonlinear signals is performed manually by trained operators, whose knowledge is used to supervise quality checking and manual detection of faults. We present a novel approach for automated Fault Detection and Isolation (FDI) based on deep learning. The approach was tested on data generated by computer-based manufacturing systems equipped with local and remote sensing devices. The results show that the approach models the different spatial/temporal patterns found in the data. The approach can successfully diagnose and locate multiple classes of faults under real-time working conditions. The proposed method is shown to outperform other established FDI methods. © 2005-2012 IEEE.","Artificial Neural Networks (ANNs); computer-aided manufacturing; deep learning; fault detection; machine learning; manufacturing automation","Automation; Computer aided instruction; Computer aided manufacturing; Deep learning; Engineering education; Learning systems; Neural networks; Quality control; Remote sensing; Automated fault detection; Fault detection and isolation; Industrial processs; Instrument clusters; Learning approach; Manufacturing assembly; Manufacturing Automation; Nonlinear signals; Fault detection"
"Towards automatic extraction and updating of VGI-based road networks using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065727619&doi=10.3390%2frs11091012&partnerID=40&md5=d373ebb32972eee3d2063231cf10cac1","This work presents an approach to road network extraction in remote sensing images. In our earlier work, we worked on the extraction of the road network using a multi-agent approach guided by Volunteered Geographic Information (VGI). The limitation of this VGI-only approach is its inability to update the new road developments as it only follows the VGI. In this work, we employ a deep learning approach to update the road network to include new road developments not captured by the existing VGI. The output of the first stage is used to train a Convolutional Neural Network (CNN) in the second stage to generate a general model to classify road pixels. Post-processing is used to correct the undesired artifacts such as buildings, vegetation, occlusions, etc. to generate a final road map. Our proposed method is tested on the satellite images acquired over Abu Dhabi, United Arab Emirates and the aerial images acquired over Massachusetts, United States of America, and is observed to produce accurate results. © 2019 by the authors.","CNN; Road extraction; Road updating; Segment connection; VGI","Antennas; Extraction; Image acquisition; Motor transportation; Multi agent systems; Neural networks; Remote sensing; Roads and streets; Traffic control; Abu Dhabi , United Arab Emirates; Convolutional neural network; Road extraction; Road network extraction; Road updating; Segment connection; United States of America; Volunteered geographic information; Deep learning"
"Deep learning meets hyperspectral image analysis: A multidisciplinary review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066846820&doi=10.3390%2fjimaging5050052&partnerID=40&md5=55bc43c7195435c942eb105cabcc1e59","Modern hyperspectral imaging systems produce huge datasets potentially conveying a great abundance of information; such a resource, however, poses many challenges in the analysis and interpretation of these data. Deep learning approaches certainly offer a great variety of opportunities for solving classical imaging tasks and also for approaching new stimulating problems in the spatial–spectral domain. This is fundamental in the driving sector of Remote Sensing where hyperspectral technology was born and has mostly developed, but it is perhaps even more true in the multitude of current and evolving application sectors that involve these imaging technologies. The present review develops on two fronts: on the one hand, it is aimed at domain professionals who want to have an updated overview on how hyperspectral acquisition techniques can combine with deep learning architectures to solve specific tasks in different application fields. On the other hand, we want to target the machine learning and computer vision experts by giving them a picture of how deep learning technologies are applied to hyperspectral data from a multidisciplinary perspective. The presence of these two viewpoints and the inclusion of application fields other than Remote Sensing are the original contributions of this review, which also highlights some potentialities and critical issues related to the observed development trends. © 2019 by the authors.","Deep learning; Hyperspectral imaging; Image processing; Machine learning; Neural networks",
"Multi-modal Remote Sensing Image Description Based on Word Embedding and Self-Attention Mechanism","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070414687&doi=10.1109%2fISASS.2019.8757726&partnerID=40&md5=6abbd9762687517efcbbad7eb0c6de75","Traditional multi-modal models are relatively weak in describing complex image content when describing and identifying objects to be identified in microwave images, the generated sentences by which are relatively simple. In this paper, a multimodal remote sensing semantic description and recognition method based on self-attention mechanism is proposed, which combined with the Ngram 2vec word embedding technique. Firstly, Ngram2ve is used to mine the semantic information and context features between the pixels to be identified in the domain window and adjacent pixels. Secondly, a self-attention mechanism is introduced to further learn the internal structure information of all pixels in the neighborhood window to generate a multidimensional representation. Finally, in order to avoid the loss of information transmitted between layers, Dense nets are used to implement information flow integration, and a multi-layered independent recurrent neural network is added between each densely connected module to solve the gradient disappearance. Experimental results show that this method is superior to traditional deep learning methods in image description and recognition. © 2019 IEEE.","Densely connected network; Gradient disappears; Independent Recurrent Neural Network; Remote sensing imagery; Word embedding","Deep learning; Embeddings; Image analysis; Pixels; Recurrent neural networks; Remote sensing; Semantics; Attention mechanisms; Densely connected networks; Recognition methods; Remote sensing imagery; Remote sensing images; Semantic descriptions; Semantic information; Word embedding; Multilayer neural networks"
"Nonlinear Multi-scale Super-resolution Using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069000640&doi=10.1109%2fICASSP.2019.8682354&partnerID=40&md5=7553e8638c9a937e4390ebb9c7d1e70f","We propose a deep learning architecture capable of performing up to 8× single image super-resolution. Our architecture incorporates an adversarial component from the super-resolution generative adversarial networks (SRGANs) and a multi-scale learning component from the multiple scale super-resolution network (MSSRNet), which only together can recover smaller structures inherent in satellite images. To further enhance our performance, we integrate progressive growing and training to our network. This, aided by feed forwarding connections in the network to move along and enrich information from previous inputs, produces super-resolved images at scaling factors of 2, 4, and 8. To ensure and enhance the stability of GANs, we employ Wasserstein GANs (WGANs) during training. Experimentally, we find that our architecture can recover small objects in satellite images during super-resolution whereas previous methods cannot. © 2019 IEEE.","dilated convolutions; GANs; remote sensing data; super-resolution","Audio signal processing; Network architecture; Optical resolving power; Remote sensing; Small satellites; Speech communication; Adversarial networks; GANs; Learning architectures; Multiple scale; Remote sensing data; Satellite images; Scaling factors; Super resolution; Deep learning"
"Automated mapping of accessibility signs with deep learning from ground-level imagery and open data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072042598&doi=10.1109%2fJURSE.2019.8808961&partnerID=40&md5=021300331128f233fd85c037680fa230","In some areas or regions, accessible parking spots are not geolocalized and therefore both difficult to find online and excluded from open data sources. In this paper, we aim at detecting accessible parking signs from street view panoramas and geolocalize them. Object detection is an open challenge in computer vision, and numerous methods exist whether based on handcrafted features or deep learning. Our method consists of processing Google Street View images of French cities in order to geolocalize the accessible parking signs on posts and on the ground where the parking spot is not available on GIS systems. To accomplish this, we rely on the deep learning object detection method called Faster R-CNN with Region Proposal Networks which has proven excellent performance in object detection benchmarks. This helps to map accurate locations of where the parking areas do exist, which can be used to build services or update online mapping services such as Open Street Map. We provide some preliminary results which show the feasibility and relevance of our approach. © 2019 IEEE.","Accessibility Sign Detection; Convolutional Neural Networks; Faster R-CNN; Ground-level imagery; Object Detection","Benchmarking; Mapping; Neural networks; Object detection; Object recognition; Open Data; Remote sensing; Traffic signs; Accurate location; Automated mapping; Convolutional neural network; Faster R-CNN; Ground level; Learning objects; Online mapping; Sign detection; Deep learning"
"Dense dilated convolutions merging network for semantic mapping of remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072051887&doi=10.1109%2fJURSE.2019.8809046&partnerID=40&md5=665f714eafaafc5418720d53fe32ecb7","We propose a network for semantic mapping called the Dense Dilated Convolutions Merging Network (DDCM-Net) to provide a deep learning approach that can recognize multi-scale and complex shaped objects with similar color and textures, such as buildings, surfaces/roads, and trees in very high resolution remote sensing images. The proposed DDCM-Net consists of dense dilated convolutions merged with varying dilation rates. This can effectively enlarge the kernels' receptive fields, and, more importantly, obtain fused local and global context information to promote surrounding discriminative capability. We demonstrate the effectiveness of the proposed DDCM-Net on the publicly available ISPRS Potsdam dataset and achieve a performance of 92.3% F1-score and 86.0% mean intersection over union accuracy by only using the RGB bands, without any post-processing. We also show results on the ISPRS Vaihingen dataset, where the DDCM-Net trained with IRRG bands, also obtained better mapping accuracy (89.8% F1-score) than previous state-of-the-art approaches. © 2019 IEEE.","deep learning; Dense Dilated Convolutions Merging (DDCM); remote sensing; semantic mapping","Convolution; Deep learning; Mapping; Merging; Semantic Web; Semantics; Textures; Color and textures; Dense Dilated Convolutions Merging (DDCM); Learning approach; Mapping accuracy; Remote sensing images; Semantic mapping; State-of-the-art approach; Very high resolution; Remote sensing"
"Using deep learning to examine street view green and blue spaces and their associations with geriatric depression in Beijing, China","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061746218&doi=10.1016%2fj.envint.2019.02.013&partnerID=40&md5=ec4118a1d96d456c6ce55ce1f12ef19a","Background: Residential green and blue spaces may be therapeutic for the mental health. However, solid evidence on the linkage between exposure to green and blue spaces and mental health among the elderly in non-Western countries is scarce and limited to exposure metrics based on remote sensing images (i.e., land cover and vegetation indices). Such overhead-view measures may fail to capture how people perceive the environment on the site. Objective: This study aimed to compare streetscape metrics derived from street view images with satellite-derived ones for the assessment of green and blue space; and to examine associations between exposure to green and blue spaces as well as geriatric depression in Beijing, China. Methods: Questionnaire data on 1190 participants aged 60 or above were analyzed cross-sectionally. Depressive symptoms were assessed through the shortened Geriatric Depression Scale (GDS-15). Streetscape green and blue spaces were extracted from Tencent Street View data by a fully convolutional neural network. Indicators derived from street view images were compared with a satellite-based normalized difference vegetation index (NDVI), a normalized difference water index (NDWI), and those derived from GlobeLand30 land cover data on a neighborhood level. Multilevel regressions with neighborhood-level random effects were fitted to assess correlations between GDS-15 scores and these green and blue spaces exposure metrics. Results: The average cumulative GDS-15 score was 3.4 (i.e., no depressive symptoms). Metrics of green and blue space derived from street view images were not correlated with satellite-based ones. While NDVI was highly correlated with GlobeLand30 green space, NDWI was moderately correlated with GlobeLand30 blue space. Multilevel regressions showed that both street view green and blue spaces were inversely associated with GDS-15 scores and achieved the highest model goodness-of-fit. No significant associations were found with NDVI, NDWI, and GlobeLand30 green and blue space. Our results passed robustness tests. Conclusion: Our findings provide support that street view green and blue spaces are protective against depression for the elderly in China, yet longitudinal confirmation to infer causality is necessary. Street view and satellite-derived green and blue space measures represent different aspects of natural environments. Both street view data and deep learning are valuable tools for automated environmental exposure assessments for health-related studies. © 2019 The Authors","China; Deep learning; Depression; Exposures; Natural environments; Street view data; The elderly","Geriatrics; Neural networks; Random processes; Remote sensing; Satellites; Vegetation; China; Depression; Exposures; Natural environments; The elderly; Deep learning; elderly population; greenspace; learning; mental health; NDVI; adult; aged; Article; China; controlled study; cross-sectional study; deep learning; environmental exposure; environmental monitoring; female; Geriatric Depression Scale; health care survey; human; image segmentation; land use; late life depression; major clinical study; male; mental health; population research; priority journal; remote sensing; vulnerable population; China; demography; depression; environment; middle aged; questionnaire; very elderly; Beijing [China]; China; Aged; Aged, 80 and over; Beijing; Deep Learning; Depression; Environment; Female; Humans; Male; Middle Aged; Residence Characteristics; Surveys and Questionnaires"
"Effective Building Extraction from High-Resolution Remote Sensing Images with Multitask Driven Deep Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057823197&doi=10.1109%2fLGRS.2018.2880986&partnerID=40&md5=cb44ede8b188fc4fd97761c86bb5c7be","Building extraction from high-resolution remote sensing images has widely been studied for its great significance in obtaining geographic information. Many methods based on deep learning have been tried for the task; however, there is still much to explore about designing layers or modules for remote sensing data and taking full use of the unique features of buildings like shape and boundary. In this letter, an end-to-end network architecture based on U-Net is proposed. The U-Net architecture is modified with Xception module for remote sensing images to extract effective features. Also, multitask learning is adopted to incorporate the structure information of buildings. Two standard data sets (Massachusetts building data set and Vaihingen Data set) of high-resolution remote sensing images are selected to test our model and it achieves state-of-the-art results. © 2004-2012 IEEE.","Building extraction; deep neural network; multitask learning; remote sensing image; Xception module","Buildings; Decoding; Deep neural networks; Extraction; Feature extraction; Image processing; Job analysis; Network architecture; Neural networks; Personnel training; Statistical tests; Building extraction; Multitask learning; Remote sensing images; Task analysis; Xception module; Remote sensing; artificial neural network; building; data set; extraction method; numerical model; remote sensing; software; Baden-Wurttemberg; Germany; Massachusetts; United States; Vaihingen an der Enz"
"Building change detection based on deep learning and belief function","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072025106&doi=10.1109%2fJURSE.2019.8808968&partnerID=40&md5=88afdb960631ed05104b0a70b77fe6ef","This paper proposes a new approach for building change detection using multioral satellite stereo data. This approach is composed of three main steps. Firstly the building probability map can be derived by a state-of-the-art deep learning approach. In the second step, a decision fusion based fusion model refines and fuses the building changes from satellite stereo imagery and the digital surface models (DSMs). In the last step, the building probability maps are further fused with the building change indicators to generate an improved change detection result. Experiments on the multioral data acquired over 5 years confirms the effectiveness of the proposed approach. © 2019 IEEE.","belief function; building probability map; Change detection; CNN","Buildings; Remote sensing; Satellite imagery; Stereo image processing; Uncertainty analysis; Belief function; Building change detection; Change detection; Change indicators; Digital surface models; Learning approach; Probability maps; State of the art; Deep learning"
"Research on scene classification method of high-resolution remote sensing images based on RFPNet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066506322&doi=10.3390%2fapp9102028&partnerID=40&md5=8473fd99774aa41a5336e95396fbc962","One of the challenges in the field of remote sensing is how to automatically identify and classify high-resolution remote sensing images. A number of approaches have been proposed. Among them, the methods based on low-level visual features and middle-level visual features have limitations. Therefore, this paper adopts the method of deep learning to classify scenes of high-resolution remote sensing images to learn semantic information. Most of the existing methods of convolutional neural networks are based on the existing model using transfer learning, while there are relatively few articles about designing of new convolutional neural networks based on the existing high-resolution remote sensing image datasets. In this context, this paper proposes a multi-view scaling strategy, a new convolutional neural network based on residual blocks and fusing strategy of pooling layer maps, and uses optimization methods to make the convolutional neural network named RFPNet more robust. Experiments on two benchmark remote sensing image datasets have been conducted. On the UC Merced dataset, the test accuracy, precision, recall, and F1-score all exceed 93%. On the SIRI-WHU dataset, the test accuracy, precision, recall, and F1-score all exceed 91%. Compared with the existing methods, such as the most traditional methods and some deep learning methods for scene classification of high-resolution remote sensing images, the proposed method has higher accuracy and robustness. © 2019 by the authors.","Convolutional neural network; Remote sensing images; ResNet; Scene classification; Semantic information; TensorFlow",
"Multi-scale semantic segmentation and spatial relationship recognition of remote sensing images based on an attention model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065713213&doi=10.3390%2frs11091044&partnerID=40&md5=8f31d6800c0ef8dbd36339f5db44de48","A comprehensive interpretation of remote sensing images involves not only remote sensing object recognition but also the recognition of spatial relations between objects. Especially in the case of different objects with the same spectrum, the spatial relationship can help interpret remote sensing objects more accurately. Compared with traditional remote sensing object recognition methods, deep learning has the advantages of high accuracy and strong generalizability regarding scene classification and semantic segmentation. However, it is difficult to simultaneously recognize remote sensing objects and their spatial relationship from end-to-end only relying on present deep learning networks. To address this problem, we propose a multi-scale remote sensing image interpretation network, called the MSRIN. The architecture of the MSRIN is a parallel deep neural network based on a fully convolutional network (FCN), a U-Net, and a long short-term memory network (LSTM). The MSRIN recognizes remote sensing objects and their spatial relationship through three processes. First, the MSRIN defines a multi-scale remote sensing image caption strategy and simultaneously segments the same image using the FCN and U-Net on different spatial scales so that a two-scale hierarchy is formed. The output of the FCN and U-Net are masked to obtain the location and boundaries of remote sensing objects. Second, using an attention-based LSTM, the remote sensing image captions include the remote sensing objects (nouns) and their spatial relationships described with natural language. Finally, we designed a remote sensing object recognition and correction mechanism to build the relationship between nouns in captions and object mask graphs using an attention weight matrix to transfer the spatial relationship from captions to objects mask graphs. In other words, the MSRIN simultaneously realizes the semantic segmentation of the remote sensing objects and their spatial relationship identification end-to-end. Experimental results demonstrated that the matching rate between samples and the mask graph increased by 67.37 percentage points, and the matching rate between nouns and the mask graph increased by 41.78 percentage points compared to before correction. The proposed MSRIN has achieved remarkable results. © 2019 by the authors.","Downscaling; Image caption; LSTM; Multi-scale; Remote sensing; Semantic segmentation; U-Net; Upscaling","Deep neural networks; Image segmentation; Long short-term memory; Object recognition; Semantics; Down-scaling; Image caption; LSTM; Multi-scale; Semantic segmentation; Upscaling; Remote sensing"
"Spatial-Temporal Fusion Algorithm for Remote Sensing Images Based on Multi-input Dense Connected Neural Network [基于多输入密集连接神经网络的 遥感图像时空融合算法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071579128&doi=10.16451%2fj.cnki.issn1003-6059.201905005&partnerID=40&md5=c0de19237b7c862a0d02c83899a88374","To solve the spatial-temporal fusion problem of images of surface reflectivity remote sensing satellites Landsat and MODIS, a spatial-temporal fusion algorithm for remote sensing images based on multi-input dense connected neural network is proposed. Firstly, a multi-input dense connected neural network is put forward to study the remote sensing images containing the difference information between continuous moments. Then, two transition images learned from the network are fused with the two known high spatial resolution images based on the difference similarity hypothesis to obtain the final predicted images. According to the fusion experiment of Landsat remote sensing images and MODIS remote sensing images, the proposed algorithm produces promising results in each quantitative index. The final predicted image by the proposed algorithm is more robust to noise with better recovered detail information. © 2019, Science Press. All right reserved.","Deep Learning; Dense Neural Network; Remote Sensing Image; Spatial-Temporal Fusion",
"Fully convolutional neural network with augmented atrous spatial pyramid pool and fully connected fusion path for high resolution remote sensing image segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067207591&doi=10.3390%2fapp9091816&partnerID=40&md5=3327f1425ae92dd47ea52feaecd3a15a","Recent developments in Convolutional Neural Networks (CNNs) have allowed for the achievement of solid advances in semantic segmentation of high-resolution remote sensing (HRRS) images. Nevertheless, the problems of poor classification of small objects and unclear boundaries caused by the characteristics of the HRRS image data have not been fully considered by previous works. To tackle these challenging problems, we propose an improved semantic segmentation neural network, which adopts dilated convolution, a fully connected (FC) fusion path and pre-trained encoder for the semantic segmentation task of HRRS imagery. The network is built with the computationally-efficient DeepLabv3 architecture, with added Augmented Atrous Spatial Pyramid Pool and FC Fusion Path layers. Dilated convolution enlarges the receptive field of feature points without decreasing the feature map resolution. The improved neural network architecture enhances HRRS image segmentation, reaching the classification accuracy of 91%, and the precision of recognition of small objects is improved. The applicability of the improved model to the remote sensing image segmentation task is verified. © 2019 by the authors.","Deep learning; Dilated convolution; Fully convolutional neural network; Remote sensing; Semantic segmentation",
"Integrating multitemporal Sentinel-1/2 data for coastal land cover classification using a multibranch convolutional neural network: A case of the Yellow River Delta","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065716294&doi=10.3390%2frs11091006&partnerID=40&md5=90923e351b24f55e22626cfd0441af1a","Coastal land cover classification is a significant yet challenging task in remote sensing because of the complex and fragmented nature of coastal landscapes. However, availability of multitemporal and multisensor remote sensing data provides opportunities to improve classification accuracy. Meanwhile, rapid development of deep learning has achieved astonishing results in computer vision tasks and has also been a popular topic in the field of remote sensing. Nevertheless, designing an effective and concise deep learning model for coastal land cover classification remains problematic. To tackle this issue, we propose a multibranch convolutional neural network (MBCNN) for the fusion of multitemporal and multisensor Sentinel data to improve coastal land cover classification accuracy. The proposed model leverages a series of deformable convolutional neural networks to extract representative features from a single-source dataset. Extracted features are aggregated through an adaptive feature fusion module to predict final land cover categories. Experimental results indicate that the proposed MBCNN shows good performance, with an overall accuracy of 93.78% and a Kappa coeffcient of 0.9297. Inclusion of multitemporal data improves accuracy by an average of 6.85%, while multisensor data contributes to 3.24% of accuracy increase. Additionally, the featured fusion module in this study also increases accuracy by about 2% when compared with the feature-stacking method. Results demonstrate that the proposed method can effectively mine and fuse multitemporal and multisource Sentinel data, which improves coastal land cover classification accuracy. © 2019 by the authors.","Convolutional neural networks; Data fusion; Land cover classification; Sentinel","Convolution; Data fusion; Deep learning; Neural networks; Remote sensing; Classification accuracy; Convolutional neural network; Land cover classification; Multi-temporal data; Multisensor remote sensing; Overall accuracies; Sentinel; Yellow River delta; Classification (of information)"
"Slum mapping in imbalanced remote sensing datasets using transfer learned deep features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072028076&doi=10.1109%2fJURSE.2019.8808965&partnerID=40&md5=5519b1b796b016cca32b6bf5e5991a51","Unprecedented urbanization, particularly in countries of the Global South, results in the formation of slums. Here, remote sensing has proven to be an extremely valuable and effective tool for mapping slums. Recent advances in transferring deep features learned in fully convolutional networks (FCN) allow the specific structural types and alignments of buildings in slums to be mapped. The class imbalance of slums is especially challenging in the context of intra-urban variability of slums themselves, and their possible similarity to other urban built-up structures. Thus, in our study we aim to analyze the transfer learning capabilities of FCNs for slum mapping with respect to training on imbalanced datasets and the quantity of available training images. When the slum sample proportion is increased an improvement of the Intersection over Union (IU) of 10% to 30% can be observed. Increasing the total number of images improves the IU up to 20% to 50%. Transfer learning proves extremely valuable in retrieving information on complex and heterogeneous urban structures such as slum patches. © 2019 IEEE.","deep learning; fully convolutional network; remote sensing; slums; transfer learning; urban poverty","Convolution; Deep learning; Housing; Image enhancement; Mapping; Built-up structures; Convolutional networks; Imbalanced Data-sets; slums; Structural type; Transfer learning; Urban poverty; Urban variability; Remote sensing"
"Large-scale building extraction in very high-resolution aerial imagery using Mask R-CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072021187&doi=10.1109%2fJURSE.2019.8808977&partnerID=40&md5=ebd4965c2a786b26d2e29e9c4925eee2","Urban areas are hotspots of complex and dynamic alterations of the Earth's surface. Using deep learning (DL) techniques in remote sensing applications can significantly contribute to document these tremendous changes. Open source building data at a very high level of detail are still scarce or incomplete for many regions, therefore, hindering research and policy to properly provide knowledge on urban structures. In this study, we use a convolutional neural network to extract buildings for the city of Santiago de Chile. We deploy the recently released Mask R-CNN and use a pretrained model (PM) which already has been trained with remote sensing imagery. We fine-tune PM with very high-resolution (VHR) airborne RGB images from our study region and generate the fine-tuned model (FM). To extend the number of training data, we test several data augmentation methods for training purposes and evaluate their performance in context of urban environments. We achieve highest overall accuracy of 92 % by using augmentations and the generated FM. Our findings encourage to use DL methods in the urban context. The presented method can be adapted and applied to other global urban regions, and, help to overcome lacks in open source building data to assess urban environments. © 2019 IEEE.","aerial images; building extraction; classification; deep learning; urban; very high-resolution","Aerial photography; Antennas; Buildings; Classification (of information); Deep learning; Extraction; Neural networks; Urban planning; Aerial images; Building extraction; Convolutional neural network; Large scale buildings; Remote sensing applications; Remote sensing imagery; urban; Very high resolution; Remote sensing"
"A novel cost function for despeckling using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072050313&doi=10.1109%2fJURSE.2019.8809042&partnerID=40&md5=9a8c6c60d5b0c14b4ce7e543068aa172","Removing speckle noise from SAR images is still an open issue. It is well know that the interpretation of SAR images is very challenging and despeckling algorithms are necessary to improve the ability of extracting information. An urban environment makes this task more heavy due to different structures and to different objects scale. Following the recent spread of deep learning methods related to several remote sensing applications, in this work a convolutional neural networks based algorithm for despeckling is proposed. The network is trained on simulated SAR data. The paper is mainly focused on the implementation of a cost function that takes account of both spatial consistency of image and statistical properties of noise. © 2019 IEEE.","cnn; deep learning; despeckling; SAR; speckle","Convolution; Cost functions; Deep learning; Image enhancement; Neural networks; Radar imaging; Speckle; Synthetic aperture radar; Convolutional neural network; De-speckling; Different structure; Extracting information; Remote sensing applications; Spatial consistency; Statistical properties; Urban environments; Remote sensing"
"A robust image zero-watermarking using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068443113&doi=10.1109%2fIWBF.2019.8739245&partnerID=40&md5=1d5890309fd125ea48a97fb9c2be96ce","In the image zero-watermarking techniques, a watermark sequence is not physically embedded into the host image but has a logical linkage with the host image. This property of zero-watermarking is desirable for some kinds of images in which a minimum distortion may cause serious detection or diagnostic errors, such as medical images and remote sensing images. In this paper, we propose a robust zero-watermarking algorithm based on the Convolutional Neural Networks (CNN) and deep learning algorithm, in which robust inherent features of image is generated by the CNN, and it is combined with the owner's watermark sequence using XOR operation. The experimental results show the watermark robustness against several attacks and common image processing. © 2019 IEEE.","Convolutional Neural Networks; Deep Learning; Robust Features; Zero-watermarking","Biometrics; Convolution; Deep learning; Deep neural networks; Diagnosis; Learning algorithms; Medical imaging; Neural networks; Remote sensing; Watermarking; Convolutional neural network; Diagnostic errors; Minimum distortions; Remote sensing images; Robust Features; Watermark robustness; Watermark sequences; Zero-watermarking; Image watermarking"
"Exploring semantic elements for urban scene recognition: Deep integration of high-resolution imagery and OpenStreetMap (OSM)","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063528424&doi=10.1016%2fj.isprsjprs.2019.03.019&partnerID=40&md5=effa59fd1cb8aa1075f8973900a5b0ae","Urban scenes refer to city blocks which are basic units of megacities, they play an important role in citizens’ welfare and city management. Remote sensing imagery with largescale coverage and accurate target descriptions, has been regarded as an ideal solution for monitoring the urban environment. However, due to the heterogeneity of remote sensing images, it is difficult to access their geographical content at the object level, let alone understanding urban scenes at the block level. Recently, deep learning-based strategies have been applied to interpret urban scenes with remarkable accuracies. However, the deep neural networks require a substantial number of training samples which are hard to satisfy, especially for high-resolution images. Meanwhile, the crowed-sourced Open Street Map (OSM) data provides rich annotation information about the urban targets but may encounter the problem of insufficient sampling (limited by the places where people can go). As a result, the combination of OSM and remote sensing images for efficient urban scene recognition is urgently needed. In this paper, we present a novel strategy to transfer existing OSM data to high-resolution images for semantic element determination and urban scene understanding. To be specific, the object-based convolutional neural network (OCNN) can be utilized for geographical object detection by feeding it rich semantic elements derived from OSM data. Then, geographical objects are further delineated into their functional labels by integrating points of interest (POIs), which contain rich semantic terms, such as commercial or educational labels. Lastly, the categories of urban scenes are easily acquired from the semantic objects inside. Experimental results indicate that the proposed method has an ability to classify complex urban scenes. The classification accuracies of the Beijing dataset are as high as 91% at the object-level and 88% at the scene level. Additionally, we are probably the first to investigate the object level semantic mapping by incorporating high-resolution images and OSM data of urban areas. Consequently, the method presented is effective in delineating urban scenes that could further boost urban environment monitoring and planning with high-resolution images. © 2019","Data fusion; Deep learning; High-resolution imagery; OpenStreetMap (OSM); Semantic classification; Urban scene recognition","Classification (of information); Computer software maintenance; Data fusion; Deep learning; Deep neural networks; Neural networks; Object detection; Semantics; Urban planning; Classification accuracy; Convolutional neural network; High resolution imagery; OpenStreetMap (OSM); Remote sensing imagery; Remote sensing images; Semantic classification; Urban scenes; Remote sensing; accuracy assessment; artificial neural network; crowdsourcing; data set; environmental monitoring; exploration; heterogeneity; image analysis; image classification; megacity; recognition; remote sensing; sampling; satellite imagery; spectral resolution; urban area; Beijing [China]; China"
"ES-CNN: An end-to-end siamese convolutional neural network for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072037808&doi=10.1109%2fJURSE.2019.8808991&partnerID=40&md5=81f78fdc754fcc9d8e6b2c317e9d2d66","In recent years, deep learning-based methods have achieved great success in remote sensing image analysis. However, especially in the context of hyperspectral image classification, there is still a lack of labelled samples to feed those data-hungry deep models. To augment the amount of input data, models operate on pixel-pairs have been proposed and Siamese convolutional neural network (S-CNN) is a typical one. S-CNN is used as a pixel-pair feature extractor and an additional classifier like SVM is required. In this paper, we propose an end-to-end version of S-CNN. Taking advantage of the pairwise input and to make better use of spatial information, a voting strategy using neighbouring pixels is also employed to determine the final class label of the centre pixel. Experimental results on real hyperspectral datasets show that the proposed method outperforms the original S-CNN by a considerable margin. © 2019 IEEE.","deep learning; hyperspectral image classification; pixel pair; Siamese neural network; voting strategy","Convolution; Deep learning; Deep neural networks; Hyperspectral imaging; Information use; Neural networks; Pixels; Remote sensing; Spectroscopy; Support vector machines; Class labels; Convolutional neural network; Feature extractor; HyperSpectral; Learning-based methods; Remote sensing images; Spatial informations; Voting strategies; Image classification"
"Road extraction by using atrous spatial pyramid pooling integrated encoder-decoder network and structural similarity loss","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065727861&doi=10.3390%2frs11091015&partnerID=40&md5=b2ae9b4ad560492bbb1a2b11f93ca721","The technology used for road extraction from remote sensing images plays an important role in urban planning, traffic management, navigation, and other geographic applications. Although deep learning methods have greatly enhanced the development of road extractions in recent years, this technology is still in its infancy. Because the characteristics of road targets are complex, the accuracy of road extractions is still limited. In addition, the ambiguous prediction of semantic segmentation methods also makes the road extraction result blurry. In this study, we improved the performance of the road extraction network by integrating atrous spatial pyramid pooling (ASPP) with an Encoder-Decoder network. The proposed approach takes advantage of ASPP's ability to extract multiscale features and the Encoder-Decoder network's ability to extract detailed features. Therefore, it can achieve accurate and detailed road extraction results. For the first time, we utilized the structural similarity (SSIM) as a loss function for road extraction. Therefore, the ambiguous predictions in the extraction results can be removed, and the image quality of the extracted roads can be improved. The experimental results using the Massachusetts Road dataset show that our method achieves an F1-score of 83.5% and an SSIM of 0.893. Compared with the normal U-net, our method improves the F1-score by 2.6% and the SSIM by 0.18. Therefore, it is demonstrated that the proposed approach can extract roads from remote sensing images more effectively and clearly than the other compared methods. © 2019 by the authors.","Deep learning; Remote sensing; Road extraction; Semantic segmentation; Structural similarity","Decoding; Deep learning; Extraction; Feature extraction; Image enhancement; Remote sensing; Roads and streets; Semantics; Learning methods; Multi-scale features; Remote sensing images; Road extraction; Semantic segmentation; Spatial pyramids; Structural similarity; Traffic management; Network coding"
"A stacked fully convolutional networks with feature alignment framework for multi-label land-cover segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065723767&doi=10.3390%2frs11091051&partnerID=40&md5=bc7a5c7900742cf3274b6262ca9abc16","Applying deep-learning methods, especially fully convolutional networks (FCNs), has become a popular option for land-cover classification or segmentation in remote sensing. Compared with traditional solutions, these approaches have shown promising generalization capabilities and precision levels in various datasets of different scales, resolutions, and imaging conditions. To achieve superior performance, a lot of research has focused on constructing more complex or deeper networks. However, using an ensemble of different fully convolutional models to achieve better generalization and to prevent overfitting has long been ignored. In this research, we design four stacked fully convolutional networks (SFCNs), and a feature alignment framework for multi-label land-cover segmentation. The proposed feature alignment framework introduces an alignment loss of features extracted from basic models to balance their similarity and variety. Experiments on a very high resolution(VHR) image dataset with six categories of land-covers indicates that the proposed SFCNs can gain better performance when compared to existing deep learning methods. In the 2nd variant of SFCN, the optimal feature alignment gains increments of 4.2% (0.772 vs. 0.741), 6.8% (0.629 vs. 0.589), and 5.5% (0.727 vs. 0.689) for its f1-score, jaccard index, and kappa coefficient, respectively. © 2019 by the authors.","Ensemble learing; Feature alignment; Fully convolutional networks; Image segmentation; Land-cover classification","Convolution; Deep learning; Image segmentation; Remote sensing; Convolutional model; Convolutional networks; Ensemble learing; Feature alignment; Generalization capability; Imaging conditions; Land cover classification; Very high resolution (VHR) image; Alignment"
"A study on growth stage classification of paddy rice by CNN using NDVI images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073879868&doi=10.1109%2fCCC.2019.000-4&partnerID=40&md5=b9cd379f4c2c55375ebe2b07b825d7d3","It is difficult for young farmers to inherit technics from skilled farmers, since the population of farmers is decreasing, and their average age is increasing in Japan. Under this background, it is expected that technology for managing rice fields with little effort using sensor or robot technology. In this study, we selected drone as the platform of remote sensing since a drone is useful for precision farming. The purpose of this study is to support farmers by estimating growth stage of paddy rice by using Deep Learning (CNN) and NDVI (Normalized Difference Vegetation Index) images. In this paper, we compared growth stage classification accuracy using images taken at the different height of the drone. © 2019 IEEE.","Growth Stage; NDVI; Paddy Rice; Remote Sensing; UAV","Agriculture; Deep learning; Drones; Image classification; Unmanned aerial vehicles (UAV); Classification accuracy; Different heights; Growth stages; NDVI; Normalized difference vegetation index; Paddy rice; Precision farming; Robot technology; Remote sensing"
"Dictionaries of deep features for land-use scene classification of very high spatial resolution images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059180070&doi=10.1016%2fj.patcog.2018.12.019&partnerID=40&md5=64132aa3e54cc0057785576e3275d9ae","Land-use classification in very high spatial resolution images is critical in the remote sensing field. Consequently, remarkable efforts have been conducted towards developing increasingly accurate approaches for this task. In recent years, deep learning has emerged as a dominant paradigm for machine learning, and methodologies based on deep convolutional neural networks have received particular attention from the remote sensing community. These methods typically utilize transfer learning and/or data augmentation to accommodate a small number of labeled images in the publicly available datasets in this field. However, they typically require powerful computers and/or a long time for training. In this work, we propose a simple and novel method for land-use classification in very high spatial resolution images, which efficiently combines transfer learning with a sparse representation. Specifically, the proposed method performs the classification of land-use scenes using a modified version of the well-known sparse representation-based classification method. While this method directly uses the training images to form dictionaries, which are employed to classify test images, our method utilizes a pre-trained deep convolutional neural network and the Gaussian mixture model to generate more robust and compact “dictionaries of deep features.” The effectiveness of the proposed method was evaluated on two publicly available datasets: UC Merced and Brazilian Cerrado–Savana. The experimental results suggest that our method can potentially outperform state-of-the-art techniques for land-use classification in very high spatial resolution images. © 2018 Elsevier Ltd","Deep learning; Dictionary learning; Feature learning; Land-use classification; Sparse representation","Convolution; Deep learning; Deep neural networks; Gaussian distribution; Image classification; Image resolution; Land use; Neural networks; Remote sensing; Deep convolutional neural networks; Dictionary learning; Feature learning; Landuse classifications; Sparse representation; Sparse representation based classifications; State-of-the-art techniques; Very high spatial resolution images; Classification (of information)"
"What data are needed for semantic segmentation in earth observation?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072024717&doi=10.1109%2fJURSE.2019.8809071&partnerID=40&md5=5ec2db1b1d2cffc8b6bf6e026dde8e6e","This paper explores different aspects of semantic segmentation of remote sensing data using deep neural networks. Learning with deep neural networks was revolutionized by the creation of ImageNet. Remote sensing benefited of these new techniques, however Earth Observation (EO) datasets remain small in comparison. In this work, we investigate how we can progress towards the ImageNet of remote sensing. In particular, two questions are addressed in this paper. First, how robust are existing supervised learning strategies with respect to data volume? Second, which properties are expected from a large-scale EO dataset? The main contributions of this work are: (i) a strong robustness analysis of existing supervised learning strategies with respect to remote sensing data, (ii) the introduction of a new, large-scale dataset named MiniFrance. © 2019 IEEE.","Deep Learning; Land Use/Land Cover Mapping; Semantic Segmentation; Supervised Learning","Deep learning; Deep neural networks; Land use; Large dataset; Machine learning; Observatories; Semantics; Supervised learning; Data volume; Earth observations; Land use/land cover; Large-scale dataset; Remote sensing data; Semantic segmentation; Strong robustness; Remote sensing"
"A comparative study of texture and convolutional neural network features for detecting collapsed buildings after earthquakes using pre- and post-event satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066730247&doi=10.3390%2frs11101202&partnerID=40&md5=6d9dd2115cb0192391258d48b579a22c","The accurate and quick derivation of the distribution of damaged building must be considered essential for the emergency response. With the success of deep learning, there is an increasing interest to apply it for earthquake-induced building damage mapping, and its performance has not been compared with conventional methods in detecting building damage after the earthquake. In the present study, the performance of grey-level co-occurrence matrix texture and convolutional neural network (CNN) features were comparatively evaluated with the random forest classifier. Preand post-event very high-resolution (VHR) remote sensing imagery were considered to identify collapsed buildings after the 2010 Haiti earthquake. Overall accuracy (OA), allocation disagreement (AD), quantity disagreement (QD), Kappa, user accuracy (UA), and producer accuracy (PA) were used as the evaluation metrics. The results showed that the CNN feature with random forest method had the best performance, achieving an OA of 87.6% and a total disagreement of 12.4%. CNNs have the potential to extract deep features for identifying collapsed buildings compared to the texture feature with random forest method by increasing Kappa from 61.7% to 69.5% and reducing the total disagreement from 16.6% to 14.1%. The accuracy for identifying buildings was improved by combining CNN features with random forest compared with the CNN approach. OA increased from 85.9% to 87.6%, and the total disagreement reduced from 14.1% to 12.4%. The results indicate that the learnt CNN features can outperform texture features for identifying collapsed buildings using VHR remotely sensed space imagery. © 2019 by the authors.","CNN; Convolutional neural network; Earthquake; Grey-level co-occurrence matrix texture; Random forest","Convolution; Damage detection; Decision trees; Deep learning; Earthquakes; Neural networks; Remote sensing; Satellite imagery; Textures; Conventional methods; Convolutional neural network; Grey level co-occurrence matrixes; Random forest classifier; Random forest methods; Random forests; Remote sensing imagery; Very high resolution; Buildings"
"Mapping human settlements with multi-seasonal sentinel-2 imagery and attention-based resnext","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072024145&doi=10.1109%2fJURSE.2019.8809009&partnerID=40&md5=e1331bbedca7f6b733aaf0d485702e50","This paper explores the potential of multi-spectral Sentinel-2 imagery for human settlement mapping, using deep learning based methods. We show first results of a study area in central Europe, with an attention-based ResNeXt to better exploit the spectral information. Reasonable mapping accuracy has been achieved, compared to the state-of-the-art products. Based on the results and comparison with the existing products, we discuss two interesting questions: How can human settlement mapping be made consistent with or complementary to the existing human settlement maps and how can further improvement in human settlement mapping be achieved by exploring deep learning-based approaches. © 2019 IEEE.","attention; classification; convolutional neural network (CNN); human settlement (HS) mapping; Sentinel-2","Classification (of information); Deep learning; Neural networks; Remote sensing; attention; Convolutional neural network; Human settlements; Learning-based approach; Learning-based methods; Mapping accuracy; Sentinel-2; Spectral information; Photomapping"
"Towards combining satellite imagery and VGI for urban LULC classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072036616&doi=10.1109%2fJURSE.2019.8808966&partnerID=40&md5=98ce70c5b8451a1fd5675320e6b524e8","In this work we introduce and evaluate a deep learning model, mbCNN, that combines together satellite imagery and Volunteer Geographical Information (VGI) data to deal with different types of built-up surfaces. Differently from most of the previous works that only consider Urban/Non-Urban settings involving only one urban LULC class, here, we investigate the possibility to go a step further and distinguish among several urban land use classes: Residential, industrial, sport fields and non-urban. Experiments on a real-world dataset covering the City of Montpellier (South of France) site are reported. Such results demonstrate the quality of Deep Learning approaches to deal with several types of Urban LULC mapping as well as the positive influence to integrate VGI knowledge in the process. © 2019 IEEE.","Deep Learning; Urban LULC Classification; VGI","Image classification; Land use; Remote sensing; Satellite imagery; Learning approach; Learning models; Montpellier; Real-world; Urban land use; Volunteer geographical informations; Deep learning"
"Adaptive Brightness Learning for Active Object Recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068972767&doi=10.1109%2fICASSP.2019.8682681&partnerID=40&md5=cc292a9e76156fbf9f12404461f4467c","State-of-the-art object detection methods based on deep learning achieved promising performances in recent years. However, the performances are limited by the passive nature of the traditional object recognition framework in ignoring the relationship between imaging configuration and recognition performance as well as the importance of recognition performance feedback for improving image quality. To address the above limitations, an active object recognition method based on reinforcement learning is proposed in this paper by taking adaptive brightness adjustment as an example. Progressive brightness adjustment strategy is learned by maximizing recognition performance on reference high-quality training samples. With the help of active object recognition and brightness adjustment strategy, low-quality images can be converted into high-quality images, and overall performances are improved without retraining the detector. © 2019 IEEE.","deep learning; deep reinforcement learning; object recognition; remote sensing images","Audio signal processing; Deep learning; Image enhancement; Luminance; Machine learning; Object detection; Reinforcement learning; Remote sensing; Speech communication; Brightness adjustments; High quality images; Imaging configurations; Low qualities; Object detection method; Performance feedback; Remote sensing images; State of the art; Object recognition"
"CoinNet: Copy Initialization Network for Multispectral Imagery Semantic Segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058082701&doi=10.1109%2fLGRS.2018.2880756&partnerID=40&md5=cb70558617b13ee36f6483ef33324cb8","Remote sensing imagery semantic segmentation refers to assigning a label to every pixel. Recently, deep convolutional neural networks (CNNs)-based methods have presented an impressive performance in this task. Due to the lack of sufficient labeled remote sensing images, researchers usually utilized transfer learning (TL) strategies to fine tune networks which were pretrained in huge RGB-scene data sets. Unfortunately, this manner may not work if the target images are multispectral/hyperspectral. The basic assumption of TL is that the low-level features extracted by the former layers are similar in most data sets, hence users only require to train the parameters in the last layers that are specific to different tasks. However, if one should use a pretrained deep model in RGB data for multispectral /hyperspectral imagery semantic segmentation, the structure of the input layer has to be adjusted. In this case, the first convolutional layer has to be trained using the multispectral /hyperspectral data sets which are much smaller. Apparently, the feature representation ability of the first convolutional layer will decrease and it may further harm the following layers. In this letter, we propose a new deep learning model, COpy INitialization Network (CoinNet), for multispectral imagery semantic segmentation. The major advantage of CoinNet is that it can make full use of the initial parameters in the pretrained network's first convolutional layer. Comparison experiments on a challenging multispectral data set have demonstrated the effectiveness of the proposed improvement. The demo and a trained network will be published in our homepage. © 2004-2012 IEEE.","CoinNet; deep convolutional network; semantic segmentation; transfer learning (TL)","Convolution; Deep neural networks; Neural networks; Remote sensing; Semantic Web; Semantics; CoinNet; Convolutional networks; Deep convolutional neural networks; Feature representation; Multi-spectral imagery; Remote sensing imagery; Semantic segmentation; Transfer learning; Color image processing; artificial neural network; computer; data set; image analysis; pixel; remote sensing; satellite imagery; segmentation"
"Efficient Convolutional Neural Network Weight Compression for Space Data Classification on Multi-fpga Platforms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068995792&doi=10.1109%2fICASSP.2019.8682732&partnerID=40&md5=a93d4d3121e8a09b91eea25c4ff10eb2","Convolutional Neural Networks (CNNs) represent the cutting edge in signal analysis tasks like classification and regression. Realization of such architectures in hardware capable of performing high throughput computations, with minimal energy consumption, is a key enabling factor towards the proliferation of analysis immediately after acquisition. Our driving problem is a satellite-based remote sensing platform in which onboard signal processing and classification tasks must take place, given strict bandwidth and energy limitations. In this work, we exploit the implementation of a CNN on Field Programmable Gate Array (FPGA) platforms and explore different ways to minimize the impact of different hardware restrictions to performance. We compare our results against competing technologies such as Graphics Processing Units (GPU) in terms of throughput, latency and energy consumption. In actual experimental runs we demonstrate competitive latency and throughput of the FPGA platform vs. GPU technology at an order-of-magnitude energy savings, which is especially important for space-borne computing. © 2019 IEEE.","Convolutional Neural Networks; Field Programmable Gate Arrays; Hardware for Deep Learning; Remote Sensing; Space Data","Audio signal processing; Computer graphics; Convolution; Deep neural networks; Energy conservation; Energy utilization; Field programmable gate arrays (FPGA); Logic gates; Neural networks; Program processors; Remote sensing; Signal receivers; Space optics; Space platforms; Speech communication; Classification tasks; Competing technologies; Convolutional neural network; Energy limitations; Hardware restrictions; High throughput; Remote sensing platforms; Space data; Graphics processing unit"
"Land-Use Detection Using Residual Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078017964&doi=10.1109%2fICASERT.2019.8934607&partnerID=40&md5=85b9faa93c6d1e078090a3d372a8c75d","Land-Use detection is an important application in the context of remote sensing. This objective is to classify, a chunk of a satellite or high-earth orbit image of the earth, as the type of land-use. In this work, we have used recent advancement in computer vision with deep learning to improve the performance to the classification task. We have used a very deep convolutional neural network with the residual connection as a building block and the concept of transfer learning to train this model. The experiment shows that the performance of the classifier is very high with transfer learning on a pre-trained model rather than training from the sketch with such small dataset that is available for land use classification task. Our proposed approach generated a better result than the previously existing method in benchmark dataset. © 2019 IEEE.","Convolutional Neural Network(CNN); Fine Tuning; Land-Use Classification; Transfer Learning","Classification (of information); Convolution; Deep neural networks; Neural networks; Orbits; Remote sensing; Robotics; Benchmark datasets; Building blockes; Classification tasks; Convolutional neural network; Fine tuning; High earth orbits; Landuse classifications; Transfer learning; Land use"
"Self-supervised convolutional neural networks for plant reconstruction using stereo imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072813602&doi=10.14358%2fPERS.85.5.389&partnerID=40&md5=074e64a83cfba0dcd33029414967c036","Stereo matching can provide complete and dense threedimensional reconstruction to study plant growth. Recently, high-quality stereo matching results were achieved combining Semi-Global Matching (SGM) with deep learning. However, due to a lack of suitable training data, this technique is not readily applicable for plant reconstruction. We propose a self-supervised Matching Cost with a Convolutional Neural Network (MC-CNN) scheme to calculate matching cost and test it for plant reconstruction. The MC-CNN network is retrained using the initial matching results obtained from the standard MC-CNN weights. For the experiment, closerange photogrammetric imagery of an in-house plant is used. The results show that the performance of self-supervised MC-CNN is superior to the Census algorithm and comparable to MC-CNN trained by a Light Detection and Ranging point cloud. Another experiment is performed using stereo imagery of a field beech tree. The proposed self-training strategy is tested and has proved capable of identifying the drought condition of trees from the reconstructed leaves. © 2019 American Society for Photogrammetry and Remote Sensing.",,"Convolution; Deep learning; Forestry; Image reconstruction; Neural networks; Optical radar; Plants (botany); Census algorithms; Close-range photogrammetric; Convolutional neural network; Drought conditions; Light detection and ranging; Plant reconstruction; Semi-global matching; Three-dimensional reconstruction; Stereo image processing; algorithm; artificial neural network; digital photogrammetry; drought stress; image analysis; reconstruction; stereo image; supervised classification; Fagus"
"Deep built-structure counting in satellite imagery using attention based re-weighting","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063910727&doi=10.1016%2fj.isprsjprs.2019.03.014&partnerID=40&md5=e9762f4ed7071f04f6f68ac101e886c4","In this paper, we attempt to address the challenging problem of counting built-structures in the satellite imagery. Building density is a more accurate estimate of the population density, urban area expansion and its impact on the environment, than the built-up area segmentation. However, building shape variances, overlapping boundaries, and variant densities make this a complex task. To tackle this difficult problem, we propose a deep learning based regression technique for counting built-structures in satellite imagery. Our proposed framework intelligently combines features from different regions of satellite image using attention based re-weighting techniques. Multiple parallel convolutional networks are designed to capture information at different granulates. These features are combined into the FusionNet which is trained to weigh features from different granularity differently, allowing us to predict a precise building count. To train and evaluate the proposed method, we put forward a new large-scale and challenging built-structure-count dataset. Our dataset is constructed by collecting satellite imagery from diverse geographical areas (planes, urban centers, deserts, etc.,) across the globe (Asia, Europe, North America, and Africa) and captures the wide density of built structures. Detailed experimental results and analysis validate the proposed technique. FusionNet has Mean Absolute Error of 3.65 and R-squared measure of 88% over the testing data. Finally, we perform the test on the 274.3×10 3 m 2 of the unseen region, with the error of 19 buildings off the 656 buildings in that area. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Attention based re-weighting; Building count; Built-up area segmentation; Deep learning; Land use; Regression","Arid regions; Buildings; Deep learning; Land use; Large dataset; Population statistics; Built-up areas; Convolutional networks; Different granularities; Impact on the environment; Population densities; Re-weighting; Regression; Regression techniques; Satellite imagery; accuracy assessment; artificial neural network; data set; environmental impact assessment; estimation method; experimental study; land use change; machine learning; population density; regression analysis; satellite imagery; segmentation; testing method; urban area; Africa; Asia; Europe; North America"
"Detection of informal graveyards in lima using fully convolutional network with VHR images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072020235&doi=10.1109%2fJURSE.2019.8808983&partnerID=40&md5=53b339ab4eb82641cab4363d158f0c14","Lima is facing rapid urban growth, including a rapid expansion of informal areas, mainly taking place within three peripheral cones. Most of the studies on that subject focused in general on informal settlements. Yet in this paper, we focus on two different informal types, graveyards and housing. They are experiencing complex, intertwined development dynamics due to a lack of land for housing and burials, causing social and public health problems. Housing invasions on burial grounds have never been systematically investigated. Yet, while challenging due to their morphological similarity, the detection of boundaries between graveyards and neighbouring and sometimes invading informal housing is essential, e.g., to prevent the spread of diseases. This study aims to distinguish those similar urban structures of which the visual features are very alike (e.g., rectangular shapes, same colours, organic organization). We used state-of-the-art Fully Convolutional Networks (FCNs) with dilated convolution of increasing spatial kernels to acquire features of deep level of abstraction on Pleiades satellites images. We found that such neural networks can reach a good level in mapping both informal developments with a F1-score of 0.819. Effective monitoring of such developments is important to inform planning and decision-making processes to allow interventions at critical locations. © 2019 IEEE.","barriadas; deep learning; Fully Convolutional Networks (FCNs); informal graveyards; informal settlements; Lima; urban remote sensing","Convolution; Decision making; Deep learning; Housing; Remote sensing; barriadas; Convolutional networks; informal graveyards; Informal settlements; Lima; Urban remote sensing; Urban growth"
"Novelty detection in very high resolution urban scenes with Density Forests","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072045155&doi=10.1109%2fJURSE.2019.8808974&partnerID=40&md5=1b6dca9fc1236a7d647ef8c0967df6a0","Uncertainty in deep learning has recently received a lot of attention. While deep neural networks have shown better accuracy than other competing methods in many benchmarks, it has been shown that they may yield wrong predictions with unreasonably high confidence. This has increased the interest in methods that help providing better confidence estimates in neural networks, some using specifically designed architectures with probabilistic building blocks, and others using a standard architecture with an additional confidence estimation step based on its output. This work proposes a confidence estimation method for Convolutional Neural Networks based on fitting a forest of randomized density estimation decision trees to the network activations before the final classification layer and compares it to other confidence estimation methods based on standard architectures. The methods are compared on a semantic labelling dataset with very high resolution satellite imagery. Our results show that methods based on intermediate network activations lead to better confidence estimates in novelty detection, i.e., in the discovery of classes that are not present in the training set. © 2019 IEEE.","Convolutional Neural Networks; Density Forest; land cover; novelty detection; Uncertainty","Chemical activation; Chemical detection; Convolution; Decision trees; Deep neural networks; Forestry; Network architecture; Remote sensing; Satellite imagery; Semantics; Confidence estimation; Convolutional neural network; Intermediate networks; Land cover; Novelty detection; Standard architecture; Uncertainty; Very high resolution satellite imagery; Multilayer neural networks"
"Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063094902&doi=10.1016%2fj.isprsjprs.2019.02.019&partnerID=40&md5=d8962e3df5fd08001cfe135f9e08e34b","Automated extraction of buildings from remotely sensed data is important for a wide range of applications but challenging due to difficulties in extracting semantic features from complex scenes like urban areas. The recently developed fully convolutional neural networks (FCNs) have shown to perform well on urban object extraction because of the outstanding feature learning and end-to-end pixel labeling abilities. The commonly used feature fusion or skip-connection refine modules of FCNs often overlook the problem of feature selection and could reduce the learning efficiency of the networks. In this paper, we develop an end-to-end trainable gated residual refinement network (GRRNet) that fuses high-resolution aerial images and LiDAR point clouds for building extraction. The modified residual learning network is applied as the encoder part of GRRNet to learn multi-level features from the fusion data and a gated feature labeling (GFL) unit is introduced to reduce unnecessary feature transmission and refine classification results. The proposed model - GRRNet is tested in a publicly available dataset with urban and suburban scenes. Comparison results illustrated that GRRNet has competitive building extraction performance in comparison with other approaches. The source code of the developed GRRNet is made publicly available for studies. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Building extraction; Convolutional neural networks; Deep learning; Image classification; Semantic segmentation","Antennas; Buildings; Convolution; Deep learning; Extraction; Image classification; Image segmentation; Neural networks; Optical radar; Semantics; Automated extraction; Automatic building extraction; Building extraction; Classification results; Convolutional neural network; High-resolution aerial images; Remotely sensed data; Semantic segmentation; Data mining; aerial photography; artificial neural network; automation; building; image analysis; image classification; lidar; photogrammetry; pixel; supervised learning"
"A new fully convolutional neural network for semantic segmentation of polarimetric SAR imagery in complex land cover ecosystem","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063541232&doi=10.1016%2fj.isprsjprs.2019.03.015&partnerID=40&md5=b58bf39f740392f99d377e5c89267b7f","Despite the application of state-of-the-art fully Convolutional Neural Networks (CNNs) for semantic segmentation of very high-resolution optical imagery, their capacity has not yet been thoroughly examined for the classification of Synthetic Aperture Radar (SAR) images. The presence of speckle noise, the absence of efficient feature expression, and the limited availability of labelled SAR samples have hindered the application of the state-of-the-art CNNs for the classification of SAR imagery. This is of great concern for mapping complex land cover ecosystems, such as wetlands, where backscattering/spectrally similar signatures of land cover units further complicate the matter. Accordingly, we propose a new Fully Convolutional Network (FCN) architecture that can be trained in an end-to-end scheme and is specifically designed for the classification of wetland complexes using polarimetric SAR (PolSAR) imagery. The proposed architecture follows an encoder-decoder paradigm, wherein the input data are fed into a stack of convolutional filters (encoder) to extract high-level abstract features and a stack of transposed convolutional filters (decoder) to gradually up-sample the low resolution output to the spatial resolution of the original input image. The proposed network also benefits from recent advances in CNN designs, namely the addition of inception modules and skip connections with residual units. The former component improves multi-scale inference and enriches contextual information, while the latter contributes to the recovery of more detailed information and simplifies optimization. Moreover, an in-depth investigation of the learned features via opening the black box demonstrates that convolutional filters extract discriminative polarimetric features, thus mitigating the limitation of the feature engineering design in PolSAR image processing. Experimental results from full polarimetric RADARSAT-2 imagery illustrate that the proposed network outperforms the conventional random forest classifier and the state-of-the-art FCNs, such as FCN-32s, FCN-16s, FCN-8s, and SegNet, both visually and numerically for wetland mapping. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional Neural Network (CNN); Deep learning; Encoder-decoder; Fully Convolutional Network (FCN); Land cover; Polarimetric Synthetic Aperture Radar (PolSAR); Wetland","Complex networks; Convolution; Decision trees; Decoding; Deep learning; Ecosystems; Image segmentation; Mapping; Network architecture; Neural networks; Optical data processing; Polarimeters; Semantic Web; Semantics; Signal encoding; Synthetic aperture radar; Wetlands; Convolutional networks; Convolutional neural network; Encoder-decoder; Land cover; Polarimetric synthetic aperture radars; Radar imaging; artificial neural network; backscatter; image classification; land cover; machine learning; radar imagery; RADARSAT; satellite imagery; segmentation; spatial resolution; synthetic aperture radar; terrestrial ecosystem; vegetation mapping; wetland"
"Classification Method of Grassland Species Based on Unmanned Aerial Vehicle Remote Sensing and Convolutional Neural Network [基于无人机遥感与卷积神经网络的草原物种分类方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066833471&doi=10.6041%2fj.issn.1000-1298.2019.04.021&partnerID=40&md5=4c7cd55aa9f1f7270e874bc285d2aa74","Grassland degradation is an ecological problem facing the world. Investigating the species composition and species distribution of grassland is extremely important for judging the degradation process of grassland. At present, satellite remote sensing technology is difficult to meet the requirements of grassland species level classification due to the limitation of spatial resolution. Unmanned aerial vehicle (UAV) hyperspectral remote sensing technology provides images of centimeter level spatial resolution and nanoscale spectral resolution required for grassland species classification. Based on the UAV hyperspectral imaging remote sensing system, the hyperspectral image data of low and mixed growth desert grassland degradation indicator species were collected in the 400~1 000 nm spectral range. Flight experiments were carried out at the flowering, fruiting and yellow blight periods of the degraded indicator species. The flying height was 30 m and the ground resolution of the hyperspectral image was about 2.3 cm. Based on the combination of feature bands extraction and deep learning convolutional neural network (CNN), a method for classification of desert grassland species was proposed. The recommended phenological phase of species classification of desert grassland in central Inner Mongolia, China, was given in combination with plant phenology. The overall classification accuracy and Kappa coefficient reached 94% and 0. 91, respectively. The results showed that the UAV hyperspectral imaging remote sensing technology and deep CNN can better classify the indicator species of desert grassland degradation. Compared with the support vector machine based on radial basis kernel function and the deep CNN based on principal component analysis, the deep CNN classification based on feature bands selection had the best effect and the highest classification accuracy. The method of CNN and the low-altitude remote sensing of UAV equipped with hyperspectral imager provided a new way to classify grassland species. The research result provided characteristic parameters for the judgment of grassland degradation succession process, and quantitative indicators for grassland ecological restoration management. © 2019, Chinese Society of Agricultural Machinery. All right reserved.","Classification; Convolutional neural network; Desert grassland; Hyperspectral remote sensing; Indicator species; Unmanned aerial vehicle","Antennas; Classification (of information); Convolution; Deep learning; Ecology; Forestry; Hyperspectral imaging; Image resolution; Landforms; Neural networks; Principal component analysis; Spectroscopy; Unmanned aerial vehicles (UAV); Convolutional neural network; Desert grassland; Hyperspectral image datas; Hyperspectral remote sensing; Hyperspectral remote sensing technology; Indicator species; Remote sensing technology; Satellite remote sensing; Remote sensing"
"Saliency-based End-to-end Target Detection Model in Optical Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065333486&doi=10.1088%2f1757-899X%2f490%2f4%2f042011&partnerID=40&md5=deb4838f9be0f4237f1240f4d3eca257","It is challenging work that detecting target such as aircraft in remote-sensing images because of the complicated background. Most existing methods are based on deep-learning network considering its high learning power of the features. However, the region proposal network is often based on deep network, which cost much time and computation on the proposal of the irrelevant regions which are useless to the target detection. Based on the above considerations, a novel end-to-end aircraft detection model based on saliency map is proposed in this paper. The saliency-based region proposal network can produce the target-like regions and filter out the most irrelevant background regions. Meanwhile, it cost less computing time compare to the network based on deep-learning network. Then, a novel target detection network is designed to extract the feature of target-like regions, and classify these features by the iterations of a coupled networks, the result of the binary classification is conducted by the classification layer, at same time the accurate bounding boxes are conducted by the regression layer. The performance of our method is evaluated by detecting aircraft targets in high resolution remote-sensing images. Superior experimental result proves the effectiveness and efficiency of proposed model. © 2019 Institute of Physics Publishing. All rights reserved.",,"Aircraft; Aircraft detection; Deep learning; Object recognition; Background region; Binary classification; Detecting target; Detection networks; Effectiveness and efficiencies; High resolution remote sensing images; Optical remote sensing; Remote sensing images; Remote sensing"
"Recognition of remote sensing image based on depth learning for agricultural monitoring","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068468359&partnerID=40&md5=78856400430176552ddcab9b09154206","With the development of space technology in recent years, various spacecraft with different sensors have been launched one after another, and there are more and more satellite remote sensing images in different situations. Remote sensing image recognition has become a hot topic in the current discussion. At the same time, remote sensing images are very helpful in social rescue, geological survey, urban design and so on. Therefore, remote sensing images have become the eyes of people to observe the earth and even space. Nowadays, it is more and more mature in the field of image target recognition. With the arrival of machine learning and depth learning, the algorithm of image recognition based on depth learning becomes more and more mature, and it is used in every field of image recognition. However, the existing remote sensing image target recognition methods based on convolution neural network only consider the deep semantic features of the network, which results in the recognition performance reaching the bottleneck stage, In this paper, in order to solve the above problems, firstly, the remote sensing image is preprocessed, including noise filtering and binary feature contour extraction. After the image preprocessing, the processed image is classified. Finally, the classified images are trained by convolutional neural network, and a remote sensing image agricultural monitoring recognition algorithm based on depth learning is proposed. The classification of remote sensing image data sets is established by establishing the general classification of remote sensing image data sets. After training, the network can basically realize the agricultural monitoring. © 2019, Universidad del Zulia. All rights reserved.","Agricultural monitoring; Convolutional neural networks; Deep learning; Remote sensing images; Target recognition",
"Deep domain adaptation with manifold aligned label transfer","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061256903&doi=10.1007%2fs00138-019-01003-1&partnerID=40&md5=37e33fee1b7b41288bc7e40e83decb80","We propose a novel deep learning domain adaptation method that performs transductive learning from the source domain to the target domain based on cluster matching between the source and target features. The proposed method combines Adaptive Batch Normalization and Locality Preserving Projection-based subspace alignment on deep features to produce a common feature space for label transfer. Adaptive Batch Normalization automatically conditions the features from the source/target domain by normalizing the activations in each layer of our network. Following Manifold Subspace Alignment, we cluster the data in each domain using Gaussian Mixture Model clustering in feature space. The clusters are matched between domains to transfer labels from the closest source cluster to each target cluster. The transfer labels are compared to the network prediction, and the samples with consistent labels are used to adapt the network on the target domain. The proposed manifold-guided label transfer method produces state-of-the-art results for deep adaptation on digit recognition datasets. Furthermore, we perform domain adaptation on remote sensing datasets. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Domain adaptation; Label transfer","Gaussian distribution; Network layers; Remote sensing; Digit recognition; Domain adaptation; Gaussian Mixture Model; Locality preserving projections; Network prediction; State of the art; Subspace alignment; Transductive learning; Deep learning"
"Land parcel-based digital soil mapping of soil nutrient properties in an alluvial-diluvia plain agricultural area in China","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059950087&doi=10.1016%2fj.geoderma.2019.01.018&partnerID=40&md5=cd26a6f50713fbb9dbe284cd3512792c","The ability to accurately and precisely perform soil nutrient mapping over large areas is essential in the decision-making processes for precision agriculture. However, existing grid-based or non-grid-based digital soil mapping (DSM) can lead to the problem of mixed units of input information, which causes the mapping results to be unsuitable for direct use in guiding the implementation of precision agriculture. Instead, the goal of this study was to achieve DSM based on land parcels, which are the basic units of agricultural management and have practical geographical significance for precision mapping in agricultural areas. This study established a convolutional neural network-based automatic extraction model to extract land parcels from high resolution remote sensing images. Thirty environmental covariates were chosen and calibrated at land parcels to establish the relationships between soils and landscapes. Four prediction algorithms, namely, ordinary kriging, cokriging, random forest and artificial neural network, were combined with the land-parcel-based DSM framework to develop and evaluate their effectiveness in predicting four topsoil nutrient properties in an alluvial-diluvia plain agricultural region located in Ningxia province, China. The results of comparisons show that, overall, the land-parcel-based RF model achieved the best prediction accuracy; its relative improvement (RMSE%) values over the competing models were 1.27, 4.23, 3.19 and 9.01 for soil organic matter, soil total nitrogen, available phosphorus and available potassium, respectively. In addition, land-parcel-based mapping can improve algorithmic efficiency by approximately 4 times by effectively reducing the mapping units for complex agricultural areas compared with the grid-based mapping results when using the same algorithm, and it also achieves a better performance at the detail level. Overall, the land-parcel-based DSM approach achieved good results in plain agricultural areas, but the model still needs improvement for land-parcel-based DSM in mountainous and hilly agricultural areas, and a challenge remains in selecting the most appropriate environmental covariates. © 2019 Elsevier B.V.","Agricultural landscape; Deep learning; Digital soil mapping; Land parcel; Soil nutrient properties","Decision making; Decision trees; Deep learning; Forecasting; Mapping; Neural networks; Nutrients; Precision agriculture; Remote sensing; Soil surveys; Soils; Agricultural landscapes; Agricultural management; Algorithmic efficiencies; Convolutional neural network; Digital soil mappings; High resolution remote sensing images; Land parcels; Soil nutrients; Landforms; agricultural land; agricultural management; alluvial plain; digital mapping; numerical model; phosphorus; precision agriculture; remote sensing; soil nutrient; topsoil; China; Ningxia Huizu"
"Automatic building footprint extraction from multi-resolution remote sensing images using a hybrid FCN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066441743&doi=10.3390%2fijgi8040191&partnerID=40&md5=3303cd5f1332c4e92d8fd833aad49eba","Recent technical developments made it possible to supply large-scale satellite image coverage. This poses the challenge of efficient discovery of imagery. One very important task in applications like urban planning and reconstruction is to automatically extract building footprints. The integration of different information, which is presently achievable due to the availability of high-resolution remote sensing data sources, makes it possible to improve the quality of the extracted building outlines. Recently, deep neural networks were extended from image-level to pixel-level labelling, allowing to densely predict semantic labels. Based on these advances, we propose an end-to-end U-shaped neural network, which efficiently merges depth and spectral information within two parallel networks combined at the late stage for binary building mask generation. Moreover, as satellites usually provide high-resolution panchromatic images, but only low-resolution multi-spectral images, we tackle this issue by using a residual neural network block. It fuses those images with different spatial resolution at the early stage, before passing the fused information to the Unet stream, responsible for processing spectral information. In a parallel stream, a stereo digital surface model (DSM) is also processed by the Unet. Additionally, we demonstrate that our method generalizes for use in cities which are not included in the training data. c 2019 by the authors.","Building footprint extraction; Deep learning; Fully convolutional neural network; Pansharpening; Stereo DSM; Stereo imagery; Unet; World View-2 Imagery",
"Detecting large-scale urban land cover changes from very high resolution remote sensing images using CNN-based classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066450551&doi=10.3390%2fijgi8040189&partnerID=40&md5=9da438d6f37a7929c09b77426768a0ad","The study investigates land use/cover classification and change detection of urban areas from very high resolution (VHR) remote sensing images using deep learning-based methods. Firstly, we introduce a fully Atrous convolutional neural network (FACNN) to learn the land cover classification. In the FACNN an encoder, consisting of full Atrous convolution layers, is proposed for extracting scale robust features from VHR images. Then, a pixel-based change map is produced based on the classification map of current images and an outdated land cover geographical information system (GIS) map. Both polygon-based and object-based change detection accuracy is investigated, where a polygon is the unit of the GIS map and an object consists of those adjacent changed pixels on the pixel-based change map. The test data covers a rapidly developing city of Wuhan (8000 km2), China, consisting of 0.5 m ground resolution aerial images acquired in 2014, and 1 m ground resolution Beijing-2 satellite images in 2017, and their land cover GIS maps. Testing results showed that our FACNN greatly exceeded several recent convolutional neural networks in land cover classification. Second, the object-based change detection could achieve much better results than a pixel-based method, and provide accurate change maps to facilitate manual urban land cover updating. © 2019 by the authors.","Atrous convolution; Change detection; Classification; Convolutional neural networks; Very-high-resolution remote sensing images",
"Fully Convolutional Network Method of Semantic Segmentation of Class Imbalance Remote Sensing Images [类别非均衡遥感图像语义分割的全卷积网络方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065559046&doi=10.3788%2fAOS201939.0428004&partnerID=40&md5=e7e114229707786be3cdd910e54d8b6f","A fully convolutional network (FCN) model based on U-Net is proposed to implement the semantic segmentation of remote sensing images with high resolution, in which the data standardization and data augmentation are adopted for data preprocessing. In addition, the Adam optimizer is used for the model training and the average Jaccard index is used as the evaluation metric. A weighted cross entropy loss function and an adaptive threshold algorithm are employed to improve the classification accuracy of small classes. The experimental results on the DSTL dataset show that the proposed method can increase the average Jaccard index of prediction results from 0.611 to 0.636, and produces an accurate end-to-end classification for high-resolution remote sensing images. © 2019, Chinese Lasers Press. All right reserved.","Class imbalance; Deep learning; Fully convolutional network; Image processing; Remote sensing images; Semantic segmentation","Classification (of information); Convolution; Deep learning; Image processing; Image segmentation; Semantic Web; Semantics; Adaptive threshold algorithm; Class imbalance; Classification accuracy; Convolutional networks; Data standardization; High resolution remote sensing images; Remote sensing images; Semantic segmentation; Remote sensing"
"Oil film classification using deep learning-based hyperspectral remote sensing technology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066453783&doi=10.3390%2fijgi8040181&partnerID=40&md5=c06ff977634c75adfb616e8b3af2663e","Marine oil spills seriously impact the marine environment and transportation. When oil spill accidents occur, oil spill distribution information, in particular, the relative thickness of the oil film, is vital for emergency decision-making and cleaning. Hyperspectral remote sensing technology is an effective means to extract oil spill information. In this study, the concept of deep learning is introduced to the classification of oil film thickness based on hyperspectral remote sensing technology. According to the spatial and spectral characteristics, the stacked autoencoder network model based on the support vector machine is improved, enhancing the algorithm’s classification accuracy in validating data sets. A method for classifying oil film thickness using the convolutional neural network is designed and implemented to solve the problem of space homogeneity and heterogeneity. Through numerous experiments and analyses, the potential of the two proposed deep learning methods for accurately classifying hyperspectral oil spill data is verified. © 2019 by the authors.","Deep learning; Oil film classification; Spectral information extraction",
"High-resolution remote sensing image building extraction combined with faster-RCNN and level-set","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071122639&doi=10.3788%2fYJYXS20193404.0439&partnerID=40&md5=70d560479945429db65f8e0f702239fb","At present, the Level-Set image segmentation method has the problem that the initial contour is greatly affected by human factors. The target segmentation and the target and background gray values are close to each other, and the ideal segmentation effect cannot be achieved. Aiming at this problem, this paper proposes a priori level set image segmentation method based on the Faster-RCNN network model to determine the initial contour and region information of the target, and builds the Caffe deep learning framework to train the Faster-RCNN network model. The IAILD data is obtained through supervised learning. The training model is assembled, the target building is detected and the outline of the building is initially extracted and combined with the shape-priority Level-Set algorithm. The experimental results show that the proposed method solves the problem that the initial contour of the image segmentation result is greatly influenced by the artificial marker frame selection in the Level-Set algorithm. It can better complete the segmentation of the occluded building, and achieve better segmentation effect for the target building and the background gray value. © 2019, Editorial Office of Chinese Journal of Liquid Crystals and Displays. All rights reserved.","Building extraction; Deep learning; Faster-RCNN; Level-Set",
"A Y-Net deep learning method for road segmentation using high-resolution visible remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059552395&doi=10.1080%2f2150704X.2018.1557791&partnerID=40&md5=1fa6e39865a0db645101b77b1e627c4b","Road segmentation from high-resolution visible remote sensing images provides an effective way for automatic road network forming. Recently, deep learning methods based on convolutional neural networks (CNNs) are widely applied in road segmentation. However, it is a challenge for most CNN-based methods to achieve high segmentation accuracy when processing high-resolution visible remote sensing images with rich details. To handle this problem, we propose a road segmentation method based on a Y-shaped convolutional network (indicated as Y-Net). Y-Net contains a two-arm feature extraction module and a fusion module. The feature extraction module includes a deep downsampling-to-upsampling sub-network for semantic features and a convolutional sub-network without downsampling for detail features. The fusion module combines all features for road segmentation. Benefiting from this scheme, the Y-Net can well segment multi-scale roads (both wide and narrow roads) from high-resolution images. The testing and comparative experiments on a public dataset and a private dataset show that Y-Net has higher segmentation accuracy than four other state-of-art methods, FCN (Fully Convolutional Network), U-Net, SegNet, and FC-DenseNet (Fully Convolutional DenseNet). Especially, Y-Net accurately segments contours of narrow roads, which are missed by the comparative methods. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Arts computing; Convolution; Extraction; Feature extraction; Image segmentation; Neural networks; Remote sensing; Roads and streets; Semantics; Signal sampling; Statistical tests; Comparative experiments; Convolutional networks; Convolutional neural network; High resolution image; High resolution visible; Remote sensing images; Segmentation accuracy; State-of-art methods; Deep learning; artificial neural network; automation; data set; downscaling; image resolution; learning; remote sensing; road; segmentation"
"Automatic extraction of urban impervious surfaces based on deep learning and multi-source remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061729503&doi=10.1016%2fj.jvcir.2018.12.051&partnerID=40&md5=5b8cabbde0034f4998d228fcad81d45b","The conventional methods of urban impervious surfaces extraction mainly use the shallow-layer machine learning algorithms based on the medium- or low-resolution remote sensing images, and always provide low accuracy and poor automation level because the potential of multi-source remote sensing data are not fully utilized and the low-level features are not effectively organized. In order to address this problem, a novel method (AEIDLMRS) is proposed to automatically extract impervious surfaces based on deep learning and multi-source remote sensing data. First, the multi-source remote sensing data consisting of LIDAR points cloud data, Landsat8 images and Pléiades-1A images are pre-processed, re-sampled and registered, and then the combined features of spectral, elevation and intensity from the multi-source data are denoised using the minimum noise fraction (MNF) method to generate some representative MNF features. A small number of reliable labelled samples are automatically extracted using the fuzzy C-means (FCM) clustering method based on the MNF features. Secondly, the convolutional neural network (CNN) is used to extract the representative features of the neighborhood windows of each pixel in the fused Pléiades-1A image through multi-layer convolution and pooling operations. Finally, the combined features of MNF features and CNN features are pre-learned via the deep belief network (DBN). The DBN parameters are globally optimized jointly using the Extreme Learning Machine (ELM) classifier on the top level and the small set of labelled samples extracted via FCM, and the urban impervious surfaces are distinguished from others based on the trained ELM classifier and morphological operations. Experiments are performed to compare the proposed method with other three related methods in three different experimental regions respectively. Experimental results demonstrate that AEIDLMRS has better accuracy and automation level than the others under relatively good efficiency, and it is more suitable for the extraction of complex urban impervious surfaces. © 2018","Deep learning; ELM classifier; Extraction of urban impervious surface; Fuzzy C means clustering; Multi-source remote sensing data","Convolution; Data mining; Extraction; Fuzzy systems; Image processing; Learning algorithms; Machine learning; Mathematical morphology; Neural networks; Partial discharges; Remote sensing; Convolutional neural network; Deep belief network (DBN); Extreme learning machine; Fuzzy C means clustering; Minimum noise fraction; Morphological operations; Remote sensing data; Urban impervious surfaces; Deep learning"
"Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062325359&doi=10.1016%2fj.isprsjprs.2019.02.017&partnerID=40&md5=a8bea0f0f3f60976b36a4101995e6479","Cloud detection is an important preprocessing step for the precise application of optical satellite imagery. In this paper, we propose a deep learning based cloud detection method named multi-scale convolutional feature fusion (MSCFF) for remote sensing images of different sensors. In the network architecture of MSCFF, the symmetric encoder-decoder module, which provides both local and global context by densifying feature maps with trainable convolutional filter banks, is utilized to extract multi-scale and high-level spatial features. The feature maps of multiple scales are then up-sampled and concatenated, and a novel multi-scale feature fusion module is designed to fuse the features of different scales for the output. The two output feature maps of the network are cloud and cloud shadow maps, which are in turn fed to binary classifiers outside the model to obtain the final cloud and cloud shadow mask. The MSCFF method was validated on hundreds of globally distributed optical satellite images, with spatial resolutions ranging from 0.5 to 50 m, including Landsat-5/7/8, Gaofen-1/2/4, Sentinel-2, Ziyuan-3, CBERS-04, Huanjing-1, and collected high-resolution images exported from Google Earth. The experimental results show that MSCFF achieves a higher accuracy than the traditional rule-based cloud detection methods and the state-of-the-art deep learning models, especially in bright surface covered areas. The effectiveness of MSCFF means that it has great promise for the practical application of cloud detection for multiple types of medium and high-resolution remote sensing images. Our established global high-resolution cloud detection validation dataset has been made available online (http://sendimage.whu.edu.cn/en/mscff/). © 2019","Cloud detection; Cloud shadow; Convolutional feature fusion; Convolutional neural network; MSCFF; Multi-scale","Convolution; Feature extraction; Maps; Network architecture; Neural networks; Remote sensing; Satellite imagery; Cloud detection; Cloud shadows; Convolutional neural network; Feature fusion; MSCFF; Multi-scale; Deep learning; accuracy assessment; algorithm; artificial neural network; detection method; image resolution; remote sensing; satellite imagery; sensor"
"On-board ship detection in micro-nano satellite based on deep learning and COTS component","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064013613&doi=10.3390%2frs11070762&partnerID=40&md5=9b6de3e9549c768100238b0fbc3e6a61","Micro-nano satellites have provided a large amount of remote sensing images for many earth observation applications. However, the hysteresis of satellite-ground mutual communication of massive remote sensing images and the low efficiency of traditional information processing flow have become the bottlenecks for the further development of micro-nano satellites. To solve this problem, this paper proposes an on-board ship detection scheme based on deep learning and Commercial Off-The-Shelf (COTS) component, which can be used to achieve near real-time on-board processing by micro-nano satellite computing platform. The on-board ship detection algorithm based on deep learning consists of a feature extraction network, Region Proposal Network (RPN) with square anchors, Global Average Pooling (GAP), and Bigger-Left Non-Maximum Suppression (BL-NMS).With the help of high performance COTS components, the proposed scheme can extract target patches and valuable information from remote sensing images quickly and accurately. A ground demonstration and verification system is built to verify the feasibility and effectiveness of our scheme. Our method achieves the performance with 95.9% recall and 80.5% precision in our dataset. Experimental results show that the scheme has a good application prospect in micro-nano satellites with limited power and computing resources. © 2019 by the authors.","COTS component; Deep learning; Micro-nano satellite; On-board processing; Ship detection","Commercial off-the-shelf; Communication satellites; Feature extraction; Image processing; Micro satellites; Nanosatellites; Remote sensing; Ships; Software packages; Commercial off-the-shelf components; COTS component; Ground demonstrations; Micro-nano; Non-maximum suppression; On-board processing; Remote sensing images; Ship detection; Deep learning"
"Analysis of the inter-dataset representation ability of deep features for high spatial resolution remote sensing image scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053024673&doi=10.1007%2fs11042-018-6548-6&partnerID=40&md5=9f27940fd1edda02479243f4eb97e19f","Recently, scene based classification has become a new trend for very high spatial resolution remote sensing image interpretation. With the advent of deep learning, the pretrained convolutional neural networks (CNNs) have been proved effective as feature extractors for scene classification tasks in the remote sensing domain, but the potential characteristics and capabilities of such deep features have not been sufficiently analyzed and fully understood. Facing with complex remote sensing scenes with huge intra-class variations, it is still not clear about the limitation of these powerful deep features in exploring essential invariant attributes of remote sensing scenes of the same kind but, in most cases, from separate sources. Therefore, this paper makes an intensive investigation in the feature representation ability of such deep features from the aspect of inter-dataset scene classification of remote sensing images. Four well-known pretrained CNN models and three different commonly used datasets are selected and summarized. Firstly, deep features extracted from various intermediate layers of these models are compared. Then, the inter-dataset feature representation ability is evaluated using cross-classification of different datasets and discussed in terms of imaging spatial resolution, image size, model structure, and time efficiency. Finally, several instructive findings are revealed and conclusions are drawn regarding the strength and weakness of the CNN features in the application of remote sensing image scene classification. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks (CNNs); Deep learning features; Inter-dataset feature representation; Remote sensing image; Scene classification","Classification (of information); Convolution; Deep learning; Image classification; Image resolution; Neural networks; Convolutional neural network; Feature representation; High spatial resolution; Intra-class variation; Remote sensing image interpretations; Remote sensing images; Scene classification; Very high spatial resolutions; Remote sensing"
"Classification of optical remote sensing images based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072839101&doi=10.1109%2fCoDIT.2019.8820307&partnerID=40&md5=2af0b5184a463c84f146e4af4adea951","Based on deep convolutional neural network, an optical remote sensing image classification method is proposed in this paper. Aiming at the particularity of remote sensing image and natural object classification, combined with the theory of deep learning convolutional neural network, a five-layer convolutional neural network was designed, which applied to classify the optical remote sensing image into two category. Testing and parameter optimization on the UC Merced Land Use data set. The convolutional neural network designed in this paper is trained and tested on the same test set. The result shows it has better effect of classifying on the current data set reach 98.15%. The experimental results indicate this network designed can apply to the scene of two-category image classification and improve the classification accuracy of aerial image. © 2019 IEEE.","Convolutional Neural Network; Deep Learning; Image Classification; Softmax Classifier","Antennas; Classification (of information); Convolution; Deep learning; Deep neural networks; Image classification; Image enhancement; Land use; Network layers; Remote sensing; Statistical tests; Aerial images; Classification accuracy; Convolutional neural network; Current data; Natural objects; Optical remote sensing; Parameter optimization; Remote sensing images; Multilayer neural networks"
"Building extraction via convolutional neural networks from an open remote sensing building dataset [遥感影像建筑物提取的卷积神经元网络与开源数据集方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066029846&doi=10.11947%2fj.AGCS.2019.20180206&partnerID=40&md5=1b2b264d5418bda78121e2af17c1d11d","Automatic extraction of buildings from remote sensing images is significant to city planning, popular estimation, map making and updating. We report several important developments in building extraction. Automatic building recognition from remote sensing data has been a scientific challenge of more than 30 years. Traditional methods based on empirical feature design can hardly realize automation. Advanced deep learning based methods show prospects but have two limitations now. Firstly, large and accurate building datasets are lacking while such dataset is the necessary fuel for deep learning. Secondly, the current researches only concern building's pixel wise semantic segmentation and the further extractions on instance-level and vector-level are urgently required. This paper proposes several solutions. First, we create a large, high-resolution, accurate and open-source building dataset, which consists of aerial and satellite images with both raster and vector labels. Second, we propose a novel structure based on fully neural network which achieved the best accuracy of semantic segmentation compared to most recent studies. Third, we propose a building instance segmentation method which expands the current studies of pixel-level segmentation to building-level segmentation. Experiments proved our dataset's superiority in accuracy and multi-usage and our methods' advancement. It is expected that our researches might push forward the challenging building extraction study. © 2019, Surveying and Mapping Press. All right reserved.","Building extraction; Convolutional neural network; Deep learning; Instance segmentation; Semantic segmentation","Antennas; Convolution; Deep learning; Extraction; Large dataset; Neural networks; Pixels; Remote sensing; Semantics; Automatic extraction; Building extraction; Convolutional neural network; Learning-based methods; Remote sensing data; Remote sensing images; Segmentation methods; Semantic segmentation; Buildings"
"Smallholder crop area mapped with a semantic segmentation deep learning method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069881076&doi=10.3390%2fRS11070888&partnerID=40&md5=4148399fba888ba9ed6ae9f2ae5738bb","The growing population in China has led to an increasing importance of crop area (CA) protection. A powerful tool for acquiring accurate and up-to-date CA maps is automatic mapping using information extracted from high spatial resolution remote sensing (RS) images. RS image information extraction includes feature classification, which is a long-standing research issue in the RS community. Emerging deep learning techniques, such as the deep semantic segmentation network technique, are effective methods to automatically discover relevant contextual features and get better image classification results. In this study, we exploited deep semantic segmentation networks to classify and extract CA from high-resolution RS images. WorldView-2 (WV-2) images with only Red-Green-Blue (RGB) bands were used to confirm the effectiveness of the proposed semantic classification framework for information extraction and the CA mapping task. Specifically, we used the deep learning framework TensorFlow to construct a platform for sampling, training, testing, and classifying to extract and map CA on the basis of DeepLabv3+. By leveraging per-pixel and random sample point accuracy evaluation methods, we conclude that the proposed approach can efficiently obtain acceptable accuracy (Overall Accuracy = 95%, Kappa = 0.90) of CA classification in the study area, and the approach performs better than other deep semantic segmentation networks (U-Net/PspNet/SegNet/DeepLabv2) and traditional machine learning methods, such as Maximum Likelihood (ML), Support Vector Machine (SVM), and RF (Random Forest). Furthermore, the proposed approach is highly scalable for the variety of crop types in a crop area. Overall, the proposed approach can train a precise and effective model that is capable of adequately describing the small, irregular fields of smallholder agriculture and handling the great level of details in RGB high spatial resolution images. © 2018 by the authors.","Agriculture; High spatial resolution images; Semantic labeling","Agriculture; Classification (of information); Crops; Decision trees; Image resolution; Image segmentation; Information retrieval; Mapping; Maximum likelihood; Remote sensing; Semantics; Support vector machines; Classification results; Feature classification; High spatial resolution; High spatial resolution images; High-resolution RS images; Machine learning methods; Semantic classification; Semantic labeling; Deep learning"
"Deep learning based hyperspectral image analysis-a survey","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067001579&doi=10.1166%2fjctn.2019.8070&partnerID=40&md5=eb8c0cfd02d4858cda83669678fdf39a","Hyperspectral imagery is used widely in remote sensing applications that considers the compositions of thousands of spectral channels over a single scene. The hyperspectral imagery demands for an accurate learning models for extracting the hyperspectral image features. The image learning model provides a core challenges due to its complicated nature of image frames due to the presence of its spatial and spectral resolution. Several attempts have been made to address its complicated nature in order to assist it during the learning process. However, these methods failed in providing deeper understanding of the hyperspectral imagery. The use of deep learning techniques addresses the problems due to the presence of mixed pixels, large amount of data and limited training samples. The deep learning process address the complex relationship among the image data. In this paper, various deep learning methods used for the learning of hyperspectral imagery is presented. A survey on different deep learning methods for various image processing techniques is presented. A system review is then carried out on various hyperspectral image learning models based on deep learning. Finally, the challenges and possible directions for future study is discussed. Copyright © 2019 American Scientific Publishers All rights reserved.","Deep Learning; Feature Extraction; Hyperspectral Image Learning",
"Application of deep-learning methods to bird detection using unmanned aerial vehicle imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064572639&doi=10.3390%2fs19071651&partnerID=40&md5=7d96da0847298c134dd400bb4997e612","Wild birds are monitored with the important objectives of identifying their habitats and estimating the size of their populations. Especially in the case of migratory bird, they are significantly recorded during specific periods of time to forecast any possible spread of animal disease such as avian influenza. This study led to the construction of deep-learning-based object-detection models with the aid of aerial photographs collected by an unmanned aerial vehicle (UAV). The dataset containing the aerial photographs includes diverse images of birds in various bird habitats and in the vicinity of lakes and on farmland. In addition, aerial images of bird decoys are captured to achieve various bird patterns and more accurate bird information. Bird detection models such as Faster Region-based Convolutional Neural Network (R-CNN), Region-based Fully Convolutional Network (R-FCN), Single Shot MultiBox Detector (SSD), Retinanet, and You Only Look Once (YOLO) were created and the performance of all models was estimated by comparing their computing speed and average precision. The test results show Faster R-CNN to be the most accurate and YOLO to be the fastest among the models. The combined results demonstrate that the use of deep-learning-based detection methods in combination with UAV aerial imagery is fairly suitable for bird detection in various environments. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Bird detection; Convolutional neural networks; Deep learning; Unmanned aerial vehicle","Aerial photography; Aircraft detection; Antennas; Birds; Convolution; Ecosystems; Neural networks; Object detection; Photographic equipment; Unmanned aerial vehicles (UAV); Aerial Photographs; Avian influenza; Bird detection; Convolutional networks; Convolutional neural network; Detection methods; Learning methods; Migratory birds; Deep learning; animal; artificial neural network; bird; machine learning; procedures; remote sensing; Animals; Birds; Deep Learning; Machine Learning; Neural Networks (Computer); Remote Sensing Technology"
"Combining iterative slow feature analysis and deep feature learning for change detection in high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065503901&doi=10.1117%2f1.JRS.13.024506&partnerID=40&md5=e0ae21b45445ca68f386edd2b0a14529","In order to make full use of local neighborhood information for high-resolution remote sensing images, this study combined iterative slow feature analysis (ISFA) and stacked denoising autoencoder (SDAE) to improve the change detection precision. First, this approach introduced ISFA for initial change detection in an unsupervised way, which enlarged the separability of changed and unchanged areas. Then, by setting different membership degrees, the changed and unchanged samples were obtained through fuzzy-means clustering. Finally, the change model was built by SDAE to represent the local neighborhood features deeply, and the change detection result can be obtained after all the samples were fed into the model. Experiments were performed on three real datasets, and the results validated the effectiveness and superiority of the proposed approach. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Change detection; Deep learning; High-resolution remote sensing image; Slow feature analysis; Stacked denoising autoencoder","Feature extraction; Image analysis; Image enhancement; Machine learning; Remote sensing; Auto encoders; Change detection; Deep feature learning; Fuzzy means clustering; High resolution remote sensing images; Local neighborhoods; Membership degrees; Slow feature analysis; Deep learning"
"Scene recognition with deep learning methods using aerial images [Hava fotoǧraflari kullanarak derin Öǧrenme yöntemleri ile sahne siniflandirma]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071989460&doi=10.1109%2fSIU.2019.8806616&partnerID=40&md5=53ee2b01ec5a390fd30562320bbc1457","In this paper, two novel deep learning architectures are proposed to solve the scene classification problem using aerial images. The results that we get using the two models that are constructed utilizing a part of ResNET50 pretrained model are investigated. In model evaluations, we used one of the largest open access dataset, i.e. NWPU-RESIS45 dataset, that contains 45 different categories and in total 31500 samples. 95.7% accuracy that we get with the developped models show a competitive performance with the state-of-the-art methods. The proposed approaches are advantageous because they present a base model that requires low processing power and memory compared to the existing approaches. Within the scope of the study, the contribution of the regulation of the categories in a hierarchical structure to the classification performance is also investigated. © 2019 IEEE.","Deep learning; NWPU-RESIS45; Remote sense scene classification; ResNET50","Antennas; Remote sensing; Signal processing; Classification performance; Competitive performance; Hierarchical structures; Learning architectures; NWPU-RESIS45; ResNET50; Scene classification; State-of-the-art methods; Deep learning"
"Extracting raft aquaculture areas in sanduao from high-resolution remote sensing images using rcf","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068676476&doi=10.3969%2fj.issn.0253-4193.2019.04.011&partnerID=40&md5=85cc94fd3a3aef27f696ca0778a51b3a","Sanduao is an important sea-breeding bay in China and holds a high economic status in aquaculture. Quickly and accurately obtaining information such as the distribution area, quantity, and aquaculture area is important for breeding area planning, production value estimation, ecological survey, and storm surge prevention. However, as the aquaculture area expands, the seawater background becomes increasingly complex and spectral characteristics differ dramatically, making it difficult to determine the aquaculture area. In this study, we used a high-resolution remote-sensing satellite GF-2 image to introduce a deep-learning Richer Convolutional Features network model to extract the raft aquaculture area in the bay. The results demonstrate that this method does not require land and water separation of the area in advance, and good extraction can be achieved in areas with more sediment and waves, with an extraction accuracy >93%, which is suitable for large-scale raft aquaculture area extraction. © 2019, Editorial Office of Haiyang Xuebao. All Rights Reserved.","Deep learning; High-resolution remote sensing; Raft aquaculture area; RCF model",
"BIM-PoseNet: Indoor camera localisation using a 3D indoor model and deep learning from synthetic images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062348063&doi=10.1016%2fj.isprsjprs.2019.02.020&partnerID=40&md5=f6874078e36477eaa3428b7f22f95ec4","The ubiquity of cameras built in mobile devices has resulted in a renewed interest in image-based localisation in indoor environments where the global navigation satellite system (GNSS) signals are not available. Existing approaches for indoor localisation using images either require an initial location or need first to perform a 3D reconstruction of the whole environment using structure-from-motion (SfM) methods, which is challenging and time-consuming for large indoor spaces. In this paper, a visual localisation approach is proposed to eliminate the requirement of image-based reconstruction of the indoor environment by using a 3D indoor model. A deep convolutional neural network (DCNN) is fine-tuned using synthetic images obtained from the 3D indoor model to regress the camera pose. Results of the experiments indicate that the proposed approach can be used for indoor localisation in real-time with an accuracy of approximately 2 m. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","3D building models; BIM; Camera pose regression; Deep learning; Indoor localisation; Synthetic images","3D modeling; Architectural design; Cameras; Deep learning; Deep neural networks; Global positioning system; Image reconstruction; Neural networks; Three dimensional computer graphics; 3D building models; 3D reconstruction; Convolutional neural network; Global Navigation Satellite Systems; Indoor environment; Localisation; Structure from motion; Synthetic images; Indoor positioning systems; accuracy assessment; artificial intelligence; artificial neural network; GNSS; image analysis; image processing; machine learning; methodology; numerical model; real time; regression analysis; three-dimensional modeling"
"Integrating remote sensing, machine learning, and citizen science in dutch archaeological prospection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064008907&doi=10.3390%2frs11070794&partnerID=40&md5=a5cd31ad9810602b9fdeaf2931d34f58","Although the history of automated archaeological object detection in remotely sensed data is short, progress and emerging trends are evident. Among them, the shift from rule-based approaches towards machine learning methods is, at the moment, the cause for high expectations, even though basic problems, such as the lack of suitable archaeological training data are only beginning to be addressed. In a case study in the central Netherlands, we are currently developing novel methods for multi-class archaeological object detection in LiDAR data based on convolutional neural networks (CNNs). This research is embedded in a long-term investigation of the prehistoric landscape of our study region. We here present an innovative integrated workflow that combines machine learning approaches to automated object detection in remotely sensed data with a two-tier citizen science project that allows us to generate and validate detections of hitherto unknown archaeological objects, thereby contributing to the creation of reliable, labeled archaeological training datasets. We motivate our methodological choices in the light of current trends in archaeological prospection, remote sensing, machine learning, and citizen science, and present the first results of the implementation of the workflow in our research area. © 2019 by the authors.","Airborne laser scanning; Archaeological prospection; Citizen science; Deep learning; The Netherlands","Deep learning; Machine learning; Neural networks; Object recognition; Remote sensing; Airborne Laser scanning; Archaeological objects; Archaeological prospection; Citizen science; Convolutional neural network; Machine learning approaches; Machine learning methods; Netherlands; Object detection"
"Assessment of convolution neural networks for wetland mapping with landsat in the central Canadian boreal forest region","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064013187&doi=10.3390%2frs11070772&partnerID=40&md5=15b918be2c258ce3d29610b6443385f2","Methods for effective wetland monitoring are needed to understand how ecosystem services may be altered from past and present anthropogenic activities and recent climate change. The large extent of wetlands in many regions suggests remote sensing as an effective means for monitoring. Remote sensing approaches have shown good performance in local extent studies, but larger regional efforts have generally produced low accuracies for detailed classes. In this research we evaluate the potential of deep-learning Convolution Neural Networks (CNNs) for wetland classification using Landsat data to bog, fen, marsh, swamp, and water classes defined by the Canada Wetland Classification System (CWCS). The study area is the northern part of the forested region of Alberta where we had access to two reference data sources. We evaluated ResNet CNNs and developed a Multi-Size/Scale ResNet Ensemble (MSRE) approach that exhibited the best performance. For assessment, a spatial extension strategy was employed that separated regions for training and testing. Results were consistent between the two reference sources. The best overall accuracy for the CWCS classes was 62-68%. Compared to a pixel-based random forest implementation this was 5-7% higher depending on the accuracy measure considered. For a parameter-optimized spatial-based implementation this was 2-4% higher. For a reduced set of classes to water, wetland, and upland, overall accuracy was in the range of 86-87%. Assessment for sampling over the entire region instead of spatial extension improved the mean class accuracies (F1-score) by 9% for the CWCS classes and for the reduced three-class level by 6%. The overall accuracies were 69% and 90% for the CWCS and reduced classes respectively with region sampling. Results in this study show that detailed classification of wetland types with Landsat remains challenging, particularly for small wetlands. In addition, further investigation of deep-learning methods are needed to identify CNN configurations and sampling methods better suited to moderate spatial resolution imagery across a range of environments. © 2019 by the authors.","Classification; Convolution neural network; Deep learning; Landsat; Machine learning; Wetlands","Classification (of information); Climate change; Convolution; Decision trees; Deep learning; Ecosystems; Learning systems; Remote sensing; Anthropogenic activity; Canadian boreal forest; Convolution neural network; Detailed classification; LANDSAT; Remote sensing approaches; Spatial resolution imagery; Wetland classification; Wetlands"
"Building footprint extraction from high-resolution images via spatial residual inception convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064008268&doi=10.3390%2frs11070830&partnerID=40&md5=cf3d683caf662565225317bf001aa9f8","The rapid development in deep learning and computer vision has introduced new opportunities and paradigms for building extraction from remote sensing images. In this paper, we propose a novel fully convolutional network (FCN), in which a spatial residual inception (SRI) module is proposed to capture and aggregate multi-scale contexts for semantic understanding by successively fusing multi-level features. The proposed SRI-Net is capable of accurately detecting large buildings that might be easily omitted while retaining global morphological characteristics and local details. On the other hand, to improve computational efficiency, depthwise separable convolutions and convolution factorization are introduced to significantly decrease the number of model parameters. The proposed model is evaluated on the Inria Aerial Image Labeling Dataset and theWuhan University (WHU) Aerial Building Dataset. The experimental results show that the proposed methods exhibit significant improvements compared with several state-of-the-art FCNs, including SegNet, U-Net, RefineNet, and DeepLab v3+. The proposed model shows promising potential for building detection from remote sensing images on a large scale. © 2019 by the authors.","Building footprints extraction; Fully convolutional network; High-resolution image; Multi-scale contexts; Semantic segmentation","Antennas; Buildings; Computational efficiency; Convolution; Deep learning; Extraction; Neural networks; Remote sensing; Semantics; Building footprint; Convolutional networks; High resolution image; Multi-scale contexts; Semantic segmentation; Image segmentation"
"Ship detection based on YOLOv2 for SAR imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063989255&doi=10.3390%2frs11070786&partnerID=40&md5=3019378fad59002307c6f5837b929dcf","Synthetic aperture radar (SAR) imagery has been used as a promising data source for monitoring maritime activities, and its application for oil and ship detection has been the focus of many previous research studies. Many object detection methods ranging from traditional to deep learning approaches have been proposed. However, majority of them are computationally intensive and have accuracy problems. The huge volume of the remote sensing data also brings a challenge for real time object detection. To mitigate this problem a high performance computing (HPC) method has been proposed to accelerate SAR imagery analysis, utilizing the GPU based computing methods. In this paper, we propose an enhanced GPU based deep learning method to detect ship from the SAR images. The You Only Look Once version 2 (YOLOv2) deep learning framework is proposed to model the architecture and training the model. YOLOv2 is a state-of-the-art real-time object detection system, which outperforms Faster Region-Based Convolutional Network (Faster R-CNN) and Single Shot Multibox Detector (SSD) methods. Additionally, in order to reduce computational time with relatively competitive detection accuracy, we develop a new architecture with less number of layers called YOLOv2-reduced. In the experiment, we use two types of datasets: A SAR ship detection dataset (SSDD) dataset and a Diversified SAR Ship Detection Dataset (DSSDD). These two datasets were used for training and testing purposes. YOLOv2 test results showed an increase in accuracy of ship detection as well as a noticeable reduction in computational time compared to Faster R-CNN. From the experimental results, the proposed YOLOv2 architecture achieves an accuracy of 90.05% and 89.13% on the SSDD and DSSDD datasets respectively. The proposed YOLOv2-reduced architecture has a similarly competent detection performance as YOLOv2, but with less computational time on a NVIDIA TITAN X GPU. The experimental results shows that the deep learning can make a big leap forward in improving the performance of SAR image ship detection. © 2019 by the authors.","Faster R-CNN; High performance computing; Ship detection; Synthetic aperture radar (SAR) images; YOLOv2; YOLOv2-reduced","Deep learning; Image enhancement; Network architecture; Object detection; Object recognition; Remote sensing; Ships; Synthetic aperture radar; Tracking radar; Vehicle performance; Faster R-CNN; High performance computing; Ship detection; Synthetic aperture radar (SAR) images; YOLOv2; YOLOv2-reduced; Radar imaging"
"A novel spatio-temporal FCN-LSTM network for recognizing various crop types using multi-temporal radar images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065037054&doi=10.3390%2frs11080893&partnerID=40&md5=a614c0749224ec52c393056b386d4ff9","In recent years, analyzing Synthetic Aperture Radar (SAR) data has turned into one of the challenging and interesting topics in remote sensing. Radar sensors are capable of imaging Earth's surface independently of the weather conditions, local time of day, penetrating of waves through clouds, and containing spatial information on agricultural crop types. Based on these characteristics, the main goal sought in this research is to reveal the SAR imaging data capability in recognizing various agricultural crops in the main growth season in a more clarified and detailed way by using a deep-learning-based method. In the present research, the multi-temporal C-band Sentinel 1 images were used to classify 14 major classes of agricultural crops plus background in Denmark. By considering the capability of a deep learning method in analyzing satellite images, a novel, optimal, and lightweight network structure was developed and implemented based on a combination of a fully convolutional network (FCN) and a convolutional long short-term memory (ConvLSTM) network. The average pixel-based accuracy and Intersection over Union obtained from the proposed network were 86% and 0.64, respectively. Winter rapeseed, winter barley, winter wheat, spring barley, and sugar beet had the highest pixel-based accuracies of 95%, 94%, 93%, 90%, and 90%; respectively. The pixel-based accuracies for eight crop types and the background class were more than 84%. The network prediction showed that in field borders the classification confidence was lower than the center regions of the fields. However, the proposed structure has been able to identify different crops in multi-temporal Sentinel 1 data of a large area of around 254 thousand hectares with high performance. © 2019 by the authors.","Crop classification; Fully convolutional network; Long-short term memory; Multi-temporal data; Sentinel; Sequential network","Brain; Convolution; Crops; Deep learning; Long short-term memory; Pixels; Remote sensing; Sugar beets; Synthetic aperture radar; Classification confidence; Convolutional networks; Crop classification; Learning-based methods; Multi-temporal data; Network structures; Sentinel; Spatial informations; Radar imaging"
"Altimeter observation-based eddy nowcasting using an improved Conv-LSTM network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064012515&doi=10.3390%2frs11070783&partnerID=40&md5=a49214e3226427e5a98e87e1c6165506","Eddies can be identified and tracked based on satellite altimeter data. However, few studies have focused on nowcasting the evolution of eddies using remote sensing data. In this paper, an improved Convolutional Long Short-Term Memory (Conv-LSTM) network named Prednet is used for eddy nowcasting. Prednet, which uses a deep, recurrent convolutional network with both bottom-up and top-down connects, has the ability to learn the temporal and spatial relationships associated with time series data. The network can effectively simulate and reconstruct the spatiotemporal characteristics of the future sea level anomaly (SLA) data. Based on the SLA data products provided by Archiving, Validation, and Interpretation of Satellite Oceanographic (AVISO) from 1993 to 2018, combined with an SLA-based eddy detection algorithm, seven-day eddy nowcasting experiments are conducted on the eddies in South China Sea. The matching ratio is defined as the percentage of true eddies that can be successfully predicted by Conv-LSTM network. On the first day of the nowcasting, matching ratio for eddies with diameters greater than 100 km is 95%, and the average matching ratio of the seven-day nowcasting is approximately 60%. In order to verify the performance of nowcasting method, two experiments were set up. A typical anticyclonic eddy shedding from Kuroshio in January 2017 was used to verify this nowcasting algorithm's performance on single eddy, with the mean eddy center error is 11.2 km. Moreover, compared with the eddies detected in the Hybrid Coordinate Ocean Model data set (HYCOM), the eddies predicted with Conv-LSTM networks are closer to the eddies detected in the AVISO SLA data set, indicating that deep learning method can effectively nowcast eddies. © 2019 by the authors.","Convolution LSTM network; Deep learning; Mesoscale oceanic eddies; Remote sensing data","Aneroid altimeters; Convolution; Deep learning; Meteorological instruments; Ocean currents; Remote sensing; Sea level; Algorithm's performance; Bottom-up and top-down; Convolutional networks; Hybrid coordinate ocean models; Oceanic eddies; Remote sensing data; Satellite altimeter data; Spatiotemporal characteristics; Long short-term memory"
"Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061769312&doi=10.1016%2fj.isprsjprs.2019.02.006&partnerID=40&md5=97fbdef3241bcf8ef1c7341e15e8aaf1","Unprecedented urbanization in particular in countries of the global south result in informal urban development processes, especially in mega cities. With an estimated 1 billion slum dwellers globally, the United Nations have made the fight against poverty the number one sustainable development goal. To provide better infrastructure and thus a better life to slum dwellers, detailed information on the spatial location and size of slums is of crucial importance. In the past, remote sensing has proven to be an extremely valuable and effective tool for mapping slums. The nature of used mapping approaches by machine learning, however, made it necessary to invest a lot of effort in training the models. Recent advances in deep learning allow for transferring trained fully convolutional networks (FCN) from one data set to another. Thus, in our study we aim at analyzing transfer learning capabilities of FCNs to slum mapping in various satellite images. A model trained on very high resolution optical satellite imagery from QuickBird is transferred to Sentinel-2 and TerraSAR-X data. While free-of-charge Sentinel-2 data is widely available, its comparably lower resolution makes slum mapping a challenging task. TerraSAR-X data on the other hand, has a higher resolution and is considered a powerful data source for intra-urban structure analysis. Due to the different image characteristics of SAR compared to optical data, however, transferring the model could not improve the performance of semantic segmentation but we observe very high accuracies for mapped slums in the optical data: QuickBird image obtains 86–88% (positive prediction value and sensitivity) and a significant increase for Sentinel-2 applying transfer learning can be observed (from 38 to 55% and from 79 to 85% for PPV and sensitivity, respectively). Using transfer learning proofs extremely valuable in retrieving information on small-scaled urban structures such as slum patches even in satellite images of decametric resolution. © 2019 The Authors","Convolutional neural networks; Deep learning; FCN; Slums; Transfer learning","Convolution; Housing; Image enhancement; Image segmentation; Mapping; Neural networks; Remote sensing; Satellite imagery; Semantic Web; Semantics; Small satellites; Urban growth; Convolutional networks; Convolutional neural network; Image characteristics; Optical satellite imagery; Semantic segmentation; Slums; Transfer learning; Very high resolution; Deep learning; algorithm; artificial neural network; data set; image resolution; QuickBird; remote sensing; satellite data; satellite imagery; segmentation; Sentinel; sustainable development; synthetic aperture radar; TerraSAR-X; United Nations; urban area; urbanization"
"Semantic Segmentation of Aerial Images using FCN-based Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070963379&doi=10.1109%2fIranianCEE.2019.8786455&partnerID=40&md5=8ab080187ef2eea082b88b3aab14a39e","In this paper, a Deep learning architecture for semantic segmentation of aerial images is proposed. One of the problems with convolutional neural network is the loss of spatial data after using convolution layers which can be retrieved using up-sampling layers. The proposed network uses skip connection with residual learning within an encoder-decoder architecture to reduce ambiguities in the up-sampling layers. We designed a FCN that uses Infra-Red-Green (IRRG), normalized Digital Surface Model (nDSM) and Normalized Difference Vegetation Index (NDVI) information from ISPRS Vaihingen dataset 2D semantic labeling and reached more than 87% as our best total accuracy. © 2019 IEEE.","Deep Learning; photogrammetry; Remote Sensing; Semantic Segmentation; Vaihingen","Antennas; Convolution; Deep learning; Multilayer neural networks; Network architecture; Photogrammetry; Remote sensing; Semantic Web; Semantics; Signal sampling; Convolutional neural network; Digital surface models; Encoder-decoder architecture; Learning architectures; Normalized difference vegetation index; Semantic labeling; Semantic segmentation; Vaihingen; Image segmentation"
"Multispectral image fusion using super-resolution conditional generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055792733&doi=10.1117%2f1.JRS.13.022002&partnerID=40&md5=88db0212e3ac0f09a92f60ac8917e082","In multispectral image fusion scenarios, deep learning has been widely applied. However, the fusion performance and image quality are still restricted by inflexible architecture and supervised learning mode. We proposed multispectral image fusion using super-resolution conditional generative adversarial networks (MS-cGANs) based on conditional cGANs, which produces the fused image through the flexible encode-and-decode procedure. In the proposed network, a least square model is extended to solve the gradients vanishing problem in cGANs. Then, to improve the fusion quality, the multiscale features are used to preserve the details. Furthermore, the image resolution is promoted by adding the perceptual loss in object function and injecting the super-resolution structure into a deconvolution procedure. In experimental results, MS-cGANs demonstrates a significant performance in fusing multispectral images and top-ranking image quality compared with the state-of-the-art methods. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","fusion; multispectral image; multispectral-conditional generative adversarial network; remote sensing","Deep learning; Fusion reactions; Image quality; Image resolution; Least squares approximations; Optical resolving power; Remote sensing; Adversarial networks; Fusion performance; Least square model; Multi-scale features; Multi-spectral image fusions; Multispectral images; State-of-the-art methods; Super resolution; Image fusion"
"Deep convolutional neural networks for land-cover classification with Sentinel-2 images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069445172&doi=10.1117%2f1.JRS.13.024525&partnerID=40&md5=a5b2500d1079e94afaa4b5d8eb88ded8","Currently, analyzing satellite images requires an unsustainable amount of manual labor. Semiautomatic solutions for land-cover classification of satellite images entail the incorporation of expert knowledge. To increase the scalability of the built solutions, methods that automate image processing and analysis pipelines are required. Recently, deep learning (DL) models have been applied to challenging vision problems with great success. We expect that the use of DL models will soon outperform shallow networks and other classification algorithms, as recently achieved in multiple domains. Here, we consider the task of land-cover classification of satellite images. This seems particularly appropriate for deep classifiers due to the combined high dimensionality of the data with the presence of compositional dependencies between pixels, which can be used to characterize a particular class. We develop a pipeline for analyzing satellite images using a deep convolutional neural network for practical applications. We present its successful application for land-cover classification, where it achieves 86% classification accuracy on unseen raw images. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","deep learning; land-cover classification; remote sensing; Sentinel-2","Convolution; Deep learning; Deep neural networks; Image analysis; Neural networks; Pipelines; Remote sensing; Satellites; Classification accuracy; Classification algorithm; Convolutional neural network; High dimensionality; Image processing and analysis; Land cover classification; Satellite images; Sentinel-2; Image classification"
"Building extraction from high-resolution aerial imagery using a generative adversarial network with spatial and channel attention mechanisms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065020057&doi=10.3390%2frs11080966&partnerID=40&md5=d51590b09b7ae21f6eff63cd0b8f953f","Segmentation of high-resolution remote sensing images is an important challenge with wide practical applications. The increasing spatial resolution provides fine details for image segmentation but also incurs segmentation ambiguities. In this paper, we propose a generative adversarial network with spatial and channel attention mechanisms (GAN-SCA) for the robust segmentation of buildings in remote sensing images. The segmentation network (generator) of the proposed framework is composed of the well-known semantic segmentation architecture (U-Net) and the spatial and channel attention mechanisms (SCA). The adoption of SCA enables the segmentation network to selectively enhance more useful features in specific positions and channels and enables improved results closer to the ground truth. The discriminator is an adversarial network with channel attention mechanisms that can properly discriminate the outputs of the generator and the ground truth maps. The segmentation network and adversarial network are trained in an alternating fashion on the Inria aerial image labeling dataset and Massachusetts buildings dataset. Experimental results show that the proposed GAN-SCA achieves a higher score (the overall accuracy and intersection over the union of Inria aerial image labeling dataset are 96.61% and 77.75%, respectively, and the F1-measure of the Massachusetts buildings dataset is 96.36%) and outperforms several state-of-the-art approaches. © 2019 by the authors.","Deep learning; Generative adversarial network; High-resolution aerial images; Inria aerial image labeling dataset; Massachusetts buildings dataset; Semantic segmentation","Aerial photography; Antennas; Buildings; Deep learning; Object recognition; Remote sensing; Semantics; Adversarial networks; Aerial images; High-resolution aerial images; Massachusetts; Semantic segmentation; Image segmentation"
"Combined multiscale segmentation convolutional neural network for rapid damage mapping from postearthquake very high-resolution images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062798474&doi=10.1117%2f1.JRS.13.022007&partnerID=40&md5=6e8dc13fd4a4ac353b54b30cf228c6f5","Classifying land use from postearthquake very high-resolution (VHR) images is challenging due to the complexity of objects in Earth surface after an earthquake. Convolutional neural network (CNN) exhibits satisfied performance in differentiating complex postearthquake objects, thanks to its automatic extraction of high-level features and accurate identification of target geo-objects. Nevertheless, in view of the scale variance of natural objects, the fact that CNN suffers from the fixed receptive field, the reduced feature resolution, and the insufficient training sample has severely contributed to its limitation in the rapid damage mapping. Multiscale segmentation technique is considered as a promising solution as it can generate the homogenous regions and provide the boundary information. Therefore, we propose a combined multiscale segmentation convolutional neural network (CMSCNN) method for postearthquake VHR image classification. First, multiscale training samples are selected based on segments derived from the multiscale segmentation. Then, CNN is directly trained to classify the original image to further produce the preliminary classification maps. To enhance the localization accuracy, the output of CNN is further refined using multiscale segmentations from fine to coarse iteratively to obtain the multiscale classification maps. As a result, the combination strategy is able to capture objects and image context simultaneously. Experimental results show that the proposed CMSCNN method can reflect the multiscale information of complex scenes and obtain satisfied classification results for mapping postearthquake damage using VHR remote sensing images. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","convolutional neural network; deep learning; multiscale segmentation; postearthquake image classification","Classification (of information); Complex networks; Convolution; Deep learning; Deep neural networks; Image segmentation; Iterative methods; Land use; Mapping; Neural networks; Remote sensing; Sampling; Classification results; Combination strategies; Convolutional neural network; Localization accuracy; Multi-scale informations; Multiscale segmentation; Remote sensing images; Very high resolution (VHR) image; Image classification"
"Effect of visual context information for super resolution problems [Süper Çözünürlük problemlerinde görsel Içerik bilgisinin etkisi]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071972564&doi=10.1109%2fSIU.2019.8806598&partnerID=40&md5=f92503ca1e82b5c52ec54d312f17bdbe","In this study, the effect of visual context information to the performance of learning-based techniques for the super resolution problem is analyzed. Beside the interpretation of the experimental results in detail, its theoretical reasoning is also achieved in the paper. For the experiments, two different visual datasets composed of natural and remote sensing scenes are utilized. From the experimental results, we observe that keeping visual context information in the course of parameter learning for convolutional neural networks yields better performance compared to the baselines. Moreover, we summarize that finetuning pre-trained parameters with the related context yet fewer samples improves the results. © 2019 IEEE.","Convolutional Neural Networks; Deep learning; Super Resolution","Convolution; Deep learning; Deep neural networks; Neural networks; Optical resolving power; Remote sensing; Convolutional neural network; Parameter learning; Super resolution; Visual context; Signal processing"
"SFTGAN: A generative adversarial network for pan-sharpening equipped with spatial feature transform layers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069457255&doi=10.1117%2f1.JRS.13.026507&partnerID=40&md5=81b55330e192044dfa7b6f388c1ef808","Pan-sharpening is an indispensable technology for remote sensing that aims to combine low-resolution multispectral images and high-resolution panchromatic images to create a multispectral image with high resolution. However, pan-sharpening approaches often encounter spectral distortion and detail distortion issues. In order to overcome the drawbacks of pan-sharpening methodologies, we propose an end-to-end pan-sharpening model consisting of an effective generative adversarial network architecture equipped with spatial feature transform layers that generate spatial detail features under spectral feature constraints. Through a large number of quantitative and visual assessments, we demonstrate that the proposed method achieves superior performance to other state-of-the-art methods. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","deep learning; multispectral image; pan-sharpening; remote image fusion; spatial feature transform","Deep learning; Image fusion; Network architecture; Adversarial networks; Low resolution multispectral images; Multispectral images; Pan-sharpening; Panchromatic images; Spatial features; Spectral distortions; State-of-the-art methods; Remote sensing"
"Unsupervised change detection method based on saliency analysis and convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069437260&doi=10.1117%2f1.JRS.13.024512&partnerID=40&md5=4e0150de46a5536d9c762d874a274aeb","Due to great advantages in deep features representation and classification for image data, deep learning is becoming increasingly popular for change detection (CD) in the remote-sensing community. An unsupervised CD method is proposed by combining deep features representation, saliency detection, and convolutional neural network (CNN). First, bitemporal images are fed into the pretrained CNN model for deep features extraction and difference image generation. Second, multiscale saliency detection is adopted to implement the uncertainty analysis for the difference image, where image pixels can be categorized into three classes: changed, unchanged, and uncertain. Then, a flexible CNN model is constructed and trained using the interested changed and unchanged pixels, and the change type of the uncertain pixels can be determined by the CNN model. Finally, object-based refinement and multiscale fusion strategies are utilized to generate the final change map. The effectiveness and reliability of our CD method are verified on three very high-resolution datasets, and the experimental results show that our proposed approach outperforms the other state-of-the-art CD methods in terms of five quantitative metrics. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","change detection; convolutional neural network; deep feature representation; multiscale fusion; saliency detection","Classification (of information); Convolution; Deep neural networks; Neural networks; Pixels; Remote sensing; Uncertainty analysis; Change detection; Convolutional neural network; Feature representation; Multiscale fusion; Saliency detection; Feature extraction"
"Unsupervised feature-learning for hyperspectral data with autoencoders","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066078114&doi=10.3390%2fRS11070864&partnerID=40&md5=59fdd91be13c41b619a7253dbe15ff3f","This paper proposes novel autoencoders for unsupervised feature-learning from hyperspectral data. Hyperspectral data typically have many dimensions and a significant amount of variability such that many data points are required to represent the distribution of the data. This poses challenges for higher-level algorithms which use the hyperspectral data (e.g., those that map the environment). Feature-learning mitigates this by projecting the data into a lower-dimensional space where the important information is either preserved or enhanced. In many applications, the amount of labelled hyperspectral data that can be acquired is limited. Hence, there is a need for feature-learning algorithms to be unsupervised. This work proposes unsupervised techniques that incorporate spectral measures from the remote-sensing literature into the objective functions of autoencoder feature learners. The proposed techniques are evaluated on the separability of their feature spaces as well as on their application as features for a clustering task, where they are compared against other unsupervised feature-learning approaches on several different datasets. The results show that autoencoders using spectral measures outperform those using the standard squared-error objective function for unsupervised hyperspectral feature-learning. © 2019 by the authors.","Autoencoders; Deep learning; Hyperspectral; Unsupervised feature-learning","Learning algorithms; Machine learning; Remote sensing; Autoencoders; Dimensional spaces; Feature learning; HyperSpectral; Hyperspectral Data; Objective functions; Unsupervised feature learning; Unsupervised techniques; Deep learning"
"Chimera: A multi-task recurrent convolutional neural network for forest classification and structural estimation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064007601&doi=10.3390%2frs11070768&partnerID=40&md5=52d599532d9c7223bc1ae4fec8605973","More consistent and current estimates of forest land cover type and forest structural metrics are needed to guide national policies on forest management, carbon sequestration, and ecosystem health. In recent years, the increased availability of high-resolution (&lt;30m) imagery and advancements in machine learning algorithms have opened up a new opportunity to fuse multiple datasets of varying spatial, spectral, and temporal resolutions. Here, we present a new model, based on a deep learning architecture, that performs both classification and regression concurrently, thereby consolidating what was previously several independent tasks and models into one stream. The model, a multi-task recurrent convolutional neural network that we call the Chimera, integrates varying resolution, freely available aerial and satellite imagery, as well as relevant environmental factors (e.g., climate, terrain) to simultaneously classify five forest cover types ('conifer', 'deciduous', 'mixed', 'dead', 'none' (non-forest)) and to estimate four continuous forest structure metrics (above ground biomass, quadratic mean diameter, basal area, canopy cover). We demonstrate the performance of our approach by training an ensemble of Chimera models on 9967 georeferenced (true locations) Forest Inventory and Analysis field plots from the USDA Forest Service within California and Nevada. Classification diagnostics for the Chimera ensemble on an independent test set produces an overall average precision, recall, and F1-score of 0.92, 0.92, and 0.92. Class-wise F1-scores were high for 'none' (0.99) and 'conifer' (0.85) cover classes, and moderate for the 'mixed' (0.74) class samples. This demonstrates a strong ability to discriminate locations with and without trees. Regression diagnostics on the test set indicate very high accuracy for ensembled estimates of above ground biomass (R2 = 0.84, RMSE = 37.28 Mg/ha), quadraticmean diameter (R2 = 0.81, RMSE = 3.74 inches), basal area (R2 = 0.87, RMSE = 25.88 ft2/ac), and canopy cover (R2 = 0.89, RMSE = 8.01 percent). Comparative analysis of the Chimera ensemble versus support vector machine and random forest approaches demonstrates increased performance over both methods. Future implementations of the Chimera ensemble on a distributed computing platform could provide continuous, annual estimates of forest structure for other forested landscapes at regional or national scales. © 2019 by the authors.","Forest classification; Forest structure; High resolution imagery; Multi-task learning; NAIP; Recurrent convolutional neural networks; Remote sensing","Antennas; Classification (of information); Climate models; Convolution; Decision trees; Deep learning; Distributed computer systems; Ecosystems; Learning algorithms; Machine learning; Recurrent neural networks; Remote sensing; Satellite imagery; Convolutional neural network; Forest classification; Forest structure; High resolution imagery; Multitask learning; NAIP; Forestry"
"Learning high-level features by fusing multi-view representation of MLS point clouds for 3D object recognition in road environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061529859&doi=10.1016%2fj.isprsjprs.2019.01.024&partnerID=40&md5=69f88d058bb213bd01e7be0ce9b2f329","Most existing 3D object recognition methods still suffer from low descriptiveness and weak robustness although remarkable progress has made in 3D computer vision. The major challenge lies in effectively mining high-level 3D shape features. This paper presents a high-level feature learning framework for 3D object recognition through fusing multiple 2D representations of point clouds. The framework has two key components: (1) three discriminative low-level 3D shape descriptors for obtaining multi-view 2D representation of 3D point clouds. These descriptors preserve both local and global spatial relationships of points from different perspectives and build a bridge between 3D point clouds and 2D Convolutional Neural Networks (CNN). (2) A two-stage fusion network, which consists of a deep feature learning module and two fusion modules, for extracting and fusing high-level features. The proposed method was tested on three datasets, one of which is Sydney Urban Objects dataset and the other two were acquired by a mobile laser scanning (MLS) system along urban roads. The results obtained from comprehensive experiments demonstrated that our method is superior to the state-of-the-art methods in descriptiveness, robustness and efficiency. Our method achieves high recognition rates of 94.6%, 93.1% and 74.9% on the above three datasets, respectively. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","3D object recognition; Convolutional neural networks; MLS point clouds; Multi-view representation; Two-stage fusion network","Convolution; Deep learning; Machine learning; Neural networks; 3d object recognition; 3D shape descriptors; Convolutional neural network; Deep feature learning; Multi-views; Point cloud; Spatial relationships; State-of-the-art methods; Object recognition; algorithm; artificial neural network; computer vision; data set; experimental design; laser method; numerical method; road; three-dimensional modeling; Australia; New South Wales; Sydney [New South Wales]"
"Ship Classification in SAR Images Using a New Hybrid CNN–MLP Classifier","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055981128&doi=10.1007%2fs12524-018-0891-y&partnerID=40&md5=0dd49e147b7580f8a5a3205e1fe61f9c","Ship detection on the SAR images for marine monitoring has a wide usage. SAR technology helps us to have a better monitoring over intended sections, without considering atmospheric conditions, or image shooting time. In recent years, with advancements in convolutional neural network (CNN), which is one of the well-known ways of deep learning, using image deep features has increased. Recently, usage of CNN for SAR image segmentation has been increased. Existence of clutter edge, multiple interfering targets, speckle and sea-level clutters makes false alarms and false detections on detector algorithms. In this letter, constant false alarm rate is used for object recognition. This algorithm, processes the image pixel by pixel, and based on statistical information of its neighbor pixels, detects the targeted pixels. Afterward, a neural network with hybrid algorithm of CNN and multilayer perceptron (CNN–MLP) is suggested for image classification. In this proposal, the algorithm is trained with real SAR images from Sentinel-1 and RADARSAT-2 satellites, and has a better performance on object classification than state of the art. © 2018, Indian Society of Remote Sensing.","Classification; Convolution neural network (CNN); Hybrid CNN–MLP; Multilayer perceptron (MLP); Neural network; SAR image processing; Ship classification; Synthetic aperture radar (SAR)","artificial neural network; image classification; image processing; monitoring system; pixel; RADARSAT; satellite imagery; segmentation; Sentinel; ship design"
"Fusion network for change detection of high-resolution panchromatic imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064068655&doi=10.3390%2fapp9071441&partnerID=40&md5=59af34f72671dab57cfa8e0cd08b5fad","This paper proposes a fusion network for detecting changes between two high-resolution panchromatic images. The proposed fusion network consists of front- and back-end neural network architectures to generate dual outputs for change detection. Two networks for change detection were applied to handle image- and high-level changes of information, respectively. The fusion network employs single-path and dual-path networks to accomplish low-level and high-level differential detection, respectively. Based on two dual outputs, a two-stage decision algorithm was proposed to efficiently yield the final change detection results. The dual outputs were incorporated into the two-stage decision by operating logical operations. The proposed algorithm was designed to incorporate not only dual network outputs but also neighboring information. In this paper, a new fused loss function was presented to estimate the errors and optimize the proposed network during the learning stage. Based on our experimental evaluation, the proposed method yields a better detection performance than conventional neural network algorithms, with an average area under the curve of 0.9709, percentage correct classification of 99%, and Kappa of 75 for many test datasets. © 2019 by the authors.","Change detection; Convolutional network; Deep learning; Panchromatic; Remote sensing",
"Deep convolutional neural networks for rice grain yield estimation at the ripening stage using UAV-based remotely sensed images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062921699&doi=10.1016%2fj.fcr.2019.02.022&partnerID=40&md5=54cf06cd10f61669a9fb67e46e24c744","Forecasting rice grain yield prior to harvest is essential for crop management, food security evaluation, food trade, and policy-making. Many successful applications have been made in crop yield estimation using remotely sensed products, such as vegetation index (VI) from multispectral imagery. However, VI-based approaches are only suitable for estimating rice grain yield at the middle stage of growth but have limited capability at the ripening stage. In this study, an efficient convolutional neural network (CNN) architecture was proposed to learn the important features related to rice grain yield from low-altitude remotely sensed imagery. In one major region for rice cultivation of Southern China, a 160-hectare site with over 800 management units was chosen to investigate the ability of CNN in rice grain yield estimation. The datasets of RGB and multispectral images were obtained by a fixed-wing, unmanned aerial vehicle (UAV), which was mounted with a digital camera and multispectral sensors. The network was trained with different datasets and compared against the traditional vegetation index-based method. In addition, the temporal and spatial generality of the trained network was investigated. The results showed that the CNNs trained by RGB and multispectral datasets perform much better than VIs-based regression model for rice grain yield estimation at the ripening stage. The RGB imagery of very high spatial resolution contains important spatial features with respect to grain yield distribution, which can be learned by deep CNN. The results highlight the promising potential of deep convolutional neural networks for rice grain yield estimation with excellent spatial and temporal generality, and a wider time window of yield forecasting. © 2019 Elsevier B.V.","CNN; Deep learning; Rice crop; UAV; Yield estimation","agricultural management; artificial neural network; crop yield; developmental stage; estimation method; machine learning; policy making; remote sensing; rice; ripening; satellite imagery; unmanned vehicle; yield response; China"
"Transfer learning and U-Net for buildings segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064274604&doi=10.1145%2f3314074.3314088&partnerID=40&md5=14f749e05f1ab844f679062ca4e48ea2","Monitoring urban changes has been considered a major concern for decision makers for decades because of the social and economic impact of the urbanization on societies. To achieve such goal, many technical approaches, combining Raster GIS and image processing techniques, have been proposed using remote sensing imageries in order to be able to identify buildings and urban changes in wider area. In other hands, we are recently witnessing a huge interest in deep learning architecture, and especially in their capacity to predict complex patterns, particularly those related to digital imageries. The aim of this paper is to explore a particular deep convolution network, U-Net, and its use for automatic semantic image building segmentation from remote sensing imageries. We show that the Unet deep learning architecture can achieve good results in buildings segmentation. © 2019 Association for Computing Machinery.","Building segmentation; Convolution; Deep learning networks; Encoder/decoder; Unet","Buildings; Classification (of information); Convolution; Decision making; Image segmentation; Network architecture; Remote sensing; Semantics; Encoder/decoder; Image processing technique; Learning architectures; Learning network; Remote sensing imagery; Social and economic impacts; Transfer learning; Unet; Deep learning"
"Low-Power Deep Learning Inference using the SpiNNaker Neuromorphic Platform","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072262348&doi=10.1145%2f3320288.3320300&partnerID=40&md5=dba37b4438d1dd74dd907dee49f9e7db","With the successes deep neural networks have achieved across a range of applications, researchers have been exploring computational architectures to more efficiently execute their operation. In addition to the prevalent role of graphics processing units (GPUs), many accelerator architectures have emerged. Neuromorphic is one such particular approach which takes inspiration from the brain to guide the computational principles of the architecture including varying levels of biological realism. In this paper we present results on using the SpiNNaker neuromorphic platform (48-chip model) for deep learning neural network inference. We use the Sandia National Laboratories developed Whetstone spiking deep learning library to train deep multi-layer perceptrons and convolutional neural networks suitable for the spiking substrate on the neural hardware architecture. By using the massively parallel nature of SpiNNaker, we are able to achieve, under certain network topologies, substantial network tiling and consequentially impressive inference throughput. Such high-throughput systems may have eventual application in remote sensing applications where large images need to be chipped, scanned, and processed quickly. Additionally, we explore complex topologies that push the limits of the SpiNNaker routing hardware and investigate how that impacts mapping software-implemented networks to on-hardware instantiations. © 2019 Association for Computing Machinery.",,"Computer graphics; Deep neural networks; Graphics processing unit; Network architecture; Program processors; Remote sensing; Topology; Accelerator architectures; Computational architecture; Convolutional neural network; High throughput systems; Learning neural networks; Multi-layer perceptrons; Remote sensing applications; Sandia National Laboratories; Multilayer neural networks"
"Hyperspectral Image Classification Algorithm Based on Locally Retained Reduced Dimensional Convolution Neural Network [基于局部保留降维卷积神经网络的高光谱图像分类算法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068933297&doi=10.6041%2fj.issn.1000-1298.2019.03.014&partnerID=40&md5=129a6e7e3fd873cd23232babc9a513ca","In order to improve the classification accuracy of hyperspectral remote sensing images, a novel hyperspectral image classification algorithm based on local preserving reduced dimensional convolutional neural network (DCNN) was proposed by using local preserving discriminant analysis and deep convolutional neural network (DCNN) algorithm. Firstly, the dimensionality reduction of hyperspectral data was analyzed by local reserved discriminant, and then the spatial tunnel information was filtered by two-dimensional Gabor filter. Secondly, the original hyperspectral data were extracted by convolution neural network to generate spectral tunnel information. Thirdly, the spatial tunnel information and spectral tunnel information were integrated to form the air-spectrum characteristic information, which was input into deep convolutional neural network to extract more effective features. Finally, the feature of the final extraction was classified by using the dual optimization classifier. The proposed method was compared with CNN, PCA-SVM, CD-CNN and CNN-PPF in the performance of Indian Pines and University of Pavia hyperspectral remote sensing databases. In the database of Indian Pines and University of Pavia, the overall recognition accuracy of the proposed method was 3.81 percentage points and 6.62 percentage points higher than that of the traditional CNN method. Experimental results on two databases showed that the proposed method was superior to the other four methods in both classification accuracy and Kappa coefficient, and it was a better classification method for hyperspectral remote sensing data classification. © 2019, Chinese Society of Agricultural Machinery. All right reserved.","DCNN deep learning; Double optimization classifier; Gabor feature; Hyperspectral image; Locally retained dimensionality reduction; Spectral combined with spatial","Convolution; Data reduction; Database systems; Deep neural networks; Discriminant analysis; Gabor filters; Hyperspectral imaging; Image enhancement; Information filtering; Neural networks; Remote sensing; Spectroscopy; Support vector machines; Convolution neural network; Convolutional neural network; Dimensionality reduction; Gabor feature; Hyperspectral remote sensing; Hyperspectral remote sensing data; Hyperspectral Remote Sensing Image; Spectral combined with spatial; Image classification"
"Studies on high-resolution remote sensing sugarcane field extraction based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064385662&doi=10.1088%2f1755-1315%2f237%2f3%2f032046&partnerID=40&md5=5e60a35b5329006df7cb154701172fe7","Sugarcane is one of the most important economic crops in Guangxi. For a long time, the sugarcane cultivated areas were estimated via sampling data statistics, while effective and accurate dynamic monitoring data keep absent. High spatial resolution is one of the advantages of high-resolution remote sensing images, through which the texture of sugarcane fields is found clear and unique; however, effective and accurate methods are lacking extracting them automatically in the past. In this paper, a novel deep learning method for sugarcane field extraction from high-resolution remote sensing images is proposed based on DeepLab V3+. It consists of blocks for multioral remote sensing images fusion, which increases the ability of DCNN temporal factors processing. The experiment shows 94.32% extraction accuracy of sugarcane field. Also, its processing speed is superior to the traditional object-oriented extraction method, which solves the problems of low extraction accuracy and slow processing speed using traditional methods. © 2019 Published under licence by IOP Publishing Ltd.",,"Energy resources; Extraction; Image processing; Remote sensing; Sampling; Textures; Dynamic monitoring; Extraction accuracy; Extraction method; High resolution remote sensing; High resolution remote sensing images; High spatial resolution; Processing speed; Remote sensing images; Deep learning"
"Compacting deep neural networks for light weight IoT & SCADA based applications with node pruning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063861909&doi=10.1109%2fICAIIC.2019.8669031&partnerID=40&md5=0176b151e8fce0724de2ad564b3b1410","Deeplearning based image classifier is getting improved day by day. The network architecture is also increasing with the accuracy. But the bigger size and resource intensive training makes this model impractical to deploy in IoT based computational units. IoT has limited resources and reckoning power. So smaller network with same accuracy is highly priced for IoT based application deployment. In this study, convolutional deeplearning neural network and how pruning filters without compromising accuracy was studied. Efficient result was achieved from the pruned deeplearning neural network. the model was configured in the experiments by pruning the filter based on absolution position of zeros value based filter ranking. SCADA applications with intelligent component to detect data abnormality and remote sensing also required neural network applications. Using compact memory efficient module in such machines will also give proper validation in such applications in real time. In the end, proposed method for the pruned network delivered same accuracy with reduced size and thus archiving memory and computation for small sized application. © 2019 IEEE.","Convolutional Neural Network; Deep learning; Handwritten Digit recognition; Neural Network Pruning; Optimization","Character recognition; Convolution; Deep learning; Image enhancement; Internet of things; Network architecture; Neural networks; Optimization; Remote sensing; Application deployment; Computational units; Convolutional neural network; Handwritten digit recognition; Image Classifiers; Intelligent components; Network pruning; Neural network application; Deep neural networks"
"Endmember extraction of farmland hyperspectral image using deep learning autoencoder and shuffled frog leaping algorithm [深度学习自编码结合混合蛙跳算法提取农田高光谱影像端元]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066431511&doi=10.11975%2fj.issn.1002-6819.2019.06.020&partnerID=40&md5=843cd705766c3954458beddd55e6c537","Hyperspectral remote sensing image includes hundreds of narrow contiguous spectral bands with high spectral resolution, and can provides a contiguous spectral curve for each pixel, which is an important tool for the cropland monitoring in rapid and large-scale way. However, due to the contradiction between spectral resolution and spatial resolution, hyperspectral remote sensing image usually possesses relative low spatial resolution. Therefore, it is very important to mix various vegetation and soil at one pixel point for spectral decomposition in hyperspectral image and spectral unmixing of farmland. The first step in spectral unmixing is usually endmember extraction. Endmember extraction from a hyperspectral remote sensing image is to find some pixels (endmembers) and regard them as pure spectral reflectance of the vegetation and soil in the image to get the best accuracy of spectral unmixing results. So endmember extraction from hyperspectral remote sensing image can be regarded as a typical discrete optimization problem, which can be solved by swarm intelligence optimization algorithm. Before optimization, the spectral dimension of the image should be reduced by deep learning. In order to solve the problem of spectral unmixing of hyperspectral images, a method of extracting farmland endmember based on deep learning and shuffled frog leaping algorithm (SFLA) is proposed in this paper. Firstly, deep learning model named stacked auto encoders (SAE) was used to extract spectral features. SAE performs a non-linear transfer from the original spectral signals to a form with significant features and less dimensions. In the low-dimension space, the candidate endmembers were selected as the input of the SFLA. The purpose of extracting the candidate endmembers is to simplified the computational complexity in the next step. Secondly, the endmember extraction of hyperspectral image was transferred into the combinatorial optimization and the objective function was constructed, then the SFLA was used to optimize the objective functionto get the best combination of endmembers. The objective function in study was designed as the RMSE (root mean square error) between the real hyperspectral remote sensing image and the simulated hyperspectral remote sensing image using the endmembers and abundance after endmember extraction and spectral unmixing. Thirdly, 2 groups experiments were carried out on the synthetic hyperspectral datasets with 3 different SNR (signal to noise ratio, 20, 30 and 40 dB) and the real AVIRIS hyperspectral remote sensing dataset in Salinas region, respectively. In experiments, the experimental results of the proposed method was compared with that of 3 traditional methods for endmember extraction including the sequential maximum angle convex cone (SMACC), N-FINDR, Vertex Component Analysis (VCA). The results were evaluated by RMSE and spectral angle. The results showed that the RMSE was 0.050 8, 0.015 9, 0.005 1, 0.006 7 for 20, 30 and 40 dB dataset, and the real dataset, respectively. The average spectral angle was 0.106 88, 0.030 32, 0.009 94 for 20, 30 and 40 dB dataset respectively. The proposed method was better than traditional methods in terms of extraction accuracy, which had wide potential applications on cropland monitoring using hyperspectral remote sensing. The method proposed in this paper reduced the influence of the non-linear factors and the noise, better endmember extraction and spectral unmixing results (both less spectral angle and less RMSE) could be obtained, and the proposed method was robust when the noise of the image increased sharply. In conclusion, the endmember extraction method proposed by this study is of significant importance for the cropland monitoring using hyperspectral remote sensing and has a prosperous future for the application on the remote sensing of agriculture. © 2019, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Crops; Endmember extraction; Hyperspectral; Image processing; Remote sensing; Shuffled frog leaping algorithm; Stacked autoencoders","Combinatorial optimization; Cones; Crops; Deep learning; Extraction; Farms; Image processing; Image resolution; Learning algorithms; Mean square error; Optimization; Pixels; Signal to noise ratio; Space optics; Spectral resolution; Spectroscopy; Vegetation; Autoencoders; Discrete optimization problems; Endmember extraction; HyperSpectral; Hyperspectral Remote Sensing Image; RMSE (root mean square error); Shuffled frog leaping algorithm (SFLA); Swarm intelligence optimization algorithm; Remote sensing"
"Detecting global urban expansion over the last three decades using a fully convolutional network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067125088&doi=10.1088%2f1748-9326%2faaf936&partnerID=40&md5=13a5f427e60fbab82e0c09f2295b577e","The effective detection of global urban expansion is the basis of understanding urban sustainability. We propose a fully convolutional network (FCN) and employ it to detect global urban expansion from 1992-2016. We found that the global urban land area increased from 274.7 thousand km2-621.1 thousand km2, which is an increase of 346.4 thousand km2 and a growth by 1.3 times. The results display a relatively high accuracy with an average kappa index of 0.5, which is 0.3 higher than those of existing global urban expansion datasets. Three major advantages of the proposed FCN contribute to the improved accuracy, including the integration of multi-source remotely sensed data, the combination of features at multiple scales, and the ability to address the lack of training samples for historical urban land. Thus, the proposed FCN has great potential to effectively detect global urban expansion. © 2019 The Author(s). Published by IOP Publishing Ltd.","deep learning; fully convolutional network; global urban expansion; land surface temperature; nighttime light data; vegetation index","Atmospheric temperature; Convolution; Deep learning; Expansion; Land surface temperature; Convolutional networks; Multiple scale; Night-time lights; Remotely sensed data; Training sample; Urban expansion; Urban sustainability; Vegetation index; Urban growth; accuracy assessment; data set; expansion; land surface; learning; remote sensing; sustainability; urban area; vegetation index"
"Ship detection based on squeeze excitation skip-connection path networks for optical remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059535603&doi=10.1016%2fj.neucom.2018.12.050&partnerID=40&md5=2bfec7f965204ce1e39dd84fe4a7026c","Ship detection plays a crucial role in remote sensing image processing, which has drawn great attention in recent years. A novel neural network architecture named squeeze excitation skip-connection path networks (SESPNets) is proposed. A bottom-up path is added to feature pyramid network to improve feature extraction capability, and path-level skip-connection structure is firstly proposed to enhance information flow and reduce parameter redundancy. Also, squeeze excitation module is adopted, which can adaptively recalibrate channel-wise feature responses by adding an extra branch after each shortcut path connection block. The multi-scale fused region of interest (ROI) align is then proposed to obtain more accurate and multi-scale proposals. Finally, soft-non-maximum suppression is utilized to overcome the problem of non-maximum suppression (NMS) in ship detection. As demonstrated in the experiments, it can be seen that the SESPNets model has achieved the state-of-the-art performance, which shows the effectiveness of proposed method. © 2018","Deep learning; Optical remote sensing images; Ship detection; Skip-connection path networks; Squeeze excitation","Deep learning; Image segmentation; Network architecture; Neural networks; Optical data processing; Ships; Connection structures; Extraction capability; Non-maximum suppression; Novel neural network; Optical remote sensing; Remote sensing image processing; Ship detection; State-of-the-art performance; Remote sensing; article; attention; excitation; feature extraction; learning; remote sensing"
"Extracting Urban Impervious Surface from WorldView-2 and Airborne LiDAR Data Using 3D Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058113382&doi=10.1007%2fs12524-018-0917-5&partnerID=40&md5=95a27d07364bc02126cba39451a8b455","The urban impervious surface has been recognized as a key quantifiable indicator in assessing urbanization and its environmental impacts. Adopting deep learning technologies, this study proposes an approach of three-dimensional convolutional neural networks (3D CNNs) to extract impervious surfaces from the WorldView-2 and airborne LiDAR datasets. The influences of different 3D CNN parameters on impervious surface extraction are evaluated. In an effort to reduce the limitations from single sensor data, this study also explores the synergistic use of multi-source remote sensing datasets for delineating urban impervious surfaces. Results indicate that our proposed 3D CNN approach has a great potential and better performance on impervious surface extraction, with an overall accuracy higher than 93.00% and the overall kappa value above 0.89. Compared with the commonly applied pixel-based support vector machine classifier, our proposed 3D CNN approach takes advantage not only of the pixel-level spatial and spectral information, but also of texture and feature maps through multi-scale convolutional processes, which enhance the extraction of impervious surfaces. While image analysis is facing large challenges in a rapidly developing big data era, our proposed 3D CNNs will become an effective approach for improved urban impervious surface extraction. © 2018, The Author(s).","Airborne light detection and ranging (LiDAR); Convolutional neural networks (CNNs); Impervious surface; Support vector machine (SVM); WorldView-2","artificial neural network; data set; environmental impact; image analysis; lidar; pixel; satellite data; satellite sensor; support vector machine; three-dimensional modeling; urban area; urbanization; WorldView"
"Compressed Remote Sensing by Using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063872500&doi=10.1109%2fISTEL.2018.8661112&partnerID=40&md5=39ca5c4fe20c9897b8a68384f5769c09","In remote sensing applications, storing and compressing images requires high memory size and consumes a lot of battery power, which can be overcome with compressed sensing (CS) and transferring complexities to the receiver. But compressed sensing has three significant challenges. At first, it is hard to find the basis in which signal is sparse. Second, compressed sensing uses recovery algorithms that are slow in time, which makes CS suitable for applications that are non-real-time. Third, CS usually use pre-specified measurement matrices, which are not optimized based on the data being analyzed, so it is possible to improve the performance of CS. In this paper, we will show that using deep learning methods in compressed remote sensing, the above challenges will be solved. In this paper, we present a deep neural network in which the matrix of measurements and reconstruction operations are optimized simultaneously. We indicate that this network closely approximates the recovery algorithms and has performance near CS recovery algorithms, but it is faster in run time. However, the complexity of deep neural networks is the training phase of the network and needs to be completed only once before using the network. © 2018 IEEE.","Compressed Remote Sensing; Compressed Sensing; Deep Learning; Remote Sensing","Compressed sensing; Computer system recovery; Deep learning; Deep neural networks; Recovery; Battery power; Compressive sensing; Learning methods; Non real time; Reconstruction operations; Recovery algorithms; Remote sensing applications; Training phase; Remote sensing"
"Urban tree species classification using a worldview-2/3 and liDAR data fusion approach and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062980001&doi=10.3390%2fs19061284&partnerID=40&md5=8505bde4326b9662e87b501e42d3766c","Urban areas feature complex and heterogeneous land covers which create challenging issues for tree species classification. The increased availability of high spatial resolution multispectral satellite imagery and LiDAR datasets combined with the recent evolution of deep learning within remote sensing for object detection and scene classification, provide promising opportunities to map individual tree species with greater accuracy and resolution. However, there are knowledge gaps that are related to the contribution of Worldview-3 SWIR bands, very high resolution PAN band and LiDAR data in detailed tree species mapping. Additionally, contemporary deep learning methods are hampered by lack of training samples and difficulties of preparing training data. The objective of this study was to examine the potential of a novel deep learning method, Dense Convolutional Network (DenseNet), to identify dominant individual tree species in a complex urban environment within a fused image of WorldView-2 VNIR, Worldview-3 SWIR and LiDAR datasets. DenseNet results were compared against two popular machine classifiers in remote sensing image analysis, Random Forest (RF) and Support Vector Machine (SVM). Our results demonstrated that: (1) utilizing a data fusion approach beginning with VNIR and adding SWIR, LiDAR, and panchromatic (PAN) bands increased the overall accuracy of the DenseNet classifier from 75.9% to 76.8%, 81.1% and 82.6%, respectively. (2) DenseNet significantly outperformed RF and SVM for the classification of eight dominant tree species with an overall accuracy of 82.6%, compared to 51.8% and 52% for SVM and RF classifiers, respectively. (3) DenseNet maintained superior performance over RF and SVM classifiers under restricted training sample quantities which is a major limiting factor for deep learning techniques. Overall, the study reveals that DenseNet is more effective for urban tree species classification as it outperforms the popular RF and SVM techniques when working with highly complex image scenes regardless of training sample size. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network (CNN); Data fusion; Deep learning; Dense convolutional network (DenseNet); Random forest (RF); Support vector machine (SVM); Tree species classification","Classification (of information); Complex networks; Convolution; Data fusion; Decision trees; Forestry; Neural networks; Object detection; Optical radar; Remote sensing; Sampling; Satellite imagery; Support vector machines; Complex urban environments; Convolutional networks; Convolutional neural network; High spatial resolution; Multispectral satellite imagery; Random forests; Remote sensing images; Tree species; Deep learning; artificial neural network; human; support vector machine; Deep Learning; Humans; Neural Networks (Computer); Support Vector Machine"
"Ai-based sensor information fusion for supporting deep supervised learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063278153&doi=10.3390%2fs19061345&partnerID=40&md5=809a7a729a3d0fa8e69d77f8d8b0c38c","In recent years, artificial intelligence (AI) and its subarea of deep learning have drawn the attention of many researchers. At the same time, advances in technologies enable the generation or collection of large amounts of valuable data (e.g., sensor data) from various sources in different applications, such as those for the Internet of Things (IoT), which in turn aims towards the development of smart cities. With the availability of sensor data from various sources, sensor information fusion is in demand for effective integration of big data. In this article, we present an AI-based sensor-information fusion system for supporting deep supervised learning of transportation data generated and collected from various types of sensors, including remote sensed imagery for the geographic information system (GIS), accelerometers, as well as sensors for the global navigation satellite system (GNSS) and global positioning system (GPS). The discovered knowledge and information returned from our system provides analysts with a clearer understanding of trajectories or mobility of citizens, which in turn helps to develop better transportation models to achieve the ultimate goal of smarter cities. Evaluation results show the effectiveness and practicality of our AI-based sensor information fusion system for supporting deep supervised learning of big transportation data. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence (AI); Data mining; Deep learning; Geographic information system (GIS); Global navigation satellite system (GNSS); Global positioning system (GPS); Information fusion; Sensor; Sensor fusion; Supervised learning; Transportation","Communication satellites; Data mining; Deep learning; Geographic information systems; Global positioning system; Information fusion; Information systems; Information use; Internet of things; Machine learning; Remote sensing; Satellite imagery; Sensors; Supervised learning; Transportation; Evaluation results; Global Navigation Satellite Systems; Internet of thing (IOT); Large amounts; Remote sensed imagery; Sensor fusion; Sensor information fusions; Transportation model; Sensor data fusion"
"A Deep Learning Framework for Automatic Airplane Detection in Remote Sensing Satellite Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068312596&doi=10.1109%2fAERO.2019.8741938&partnerID=40&md5=6e20b658c9438cf2c9a535b6de66e0c4","Automated object detection in high-resolution remote sensing satellite images(HRRSSI) is a proper solution for this task rather than manual detection using professional specialists. However, it is more complex due to the varying size, type, orientation, and complex background of the objects to detect. Utilizing artificial intelligence using deep learning is the state of the art technique to achieve this task. The number of labeled satellite images is limited for training a deep neural network therefore; transfer learning techniques were adopted for this task. This paper proposes a framework for airplane detection based on Convolution Neural network (CNN). Faster Region Based CNN (Faster R-CNN) framework is used to perform automatic airplane detection through transfer learning. Inception v2 is added to the network for feature extraction to enhance detection accuracy. The problem of information reduction of the objects due to the resizing of large size satellite image in test phase has been solved by adding a split layer before the input layer, together with a mosaic layer after detection output layer. Dataset is used to build and test the model is collected from Google Earth. Experimental results prove that the proposed developed model is extremely accurate for satellite images object detection. © 2019 IEEE.",,"Aircraft; Communication satellites; Complex networks; Deep neural networks; Feature extraction; Object detection; Object recognition; Remote sensing; Satellites; Statistical tests; Airplane detections; Convolution neural network; Detection accuracy; High resolution remote sensing; Information reduction; Learning frameworks; Remote sensing satellites; State-of-the-art techniques; Aircraft detection"
"Scalable database indexing and fast image retrieval based on deep learning and hierarchically nested structure applied to remote sensing and plant biology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067646205&doi=10.3390%2fjimaging5030033&partnerID=40&md5=8979b8a64669c62bbae20b6a2525d247","Digitalisation has opened a wealth of new data opportunities by revolutionizing how images are captured. Although the cost of data generation is no longer a major concern, the data management and processing have become a bottleneck. Any successful visual trait system requires automated data structuring and a data retrieval model to manage, search, and retrieve unstructured and complex image data. This paper investigates a highly scalable and computationally efficient image retrieval system for real-time content-based searching through large-scale image repositories in the domain of remote sensing and plant biology. Images are processed independently without considering any relevant context between sub-sets of images. We utilize a deep Convolutional Neural Network (CNN) model as a feature extractor to derive deep feature representations from the imaging data. In addition, we propose an effective scheme to optimize data structure that can facilitate faster querying at search time based on the hierarchically nested structure and recursive similarity measurements. A thorough series of tests were carried out for plant identification and high-resolution remote sensing data to evaluate the accuracy and the computational efficiency of the proposed approach against other content-based image retrieval (CBIR) techniques, such as the bag of visual words (BOVW) and multiple feature fusion techniques. The results demonstrate that the proposed scheme is effective and considerably faster than conventional indexing structures. © 2019 by the authors.","Bag of visual words; Content-based image retrieval; Data indexing; Deep convolutional neural networks; Deep learning; Information retrieval; Recursive similarity measurement; Remote sensing",
"DuPLO: A DUal view Point deep Learning architecture for time series classificatiOn","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060326152&doi=10.1016%2fj.isprsjprs.2019.01.011&partnerID=40&md5=5eece83a77e9cb11c9ec8c1655ebcd87","Nowadays, modern Earth Observation systems continuously generate huge amounts of data. A notable example is represented by the Sentinel-2 mission, which provides images at high spatial resolution (up to 10 m) with high temporal revisit period (every 5 days), which can be organized in Satellite Image Time Series (SITS). While the use of SITS has been proved to be beneficial in the context of Land Use/Land Cover (LULC) map generation, unfortunately, most of machine learning approaches commonly leveraged in remote sensing field fail to take advantage of spatio-temporal dependencies present in such data. Recently, new generation deep learning methods allowed to significantly advance research in this field. These approaches have generally focused on a single type of neural network, i.e., Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), which model different but complementary information: spatial autocorrelation (CNNs) and temporal dependencies (RNNs). In this work, we propose the first deep learning architecture for the analysis of SITS data, namely DuPLO (DUal view Point deep Learning architecture for time series classificatiOn), that combines Convolutional and Recurrent neural networks to exploit their complementarity. Our hypothesis is that, since CNNs and RNNs capture different aspects of the data, a combination of both models would produce a more diverse and complete representation of the information for the underlying land cover classification task. Experiments carried out on two study sites characterized by different land cover characteristics (i.e., the Gard site in Mainland France and Reunion Island, a overseas department of France in the Indian Ocean), demonstrate the significance of our proposal. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Land cover classification; Satellite image time series; Sentinel-2","Classification (of information); Convolution; Land use; Network architecture; Recurrent neural networks; Remote sensing; Time series; Time series analysis; Convolutional neural network; Land cover classification; Machine learning approaches; Recurrent neural network (RNNs); Satellite images; Sentinel-2; Spatio-temporal dependencies; Time series classifications; Deep learning; autocorrelation; EOS; land cover; machine learning; remote sensing; satellite imagery; Sentinel; spatial resolution; spatiotemporal analysis; time series analysis; France; Gard; Indian Ocean; Mainland [Shetland]; Mascarene Islands; Occitanie; Reunion; Scotland; Shetland; United Kingdom"
"Rural construction land extraction from high spatial resolution remote sensing image based on SegNet semantic segmentation model [基于SegNet语义模型的高分辨率遥感影像农村建设用地提取]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066431961&doi=10.11975%2fj.issn.1002-6819.2019.05.031&partnerID=40&md5=fdbb549adabf325468c15db955a516fc","With the advancement of remote sensing technology, the high spatial resolution remote sensing image contains rich special information with a great detail. At the same time, the complexity of high spatial resolution remote sensing images also requires higher the classification technology of remote sensing images. However, in the face of high spatial resolution remote sensing image more obvious geometrical structure and the more rich texture characteristics, how to design rational system of characteristics, select the appropriate sorting algorithms to accurately and quickly grasp the number of rural land of building and its distribution status, are of great significance to balance urban and rural areas, save land, and realize sustainable development. This will help in exploring the application of deep learning model in high spatial resolution remote sensing image building extraction, and have research significance for improving the classification accuracy of high resolution remote sensing image. In this paper, the semantic segmentation model (SegNet) was used for extracting buildings. SegNet is mainly composed of encoder network, decoder network and pixel-wise classification layer. The encoder network transforms high-dimensional vectors into low-dimensional vectors, enabling low-dimensional extraction of high-dimensional features. The decoder network maps low-resolution feature maps to high spatial resolution feature maps, realizing the reconstruction of low-dimensional vectors to high-dimensional vectors. The softmax classifier separately classifies each pixel, which outputs the probability that each pixel belongs to each class. In this paper, a 3000 pixel × 3000 pixel and two 2000 pixel × 2000 pixel slices were taken from the global remote sensing image of Bazhou City, Hebei Province as training samples, and a 3000 pixel × 3000 pixel slice was taken as the verification sample. In this paper, five comparative experiments were used to extract the buildings, including PSPNet, support vector machine, random forest, ISO clustering and maximum likelihood method. The confusion matrix of each classification method was obtained by calculating the difference between the classification results of the comparison experiment and the real value. From the traditional classification algorithm to the shallow learning algorithm to the deep learning algorithm, the Kappa coefficient and overall accuracy of classification kept constantly increasing, among which SegNet semantic segmentation algorithm based on the deep convolutional network performed better than the other five algorithms in extracting buildings from high spatial resolution remote sensing image. The Kappa coefficient and the overall accuracy of SegNet semantic segmentation algorithm were 0.90 and 96.61%, respectively, and the ground truth value was basically the same as the classification result. The F1Score of building extraction of SegNet semantic segmentation algorithm based on deep convolution network was 0.91, but the other five algorithms were below 0.87. SegNet had the lowest error rate of 9.71% for buildings, indicating that the ability to identify buildings of semantic segmentation algorithm from high spatial resolution remote sensing was superior to traditional classification algorithms, shallow layer learning algorithms based on machine learning, and PSPNet semantic segmentation algorithm based on deep convolution network. The Kappa coefficient and overall accuracy of the remaining five classification algorithms were respectively below 0.83 and 94.68%, and the difference between the ground truth value and the classification result was relatively large. SegNet can not only make use of spectral information but also make full use of abundant spatial information. During SegNet training, more essential features can be learned, and more ideal features suitable for pattern classification were finally formed, which can enhance the ability of convergence and generalization of the model and improve the classification accuracy. Traditional classification algorithms, such as ISO clustering and maximum likelihood method, failed to make use of the rich spatial information of the high-resolution remote sensing image, so the accuracy was relatively low. Due to limited computing units and large amount of high spatial resolution remote sensing image data, shallow layer learning algorithms based on machine learning such as support vector machines and random forest cannot effectively express complex features of ground objects, so their advantages are not obvious in building extraction from the high spatial resolution remote sensing images. The experimental results showed that the SegNet based on deep learning has the best performance, and it has important theoretical significance to explore the application of deep learning model to remote sensing image classification methods. At the same time, the research results also provide a reference for improving the classification accuracy of high resolution remote sensing images. © 2019, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Algorithms; Deep learning; High-resolution remote sensing image; Image segmentation; Remote sensing; Rural construction land extraction; SegNet semantic segmentation model","Algorithms; Buildings; Classification (of information); Clustering algorithms; Complex networks; Convolution; Data mining; Decision trees; Decoding; Deep learning; Extraction; Image enhancement; Image resolution; Information use; Learning algorithms; Machine learning; Maximum likelihood; Network coding; Pixels; Remote sensing; Rural areas; Semantics; Support vector machines; Textures; Vectors; Classification algorithm; Classification technology; High resolution remote sensing images; Maximum likelihood methods; Remote sensing image classification; Remote sensing technology; Rural constructions; Semantic segmentation; Image segmentation"
"Remote Sensing Image Scene Classification Using Rearranged Local Features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062863621&doi=10.1109%2fTGRS.2018.2869101&partnerID=40&md5=40d5470a9a88850fb175b639d3b8aef4","Remote sensing image scene classification is a fundamental problem, which aims to label an image with a specific semantic category automatically. Recently, deep learning methods have achieved competitive performance for remote sensing image scene classification, especially the methods based on a convolutional neural network (CNN). However, most of the existing CNN methods only use feature vectors of the last fully connected layer. They give more importance to global information and ignore local information of images. It is common that some images belong to different categories, although they own similar global features. The reason is that the category of an image may be highly related to local features, other than the global feature. To address this problem, a method based on rearranged local features is proposed in this paper. First, outputs of the last convolutional layer and the last fully connected layer are employed to depict the local and global information, respectively. After that, the remote sensing images are clustered to several collections using their global features. For each collection, local features of an image are rearranged according to their similarities with local features of the cluster center. In addition, a fusion strategy is proposed to combine global and local features for enhancing the image representation. The proposed method surpasses the state of the arts on four public and challenging data sets: UC-Merced, WHU-RS19, Sydney, and AID. © 1980-2012 IEEE.","Feature fusion; rearranged local features; remote sensing image; representation scene classification","Convolution; Deep learning; Image enhancement; Image fusion; Neural networks; Remote sensing; Semantics; Competitive performance; Convolutional neural network; Feature fusion; Global informations; Image representations; Local feature; Remote sensing images; Scene classification; Image classification; artificial neural network; cluster analysis; data interpretation; image analysis; image classification; information management; remote sensing; satellite imagery; Australia; New South Wales; Sydney [New South Wales]"
"Remote sensing image scene classification using CNN-CapsNet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062947435&doi=10.3390%2frs11050494&partnerID=40&md5=6e4ea2fc1e40eaa17db792bbbf0fee16","Remote sensing image scene classification is one of the most challenging problems in understanding high-resolution remote sensing images. Deep learning techniques, especially the convolutional neural network (CNN), have improved the performance of remote sensing image scene classification due to the powerful perspective of feature learning and reasoning. However, several fully connected layers are always added to the end of CNN models, which is not efficient in capturing the hierarchical structure of the entities in the images and does not fully consider the spatial information that is important to classification. Fortunately, capsule network (CapsNet), which is a novel network architecture that uses a group of neurons as a capsule or vector to replace the neuron in the traditional neural network and can encode the properties and spatial information of features in an image to achieve equivariance, has become an active area in the classification field in the past two years. Motivated by this idea, this paper proposes an effective remote sensing image scene classification architecture named CNN-CapsNet to make full use of the merits of these two models: CNN and CapsNet. First, a CNN without fully connected layers is used as an initial feature maps extractor. In detail, a pretrained deep CNN model that was fully trained on the ImageNet dataset is selected as a feature extractor in this paper. Then, the initial feature maps are fed into a newly designed CapsNet to obtain the final classification result. The proposed architecture is extensively evaluated on three public challenging benchmark remote sensing image datasets: the UC Merced Land-Use dataset with 21 scene categories, AID dataset with 30 scene categories, and the NWPU-RESISC45 dataset with 45 challenging scene categories. The experimental results demonstrate that the proposed method can lead to a competitive classification performance compared with the state-of-the-art methods. © 2019 by the authors.","CapsNet; Capsule; CNN; PrimaryCaps; Remote sensing; Scene classification","Classification (of information); Deep learning; Image enhancement; Land use; Network architecture; Neural networks; Remote sensing; CapsNet; Capsule; Classification performance; Convolutional neural network; High resolution remote sensing images; PrimaryCaps; Scene classification; State-of-the-art methods; Image classification"
"Characterization of food cultivation along roadside transects with Google Street View imagery and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060673846&doi=10.1016%2fj.compag.2019.01.014&partnerID=40&md5=e69f55b6d251648da66557921342733d","We describe the development of tools to exploit the enormous resource of street-level imagery in Google Street View to characterize food cultivation practices along roadside transects at very high spatial resolution as a potential complement to traditional remote sensing approaches. We report on two software tools for crop identification using a deep convolutional neural network (CNN) applied to Google Street View imagery. The first, a multi-class classifier distinguishes seven regionally common cultivated plant species, as well as uncultivated vegetation, built environment, and water along the roads. The second, a prototype specialist detector, recognizes the presence of a single plant species: in our case, banana. These two classification tools were tested along roadside transects in two areas of Thailand, a country where there is good Google Street View coverage. On the entire test set, the overall accuracy of the multi-class classifier was 83.3%. For several classes, (banana, built, cassava, maize, rice, and sugarcane), the producer's accuracy was over 90%, meaning that the classifier was infrequently making omission errors. This performance on roadside transects is comparable with that of some remote-sensing classifiers, yet ours does not require any additional site-visits for ground-truthing. Moreover, the overall accuracy of the classifier on the 40% of images it is most sure about is excellent: 99.0%. For the prototype specialist detector, the area under the ROC curve was 0.9905, indicating excellent performance in detecting the presence of banana plants. While initially tested over the road network in a small area, this technique could readily be deployed on a regional or even national scale to supplement remote sensing data and yield a fine-grained analysis of food cultivation activities along roadside transects. © 2019","Computer vision or image classification; Convolutional neural network; Deep learning; Food cultivation practices; Google street view imagery; Transfer learning","Convolution; Deep learning; Deep neural networks; Fruits; Neural networks; Remote sensing; Roadsides; Area under the ROC curve; Convolutional neural network; Fine-grained analysis; Google street view imagery; Multi-class classifier; Remote sensing approaches; Transfer learning; Very high spatial resolutions; Classification (of information); accuracy assessment; artificial neural network; computer vision; cultivation; food; image analysis; image classification; imagery; machine learning; performance assessment; remote sensing; roadside environment; software; spatial resolution; vegetation classification; Thailand; Manihot esculenta; Zea mays"
"An road extraction method for remote sensing image based on Encoder-Decoder network [基于Encoder-Decoder网络的遥感影像道路提取方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064378446&doi=10.11947%2fj.AGCS.2019.20180005&partnerID=40&md5=8ef43d143e8dda1e7d6fc583ce75867d","According to the characteristics of the road features, an Encoder-Decoder deep semantic segmentation network is designed for road extraction of remote sensing images. Firstly, as the features of the road target are rich in local details and simple in semantic features, an Encoder-Decoder network with shallow layers and high resolution is designed to improve the ability of representing detail information. Secondly, as the road area is small proportion in remote sensing images, the cross-entropy loss function is improved, which solves the imbalance between positive and negative samples in training process. Experiments on large road extraction dataset show that, the proposed method gets the recall rate 83.9%, precision 82.5% and F1-score 82.9%, which can extract the road targets in remote sensing images completely and accurately. The Encoder-Decoder network designed in this paper performs well in road extraction task and needs less artificial participation, so it has a good application prospect. © 2019, Surveying and Mapping Press. All right reserved.","Deep learning; Encoder-Decoder network; Remote sensing; Road extraction; Semantic segmentation","Decoding; Deep learning; Extraction; Feature extraction; Image enhancement; Image segmentation; Large dataset; Network layers; Remote sensing; Roads and streets; Semantics; Application prospect; Encoder-decoder; Remote sensing images; Road extraction; Road extraction method; Semantic features; Semantic segmentation; Training process; Network coding; accuracy assessment; algorithm; extraction method; image resolution; numerical method; precision; remote sensing; satellite imagery; segmentation"
"Retro-Remote Sensing: Generating Images from Ancient Texts","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063894914&doi=10.1109%2fJSTARS.2019.2895693&partnerID=40&md5=300d238b0b3891057325948999c8e038","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area. © 2008-2012 IEEE.","Convolutional neural networks (CNN); deep learning; generative adversarial networks (GAN); multimodal learning; remote sensing; text-to-image synthesis","Deep learning; Deep neural networks; Image processing; Neural networks; Adversarial networks; Convolutional neural network; Geographical area; Image synthesis; Multi-modal learning; Multimodal sources; Remote sensing images; Statistical properties; Remote sensing; artificial neural network; civilization; geographical variation; image analysis; learning; pixel; remote sensing"
"Semantic Segmentation of High-Resolution Remote Sensing Image Based on Deep Residual Network [利用深度残差网络的高分遥感影像语义分割]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066242480&doi=10.3969%2fj.issn.0255-8297.2019.02.013&partnerID=40&md5=28e5c5c0dbd9b578e4099e58e89521d9","As an important part of image interpretation and analysis, segmentation of remote sensing images has been widely researched. However, traditional segmentation method based on hand-crafted features has its limitations on accuracy and generalization, state-of-the-art methods are mainly relied on deep learning in recent years. In this paper, we propose a new segmentation method based on multi-scale deep residual neural networks, which aims at improving segmentation accuracy, especially on small-scale objects. We firstly utilize Residual Network (ResNet) and transform it to fully convolution networks (FCN), in which, Atrous convolution is introduced during the up-sampling process to ensure the field of view on each layer. Then we add multi-scale data augmentation to improve the robustness for small objects. The proposed approach is applied on ISPRS 2D Vaihingen semantic labeling contest dataset, and yields high accuracy at 89.7%, outperforming most state-of-the-art methods. © 2019, Editorial Office of Journal of Applied Sciences. All right reserved.","Atrous convolution; Deep residual network; Multi-scale data augmentation; Semantic segmentation of remote sensing image",
"Caffe CNN-based classification of hyperspectral images on GPU","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043368071&doi=10.1007%2fs11227-018-2300-2&partnerID=40&md5=f1da552ce2d4ed6ebd4722b17ea5cc86","Deep learning techniques based on Convolutional Neural Networks (CNNs) are extensively used for the classification of hyperspectral images. These techniques present high computational cost. In this paper, a GPU (Graphics Processing Unit) implementation of a spatial-spectral supervised classification scheme based on CNNs and applied to remote sensing datasets is presented. In particular, two deep learning libraries, Caffe and CuDNN, are used and compared. In order to achieve an efficient GPU projection, different techniques and optimizations have been applied. The implemented scheme comprises Principal Component Analysis (PCA) to extract the main features, a patch extraction around each pixel to take the spatial information into account, one convolutional layer for processing the spectral information, and fully connected layers to perform the classification. To improve the initial GPU implementation accuracy, a second convolutional layer has been added. High speedups are obtained together with competitive classification accuracies. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Caffe; Classification; Convolutional neural network; CuDNN; Deep learning; GPU; Hyperspectral","Classification (of information); Computer graphics; Computer graphics equipment; Convolution; Deep learning; Image classification; Neural networks; Principal component analysis; Program processors; Remote sensing; Spectroscopy; Caffe; Classification accuracy; Convolutional neural network; CuDNN; Fully-connected layers; HyperSpectral; Spectral information; Supervised classification; Graphics processing unit"
"Temporal convolutional neural network for the classification of satellite image time series","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062968414&doi=10.3390%2frs11050523&partnerID=40&md5=62ba3e85214462108e82dc2e99783a3b","Latest remote sensing sensors are capable of acquiring high spatial and spectral Satellite Image Time Series (SITS) of the world. These image series are a key component of classification systems that aim at obtaining up-to-date and accurate land cover maps of the Earth's surfaces. More specifically, current SITS combine high temporal, spectral and spatial resolutions, which makes it possible to closely monitor vegetation dynamics. Although traditional classification algorithms, such as Random Forest (RF), have been successfully applied to create land cover maps from SITS, these algorithms do not make the most of the temporal domain. This paper proposes a comprehensive study of Temporal Convolutional Neural Networks (TempCNNs), a deep learning approach which applies convolutions in the temporal dimension in order to automatically learn temporal (and spectral) features. The goal of this paper is to quantitatively and qualitatively evaluate the contribution of TempCNNs for SITS classification, as compared to RF and Recurrent Neural Networks (RNNs) -a standard deep learning approach that is particularly suited to temporal data. We carry out experiments on Formosat-2 scene with 46 images and one million labelled time series. The experimental results show that TempCNNs are more accurate than the current state of the art for SITS classification. We provide some general guidelines on the network architecture, common regularization mechanisms, and hyper-parameter values such as batch size; we also draw out some differences with standard results in computer vision (e.g., about pooling layers). Finally, we assess the visual quality of the land cover maps produced by TempCNNs. © 2019 by the authors.","Classification; Land cover mapping; Remote sensing; Satellite images; Temporal Convolutional Neural Network (TempCNN); Time series","Classification (of information); Convolution; Decision trees; Deep learning; Image classification; Network architecture; Recurrent neural networks; Remote sensing; Satellites; Time series; Classification algorithm; Classification system; Convolutional neural network; Land cover mapping; Recurrent neural network (RNNs); Regularization mechanism; Remote sensing sensors; Satellite images; Mapping"
"Overview of hyperspectral image classification [高光谱图像分类的研究进展]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066147756&doi=10.3788%2fOPE.20192703.0680&partnerID=40&md5=15a4671c0ca94cf60c76b3ae73ecc2a2","Hyperspectral image classification comprises the classification of every pixel in an image by applying the combination of hyperspectral data atlas and rich spectral information, which can be employed for achieving high-precision classification and automatic recognition of ground objects. Hyperspectral image classification plays an important role in earth observation. Based on the analysis of the characteristics of hyperspectral images with respect to two aspects of general machine learning and deep learning, the progress in associated research and comparison of the effects of pixel-level classification of hyperspectral images are summarized and discussed in this study. The advantages and disadvantages of various algorithms were visually illustrated by comparing the corresponding results. Research objectives and development prospects of hyperspectral image classification are analyzed with respect to two aspects. Firstly, various algorithms need to be studied. A hyperspectral classification algorithm can guarantee classification accuracy required for reducing the algorithm complexity by incorporating multi-source remote sensing data with multi-feature and multi-scale composites. Such an algorithm can improve the classification accuracy of a small sample of the classification model with few parameters, and it can adapt to the intelligent and rapid development requirements of earth observation. Secondly, market applications need to be closely integrated. Practical applications of hyperspectral images should be considered and efficient classification algorithms with marketable competency should be investigated for enhancing the applicability of hyperspectral image classification in remote sensing applications. © 2019, Science Press. All right reserved.","Deep learning; Hyperspectral image; Machine learning; Pixel-level classification","Classification (of information); Computational complexity; Data mining; Deep learning; Hyperspectral imaging; Image analysis; Image enhancement; Learning systems; Machine learning; Observatories; Pixels; Remote sensing; Spectroscopy; Automatic recognition; Classification accuracy; Classification algorithm; Development prospects; Hyper-spectral classification; Multi-scale composites; Pixel level; Remote sensing applications; Image classification"
"Conditional Random Field and Deep Feature Learning for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053633632&doi=10.1109%2fTGRS.2018.2867679&partnerID=40&md5=b07fbcbcf05aebb4a23313e4a04ee956","Image classification is considered to be one of the critical tasks in hyperspectral remote sensing image processing. Recently, a convolutional neural network (CNN) has established itself as a powerful model in classification by demonstrating excellent performances. The use of a graphical model such as a conditional random field (CRF) contributes further in capturing contextual information and thus improving the classification performance. In this paper, we propose a method to classify hyperspectral images by considering both spectral and spatial information via a combined framework consisting of CNN and CRF. We use multiple spectral band groups to learn deep features using CNN, and then formulate deep CRF with CNN-based unary and pairwise potential functions to effectively extract the semantic correlations between patches consisting of 3-D data cubes. Furthermore, we introduce a deep deconvolution network that improves the final classification performance. We also introduced a new data set and experimented our proposed method on it along with several widely adopted benchmark data sets to evaluate the effectiveness of our method. By comparing our results with those from several state-of-the-art models, we show the promising potential of our method. © 1980-2012 IEEE.","Conditional random field (CRF); convolutional neural network (CNN); deep learning; image classification","Convolution; Deep learning; Image classification; Independent component analysis; Neural networks; Random processes; Remote sensing; Semantics; Spectroscopy; Classification performance; Conditional random field; Contextual information; Convolutional Neural Networks (CNN); Deep feature learning; Hyperspectral Remote Sensing Image; Potential function; Spatial informations; Classification (of information); artificial neural network; benchmarking; data set; experimental study; field method; image classification; image processing; machine learning; numerical model; performance assessment; remote sensing; spatial analysis; spectral analysis"
"Fully convolutional networks and geographic object-based image analysis for the classification of VHR imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062995919&doi=10.3390%2frs11050597&partnerID=40&md5=f6504031b624380687a8653025073b77","Land cover Classified maps obtained from deep learning methods such as Convolutional neural networks (CNNs) and fully convolutional networks (FCNs) usually have high classification accuracy but with the detailed structures of objects lost or smoothed. In this work, we develop a methodology based on fully convolutional networks (FCN) that is trained in an end-to-end fashion using aerial RGB images only as input. Skip connections are introduced into the FCN architecture to recover high spatial details from the lower convolutional layers. The experiments are conducted on the city of Goma in the Democratic Republic of Congo. We compare the results to a state-of-the art approach based on a semi-automatic Geographic object image-based analysis (GEOBIA) processing chain. State-of-the art classification accuracies are obtained by both methods whereby FCN and the best baseline method have an overall accuracy of 91.3% and 89.5% respectively. The maps have good visual quality and the use of an FCN skip architecture minimizes the rounded edges that is characteristic of FCN maps. Additional experiments are done to refine FCN classified maps using segments obtained from GEOBIA generated at different scale and minimum segment size. High OA of up to 91.5% is achieved accompanied with an improved edge delineation in the FCN maps, and future work will involve explicitly incorporating boundary information from the GEOBIA segmentation into the FCN pipeline in an end-to-end fashion. Finally, we observe that FCN has a lower computational cost than the standard patch-based CNN approach especially at inference. © 2019 by the authors.","Convolutional neural networks; Fully convolutional networks; Geographical object-based image analysis; Landcover classification; Remote sensing; Very high resolution","Antennas; Convolution; Deep learning; Image analysis; Network architecture; Neural networks; Remote sensing; Convolutional networks; Convolutional neural network; Geographical objects; Land-cover classification; Very high resolution; Image classification"
"Flotation froth image recognition with convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058938052&doi=10.1016%2fj.mineng.2018.12.011&partnerID=40&md5=30dce0742fb88d2f5c08ebbecbc892c8","Computer vision systems designed for flotation froth image analysis are well established in industry, where their ability to measure froth flow velocities and stability are used to control recovery. However, the use of froth image analysis to estimate the concentrations of mineral species in the froth phase is less well established and the reliability of these algorithms depends on the quality of the features that can be extracted from the froth images. Over less than a decade, convolutional neural networks have significantly pushed the boundaries with regard to image recognition in range of technical applications, notably cancer diagnosis, face recognition, remote sensing, as well as applications in the food industry. With the exception of the exploration geosciences, they are yet to make meaningful inroads in the mineral process industries. In this study, the use of three pretrained neural networks architectures to estimate froth grades from industrial image data, namely AlexNet, VGG16 and ResNet is considered. In its pretrained format, AlexNet outperformed previously proposed methods by a significant margin. This margin could be increased markedly via partial retraining of the VGG16 and ResNet34 networks. © 2018 Elsevier Ltd","AlexNet; Convolutional neural networks; Deep learning; Flotation; Image analysis; ResNet34; VGG16","Computer control systems; Convolution; Deep learning; Face recognition; Flotation; Froth flotation; Neural networks; Quality control; Reliability analysis; Remote sensing; AlexNet; Computer vision system; Convolutional neural network; Industrial images; Neural networks architecture; ResNet34; Technical applications; VGG16; Image analysis"
"Detection of Carolina Geranium (Geranium carolinianum) Growing in Competition with Strawberry Using Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064181743&doi=10.1017%2fwsc.2018.66&partnerID=40&md5=f63d152216c0fd44893c7f7959e1cb7f","Weed interference during crop establishment is a serious concern for Florida strawberry [Fragaria × ananassa (Weston) Duchesne ex Rozier (pro sp.) [chiloensis × virginiana]] producers. In situ remote detection for precision herbicide application reduces both the risk of crop injury and herbicide inputs. Carolina geranium (Geranium carolinianum L.) is a widespread broadleaf weed within Florida strawberry production with sensitivity to clopyralid, the only available POST broadleaf herbicide. Geranium carolinianum leaf structure is distinct from that of the strawberry plant, which makes it an ideal candidate for pattern recognition in digital images via convolutional neural networks (CNNs). The study objective was to assess the precision of three CNNs in detecting G. carolinianum. Images of G. carolinianum growing in competition with strawberry were gathered at four sites in Hillsborough County, FL. Three CNNs were compared, including object detection-based DetectNet, image classification-based VGGNet, and GoogLeNet. Two DetectNet networks were trained to detect either leaves or canopies of G. carolinianum. Image classification using GoogLeNet and VGGNet was largely unsuccessful during validation with whole images (Fscore < 0.02). CNN training using cropped images increased G. carolinianum detection during validation for VGGNet (Fscore = 0.77) and GoogLeNet (Fscore = 0.62). The G. carolinianum leaf-trained DetectNet achieved the highest Fscore (0.94) for plant detection during validation. Leaf-based detection led to more consistent detection of G. carolinianum within the strawberry canopy and reduced recall-related errors encountered in canopy-based training. The smaller target of leaf-based DetectNet did increase false positives, but such errors can be overcome with additional training images for network desensitization training. DetectNet was the most viable CNN tested for image-based remote sensing of G. carolinianum in competition with strawberry. Future research will identify the optimal approach for in situ detection and integrate the detection technology with a precision sprayer. © 2018 Weed Science Society of America.","Deep-learning GPU training system; Fscore; image classification; machine vision; object detection","artificial neural network; crop plant; fruit production; growth response; hardware; herb; identification method; image analysis; image classification; pattern recognition; remote sensing; training; weed; Florida [United States]; Hillsborough County [Florida]; United States; Fragaria x ananassa; Geranium carolinianum"
"Pixel-based and object-based terrace extraction using feed-forward deep neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065577366&doi=10.5194%2fisprs-annals-IV-3-W1-1-2019&partnerID=40&md5=2a4798c0068a4fef59d124f6c52b527f","In this paper, we present the identification of terrace field by using Feed-forward back propagation deep neural network in pixel-based and several cases of object-based approaches. Terrace field of Lao Cai area in Vietnam is identified from 5-meter RapidEye image. The image includes 5 bands: red, green, blue, rededge and nir-infrared. Reference data are set of terrace points and nonterrace points, which are generated by randomly selected from reference map. The reference data is separated into three sets: training set for training processing, validation set for generating optimal parameters of deep neural network model, and test set for assessing the accuracy of classification. Six optimal thresholds (T): 0.06, 0.09, 0.12, 0.14, 0.2 and 0.22 are chosen from Rate of Change graph, and then used to generate six cases of object-based classification. Deep neural network (DNN) model is built with 8 hidden layers, input units are 5 bands of RapidEye, and output is terrace and non-terrace classes. Each hidden layer includes 256 units - a large number, to avoid under-fitting. Activation function is Rectifier. Dropout and two regularization parameters are applied to avoid overfitting. Seven terrace maps are generated. The classification results show that the DNN is able to identify terrace field effectively in both pixel-based and object-based approaches. Pixel-based classification is the most accurate approach, achieves 90% accuracy. The values of object-based approaches are 88.5%, 87.3%, 86.7%, 86.6%, 85% and 85.3% correspond to the segmentation thresholds. © 2019 Copernicus GmbH. All rights reserved.","Deep learning; Feed forward; Object-based; Remote sensing; Terrace field","Backpropagation; Classification (of information); Deep learning; Pixels; Remote sensing; Accuracy of classifications; Feed forward; Feed-forward back propagation; Object based; Object-based classifications; Pixel based classifications; Regularization parameters; Terrace field; Deep neural networks"
"Spectral-Spatial Feature Extraction and Classification by ANN Supervised with Center Loss in Hyperspectral Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054375205&doi=10.1109%2fTGRS.2018.2869004&partnerID=40&md5=54e64b7b7d63c8b3eb9f12e7001cb06f","In this paper, we propose a spectral-spatial feature extraction and classification framework based on an artificial neuron network in the context of hyperspectral imagery. With limited labeled samples, only spectral information is exploited for training and spatial context is integrated posteriorly at the testing stage. Taking advantage of recent advances in face recognition, a joint supervision symbol that combines softmax loss and center loss is adopted to train the proposed network, by which intraclass features are gathered while interclass variations are enlarged. Based on the learned architecture, the extracted spectrum-based features are classified by a center classifier. Moreover, to fuse the spectral and spatial information, an adaptive spectral-spatial center classifier is developed, where multiscale neighborhoods are considered simultaneously, and the final label is determined using an adaptive voting strategy. Finally, experimental results on three well-known data sets validate the effectiveness of the proposed methods compared with the state-of-the-art approaches. © 1980-2012 IEEE.","Artificial neural networks (ANN); deep learning; feature extraction; hyperspectral image classification","Deep learning; Extraction; Face recognition; Feature extraction; Hyperspectral imaging; Image classification; Iron; Neural networks; Neurons; Personnel training; Remote sensing; Spectroscopy; Testing; Artificial neuron networks; Hyper-spectral imageries; Spatial context; Spatial features; Spatial informations; Spectral information; State-of-the-art approach; Voting strategies; Classification (of information); artificial neural network; image classification; imagery; machine learning; spatial analysis; spectral analysis"
"Automated detection of bird roosts using NEXRAD radar data and Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052453968&doi=10.1002%2frse2.92&partnerID=40&md5=361ac9401afbdec28e5d030aaa4fb389","Although NEXRAD radars have proven to be an effective tool for detecting airborne animals, detecting biological phenomena in radar images often involves a manual, time- consuming data-extraction process. This paper focuses on applying machine learning to automatically find radar data that snapshots large aggregations of birds (specifically Purple Martins and Tree Swallows) as they depart en masse from roosting sites. These aggregations are evident in radar images as rings of elevated reflectivity that appear early in the morning as birds depart from roost sites. Our goal was to develop an algorithm that could determine whether an individual radar image contained at least one Purple Martin or Tree Swallow roost. We use a dataset of known roost locations to train three machine learning algorithms that employed (1) a traditional Artificial Neural Network (ANN), (2) a sophisticated preexisting Convolutional Neural Network (CNN) called Inception-v3, and (3) a shallow CNN built from scratch. The resulting programs were all effective at finding bird roosts, with both the shallow CNN and the Inception-v3 network making correct determinations about 90 per cent of the time with an AUC above.9. To the best of our knowledge, this study is the first to apply neural networks in the analysis of bird roosts in radar imagery, and these analytical tools offer new avenues of research into the ecology and behavior of flying animals, with practical applications to wind farm placement, air traffic administration and wildlife conservation. The NEXRAD radar network offers a tremendous archive of continental-scale data and has the potential to capture entire vertebrate populations. We apply existing machine learning models to a new dataset which constitutes a valuable approach to extracting information from this archive. © 2018 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London.","Aeroecology; bird roosts; deep learning; machine learning",
"Smart Vessel Detection using Deep Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063189478&doi=10.1109%2fCTIT.2018.8649543&partnerID=40&md5=d0167af6dbede60f524085a00baf2993","Detecting ships automatically from the satellite images is one of the major challenging constraint due to lot of disturbances because of cluttered scenes, ship sizes etc. Small object detection in satellite images like ships, boat, vessels is a challenging task. In the state of art method, numbers of features are used to enhance the accuracy of detection but that mostly happens in homogeneous environment. Few research articles deals with heterogeneous environment such as sea shore areas, harbors, islands etc. Deep learning is one of the emerging technology in the recent days especially in the field of computer vision to provide an accurate result. It paves the new channel to the entire machine learning paradigm and provides the advancement of artificial intelligence utilization and also enhances the human computer interaction in a much feasible manner. In this article the deep learning algorithms such as convolutional neural network is used with the proper hybrid and the performance is compared with the state of art method. The deep learning algorithms result excellent accuracy in terms of classification of satellite images without any manual feature extraction. Since the deep learning algorithms does not have the manual feature extraction, it provides an excellent experimental results in the classification of ships in satellite images that has complex background. © 2018 IEEE.","Artificial Intelligence; Deep learning; Image classification; Remote sensing; Satellite images; Ship detection","Artificial intelligence; Arts computing; Convolution; Data mining; Deep learning; Deep neural networks; Engineering education; Extraction; Feature extraction; Human computer interaction; Image classification; Neural networks; Object detection; Remote sensing; Ships; Small satellites; Complex background; Convolutional neural network; Emerging technologies; Heterogeneous environments; Satellite images; Ship detection; Small object detection; State-of-art methods; Learning algorithms"
"Deep learning approaches for unwrapping phase images with steep spatial gradients: A simulation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063144430&doi=10.1109%2fICSEE.2018.8646266&partnerID=40&md5=9c844d0f3779fb89e02f6cae2faabcac","We explore different deep learning-based approaches to solve the problem of phase unwrapping in objects with high spatial gradients, which is applicable to many fields in medicine, biology and remote sensing. We simulate data with high spatial gradients to compare the quality of the solution and the runtime obtained when addressing this problem either as an inverse problem or as a semantic segmentation problem. © 2018 IEEE.","Deep learning; inverse problems; phase imaging; phase unwrapping; semantic segmentation","Inverse problems; Problem solving; Remote sensing; Semantics; Learning approach; Learning-based approach; Phase image; Phase imaging; Phase unwrapping; Runtimes; Semantic segmentation; Spatial gradients; Deep learning"
"GAN-NL: Unsupervised representation learning for remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063079226&doi=10.1109%2fGlobalSIP.2018.8646414&partnerID=40&md5=dc241ed07157416d3fbe8a0c0c5baa1b","Recently, deep learning methods have greatly enhanced the classification performance because of their strong representation ability in the local receptive field. However, the non-local spatial information always exist in the images. Moreover, the limited amount of the labeled data imposes great challenges on the supervised representation learning model, especially the remote sensing images. With the consideration, we propose a generative adversarial network with non-local spatial information (GAN-NL) for remote sensing image classification. Specifically, a non-local layer is incorporated into a generative adversarial network for unsupervised representation learning. Then, a classification network is designed to infer the labels of the images. The classification results on the challenging NWPU-RESISC45 remote sensing image dataset show that our proposed method performs favorably against the state-of-the-art methods in terms of the classification accuracy without any pre-training. © 2018 IEEE.","Generative adversarial networks (GANs); Non-local spatial information; Remote sensing image classification; Unsupervised representation learning","Classification (of information); Deep learning; Remote sensing; Adversarial networks; Classification accuracy; Classification networks; Classification performance; Remote sensing image classification; Spatial informations; State-of-the-art methods; Unsupervised representation learning; Image classification"
"Detection of Water-Bodies Using Semantic Segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063427738&doi=10.1109%2fCSPIS.2018.8642743&partnerID=40&md5=88bef6dbb20768d3911c945bb1064cd8","This paper proposes a semantic segmentation technique to automatically detect water-bodies from DubaiSat-2 images. The proposed method uses a deep convolutional neural network transfer-learning model. Several evaluation metrics such as accuracy, precision, and Jaccard coefficient are used to test our proposed algorithm. The overall accuracy for the prediction of water-bodies in DubaiSat-2 image dataset is 99.86%. © 2018 IEEE.","Deep learning; neural network; remote sensing; satellite images; semantic segmentation","Deep learning; Deep neural networks; Neural networks; Remote sensing; Security of data; Semantics; Convolutional neural network; Evaluation metrics; Image datasets; Jaccard coefficients; Overall accuracies; Satellite images; Semantic segmentation; Waterbodies; Image segmentation"
"Comparison of CNNs for Remote Sensing Scene Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063149441&doi=10.1109%2fICCES.2018.8639467&partnerID=40&md5=68765c98e34a3c431c1d4d36f723511d","Nowadays, deep learning are used widely in many applications related to remote sensing i.e. earth observation, urban planning, earth's scene classification, and so on. The deep learning manner, especially CNNs, has proved its accuracy for these practical applications. Hence, in this article, CNNs models are reviewed and its five different architectures are applied for comparisons; namely, AlexNet, VGGNet, GoogleNet, Inception-V3, and ResNet-101. These models are carried out on seven different remote-sensing image datasets for image scene classification purpose; namely, WHU-RS19, UC-Merced Land Use, SIRI-WHU, RSSCN7, AID, PatternNet, and NWPU-RESISC45. These datasets have different spatial resolutions, ranging from 0.2 to 30, to differentiate the classification accuracy of the low and high resolution images. As well, the classification accuracy of each model is assessed by trying five different classifiers; namely, Naïve Bayes, Decision Tree, Random Forest, K-Nearest Neighbor (KNN), and Support Vector Machine (SVM). The best accuracy credits to ResNet-101 model with SVM classifier; it has reached about 98.6±0.02 % of the high resolution dataset, PatternNet. © 2018 IEEE.","Convolutional Neural Networks; Deep Learning; Remote-Sensing; Satellite Images; Scene Classification","Classification (of information); Decision trees; Deep learning; Image classification; Land use; Nearest neighbor search; Neural networks; Support vector machines; Classification accuracy; Convolutional neural network; High resolution image; Image scene classification; K nearest neighbor (KNN); Remote sensing images; Satellite images; Scene classification; Remote sensing"
"Bidirectional adaptive feature fusion for remote sensing scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054446764&doi=10.1016%2fj.neucom.2018.03.076&partnerID=40&md5=be11b3ec60bc5b98ad289b77a3b9e654","Scene classification has become an effective way to interpret the High Spatial Resolution (HSR) remote sensing images. Recently, Convolutional Neural Networks (CNN) have been found to be excellent for scene classification. However, only using the deep models as feature extractor on the aerial image directly is not proper, because the extracted deep features can not capture spatial scale variability and rotation variability in HSR remote sensing images. To relieve this limitation, a bidirectional adaptive feature fusion strategy is investigated to deal with the remote sensing scene classification. The deep learning feature and the SIFT feature are fused together to get a discriminative image presentation. The fused feature can not only describe the scenes effectively by employing deep learning feature but also overcome the scale and rotation variability with the usage of the SIFT feature. By fusing both SIFT feature and global CNN feature, our method achieves state-of-the-art scene classification performances on the UCMerced, the Sydney and the AID datasets. © 2018 Elsevier B.V.","Bidirectional adaptive feature fusion; High spatial resolution remote Sensing images; Scene classification","Antennas; Deep learning; Image resolution; Neural networks; Remote sensing; Adaptive features; Convolutional Neural Networks (CNN); High spatial resolution; Image presentations; Remote sensing images; Scale and rotation; Scene classification; Spatial-scale variability; Classification (of information); article; learning; remote sensing; rotation"
"Wildfire mapping in interior alaska using deep neural networks on imbalanced datasets","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062879064&doi=10.1109%2fICDMW.2018.00116&partnerID=40&md5=5638685f44d5d4facdcf2a36faac1d1c","Wildfires are the dominant disturbance impacting many regions in Alaska and are expected to intensify due to climate change. Accurate tracking and quantification of wildfires are important for climate modeling and ecological studies in this region. Remote sensing platforms (e.g., MODIS, Landsat) are valuable tools for mapping wildfire events (burned or burning areas) in Alaska. Deep neural networks (DNN) have exhibited superior performance in many classification problems, such as high-dimensional remote sensing data. Detection of wildfires is an imbalanced classification problem where one class contains a much smaller or larger sample size, and performance of DNNs can decline. We take a known weight-selection strategy during DNN training and apply those weights to MODIS variables (e.g., NDVI, surface reflectance) for binary classification (i.e., wildfire or no-wildfire) across Alaska during the 2004 wildfire year, when Alaska experienced a record number of large wildfires. The method splits the input training data into subsets, one for training the DNN to update weights and the other for performance validation to select the weights based on the best validation-loss score. This approach was applied to two sampled datasets, such as where the no-wildfire class can significantly outweigh the wildfire class. The normal DNN training strategy was unable to map wildfires for the highly imbalanced dataset; however, the weight-selection strategy was able to map wildfires very accurately (0.96 recall score for 78,702 wildfire pixels (500 × 500 m)). © 2018 IEEE.","Deep Learning; Imbalanced Classification; MODIS; Wildfire","Climate change; Data mining; Deep learning; Fires; Mapping; Radiometers; Remote sensing; Binary classification; Imbalanced classification; Imbalanced Data-sets; MODIS; Performance validation; Remote sensing data; Remote sensing platforms; Wildfire; Deep neural networks"
"Research on Semantic Segmentation of High-resolution Remote Sensing Image Based on Full Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062887881&doi=10.1109%2fISAPE.2018.8634106&partnerID=40&md5=2f54946ecdf86422697f9c3228384d0b","Remote sensing data is an important way to reflect the comprehensive information of surface. In this paper, based on the semantic segmentation of high-resolution remote sensing images, a segmentation method based on full convolutional neural network (FCN) is proposed. The method improves the traditional convolutional neural network (CNN) and replaces the final fully connected layer of the CNN network with a convolutional layer. And then optimize the convolution operation by using the matrix expansion technique. The experimental results show that the FCN network with sufficient training and fine-tuning can effectively perform automatic semantic segmentation of high-resolution remote sensing images. The correct segmentation accuracy is higher than 85%, which improves the efficiency of convolution operations. © 2018 IEEE.","deep learning; full convolutional neural network; semantic image segmentation","Convolution; Deep learning; Neural networks; Remote sensing; Semantic Web; Semantics; Comprehensive information; Convolutional neural network; High resolution remote sensing images; Remote sensing data; Segmentation accuracy; Segmentation methods; Semantic image segmentations; Semantic segmentation; Image segmentation"
"Local deep descriptor for remote sensing image feature matching","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062534749&doi=10.3390%2frs11040430&partnerID=40&md5=46a1a830c2e72fbcd0a40057d1c14483","Feature matching via local descriptors is one of the most fundamental problems in many computer vision tasks, as well as in the remote sensing image processing community. For example, in terms of remote sensing image registration based on the feature, feature matching is a vital process to determine the quality of transform model. While in the process of feature matching, the quality of feature descriptor determines the matching result directly. At present, the most commonly used descriptor is hand-crafted by the designer's expertise or intuition. However, it is hard to cover all the different cases, especially for remote sensing images with nonlinear grayscale deformation. Recently, deep learning shows explosive growth and improves the performance of tasks in various fields, especially in the computer vision community. Here, we created remote sensing image training patch samples, named Invar-Dataset in a novel and automatic way, then trained a deep learning convolutional neural network, named DescNet to generate a robust feature descriptor for feature matching. A special experiment was carried out to illustrate that our created training dataset was more helpful to train a network to generate a good feature descriptor. A qualitative experiment was then performed to show that feature descriptor vector learned by the DescNet could be used to register remote sensing images with large gray scale difference successfully. A quantitative experiment was then carried out to illustrate that the feature vector generated by the DescNet could acquire more matched points than those generated by hand-crafted feature Scale Invariant Feature Transform (SIFT) descriptor and other networks. On average, the matched points acquired by DescNet was almost twice those acquired by other methods. Finally, we analyzed the advantages of our created training dataset Invar-Dataset and DescNet and gave the possible development of training deep descriptor network. © 2019 by the authors.","Deep descriptor network; Deep learning; Feature descriptor; Feature matching; Image registration","Computer vision; Deep learning; Image registration; Neural networks; Convolutional neural network; Descriptors; Feature descriptors; Feature matching; Qualitative experiments; Quantitative experiments; Remote sensing image processing; Scale invariant feature transforms; Remote sensing"
"Joint Deep Learning for land cover and land use classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056770380&doi=10.1016%2fj.rse.2018.11.014&partnerID=40&md5=490a664d0d6aa2e6f5c4acefed01dbfe","Land cover (LC) and land use (LU) have commonly been classified separately from remotely sensed imagery, without considering the intrinsically hierarchical and nested relationships between them. In this paper, for the first time, a highly novel Joint Deep Learning framework is proposed and demonstrated for LC and LU classification. The proposed Joint Deep Learning (JDL) model incorporates a multilayer perceptron (MLP) and convolutional neural network (CNN), and is implemented via a Markov process involving iterative updating. In the JDL, LU classification conducted by the CNN is made conditional upon the LC probabilities predicted by the MLP. In turn, those LU probabilities together with the original imagery are re-used as inputs to the MLP to strengthen the spatial and spectral feature representations. This process of updating the MLP and CNN forms a joint distribution, where both LC and LU are classified simultaneously through iteration. The proposed JDL method provides a general framework within which the pixel-based MLP and the patch-based CNN provide mutually complementary information to each other, such that both are refined in the classification process through iteration. Given the well-known complexities associated with the classification of very fine spatial resolution (VFSR) imagery, the effectiveness of the proposed JDL was tested on aerial photography of two large urban and suburban areas in Great Britain (Southampton and Manchester). The JDL consistently demonstrated greatly increased accuracies with increasing iteration, not only for the LU classification, but for both the LC and LU classifications, achieving by far the greatest accuracies for each at around 10 iterations. The average overall classification accuracies were 90.18% for LC and 87.92% for LU for the two study sites, far higher than the initial accuracies and consistently outperforming benchmark comparators (three each for LC and LU classification). This research, thus, represents the first attempt to unify the remote sensing classification of LC (state; what is there?) and LU (function; what is going on there?), where previously each had been considered separately only. It, thus, has the potential to transform the way that LC and LU classification is undertaken in future. Moreover, it paves the way to address effectively the complex tasks of classifying LC and LU from VFSR remotely sensed imagery via joint reinforcement, and in an automatic manner. © 2018 Elsevier Inc.","Convolutional neural network; Land cover and land use classification; Multilayer perceptron; Object-based CNN; VFSR remotely sensed imagery","Aerial photography; Antennas; Classification (of information); Convolution; Iterative methods; Land use; Markov processes; Multilayer neural networks; Multilayers; Neural networks; Remote sensing; Classification accuracy; Convolutional neural network; Convolutional Neural Networks (CNN); Landuse classifications; Object based; Remote sensing classification; Remotely sensed imagery; Urban and suburban areas; Deep learning; aerial photography; algorithm; artificial neural network; land cover; land use change; remote sensing; satellite imagery; spatial resolution; suburban area; urban area; England; Manchester [England]; Southampton [England]; United Kingdom"
"Deep learning based multi-temporal crop classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057535465&doi=10.1016%2fj.rse.2018.11.032&partnerID=40&md5=d3d7bb976ee561ce56ccd54d0d099bfb","This study aims to develop a deep learning based classification framework for remotely sensed time series. The experiment was carried out in Yolo County, California, which has a very diverse irrigated agricultural system dominated by economic crops. For the challenging task of classifying summer crops using Landsat Enhanced Vegetation Index (EVI) time series, two types of deep learning models were designed: one is based on Long Short-Term Memory (LSTM), and the other is based on one-dimensional convolutional (Conv1D) layers. Three widely-used classifiers were also tested for comparison, including a gradient boosting machine called XGBoost, Random Forest, and Support Vector Machine. Although LSTM is widely used for sequential data representation, in this study its accuracy (82.41%) and F1 score (0.67) were the lowest among all the classifiers. Among non-deep-learning classifiers, XGBoost achieved the best result with 84.17% accuracy and an F1 score of 0.69. The highest accuracy (85.54%) and F1 score (0.73) were achieved by the Conv1D-based model, which mainly consists of a stack of Conv1D layers and an inception module. The behavior of the Conv1D-based model was inspected by visualizing the activation on different layers. The model employs EVI time series by examining shapes at various scales in a hierarchical manner. Lower Conv1D layers of the optimized model capture small scale temporal variations, while upper layers focus on overall seasonal patterns. Conv1D layers were used as an embedded multi-level feature extractor in the classification model which automatically extracts features from input time series during training. The automated feature extraction reduces the dependency on manual feature engineering and pre-defined equations of crop growing cycles. This study shows that the Conv1D-based deep learning framework provides an effective and efficient method of time series representation in multi-temporal classification tasks. © 2018","Artificial neural network; Convolutional neural network; Crop classification; Deep learning; Landsat; Multi-temporal classification","Classification (of information); Convolution; Crops; Cultivation; Decision trees; Long short-term memory; Neural networks; Time series; Water management; Automated feature extraction; Classification framework; Classification models; Convolutional neural network; Crop classification; Enhanced vegetation index; LANDSAT; Multitemporal classification; Deep learning; accuracy assessment; artificial neural network; experimental study; farming system; Landsat; machine learning; numerical model; remote sensing; summer; time series analysis; vegetation classification; vegetation index; visualization; California; United States; Yolo County"
"Deep Neural Network Pruning Based Two-Stage Remote Sensing Image Object Detection [基于深度神经网络剪枝的两阶段遥感图像目标检测]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065881046&doi=10.12068%2fj.issn.1005-3026.2019.02.005&partnerID=40&md5=0747b9439b1716fec14d819346caac6a","In the object detection of high-resolution remote-sensing images, affected by cloud, light, complex background, noise and other factors, the existing object detection method has high false alarm, low speed and low precision. So we propose a two-stage object detection method based on deep pruning. First, we propose deep pruning, and then based on the deep pruning we propose an algorithm that learns region proposal network automatically and an algorithm that we train classification network with optimizing training method. We then apply the two algorithms to convolutional neural network and get a two-stage object detection model. The experiment result shows that our method has a certain improvement on precision and speed compared with the state-of-the-art method. © 2019, Editorial Department of Journal of Northeastern University. All right reserved.","Computer vision; Convolutional neural network; Deep learning; High-resolution remote sensing image; Object detection","Computer vision; Convolution; Deep learning; Deep neural networks; Neural networks; Object recognition; Remote sensing; Classification networks; Complex background; Convolutional neural network; High resolution remote sensing images; Object detection method; Remote sensing images; State-of-the-art methods; Training methods; Object detection"
"Scene classification with recurrent attention of VHR remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052862263&doi=10.1109%2fTGRS.2018.2864987&partnerID=40&md5=c0b653952d49e6625b9d194b0f0d6d5a","Scene classification of remote sensing images has drawn great attention because of its wide applications. In this paper, with the guidance of the human visual system (HVS), we explore the attention mechanism and propose a novel end-to-end attention recurrent convolutional network (ARCNet) for scene classification. It can learn to focus selectively on some key regions or locations and just process them at high-level features, thereby discarding the noncritical information and promoting the classification performance. The contributions of this paper are threefold. First, we design a novel recurrent attention structure to squeeze high-level semantic and spatial features into several simplex vectors for the reduction of learning parameters. Second, an end-to-end network named ARCNet is proposed to adaptively select a series of attention regions and then to generate powerful predictions by learning to process them sequentially. Third, we construct a new data set named OPTIMAL-31, which contains more categories than popular data sets and gives researchers an extra platform to validate their algorithms. The experimental results demonstrate that our model makes great promotion in comparison with the state-of-the-art approaches. © 2018 IEEE.","Attention; convolutional neural network (CNN); deep learning; long short-term memory (LSTM); recurrent neural networks (RNN); remote sensing; scene classification","Classification (of information); Convolution; Deep learning; Image classification; Remote sensing; Semantics; Attention; Classification performance; Convolutional networks; Convolutional Neural Networks (CNN); Human visual system (HVS); Recurrent neural network (RNN); Scene classification; State-of-the-art approach; Long short-term memory; artificial neural network; data set; design; image classification; information; remote sensing"
"Fast spectral clustering for unsupervised hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062486181&doi=10.3390%2frs11040399&partnerID=40&md5=22d9b7bb74348a25b693cd130ceefd5f","Hyperspectral image classification is a challenging and significant domain in the field of remote sensing with numerous applications in agriculture, environmental science, mineralogy, and surveillance. In the past years, a growing number of advanced hyperspectral remote sensing image classification techniques based on manifold learning, sparse representation and deep learning have been proposed and reported a good performance in accuracy and efficiency on state-of-the-art public datasets. However, most existing methods still face challenges in dealing with large-scale hyperspectral image datasets due to their high computational complexity. In this work, we propose an improved spectral clustering method for large-scale hyperspectral image classification without any prior information. The proposed algorithm introduces two efficient approximation techniques based on Nyström extension and anchor-based graph to construct the affinity matrix. We also propose an effective solution to solve the eigenvalue decomposition problem by multiplicative update optimization. Experiments on both the synthetic datasets and the hyperspectral image datasets were conducted to demonstrate the efficiency and effectiveness of the proposed algorithm. © 2019 by the authors.","Hyperspectral image classification; Manifold learning; Remote sensing; Spectral clustering; Unsupervised learning","Approximation algorithms; Classification (of information); Cluster analysis; Clustering algorithms; Deep learning; Efficiency; Eigenvalues and eigenfunctions; Hyperspectral imaging; Image enhancement; Large dataset; Minerals; Remote sensing; Spectroscopy; Unsupervised learning; Approximation techniques; Eigenvalue decomposition; Environmental science; Hyperspectral Remote Sensing Image; Manifold learning; Multiplicative updates; Spectral clustering; Spectral clustering methods; Image classification"
"UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062501961&doi=10.3390%2frs11040410&partnerID=40&md5=5b1bb852fc3e2c2fbaeedeaf90a1719e","Traditional plant breeding evaluation methods are time-consuming, labor-intensive, and costly. Accurate and rapid phenotypic trait data acquisition and analysis can improve genomic selection and accelerate cultivar development. In this work, a technique for data acquisition and image processing was developed utilizing small unmanned aerial vehicles (UAVs), multispectral imaging, and deep learning convolutional neural networks to evaluate phenotypic characteristics on citrus crops. This low-cost and automated high-throughput phenotyping technique utilizes artificial intelligence (AI) and machine learning (ML) to: (i) detect, count, and geolocate trees and tree gaps; (ii) categorize trees based on their canopy size; (iii) develop individual tree health indices; and (iv) evaluate citrus varieties and rootstocks. The proposed remote sensing technique was able to detect and count citrus trees in a grove of 4,931 trees, with precision and recall of 99.9% and 99.7%, respectively, estimate their canopy size with overall accuracy of 85.5%, and detect, count, and geolocate tree gaps with a precision and recall of 100% and 94.6%, respectively. This UAV-based technique provides a consistent, more direct, cost-effective, and rapid method to evaluate phenotypic characteristics of citrus varieties and rootstocks. © 2019 by the authors.","Artificial intelligence; Deep learning; Machine learning; Neural networks; Precision agriculture; Smart agriculture; UAV","Antennas; Artificial intelligence; Cost effectiveness; Data acquisition; Forestry; Image processing; Learning systems; Machine learning; Neural networks; Precision agriculture; Remote sensing; Throughput; Unmanned aerial vehicles (UAV); Convolutional neural network; Evaluation methods; High-throughput phenotyping; Multispectral imaging; Overall accuracies; Precision and recall; Remote sensing techniques; Smart agricultures; Deep learning"
"DCNR: deep cube CNN with random forest for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046163211&doi=10.1007%2fs11042-018-5986-5&partnerID=40&md5=ea103715562d59fe6fe10b354474cae1","Hyperspectral Image (HSI) classification is one of the fundamental tasks in the field of remote sensing data analysis. CNN (Convolutional Neural Network) has been proven to be an effective deep learning model, which can extract high-level features directly from the raw data and thereby utilize rich information contained in HSI data. However, labor cost to label enough HIS data for training model is usually expensive, so that it is a strong demand of utilizing limited training data to get a satisfied classification accuracy. In this paper, we put forward a deep cube CNN model – DCNR, which is composed of a cube neighbor HSI pixels strategy, a deep CNN and a random forest classifier. In DCNR model, cubic samples, containing spectral-spatial information, are generated by putting each target pixel and its neighbors together. Then features with high representative ability, extracted by applying a specially designed cube CNN model on each cubic sample, are fed into the random forest classifier for the classification of the target pixel. Results show that DCNR model can achieve classification accuracy of 96.78%, 96.08% and 94.85% on KSC, IP and SA datasets respectively with 20% samples as training set, and 85.03%, 83.45 and 62.17% on KSC, IP and SA datasets respectively with only 1% samples as training set, significantly outperforming random forest and cube CNN models. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","CNN; Deep learning; HSI classification; Random forest; Spectral-spatial feature","Decision trees; Deep learning; Geometry; Image classification; Internet protocols; Neural networks; Pixels; Remote sensing; Spectroscopy; Wages; Classification accuracy; Convolutional neural network; Limited training data; Random forest classifier; Random forests; Remote sensing data; Spatial features; Spatial informations; Classification (of information)"
"Earth observation and machine learning to meet Sustainable Development Goal 8.7: Mapping sites associated with slavery from space","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061380866&doi=10.3390%2frs11030266&partnerID=40&md5=193283e10d071efe099df68e9ea1082d","A large proportion of the workforce in the brick kilns of the Brick Belt of Asia are modern-day slaves. Work to liberate slaves and contribute to UN Sustainable Development Goal 8.7 would benefit from maps showing the location of brick kilns. Previous work has shown that brick kilns can be accurately identified and located visually from fine spatial resolution remote-sensing images. Furthermore, via crowdsourcing, it would be possible to map very large areas. However, concerns over the ability to maintain a motivated crowd to allow accurate mapping over time together with the development of advanced machine learning methods suggest considerable potential for rapid, accurate and repeatable automated mapping of brick kilns. This potential is explored here using fine spatial resolution images of a region of Rajasthan, India. A contemporary deep-learning classifier founded on region-based convolution neural networks (R-CNN), the Faster R-CNN, was trained to classify brick kilns. This approach mapped all of the brick kilns within the study area correctly, with a producer's accuracy of 100%, but at the cost of substantial over-estimation of kiln numbers. Applying a second classifier to the outputs substantially reduced the over-estimation. This second classifier could be visual classification, which, as it focused on a relatively small number of sites, should be feasible to acquire, or an additional automated classifier. The result of applying a CNN classifier to the outputs of the original classification was a map with an overall accuracy of 94.94% with both low omission and commission error that should help direct anti-slavery activity on the ground. These results indicate that contemporary Earth observation resources and machine learning methods may be successfully applied to help address slavery from space. © 2019 by the authors.","Convolutional neural network; Crowdsourcing; Mapping; Object-based target detection; Slavery","Brickmaking; Convolution; Crowdsourcing; Deep learning; Earth (planet); Image resolution; Kilns; Machine learning; Mapping; Neural networks; Observatories; Planning; Remote sensing; Sustainable development; Convolution neural network; Convolutional neural network; Machine learning methods; Object based; Omission and commission errors; Remote sensing images; Slavery; Spatial resolution images; Brick"
"A framework for evaluating land use and land cover classification using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061383356&doi=10.3390%2frs11030274&partnerID=40&md5=5d93ebca7b7ddb4bfdd0b2af159bc9d5","Analyzing land use and land cover (LULC) using remote sensing (RS) imagery is essential for many environmental and social applications. The increase in availability of RS data has led to the development of new techniques for digital pattern classification. Very recently, deep learning (DL) models have emerged as a powerful solution to approach many machine learning (ML) problems. In particular, convolutional neural networks (CNNs) are currently the state of the art for many image classification tasks. While there exist several promising proposals on the application of CNNs to LULC classification, the validation framework proposed for the comparison of different methods could be improved with the use of a standard validation procedure for ML based on cross-validation and its subsequent statistical analysis. In this paper, we propose a general CNN, with a fixed architecture and parametrization, to achieve high accuracy on LULC classification over RS data from different sources such as radar and hyperspectral. We also present a methodology to perform a rigorous experimental comparison between our proposed DL method and other ML algorithms such as support vector machines, random forests, and k-nearest-neighbors. The analysis carried out demonstrates that the CNN outperforms the rest of techniques, achieving a high level of performance for all the datasets studied, regardless of their different characteristics. © 2019 by the authors.","Convolutional neural network; Cross-validation; Deep learning; Land cover classification; Land use classification; Remote sensing; Statistical analysis","Convolution; Decision trees; Deep learning; Nearest neighbor search; Neural networks; Remote sensing; Statistical methods; Convolutional neural network; Cross validation; Experimental comparison; Land cover classification; Land use and land cover; Land-use and land cover classifications; Landuse classifications; Standard validations; Land use"
"dPEN: deep Progressively Expanded Network for mapping heterogeneous agricultural landscape using WorldView-3 satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058964035&doi=10.1016%2fj.rse.2018.11.031&partnerID=40&md5=5d08842a9419cd4f6c9dc57a24496aab","Accurately mapping heterogeneous agricultural landscape is an important prerequisite for agricultural field management (e.g., weed control), plant phenotyping and yield prediction, as well as ecological characterization. Compared to traditional mapping practices that require intensive field surveys, remote sensing technologies offer efficient and cost-effective means for crop type mapping from regional to global scales. However, mapping heterogeneous agricultural landscape is a challenge because of diverse and complex spectral profiles of crops. We propose a novel deep learning method, namely deep progressively expanded network (dPEN), for mapping nineteen different objects including crop types, weeds and crop residues, in a heterogeneous agricultural field using WorldView-3 (WV-3) imagery. To assess the mapping accuracy of dPEN, we created a calibrated WV-3 dataset with the corresponding ground truth. In addition, the suitability of visible/near-infrared (VNIR, 400–1040 nm) and short-wave infrared (SWIR, 1195 nm–2365 nm) bands of WV-3 to classification accuracy were examined and discussed in detail. To the best of our knowledge, this is the first effort to explore the significance of all SWIR bands in WV-3 for classification accuracy in a heterogeneous agricultural landscape. The results demonstrated that: (1) The proposed dPEN allows for building a deeper neural network from multispectral data which was the limitation of many convolutional neural networks; (2) dPEN was able to extract more discriminative features from VNIR and SWIR bands by producing the highest overall accuracy (OA: 86.06%) over competing methods such as support vector machine and random forest; (3) The inclusion of WV-3 SWIR bands greatly improved the classification accuracy; (4) SWIR bands were particularly beneficial to improve the classification accuracy of some individual classes such as weeds, crop residues, and corn and soybean during late developmental stages; (5) The red-edge band (705–745 nm) was identified as the most important band affecting the classification accuracy nearly 10%, whereas the coastal band (400–450 nm) provided the lowest contribution; and (6) SWIR-5 band (2145–2185 nm) contributed most to OA by enhancing it approximately 4% when combined with VNIR bands, while SWIR-1 (1195–1225 nm) yielded the lowest improvement (1.55%) for OA. These research outcomes provide useful information for efficiently mapping agricultural landscape, and indicate the potential practices of dPEN and contributions of spectral bands in WV-3 for plant phenotyping, weed control, and crop residue retention. © 2018 Elsevier Inc.","Convolutional neural network; Crops; Deep learning; Image classification; Residues; SWIR; VNIR; Weeds; WorldView-3","Agricultural wastes; Convolution; Cost effectiveness; Crops; Decision trees; Deep learning; Image classification; Neural networks; Photomapping; Plants (botany); Remote sensing; Satellite imagery; Weed control; Convolutional neural network; Residues; SWIR; VNIR; Weeds; WorldView-3; Infrared radiation; accuracy assessment; agricultural land; artificial neural network; crop plant; crop residue; heterogeneity; image classification; machine learning; mapping method; remote sensing; satellite imagery; spectral analysis; support vector machine; WorldView; Glycine max; Zea mays"
"Unsupervised deep noise modeling for hyperspectral image change detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061350307&doi=10.3390%2frs11030258&partnerID=40&md5=ebb1ae91e57e5238b6e19128e9b284ab","Hyperspectral image (HSI) change detection plays an important role in remote sensing applications, and considerable research has been done focused on improving change detection performance. However, the high dimension of hyperspectral data makes it hard to extract discriminative features for hyperspectral processing tasks. Though deep convolutional neural networks (CNN) have superior capability in high-level semantic feature learning, it is difficult to employ CNN for change detection tasks. As a ground truth map is usually used for the evaluation of change detection algorithms, it cannot be directly used for supervised learning. In order to better extract discriminative CNN features, a novel noise modeling-based unsupervised fully convolutional network (FCN) framework is presented for HSI change detection in this paper. Specifically, the proposed method utilizes the change detection maps of existing unsupervised change detection methods to train the deep CNN, and then removes the noise during the end-to-end training process. The main contributions of this paper are threefold: (1) A new end-to-end FCN-based deep network architecture for HSI change detection is presented with powerful learning features; (2) An unsupervised noise modeling method is introduced for the robust training of the proposed deep network; (3) Experimental results on three datasets confirm the effectiveness of the proposed method. © 2019 by the authors.","Change detection; Convolutional neural networks (CNN); Deep learning; Hyperspectral images (HSI); Noise modeling","Convolution; Deep learning; Deep neural networks; Image enhancement; Machine learning; Network architecture; Neural networks; Remote sensing; Semantics; Spectroscopy; Change detection; Change detection algorithms; Convolutional neural network; Discriminative features; High-level semantic features; Noise modeling; Remote sensing applications; Unsupervised change detection; Feature extraction"
"Fusion of multiscale convolutional neural networks for building extraction in very high-resolution images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061370049&doi=10.3390%2frs11030227&partnerID=40&md5=e8b04a3bcef350ecdd9403301d1ad094","Extracting buildings from very high resolution (VHR) images has attracted much attention but is still challenging due to their large varieties in appearance and scale. Convolutional neural networks (CNNs) have shown effective and superior performance in automatically learning high-level and discriminative features in extracting buildings. However, the fixed receptive fields make conventional CNNs insufficient to tolerate large scale changes. Multiscale CNN (MCNN) is a promising structure to meet this challenge. Unfortunately, the multiscale features extracted by MCNN are always stacked and fed into one classifier, which make it difficult to recognize objects with different scales. Besides, the repeated sub-sampling processes lead to a blurred boundary of the extracted features. In this study, we proposed a novel parallel support vector mechanism (SVM)-based fusion strategy to take full use of deep features at different scales as extracted by the MCNN structure. We firstly designed a MCNN structure with different sizes of input patches and kernels, to learn multiscale deep features. After that, features at different scales were individually fed into different support vector machine (SVM) classifiers to produce rule images for pre-classification. A decision fusion strategy is then applied on the pre-classification results based on another SVM classifier. Finally, superpixels are applied to refine the boundary of the fused results using region-based maximum voting. For performance evaluation, the well-known International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam dataset was used in comparison with several state-of-the-art algorithms. Experimental results have demonstrated the superior performance of the proposed methodology in extracting complex buildings in urban districts. © 2019 by the authors.","Building extraction; Convolutional neural network; Deep learning;multiscale; VHR images","Buildings; Convolution; Deep learning; Extraction; Neural networks; Petroleum reservoir evaluation; Remote sensing; Support vector machines; Building extraction; Classification results; Convolutional neural network; Discriminative features; International society; State-of-the-art algorithms; Very high resolution (VHR) image; VHR images; Image classification"
"Low Altitude Aerial Scene Synthesis Using Generative Adversarial Networks for Autonomous Natural Resource Management","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068340213&doi=10.1109%2fKBEI.2019.8734904&partnerID=40&md5=afa986e04a7d6dd53b4d065ce1f74d49","Deep neural networks are currently the best solution for aerial scene interpretation for Unmanned Aerial Vehicle (UAV) based remote sensing. A problem faced by the deep neural networks is that the deep models require significantly large training datasets which should cover almost all of the scenarios. Gathering these datasets is usually very time consuming and expensive. In this paper, data augmentation and generative adversarial network are used for autonomous synthesis of low altitude aerial scenes for creating a training dataset for deep low altitude aerial video interpretation. The proposed system is evaluated using a real world scenario of road following under foliage in a jungle and the experimental results show that the proposed framework is capable of producing high accuracy training datasets for UAV vision system in natural resource management scenarios. © 2019 IEEE.","deep learning; generative adversarial network; natural environment management; remote sensing","Deep learning; Deep neural networks; Engineering research; Knowledge based systems; Large dataset; Natural resources management; Remote sensing; Resource allocation; Unmanned aerial vehicles (UAV); Adversarial networks; Data augmentation; Natural environments; Natural resource management; Real-world scenario; Scene interpretation; Training data sets; Training dataset; Antennas"
"Deep pyramidal residual networks for spectral-spatial hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052694011&doi=10.1109%2fTGRS.2018.2860125&partnerID=40&md5=7cf80892c647b9b06de5600813953b45","Convolutional neural networks (CNNs) exhibit good performance in image processing tasks, pointing themselves as the current state-of-the-art of deep learning methods. However, the intrinsic complexity of remotely sensed hyperspectral images still limits the performance of many CNN models. The high dimensionality of the HSI data, together with the underlying redundancy and noise, often makes the standard CNN approaches unable to generalize discriminative spectral-spatial features. Moreover, deeper CNN architectures also find challenges when additional layers are added, which hampers the network convergence and produces low classification accuracies. In order to mitigate these issues, this paper presents a new deep CNN architecture specially designed for the HSI data. Our new model pursues to improve the spectral-spatial features uncovered by the convolutional filters of the network. Specifically, the proposed residual-based approach gradually increases the feature map dimension at all convolutional layers, grouped in pyramidal bottleneck residual blocks, in order to involve more locations as the network depth increases while balancing the workload among all units, preserving the time complexity per layer. It can be seen as a pyramid, where the deeper the blocks, the more feature maps can be extracted. Therefore, the diversity of high-level spectral-spatial attributes can be gradually increased across layers to enhance the performance of the proposed network with the HSI data. Our experiments, conducted using four well-known HSI data sets and 10 different classification techniques, reveal that our newly developed HSI pyramidal residual model is able to provide competitive advantages (in terms of both classification accuracy and computational time) over the state-of-the-art HSI classification methods. © 2018 IEEE.","Convolutional neural networks (CNNs); hyperspectral imaging (HSI); residual networks (ResNets)","Classification (of information); Competition; Complex networks; Convolution; Data structures; Deep learning; Feature extraction; Image classification; Independent component analysis; Learning systems; Network architecture; Neural networks; Personnel training; Spectroscopy; Classification accuracy; Classification methods; Classification technique; Competitive advantage; Convolutional neural network; Hyperspectral imaging (HSI); Network convergence; residual networks (ResNets); Hyperspectral imaging; accuracy assessment; artificial neural network; complexity; data processing; data set; experimental study; image classification; imaging method; numerical model; remote sensing; spatial analysis; spectral analysis"
"A random forest classifier based on pixel comparison features for urban LiDAR data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059031502&doi=10.1016%2fj.isprsjprs.2018.12.009&partnerID=40&md5=551f4729fea3bb99e49311b588237caf","The outstanding accuracy and spatial resolution of airborne light detection and ranging (LiDAR) systems allow for very detailed urban monitoring. Classification is a crucial step in LiDAR data processing, as many applications, e.g., 3D city modeling, building extraction, and digital elevation model (DEM) generation, rely on classified results. In this study, we present a novel LiDAR classification approach that uses simple pixel comparison features instead of the manually designed features used in many previous studies. The proposed features are generated by the computed height difference between two randomly selected neighboring pixels. In this way, the feature design does not require prior knowledge or human effort. More importantly, the features encode contextual information and are extremely quick to compute. We apply a random forest classifier to these features and a majority analysis postprocessing step to refine the classification results. The experiments undertaken in this study achieved an overall accuracy of 87.2%, which can be considered good given that only height information from the LiDAR data was used. The results were better than those obtained by replacing the proposed features with five widely accepted man-made features. We conducted algorithm parameter setting tests and an importance analysis to explore how the algorithm works. We found that the pixel pairs directing along the object structure and with a distance of the approximate object size can generate more discriminative pixel comparison features. Comparison with other benchmark results shows that this algorithm can approach the performance of state-of-the-art deep learning algorithms and exceed them in computational efficiency. We conclude that the proposed algorithm has high potential for urban LiDAR classification. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","LiDAR classification; Pixel comparison; Random forest; Urban","Benchmarking; Computational efficiency; Data handling; Data mining; Decision trees; Deep learning; Optical radar; Pixels; Surveying; Three dimensional computer graphics; Classification approach; Classification results; Contextual information; Digital elevation model; Light detection and ranging systems; Random forest classifier; Random forests; Urban; Classification (of information); accuracy assessment; algorithm; data processing; digital elevation model; lidar; pixel; satellite data; spatial resolution; urban region"
"Mapping impervious surfaces in town-rural transition belts using China's GF-2 imagery and object-based deep CNNs","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061391230&doi=10.3390%2frs11030280&partnerID=40&md5=00da49534c9fc497a9ba7f7d6fb3a36b","Impervious surfaces play an important role in urban planning and sustainable environmental management. High-spatial-resolution (HSR) images containing pure pixels have significant potential for the detailed delineation of land surfaces. However, due to high intraclass variability and low interclass distance, the mapping and monitoring of impervious surfaces in complex town-rural areas using HSR images remains a challenge. The fully convolutional network (FCN) model, a variant of convolution neural networks (CNNs), recently achieved state-of-the-art performance in HSR image classification applications. However, due to the inherent nature of FCN processing, it is challenging for an FCN to precisely capture the detailed information of classification targets. To solve this problem, we propose an object-based deep CNN framework that integrates object-based image analysis (OBIA) with deep CNNs to accurately extract and estimate impervious surfaces. Specifically, we also adopted two widely used transfer learning technologies to expedite the training of deep CNNs. Finally, we compare our approach with conventional OBIA classification and state-of-the-art FCN-based methods, such as FCN-8s and the U-Net methods. Both of these FCN-based methods are well designed for pixel-wise classification applications and have achieved great success. Our results show that the proposed approach effectively identified impervious surfaces, with 93.9% overall accuracy. Compared with the existing methods, i.e., OBIA, FCN-8s and U-Net methods, it shows that our method achieves obviously improvement in accuracy. Our findings also suggest that the classification performance of our proposed method is related to training strategy, indicating that significantly higher accuracy can be achieved through transfer learning by fine-tuning rather than feature extraction. Our approach for the automatic extraction and mapping of impervious surfaces also lays a solid foundation for intelligent monitoring and themanagement of land use and land cover. © 2019 by the authors.","Deep learning; Object-based image analysis (OBIA); Remote sensing; Transfer learning","Classification (of information); Convolution; Environmental management; Extraction; Image analysis; Land use; Mapping; Pixels; Remote sensing; Rural areas; Sustainable development; Classification performance; Convolution neural network; High spatial resolution; Land use and land cover; Object based image analysis (OBIA); State-of-the-art performance; Sustainable environmental management; Transfer learning; Deep learning"
"It Is Time for Us to Get Artificially Intelligent!","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061134486&doi=10.1016%2fj.jacep.2018.12.003&partnerID=40&md5=36d45d8ca4dfb31ca21bf409ec361c8b",[No abstract available],,"artificial intelligence; artificial neural network; big data; burnout; cardiovascular disease; catheter ablation; clinical practice; clinical trial (topic); coronary sinus; deep learning; Editorial; electronic health record; endocardium; eye disease; frequency modulation; health care; health care need; heart arrhythmia; heart electrophysiology; heart failure; heart failure with preserved ejection fraction; heart failure with reduced ejection fraction; heart left ventricle function; heart pacing; heart right ventricle; His bundle; hospital readmission; human; image analysis; learning algorithm; machine learning; major adverse cardiac event; nurse practitioner; outcome assessment; personalized medicine; practice guideline; priority journal; reimbursement; remote sensing; self care; signal processing; sudden death; time; algorithm; heart function test; Algorithms; Artificial Intelligence; Electrophysiologic Techniques, Cardiac; Humans"
"DeepExt: A Convolution Neural Network for Road Extraction using RGB images captured by UAV","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062796548&doi=10.1109%2fSSCI.2018.8628717&partnerID=40&md5=0d8faadee5d7457af7f29412354668be","In this paper, we propose automatic road extraction using Unmanned Aerial Vehicle (UAV) based Remote Sensing data. Road extraction using UAV data is very useful in traffic management, city planning, GPS based applications, etc. Deep learning techniques namely, Fully Convolutional Network (FCN) and conditional Generative Adversarial Networks (GAN) are used to extract roads from a UAV dataset available in the literature. FCN performs semantic segmentation on the image whereas the GAN generates output images from the model it learns. The results demonstrate the efficiency of the deep learning methods for the task of road extraction. © 2018 IEEE.","convolutional neural nenvork; generative adversarial networks; road extraction; semantic segmentation","Antennas; Artificial intelligence; Color image processing; Convolution; Data mining; Deep learning; Extraction; Image segmentation; Information management; Remote sensing; Roads and streets; Semantics; Unmanned aerial vehicles (UAV); Adversarial networks; Automatic road extraction; Convolution neural network; Convolutional networks; convolutional neural nenvork; Remote sensing data; Road extraction; Semantic segmentation; Feature extraction"
"TPMT based Automatic Road Extraction from 3D Real Scenes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062719660&doi=10.1109%2fBGDDS.2018.8626851&partnerID=40&md5=698853ab9b7b85361548296bb5233f65","The Generation of digital 3D buildings and roads from a number of 3D real scenes is a very difficult and important task in modern city planning and management. A TPMT (Tilt Photographic Measurement Technique) based automatic road extraction method with 3D real sceneries is studied and demonstrated. Firstly, TPMT is used to obtain 3D data of the city and form its stereoscope image sequences. Secondly, Seamless stitching of the stereoscope image sequences and high-resolution remote sensing images are conducted to produce an integrated urban digital road feature database. Thirdly, image processing and pattern recognition methods are employed to identify and extract road from the urban digital road feature database and form a vector contour image of the road. Finally, the vector contours are classified with a pattern classifier. The theoretical analysis and practical demonstrations have shown that our approach has higher precision in automatic 3D road extraction from a number of 3D real scenes, which can meet the demand of unmanned driving for high-precision roads and 3D maps. Moreover, it significantly reduces the cost of road information acquisition compared to the existing method using Lidar and has potential to play a key role to promote and support ever growing urban planning, construction, management and emergency response. Faced with a big data problem in huge and various 3D real scenes, a deep learning approach is under consideration in our undergoing study and development. © 2018 IEEE.","3D Real Scenes; Automatic extraction; City 3D mapping; Oblique Photography 3D Reconstruction; Road Extraction","Deep learning; Extraction; Pattern recognition; Photography; Remote sensing; Roads and streets; Stereo image processing; 3-D mapping; 3D Real Scenes; 3D reconstruction; Automatic extraction; Road extraction; Highway administration"
"Spectral and multi-spatial-feature based deep learning for hyperspectral remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062518480&doi=10.1109%2fRCAR.2018.8621652&partnerID=40&md5=0042ece313456f9c481562a650c11511","Hyperspectral data has a strong ability in information expression. In this paper, we will extract a variety of spectral features and Multi-spatial-dominated features. In order to make better use of the relationship between spatial neighborhood pixels, we introduce spatial features with two different window scales, which can be give us more abundant spatial information, and then we used a novel framework to merge this extracted features. This deep learning framework is made of sparse component analysis (SPCA), deep learning architecture, and logistic regression. For hyperspectral image classification, stacked autoencoders is an efficient deep learning framework. In detail, compared with principle component analysis (PCA), SPCA has a better effect on dimensionality reduction of nonlinear data, especially for hyperspectral data. The public data set Pavia Centre scene and Pavia University scene are used to test our proposed algorithm. Experimental results demonstrate that the proposed approach outperforms the compared. It also shows that the hyperspectral data classification based on deep learning has an excellent application prospect. © 2018 IEEE.","autoencoder (AE); deep learning; feature extraction; Hyperspectral image classification; logistic regression (LR); stacked autoencoder (SAE); support vector machine (SVM)","Classification (of information); Feature extraction; Hyperspectral imaging; Image classification; Principal component analysis; Regression analysis; Remote sensing; Robotics; Spectroscopy; Statistical tests; Support vector machines; Auto encoders; Dimensionality reduction; Hyperspectral data classification; Hyperspectral Remote Sensing Image; Learning architectures; Logistic regressions; Principle component analysis; Sparse component analysis; Deep learning"
"Deep learning activities on remote sensed hyperspectral images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062554461&doi=10.1109%2fIDAP.2018.8620750&partnerID=40&md5=b6fdddf001ed722506a0dc6ba0ac94dd","In recent years, deep learning models have been widely used on remote sensing images. Deep learning is held on remote sensing images as well as in every area; is to be able to perform better performance classification than existing approaches and to perceive the feature inferences on its own. In remote sensing, more studies are made especially on hyperspectral images. The most important reason for this is that it can carry a large number of data features. The large number of data features means that there are a large number of attributes for that image. The most important disadvantage of hyperspectral images is; due to the influence of the environment of the device which is shooting the image, various noises may occur. There may be a variety of information loss on this image. Various algorithms techniques have been developed to prevent these losses, while hyperspectral images have been better classified by deep learning models. Recent advances in deep learning models in technological firms and the creation and development of their own deep learning model are evidence of how intense this interest is in this area. Our aim is to examine recent developments in deep learning activities on remote sensing images in this article; to compare the performances obtained by deep learning model and to give brief information about the methods used in this area. © 2018 IEEE.","CNN; Deep Learning; Image Processing; Remote Sensing; Remote Sensing Images","Artificial intelligence; Data handling; Image processing; Remote sensing; Spectroscopy; Information loss; Learning Activity; Learning models; Number of datum; Remote sensing images; Deep learning"
"A multi-temporal convolutional autoencoder neural network for cloud removal in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062230399&doi=10.1109%2fECTICon.2018.8620054&partnerID=40&md5=a4e6bbbbbd6b49533417cdeb24e9b207","The uncontrollable weather conditions can cause a serious problem to remote sensing imaginary. One of the weather conditions is a resulting from cloud contamination. As a result, this paper proposed the use of the convolutional autoencoder neural networks to remove clouds from cloud-contaminated images by training on a multi-temporal remote sensing dataset. Here, the observations from different spectral bands are assumed to be independent since their spectral responses are usually non-overlapped. From this assumption, each convolutional autoencoder neural networks are trained with the observation from only one spectral band. In our method, we have three convolutional autoencoder neural networks for red, green and blue spectral bands. The experiments were conducted on both synthesis and real dataset derived from the actual LANDSAT 8 images from the central part of Thailand where our algorithm has shown to have a superb performance. © 2018 IEEE","Cloud Removal; Convolutional Autoencoder; Deep Learning; Image Restoration; Multi-Temporal; Remote Sensing","Convolution; Deep learning; Image reconstruction; Meteorology; Auto encoders; Cloud contamination; Cloud removal; Multi-temporal; Multi-temporal remote sensing; Red , green and blues; Remote sensing images; Spectral response; Remote sensing"
"Very high resolution remote sensing image classification with SEEDS-CNN and scale effect analysis for superpixel CNN classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053519750&doi=10.1080%2f01431161.2018.1513666&partnerID=40&md5=ba45ceafe0e6aad2229eba7d70ee46db","Pixel-based convolutional neural network (CNN) has demonstrated good performance in the classification of very high resolution images (VHRI) from which abstract deep features are extracted. However, conventional pixel-based CNN demands large resources in terms of processing time and disk space. Therefore, superpixel CNN classification has recently become a focus of attention. We therefore propose a CNN based deep learning method combining superpixels extracted via energy-driven sampling (SEEDS) for VHRI classification. The approach consists of three main steps. First, based on the concept of geographic object-based image analysis (GEOBIA), the image is segmented into homogeneous superpixels using the SEEDS based superpixel segmentation method thereby decreasing the number of processing units. Second, the training data and testing data are extracted from the image and concatenated on a superpixel level at a variety of scales for CNN. Third, the training data are input to train the parameters of CNN and abstract deep features are extracted from the VHRI. Using these extracted deep features, we classify two VHRI data sets at single scales and multiple scales. To verify the effectiveness of SEEDS based CNN classification, the performance of SEEDS and three others superpixel segmentation algorithms are compared, and the superpixel extraction via SEEDS method was found to be the optimal superpixel segmentation approach for CNN classification. The scale effect on CNN classification accuracy was investigated by comparing the four superpixel segmentation methods. We found that (1) There is no strong evidence that using scales combinations is better than a single scale in some specific situations; (2) Natural objects with low complexity are not as sensitive to scale as artificial objects; (3) For a simple VHRI that contains clear artificial objects and simple texture, the classification result with multiple scales performs better a the single scale; (4) In contrast, for the complex VHRI containing a large number of complex objects, the classification result with a single small-scale best. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Classification (of information); Complex networks; Deep learning; Image analysis; Image classification; Image segmentation; Neural networks; Pixels; Remote sensing; Superpixels; Artificial objects; Classification accuracy; Classification results; Convolutional Neural Networks (CNN); Geographic object-based image analysis; Superpixel segmentations; Very high resolution; Very high resolution (VHR) image; Data mining; artificial neural network; image analysis; image classification; image resolution; performance assessment; pixel; remote sensing; segmentation"
"Using long short-term memory recurrent neural network in land cover classification on Landsat and Cropland data layer time series","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055085914&doi=10.1080%2f01431161.2018.1516313&partnerID=40&md5=175e30b34b7bc04982959a4f5dc1fe83","Land cover maps are significant in assisting agricultural decision making. However, the existing workflow of producing land cover maps is very complicated and the result accuracy is ambiguous. This work builds a long short-term memory (LSTM) recurrent neural network (RNN) model to take advantage of the temporal pattern of crops across image time series to improve the accuracy and reduce the complexity. An end-to-end framework is proposed to train and test the model. Landsat scenes are used as Earth observations, and some field-measured data together with CDL (Cropland Data Layer) datasets are used as reference data. The network is thoroughly trained using state-of-the-art techniques of deep learning. Finally, we tested the network on multiple Landsat scenes to produce five-class and all-class land cover maps. The maps are visualized and compared with ground truth, CDL, and the results of SegNet CNN (convolutional neural network). The results show a satisfactory overall accuracy (> 97% for five-class and > 88% for all-class) and validate the feasibility of the proposed method. This study paves a promising way for using LSTM RNN in the classification of remote sensing image time series. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Brain; Decision making; Deep learning; Image enhancement; Remote sensing; Time series; Classification of remote sensing image; Convolutional neural network; Earth observations; Field-measured data; Land cover classification; Overall accuracies; Recurrent neural network (RNN); State-of-the-art techniques; Long short-term memory; accuracy assessment; artificial neural network; complexity; data set; land classification; land cover; Landsat; time series analysis"
"Learning Orientation-Estimation Convolutional Neural Network for Building Detection in Optical Remote Sensing Image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062214563&doi=10.1109%2fDICTA.2018.8615859&partnerID=40&md5=f36a5e81d3a0e37e757bad700c713a9d","Benefiting from the great success of deep learning in computer vision, object detection with Convolutional Neural Network (CNN) based methods have drawn significant attentions. Various frameworks have been proposed which show awesome and robust performance for a large range of datasets. However, for building detection in remote sensing images, buildings always pose a diversity of orientation which makes it a challenge for the application of off-the-shelf methods to building detection in remote sensing images. In this work, we aim to integrate orientation regression into the popular axis-aligned bounding box to tackle this problem. To adapt the axis-aligned bounding boxes to arbitrarily orientated ones, we also develop an algorithm to estimate the Intersection Over Union (IOU) overlap between any two arbitrarily oriented boxes which is convenient to implement in Graphics Processing Unit (GPU) for fast computation. The proposed method utilizes CNN for both robust feature extraction and bounding box regression. We present our model in an end-to-end fashion making it easy to train. The model is formulated and trained to predict both orientation and location simultaneously obtaining tighter bounding box and hence, higher mean average precision (mAP). Experiments on remote sensing images of different scales shows a promising performance over the conventional one. © 2018 IEEE.","building detection; convolutional neural network; intersection over union estimation; orientation regression; remote sensing image","Buildings; Computer graphics; Computer graphics equipment; Convolution; Deep learning; Graphics processing unit; Large dataset; Neural networks; Object detection; Program processors; Regression analysis; Axis-aligned bounding boxes; Building detection; Convolutional neural network; Learning orientation; Optical remote sensing; Remote sensing images; Robust feature extractions; Robust performance; Remote sensing"
"Band Weighting Network for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062217931&doi=10.1109%2fDICTA.2018.8615876&partnerID=40&md5=b8887f97091a6612374ee3199da64ddb","Hyperspectral remote sensing images use hundreds of bands to describe the fine spectral information of the ground area. However, they inevitably contain a large amount of redundancy as well as noisy bands. Discovering the most informative bands and modeling the relationship among the bands are effective means to process the data and improve the performance of the subsequent classification task. Attention mechanism is used in computer vision and natural language processing to guide the algorithm towards the most relevant information in the data. In this paper, we propose a band weighting network by designing and integrating an attention module in the traditional convolutional neural network for hyperspectral image classification. Our proposed band weighting network has the capability to model the relationship among the bands and weight them according to their joint contribution to classification. One prominent feature of our proposed method is that it can assign different weights to different samples. The experimental results demonstrate the effectiveness and superiority of our approach. © 2018 IEEE.","Band weighting; Deep learning; Hyperspectral image classification; Neural network","Deep learning; Hyperspectral imaging; Natural language processing systems; Neural networks; Remote sensing; Spectroscopy; Attention mechanisms; Band weighting; Classification tasks; Convolutional neural network; Hyperspectral Remote Sensing Image; NAtural language processing; Prominent features; Spectral information; Image classification"
"Convolutional Neural Networks for Semantic Segmentation of Multispectral Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062063685&doi=10.1109%2fLATINCOM.2018.8613216&partnerID=40&md5=011fdf645d7548c7185083c87291c66b","The recent impulse in development of artificial intelligence (AI) methodologies has simplified the application of this in multiple research areas. This simplification was not favorable before, due to the limitations in dimensionality, processing time, computational resources, among others. Working with multispectral remote sensing (RS) images, in an artificial neural network (NN) was quite complex. Due the methods used required millions of processes that took a long time to be executed and produce competitive results compared with the state of the art (SoA). Deep learning (DL) strategies have been applied to alleviate these limitations and have greatly improved the use of neural networks. Therefore, this paper presents the analysis of DL-NNs to perform semantic segmentation of multispectral RS images. Images are captured by the constellation of satellites Sentinel-2 from the European Space Agency. The objective of this research is to classify each pixel of a scene into five categories: 1-vegetation, 2-soil, 3-water, 4-clouds and 5-cloud shadows. The selection of spectral bands for the formation of input datasets for segmentation of these classes is very important. The spectral signatures of each material aid to discern among several classes. Results presented in this work, show that the AI strategy proposed offer better accuracy segmentation than other methods of the SoA in competitive processing time. © 2018 IEEE.","Convolutional neural networks; Multispectral images; Remote sensing; Semantic segmentation","Convolution; Deep learning; Image segmentation; Neural networks; Semantic Web; Semantics; Space optics; Computational resources; Constellation of satellites; Convolutional neural network; European Space Agency; Multispectral images; Multispectral remote sensing; Multispectral remote sensing image; Semantic segmentation; Remote sensing"
"Advanced Satellite Image Classification of Various Resolution Image Using a Novel Approach of Deep Neural Network Classifier","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054671412&doi=10.1007%2fs11277-018-6024-7&partnerID=40&md5=55c8f9489ce3237dda94261dfcaa6e37","Image registration is computationally intensive and applied in a variety of applications, for example, multispectral classification, change recognition, climate prediction and multi-view analysis in GIS and medicine. There are three types of registration namely multi-view, multimodal and multi-temporal. In multi-view based registration, the images of the same scene taken at different viewpoints are analyzed and modeled for the requirement. Hence stereoscopic image sequences of the same view are acquired, and accurate comparison for the image classification is essential. This paper presents a robust method which has three steps. The first phase includes obtaining hyper spectral (satellite) images and preprocessing of them, the second period subdivides into image blocks for alignment, and the final step focuses on classification based on hyper graph structure using deep learning approach. For processing of satellite images, a new method linear iterative clustering and deep neural network classification are employed. Previous works in remote sensing applications involve training samples and hence prior knowledge of image sets which incurs more computational time. The implementation of this method shows an automatic, achieving better accuracy and dynamic reconfigurable image registration in reduced complexity. The mathematical model used is hidden markov chain model for clustering which provides region-wise feature construction for evaluating region shape and contextual information. The work yields classification accuracy of 94.12% which is far better than past outcomes in this engaged field of research. The execution of the usage is examined, a comparison is additionally influenced regarding false classification ratio, time complexity and clustering accuracy is demonstrated. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Deep neural network; Multiview image registration; Satellite image","Climate change; Complex networks; Deep neural networks; Image analysis; Image registration; Iterative methods; Markov processes; Neural networks; Remote sensing; Satellite imagery; Stereo image processing; Hidden Markov chain model; Multispectral classification; Neural network classification; Neural network classifier; Remote sensing applications; Satellite image classification; Satellite images; Stereoscopic image sequences; Image classification"
"Segmented Autoencoders for Unsupervised Embedded Hyperspectral Band Selection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062707922&doi=10.1109%2fEUVIP.2018.8611643&partnerID=40&md5=9b47c73731fed37141b15076fc9f3c46","One of the major challenges in hyperspectral imaging (HSI) is the selection of the most informative wavelengths within the vast amount of data in a hypercube. Band selection can reduce the amount of data and computational cost as well as counteracting the negative effects of redundant and erroneous information. In this paper, we propose an unsupervised, embedded band selection algorithm that utilises the deep learning framework. Autoencoders are used to reconstruct measured spectral signatures. By putting a sparsity constraint on the input weights, the bands that contribute most to the reconstruction can be identified and chosen as the selected bands. Additionally, segmenting the input data into several spectral regions and distributing the number of desired bands according to a density measure among these segments, the quality of the selected bands can be increased and the computational time reduced by training several autoencoders. Results on a benchmark remote sensing HSI dataset show that the proposed algorithm improves classification accuracy compared to other state of the art band selection algorithms and thereby builds the basis for a framework of embedded band selection in HSI. © 2018 IEEE.","autoencoder; band selection; Hyperspectral imaging","Classification (of information); Deep learning; Remote sensing; Spectroscopy; Auto encoders; Band selection; Classification accuracy; Computational costs; Computational time; Learning frameworks; Sparsity constraints; Spectral signature; Hyperspectral imaging"
"Image classification based on log-euclidean fisher vectors for covariance matrix descriptors","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061922622&doi=10.1109%2fIPTA.2018.8608154&partnerID=40&md5=a7b19b04db4439bf777c5de478387260","This paper introduces an image classification method based on the encoding of a set of covariance matrices. This encoding relies on Fisher vectors adapted to the log-Euclidean metric: The log-Euclidean Fisher vectors (LE FV). This approach is next extended to full local Gaussian descriptors composed by a set of local mean vectors and local covariance matrices. For that, the local Gaussian model is transformed to a zero-mean Gaussian model with an augmented covariance matrix. All these approaches are used to encode handcrafted or deep learning features. Finally, they are applied in a remote sensing application on the UC Merced dataset which consists in classifying land cover images. A sensitivity analysis is carried out to evaluate the potential of the proposed LE FV. © 2018 IEEE.","classification; covariance matrices; deep neural network; Fisher vector; log-Euclidean metric; SIFT descriptors; vector of locally aggregated descriptors","Classification (of information); Deep neural networks; Encoding (symbols); Gaussian distribution; Image classification; Remote sensing; Sensitivity analysis; Signal encoding; Vectors; Covariance matrices; Fisher vectors; Log-Euclidean; SIFT descriptors; Vector of locally aggregated descriptors; Covariance matrix"
"Automatic building extraction from google earth images under complex backgrounds based on deep instance segmentation network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060140240&doi=10.3390%2fs19020333&partnerID=40&md5=068e26660ee156339435d19516a226bf","Building damage accounts for a high percentage of post-natural disaster assessment. Extracting buildings from optical remote sensing images is of great significance for natural disaster reduction and assessment. Traditional methods mainly are semi-automatic methods which require human-computer interaction or rely on purely human interpretation. In this paper, inspired by the recently developed deep learning techniques, we propose an improved Mask Region Convolutional Neural Network (Mask R-CNN) method that can detect the rotated bounding boxes of buildings and segment them from very complex backgrounds, simultaneously. The proposed method has two major improvements, making it very suitable to perform building extraction task. Firstly, instead of predicting horizontal rectangle bounding boxes of objects like many other detectors do, we intend to obtain the minimum enclosing rectangles of buildings by adding a new term: the principal directions of the rectangles θ. Secondly, a new layer by integrating advantages of both atrous convolution and inception block is designed and inserted into the segmentation branch of the Mask R-CNN to make the branch to learn more representative features. We test the proposed method on a newly collected large Google Earth remote sensing dataset with diverse buildings and very complex backgrounds. Experiments demonstrate that it can obtain promising results. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Building extraction; Deep learning; Instance segmentation; Mask R-CNN; Receptive field block; Rotation bounding box","Complex networks; Convolution; Damage detection; Deep learning; Disasters; Extraction; Geometry; Human computer interaction; Image segmentation; Neural networks; Object detection; Remote sensing; Statistical tests; Automatic building extraction; Bounding box; Building extraction; Convolutional neural network; Minimum enclosing rectangle; Optical remote sensing; Receptive fields; Semiautomatic methods; Buildings"
"A novel spectral-spatial classification technique for multispectral images using extended multi-attribute profiles and sparse autoencoder","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059629080&doi=10.1080%2f2150704X.2018.1523581&partnerID=40&md5=1ae4bf76f9d37604f2c2d970e797c80a","Image classification is a prominent topic and a challenging task in the field of remote sensing. Recently many various classification methods have been proposed for satellite images specifically the frameworks based on spectral-spatial feature extraction techniques. In this paper, a feature extraction strategy of multispectral data is taken into account in order to develop a new classification framework by combining Extended Multi-Attribute Profiles (EMAP) and Sparse Autoencoder (SAE). Extended Multi-Attribute Profiles is employed to extract the spatial information, then it is joined to the original spectral information to describe the spectral-spatial property of the multispectral images. The obtained features are fed into a Sparse Autoencoder as input. Finally, the learned spectral-spatial features are embedded into the Support Vector Machine (SVM) for classification. Experiments are conducted on two multispectral (MS) images such as we construct the ground truth maps of the corresponding images. Our approach based on EMAP and deep learning (DL), proves its huge potential to achieve a high classification accuracy in reasonable running time and outperforms traditional classifiers and others classification approaches. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Deep learning; Extraction; Feature extraction; Remote sensing; Support vector machines; Classification accuracy; Classification approach; Classification framework; Classification methods; Multispectral images; Spatial informations; Spectral information; Spectral-spatial classification; Image classification; data interpretation; image classification; multispectral image; satellite imagery; spatial analysis; spectral reflectance; support vector machine"
"A sparse deep learning model for privacy attack on remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062874251&doi=10.3934%2fmbe.2019063&partnerID=40&md5=9ac195f811b5cac34cb422773df071ef","Deep learning tools have been a new way for privacy attacks on remote sensing images. However, since labeled data of privacy objects in remote sensing images are less, the samples for training are commonly small. Besides, traditional deep neural networks have a huge amount of parameters which leads to over complexity of models and have a great heavy of computation. They are not suitable for small sample image classification task. A sparse method for deep neural network is proposed to reduce the complexity of deep learning model with small samples. A singular value decomposition algorithm is employed to reduce the dimensions of the output feature map of the upper convolution layers, which can alleviate the input burden of the current convolution layer, and decrease a large number of parameters of the deep neural networks, and then restrain the number of redundant or similar feature maps generated by the over-complete schemes in deep learning. Experiments with two remote sensing image data sets UCMLU and WHURS show that the image classification accuracy with our sparse model is better than the plain model,which is improving the accuracy by 3%,besides, its convergence speed is faster. © 2019 the Author(s), licensee AIMS Press.","Convolutional neural network; Image classification; Singular value decomposition; Sparse model","Complex networks; Convolution; Convolutional neural networks; Deep neural networks; Image classification; Image enhancement; Labeled data; Learning systems; Multilayer neural networks; Parameter estimation; Remote sensing; Singular value decomposition; Classification accuracy; Convergence speed; Learning models; Privacy Attacks; Remote sensing images; Singular value decomposition algorithms; Sparse methods; Sparse modeling; Deep learning; algorithm; artificial neural network; computer simulation; geographic information system; image processing; privacy; procedures; remote sensing; reproducibility; software; statistical analysis; Algorithms; Computer Simulation; Data Interpretation, Statistical; Deep Learning; Geographic Information Systems; Image Processing, Computer-Assisted; Neural Networks (Computer); Privacy; Remote Sensing Technology; Reproducibility of Results; Software"
"Deep learning analysis for big remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074203419&doi=10.5220%2f0008166303550362&partnerID=40&md5=48e64be51fec5ab4230dd567c50e2ecd","Large data remote sensing has various special characteristics, including multi-source, multi-scale, large scale, dynamic and non-linear characteristics. Data set collections are so large and complex that it becomes difficult to process them using available database management tools or traditional data processing applications. In addition, traditional data processing techniques have different limitations in processing massive volumes of data, as the analysis of large data requires sophisticated algorithms based on machine learning and deep learning techniques to process the data in real time with great accuracy and efficiency. Therefore Deep learning methods are used in various domains such as speech recognition, image classifications, and learning methods in language processing. However, recent researches merged different deep learning techniques with hybrid learning-training mechanisms and processing data with high speed. In this paper we propose a hybrid approach for RS image classification combining a deep learning algorithm and an explanatory classification algorithm. We show how deep learning techniques can benefit to Big remote sensing. Through deep learning we seek to extract relevant features from images via a DL architecture. Then these characteristics are the entry points for the MLlib classification algorithm to understand the correlations that may exist between characteristics and classes. This architecture combines Spark RDD image coding to consider image’s local regions, pre-trained Vggnet and U-net for image segmentation and spark Machine Learning like random Forest and KNN to achieve labeling task. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved","Big Data; Classification; Deep Learning; Remote Sensing; Spark; Tensorflow","Big data; Classification (of information); Data handling; Decision trees; Electric sparks; Image classification; Image coding; Image segmentation; Knowledge engineering; Knowledge management; Learning algorithms; Machine learning; Remote sensing; Speech recognition; Classification algorithm; Data processing applications; Data processing techniques; Database management; Language processing; Nonlinear characteristics; Remote sensing image classification; Tensorflow; Deep learning"
"A high-resolution remote sensing image building extraction method based on deep learning [基于深度学习的高分辨率遥感影像建筑物提取方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062002479&doi=10.11947%2fj.AGCS.2019.20170638&partnerID=40&md5=24039333d22b9a1110a76405a6545146","Traditional building extraction from very high resolution remote sensing optical imagery is limited by low precision and incomplete boundary. In this paper, a high-resolution remote sensing image building extraction method based on deep learning is proposed. Firstly, Principal Component Analysis is used to pre-train network structure in an unsupervised way and obtain the characteristics of remote sensing image. Secondly, an adaptive pooling model is proposed to reduce the feature information loss in the pooling process. The texture features are extracted by non-subsampled contour wave transformation and introduced to the network to improve the building extraction. Finally, the obtained image features are inputted into the softmax classifier for classification and building extraction results. A typical experiment areas selected. The comparison with typical building extraction method, the experimental results shows that the proposed method can extract the buildings with higher accuracy, especially the clearer and more complete boundary. © 2019, Surveying and Mapping Press. All right reserved.","Adaptive pooling model; Building information extraction; Deep Learning; High resolution remote sensing image","Buildings; Image processing; Principal component analysis; Remote sensing; Textures; Building extraction; Feature information; High resolution remote sensing images; Incomplete boundaries; Remote sensing images; Traditional buildings; Very high resolution; Wave transformations; Deep learning; adaptive management; classification; extraction method; image analysis; image classification; image resolution; information management; machine learning; optical property; precision; remote sensing; texture"
"Deep Learning-Based Classification Methods for Remote Sensing Images in Urban Built-Up Areas","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063957069&doi=10.1109%2fACCESS.2019.2903127&partnerID=40&md5=84936ef859c4a087e0735ac668daca04","Urban areas have been focused recently on the remote sensing applications since their function closely relates to the distribution of built-up areas, where reflectivity or scattering characteristics are the same or similar. Traditional pixel-based methods cannot discriminate the types of urban built-up areas very well. This paper investigates a deep learning-based classification method for remote sensing images, particularly for high spatial resolution remote sensing (HSRRS) images with various changes and multi-scene classes. Specifically, to help develop the corresponding classification methods in urban built-up areas, we consider four deep neural networks (DNNs): 1) convolutional neural network (CNN); 2) capsule networks (CapsNet); 3) same model with a different training rounding based on CNN (SMDTR-CNN); and 4) same model with different training rounding based on CapsNet (SMDTR-CapsNet). The performances of the proposed methods are evaluated in terms of overall accuracy, kappa coefficient, precision, and confusion matrix. The results revealed that SMDTR-CNN obtained the best overall accuracy (95.0%) and kappa coefficient (0.944) while also improving the precision of parking lot and resident samples by 1% and 4%, respectively. © 2013 IEEE.","capsule network; convolution neural network; Deep learning; high resolution remote sensing classification; model ensemble; urban built-up area","Convolution; Deep learning; Deep neural networks; Image classification; Image resolution; Neural networks; Built-up areas; Convolution neural network; Convolutional neural network; High resolution remote sensing; High spatial resolution; Model ensembles; Remote sensing applications; Scattering char-acteristics; Remote sensing"
"A framework for remote sensing images processing using deep learning techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053343827&doi=10.1109%2fLGRS.2018.2867949&partnerID=40&md5=f00063c641353adf2f80597c6b26595d","Deep learning (DL) techniques are becoming increasingly important to solve a number of image processing tasks. Among common algorithms, convolutional neural network- and recurrent neural network-based systems achieve state-of-the-art results on satellite and aerial imagery in many applications. While these approaches are subject to scientific interest, there is currently no operational and generic implementation available at the user level for the remote sensing (RS) community. In this letter, we present a framework enabling the use of DL techniques with RS images and geospatial data. Our solution takes roots in two extensively used open-source libraries, the RS image processing library Orfeo ToolBox and the high-performance numerical computation library TensorFlow. It can apply deep nets without restriction on image size and is computationally efficient, regardless of hardware configuration. © 2004-2012 IEEE.","Aerial images; deep learning (DL); neural networks; Orfeo Toolbox (OTB); remote sensing (RS); TensorFlow (TF)","Aerial photography; Antennas; Image processing; Learning systems; Libraries; Neural networks; Pipeline processing systems; Pipelines; Radio broadcasting; Recurrent neural networks; Remote sensing; Satellite imagery; Tensile stress; Aerial images; Computationally efficient; Convolutional neural network; Hardware configurations; Numerical computations; Orfeo Toolbox (OTB); Radio frequencies; TensorFlow (TF); Deep learning; aerial survey; algorithm; artificial neural network; hardware; image processing; machine learning; remote sensing"
"Spectral perturbation method for deep learning-based classification of remote sensing hyperspectral images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078201163&doi=10.1117%2f12.2534459&partnerID=40&md5=bb7ee96786e4962f692bfa19a59cc36f","Deep learning-based hyperspectral image classification has become very popular recently. It has been widely used in the domain of classification of remote sensing hyperspectral images and has shown encouraging results. Although presence of many spectral bands in a hyperspectral image provides valuable features for the classification purposes, lack of adequate training samples makes training a deep learning model a challenging task. In this paper, we employed stacked auto-encoder (SAE) as the deep learning architecture to extract deep spectral features of the input data and to address the problem of absence of enough training samples, we proposed an effective data augmentation approach to boost the number of training data. Also, in order to alleviate the noise in the output image, we used the majority voting strategy to smooth the final classification map. We applied our method on the Indian Pines hyperspectral dataset including very few training examples. Experimental results show the superiority of our method comparing to some of the state of the arts algorithms. © 2019 SPIE.","Data Augmentation; Deep Learning; Hyperspectral Images; Remote Sensing; Stacked Auto-Encoder","Hyperspectral imaging; Image classification; Perturbation techniques; Remote sensing; Sampling; Signal encoding; Spectroscopy; Auto encoders; Data augmentation; Learning architectures; Perturbation method; Spectral feature; State of the art; Training example; Voting strategies; Deep learning"
"OpenStreetMap Data Quality Assessment via Deep Learning and Remote Sensing Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077113493&doi=10.1109%2fACCESS.2019.2957825&partnerID=40&md5=d0f16c6ef3e0d9d31cb4f5ef6473f007","The number of applications associated with OpenStreetMap (OSM), one of the most famous crowd-sourced projects for volunteered geographic information (VGI), have increased because OSM data is both 'free' and 'up-to-date'. However, limited by the ability of the providers, the quality of the collected data remains a valid concern. This work focuses on how to assess the quality of OSM via deep learning and high-resolution remote imagery. First, considering that high-resolution remote sensing imagery is clear enough for recognizing buildings, we proposed using multi-task deep-convolutional networks to extract buildings in pixel level. The extracted buildings were converted into polygons with geographical coordinates, which were treated as reference data. Then, OSM footprint data were matched with the reference data. Quality was assessed in terms of both positional accuracy and data completeness. Finally, the building footprint data of OSM for the city of Las Vegas, Nevada, USA, were evaluated. The experiments show that the proposed method can assess OSM effectively and accurately. The results show that building extracted by the proposed method can be treated as a new data source for assessing OSM quality and can also be used for urban planning in regions where OSM lacks building data. © 2013 IEEE.","Building footprint; deep learning; OpenStreetMap; quality assessment; remote sensing imagery","Buildings; Computer software maintenance; Data mining; Image quality; Remote sensing; Building footprint; Data quality assessment; Geographical coordinates; High resolution remote sensing imagery; OpenStreetMap; Quality assessment; Remote sensing imagery; Volunteered geographic information; Deep learning"
"Hyperspectral remote sensing image change detection based on tensor and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057868949&doi=10.1016%2fj.jvcir.2018.11.004&partnerID=40&md5=55e17647db848a81e6646493f6b1025d","Considering the bottleneck in improving the performance of the existing multi-temporal hyperspectral remote sensing (HSRS) image change detection methods, a HSRS image change detection solution based on tensor and deep learning is proposed in this study. At first, a tensor-based information model (TFS-Cube) of underlying features change in HSRS images is established. The wavelet texture feature change, spectral feature change and spatio-temporal autocorrelation coefficient of different-temporal related pixels are combined with three-order tensor, so as to make full use of the underlying features change information of HSRS images, optimize the organization mode and maintain the integrity of constraints between different underlying features. Secondly, a restricted Boltzmann Machine based on three-order tensor (Tensor3-RBM) is designed. The input, output and unsupervised learning of TFS-Cube tensor data are realized by multi-linear operations in Tensor3-RBMs. A large number of unlabeled samples are trained layer by layer through multilayer Tensor3-RBMs. Finally, the traditional BP neural network on the top layer of deep belief network (DBN) is replaced with support tensor machine (STM), and a deep belief network with multi-layer Tensor3-RBM and STM (TRS-DBN) is constructed. A small number of labeled samples are used for supervised learning and TRS-DBN global parameters optimization to improve the accuracy of TRS-DBN change detection. Two types of HSRS images from different sensors, AVIRIS and EO-1 Hyperion, are used as the data sources (double-temporal). Four representative experimental regions are randomly selected from the two areas covered by AVIRIS and EO-1 Hyperion HSRS images respectively (two regions in each area) to detect the land use changes. Experimental results demonstrate that TRS-DBN has higher change detection accuracy than similar methods and a good automation level. © 2018","Change detection; Deep learning; Hyperspectral remote sensing images; Support tensor machine; Tensor model","Image enhancement; Infrared spectrometers; Land use; Network layers; Neural networks; Remote sensing; Tensors; Autocorrelation coefficient; Change detection; Hyperspectral remote sensing; Hyperspectral Remote Sensing Image; Restricted boltzmann machine; Support tensor machine (STM); Support tensor machines; Tensor model; Deep learning"
"Automatic extraction of impervious surfaces from high resolution remote sensing images based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058560104&doi=10.1016%2fj.jvcir.2018.11.041&partnerID=40&md5=ee2c4ac5fc6aa4e85af59ce0b70bf16f","Due to the complexity of urban surface, the differences in impervious surface materials, the mutual interference between the spectra of ground objects and the huge impact of ground object shadows in high-resolution remote sensing (HRRS) images, it is improper to directly use shallow machine learning algorithms and conventional object-oriented segmentation methods to extract urban impervious surfaces from HRRS images. Therefore, a method for automatic extraction of impervious surfaces from HRRS images based on deep learning (AEISHIDL) is proposed to address this problem. Firstly, the original HRRS images are pre-processed and the Gram-Schmidt algorithm is employed for the fusion of panchromatic and multi-spectral bands in HRRS images. In addition, an enhanced bilateral filtering method considering edge characteristics (EBFCEC) is designed and adopted to remove noises and enhance edges of man-made objects in original HRRS images. Secondly, the EBFCEC filtered images are partitioned into multi-layer object sets by using improved marker watershed based on LAB color space (IMWLCS), and the related objects in different sets are re-segmented to have the same edges through edge integration, after which we extract spectral feature averages and shape feature values of all objects while the convolutional neural network (CNN) is used to calculate the CNN feature averages of all pixel neighborhoods in each object. Finally, the fuzzy c-means clustering (FCM) algorithm is employed jointly considering the spectrum, shape and CNN features of the segmented objects in HRRS images to judge whether the objects belong to impervious surfaces, thereby effectively increasing the accuracy of automatically extracting impervious surfaces. Two different experimental regions are selected from two different types of HRRS images (WorldView 2 and Pléiades-1A) respectively (4 experimental regions in all). The experimental results show that AEISHIDL has higher accuracy and automation level compared with other four representative methods in urban impervious surfaces extraction from HRRS images. © 2018","Bilateral filtering; Convolutional neural network; High-resolution remote sensing images; Impervious surfaces extraction; Improved watershed algorithm","C (programming language); Clustering algorithms; Convolution; Deep learning; Extraction; Image segmentation; Learning algorithms; Neural networks; Nonlinear filtering; Object recognition; Remote sensing; Space optics; Watersheds; Bilateral filtering; Convolutional neural network; High resolution remote sensing images; Impervious surface; Water-shed algorithm; Image enhancement"
"Quality assessment of remote sensing images iiased on deep learning and human visual system","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063962758&doi=10.3788%2fLOP56.061101&partnerID=40&md5=05ed4c05d9a6c31ac5305daa117f18f4","A quality assessment method of remote sensing images is proposed based on deep learning and the human visual characteristics. The convolutional neural network and the back propagation neural network classifiers are used for the simultaneous feature learning and grade classification of blur and noise intensity for the remote sensing images. The masking effect and the corrected assessment model of the perceptual weighting factors are used to obtain the quality assessment results of remote sensing images, which are more in line with the human visual characteristics. The research results show that the proposed method can effectively solve the difficulty in the quality assessment of remote sensing images with both blur and noise. Moreover, the quality of remote sensing images can be effectively and accurately evaluated, and the results are well in good agreement with both the subjective evaluation results and the human visual experiences. © 2019 Universitat zu Koln. All rights reserved.","Convolutional neural network; Deep learning; Human visual system; Image quality assessment; Imaging systems",
"Deep Learning for Satellite Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053518278&doi=10.1007%2f978-3-319-99010-1_35&partnerID=40&md5=83c9e299aded4ee9fd937a9674d14477","Nowadays, large amounts of high resolution remote-sensing images are acquired daily. However, the satellite image classification is requested for many applications such as modern city planning, agriculture and environmental monitoring. Many researchers introduce and discuss this domain but still, the sufficient and optimum degree has not been reached yet. Hence, this article focuses on evaluating the available and public remote-sensing datasets and common different techniques used for satellite image classification. The existing remote-sensing classification methods are categorized into four main categories according to the features they use: manually feature-based methods, unsupervised feature learning methods, supervised feature learning methods, and object-based methods. In recent years, there has been an extensive popularity of supervised deep learning methods in various remote-sensing applications, such as geospatial object detection and land use scene classification. Thus, the experiments, in this article, carried out on one of the popular deep learning models, Convolution Neural Networks (CNNs), precisely AlexNet architecture on a standard sounded dataset, UC-Merceed Land Use. Finally, a comparison with other different techniques is introduced. © Springer Nature Switzerland AG 2019.","Convolution Neural Networks (CNNs); Deep learning; Parallel computing; Remote-sensing; Satellite image; UC-Merceed land use","Classification (of information); Convolution; Image classification; Intelligent systems; Land use; Object detection; Object recognition; Parallel processing systems; Remote sensing; Satellite imagery; Convolution neural network; Environmental Monitoring; High resolution remote sensing images; Remote sensing applications; Remote sensing classification; Satellite image classification; Satellite images; Unsupervised feature learning; Deep learning"
"Forest damage assessment using deep learning on high resolution remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071971307&doi=10.3390%2frs11171976&partnerID=40&md5=610278318505842a02f95a90c11455e5","Storms can cause significant damage to forest areas, affecting biodiversity and infrastructure and leading to economic loss. Thus, rapid detection and mapping of windthrows are crucially important for forest management. Recent advances in computer vision have led to highly-accurate image classification algorithms such as Convolutional Neural Network (CNN) architectures. In this study, we tested and implemented an algorithm based on CNNs in an ArcGIS environment for automatic detection and mapping of damaged areas. The algorithm was trained and tested on a forest area in Bavaria, Germany. It is a based on a modified U-Net architecture that was optimized for the pixelwise classification of multispectral aerial remote sensing data. The neural network was trained on labeled damaged areas from after-storm aerial orthophotos of a ca. 109km2 forest area with RGB and NIR bands and 0.2-m spatial resolution. Around 107 pixels of labeled data were used in the process. Once the network is trained, predictions on further datasets can be computed within seconds, depending on the size of the input raster and the computational power used. The overall accuracy on our test dataset was 92%. During visual validation, labeling errors were found in the reference data that somewhat biased the results because the algorithm in some instance performed better than the human labeling procedure, while missing areas affected by shadows. Our results are very good in terms of precision, and the methods introduced in this paper have several additional advantages compared to traditional methods: CNNs automatically detect high- and low-level features in the data, leading to high classification accuracies, while only one after-storm image is needed in comparison to two images for approaches based on change detection. Furthermore, flight parameters do not affect the results in the same way as for approaches that require DSMs and DTMs as the classification is only based on the image data themselves, and errors occurring in the computation of DSMs and DTMs do not affect the results with respect to the z component. The integration into the ArcGIS Platform allows a streamlined workflow for forest management, as the results can be accessed by mobile devices in the field to allow for high-accuracy ground-truthing and additional mapping that can be synchronized back into the database. Our results and the provided automatic workflow highlight the potential of deep learning on high-resolution imagery and GIS for fast and efficient post-disaster damage assessment as a first step of disaster management. © 2019 by the authors.","Convolutional neural networks; Forest damage assessment; GIS; Remote sensing; Windthrow","Antennas; Biodiversity; Classification (of information); Convolution; Damage detection; Disaster prevention; Disasters; Forestry; Geographic information systems; Losses; Mapping; Network architecture; Neural networks; Remote sensing; Statistical tests; Storms; Classification accuracy; Convolutional neural network; Forest damage; High resolution imagery; High resolution remote sensing; Image classification algorithms; Pixelwise classification; Windthrows; Deep learning"
"Research on building extraction of multi-temporal remote sensing image based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084432866&doi=10.1364%2fFIO.2019.JW3A.102&partnerID=40&md5=ffb880c1f1221c62d0114d2b7c927da0","The building texture features of multi-temporal remote sensing images were introduced into the VGG neural network. Building extraction accuracy of multi-temporal images can reach 90.6%, which is significantly higher than that of single-phase images. © 2019 The Author (s).",,"Extraction; Image processing; Remote sensing; Textures; Building extraction; Multi-temporal image; Multi-temporal remote sensing; Single phase; Texture features; Deep learning"
"A deep learning based objection detection method for high resolution remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072853052&doi=10.1007%2f978-981-32-9987-0_5&partnerID=40&md5=ba6c2cb69ad3341d31e06ec7636ab829","Automatic building detection from remote sensing images plays an important role in a wide range of applications. In this paper, we apply improved U-NET and HF-FCN as main models to detect small building which is more difficult than big building. MUL-Pan Sharpen and PAN data used as the training data. Improved U-NET and HF-FCN were selected as main models. In order to detect small building, we oversample small building areas and under sample large building areas. We adapt morphological methods to dilate and erode output of the mod-el. With the optimization of model’s outputs, we can fill in the disconnected area, but also eliminates part of the false detection noise. © Springer Nature Singapore Pte Ltd 2019.","Building extraction; Deep learning; HF-FCN; Remote sensing; U-NET","Advanced Analytics; Buildings; Edge computing; Remote sensing; Automatic building detection; Building extraction; Detection methods; False detections; High resolution remote sensing images; Large buildings; Remote sensing images; Small buildings; Deep learning"
"A big remote sensing data analysis using deep learning framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073259664&doi=10.33965%2fbigdaci2019_201907l015&partnerID=40&md5=da899d22e4df010cd43ec524d8733796","Spaceborne and airborne sensors deliver a huge number of Earth Observation Data every day. In this context, we can easily observe the whole earth from its different sides. Therefore, this big data is important in remote sensing and could be exploited in several domains requiring image classification, natural hazard monitoring, global climate change, agriculture, urban planning. Over the last five years, Convolutional Neural Networks (CNN) emerged as the most successful technique for the image classification task, as well as a number of other computer vision tasks. However, to train millions of parameters in CNN one requires a huge amount of annotated data. This requirement leads to a significant challenge if the available training data is limited for a target task at hand. To address this challenge, in the recent literature, researchers proposed various ways to apply a technique called Transfer Learning to transfer the knowledge gained by training CNNs parameters on some large annotated dataset to the target task with limited availability of training data. Most of our work in this paper was dedicated to proposing a hybrid classification of remote sensing images. This architecture combines Spark RDD image coding to consider image's local regions, pre-trained VGGNET-16 and UNET for image segmentation and SVM (Support Vector Machines) from spark Machine Learning to achieve labeling task. © Multi Conference on Computer Science and Information Systems, MCCSIS 2019. All rights reserved.","Big Data; Deep Learning; Feature extraction; Multi-label Classification; Remote Sensing; Support Vector Machines","Advanced Analytics; Big data; Classification (of information); Climate change; Computation theory; Deep learning; Feature extraction; Hazards; Image classification; Image coding; Image segmentation; Large dataset; Neural networks; Remote sensing; Support vector machines; Convolutional neural network; Earth observation data; Global climate changes; Hybrid classification; Learning frameworks; Multi label classification; Remote sensing data; Transfer learning; Data mining"
"A futuristic deep learning framework approach for land use-land cover classification using remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049899327&doi=10.1007%2f978-981-13-0680-8_9&partnerID=40&md5=3beb6980feabc498fdd09dd42eea2e8b","Our aim is to propose a new deep learning framework approach which uses an ensemble of convolutional neural network (CNN) for land use-land cover mapping. Every CNN layer was fed with diverse combination of multispectral and geospatial satellite bands provided by Sentinel 2 satellite imagery (spatial resolution of 10 m), topographic and derived texture parameters, of New Delhi (28.6139° N, 77.2090° E) region, India. Several classes were identified like forest, parking, residential areas, slums, wasteland, water bodies. It was observed that our proposed framework outperformed with classification accuracy of 89.43%, compared to the current state-of-the-art algorithms (support vector machine (SVM), K-nearest neighbor (KNN), and random forest (RF)). Accuracy assessment was done by means of following statistic measures (precision, recall, specificity, and area under curve (AUC)) and receiver operating characteristic (ROC) curve. © 2019, Springer Nature Singapore Pte Ltd.","Convolutional neural network; Deep learning; Land use-land cover; Sentinel 2","Convolution; Decision trees; Forestry; Housing; Image classification; Land use; Nearest neighbor search; Neural networks; Remote sensing; Satellite imagery; Support vector machines; Classification accuracy; Convolutional neural network; Convolutional Neural Networks (CNN); K nearest neighbor (KNN); Land use/ land covers; Receiver Operating Characteristic (ROC) curves; Sentinel 2; State-of-the-art algorithms; Deep learning"
"Deep learning and optimization algorithm for high efficient searching and detection of aircraft targets in remote sensing images [遥感图像飞机目标高效搜检深度学习优化算法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062846476&doi=10.13700%2fj.bh.1001-5965.2018.0239&partnerID=40&md5=c0fc486b5aae77562a20cf7e811705ab","In order to achieve high-performance detection and accurate positioning of aircraft targets in large-scale remote sensing images, in this paper, a kind of efficient aircraft target detection algorithm based on synthetic integration of searching and detection is presented. First, through the end-to-end deep neural networks (DNN), the efficient and accurate pixel-level segmentation of apron and runway area is achieved so that the searching range of aircraft targets is greatly narrowed and the probability of false alarm is also reduced effectively and the goal of high speed searching of aircraft targets candidate detection areas is achieved accordingly. In view of the segmented areas of apron and runway, the strategy of transfer reinforcement learning is employed to pre-trained YOLO networks with supervised signals of positive datasets by manual labelling. In this way, pre-trained networks can make up the deficiency of capacity of manual data sets, and the advantage of real-time property of YOLO networks can also be utilized to deal with the classification and locations of aircraft targets so as to achieve a satisfied solution of regression problems and promote the efficiency of detection significantly. It is obvious that the apron and runway segmentation with DNN networks can play important role in getting performance superiority of high precision and efficiency. Meanwhile, YOLO networks based on transfer reinforcement learning not only possess the characteristics of high efficiency, but also maintain the precision of detection at a high level. A series of comprehensive experiments and comparative analyses verify the effectiveness and good performance of the proposed searching and detection algorithm of aircraft targets with DNN cascade combination and synthetic integration. © 2019, Editorial Board of JBUAA. All right reserved.","Aircraft target detection; Apron and runway segmentation; Deep learning; Deep neural networks; Large-scale remote sensing image","Aircraft; Classification (of information); Deep learning; Deep neural networks; Efficiency; Image segmentation; Machine learning; Reinforcement learning; Remote sensing; Signal detection; Vehicle performance; Aircraft targets; Comparative analysis; Detection algorithm; Optimization algorithms; Probability of false alarm; Real-time properties; Remote sensing images; Synthetic integrations; Aircraft detection"
"Deep Learning Based Adversarial Images Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076835188&doi=10.1007%2f978-3-030-36402-1_30&partnerID=40&md5=a67b717b9eff5404740da095cc85bbdc","The threat of attack against deep learning based network is gradually strengthened in computer vision. The adversarial examples or images are produced by applying intentional a slight perturbation, which is not recognized by human, but can confuse the deep learning based classifier. To enhance the robustness of image classifier, we proposed several deep learning based algorithms (i.e., CNN-SVM, CNN-KNN, CNN-RF) to detect adversarial images. To improve the utilization rate of multi-layer features, an ensemble model based on two layer features generated by CNN is applied to detect adversarial examples. The accuracy, detection probability, fake alarm probability and miss probability are applied to evaluate our proposed algorithms. The results show that the ensemble model based on SVM can achieve the best performance (i.e., 94.5%) than other methods for testing remote sensing image dataset. © 2019, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Adversarial detection; Deep learning; Ensemble model; K-nearest neighbors (KNN); Random forest (RF); Support vector machine (SVM)","Decision trees; Image enhancement; Nearest neighbor search; Probability; Remote sensing; Statistical tests; Support vector machines; Detection probabilities; Ensemble modeling; Image Classifiers; K nearest neighbor (KNN); Learning-based algorithms; Random forests; Remote sensing images; Utilization rates; Deep learning"
"Learning transfer using deep convolutional features for remote sensing image retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077162550&partnerID=40&md5=f181c06b90713496d73efb66fed7375f","Convolutional neural networks (CNNs) have recently witnessed a notable interest due to their superior performance demonstrated in computer vision applications; including image retrieval. This paper introduces an optimized bilinear-CNN architecture applied in the context of remote sensing image retrieval, which investigates the capability of deep neural networks in learning transfer from general data to domain-specific application, i.e. remote sensing image retrieval. The proposed deep learning model involves two parallel feature extractors to formulate image representations from local patches at deep convolutional layers. The extracted features are approximated into low-dimensional features by a polynomial kernel projection. Each single geographic image is represented by a discriminating compact descriptor using a modified compact pooling scheme followed by feature normalization. An end-to-end deep learning is performed to generate the final fine-tuned network model. The model performance is evaluated on the standard UCMerced land-use/land-cover (LULC) dataset with high-resolution aerial imagery. The conducted experiments on the proposed model show high performance in extracting and learning complex image features, which affirms the superiority of deep bilinear features in the context of remote sensing image retrieval. © 2019 International Association of Engineers.","Deep learning; Image retrieval; Remote sensing","Aerial photography; Antennas; Convolution; Deep learning; Deep neural networks; Land use; Neural networks; Remote sensing; Compact descriptor; Computer vision applications; Convolutional neural network; Domain-specific application; Feature normalization; High resolution aerial imagery; Image representations; Remote sensing image retrieval; Image retrieval"
"Research on Spatial Target Classification and Recognition Technology Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070512876&doi=10.1007%2f978-3-030-27538-9_28&partnerID=40&md5=5605d8c97eb47c97eafc0ce5ca7bdf9b","With the progress of human space technology, mankind stepped into more distant space. Due to the long distance from the earth in deep space exploration, higher requirements were put forward for intelligent cognition of targets and environment. In this paper, the space target classification and recognition technology based on deep learning was studied by taking the classification of three types of satellites as an example. A satellite simulation sample set for deep learning was established, and a ResNet multi-layer convolutional neural network model suitable for spatial target characteristics was constructed. The training and test of satellite intelligent classification were completed, and the feature extraction results of the neural network were visualized. The accuracy rate of satellite classification identification for the remote sensing satellites, communication satellites and navigation satellites reached 90%, which provided a reference for the development of intelligent classification and identification technology of space targets in the field of deep space exploration. © 2019, Springer Nature Switzerland AG.","Deep learning; Deep space exploration; Intelligent cognition","Communication satellites; Earth (planet); Interplanetary flight; Multilayer neural networks; Network layers; Remote sensing; Robotics; Robots; Satellites; Space research; Classification identification; Convolutional neural network; Deep-space exploration; Intelligent classification; Intelligent cognition; Navigation satellites; Remote sensing satellites; Target Classification; Deep learning"
"A parallel segmentation after classification algorithm of multi-spectral image of k-means of deep learning and panchromatic based on wavelet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072615408&doi=10.1117%2f12.2539657&partnerID=40&md5=6858c0ec8c482aa6db8cd094be7a513c","[Purpose]To take the advantages of a variety of remote sensing data, the application of remote sensing image classification is a very important choice.Remote sensing image classification is large in computing capacity and time-consuming, and with the development of modern remote sensing technology, the amount of various remote sensing data obtained is getting larger and larger,the issue of how to fuse remote sensing image quickly and accurately and of getting useful information is becoming more and more urgent especially in some remote sensing applications such as disaster monitoring, prevention and relief, etc.In this paper, in order to fuse remote sensing image quickly and accurately, a parallel classification algorithm of multi-spectral image and panchromatic image based on wavelet transform is proposed.[Methods]In the method, based on parallel computing, the low-frequency components of wavelet decomposition are fused with the classification rule based on the feature matching, and the high-frequency components of wavelet decomposition are fused with the classification rule based on the sub-region variance. Then the low-frequency components and the high-frequency components after classification are processed with the inverse wavelet transform, and the fused image is obtained. According to the statistical characteristics of SAR images and the semantics of fuzzy neural networks analysis, an efficient image segmentation method based on Deep Learning Semantic analysis and wavelet transform is proposed to achieve precision of classification.[ Results] The experiment results show that the proposed method can get better classification results and faster computing speed for multi-spectral image and panchromatic image. Originality. In the proposed classification algorithm of multi-spectral image and panchromatic image, wavelet transform and different proper classification rules for low-frequency components and high frequency components of wavelet decomposition are used. To get a high speed, parallel computing is also taken in some complex parts of the proposed classification algorithm. Thus, better classification results and faster computing speed are obtained. [Conclusions] Practical value. The experiments have proved that the proposed algorithm can quickly get good classification results for remote sensing images, and it is useful in remote sensing applications in some aspects such as disaster monitoring, prevention and relief, Hybrid parallel Computing method for tasks and data (Pixels) etc. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Data clustering; Image classification; Multi-spectral image; Panchromatic image; Parallel computing; Remote sensing; Wavelet transform","Classification (of information); Deep learning; Disaster prevention; Electric arcs; Emergency services; Fuzzy neural networks; Image compression; Image segmentation; Inverse problems; K-means clustering; Parallel processing systems; Remote sensing; Semantics; Spectroscopy; Synthetic aperture radar; Wavelet decomposition; Wavelet transforms; Data clustering; High frequency components; Inverse wavelet transforms; Multispectral images; Panchromatic images; Remote sensing applications; Remote sensing image classification; Statistical characteristics; Image classification"
"Deep learning for inversion of significant wave height based on actual sea surface backscattering coefficient model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069692790&doi=10.1007%2fs11042-019-07967-6&partnerID=40&md5=57d567082564f58bc46df1e0addd1e9e","Ocean waves are complex systems with the contributions of wind waves and swells. The study on interaction mechanism between electromagnetic wave and actual sea surface is of significant importance in ocean remote sensing and engineering application, which is also helpful in the prediction and inversion of wave information. In this paper, an efficient model for estimating backscattering coefficient is built, considering the characteristics of the wind-wave regime based on the inverse wave age. The backscattering coefficient results have been verified by comparing with the data collected in Lingshan Island during the period of October and November 2014 at low grazing angles and the Ku-band measurements at moderate grazing angles. The results indicate perfect agreement (within about 2 dB) with field data. Deep learning is an excellent method that can be used not only for classification but also for inversion and fitting of non-linear functions. In order to simulate the application of actual radar detection and inversion technology, the inversion of significant wave height from actual sea surface backscattering coefficients train data sets has been performed by using deep learning technology. The accuracy of 99.01% has been achieved under the condition of three hidden layers and iterating 100 times. The root mean square errors of the test data sets are less than 0.10, which indicates that deep learning is available in the inversion of significant wave height. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Backscattering coefficient model; Deep learning technology; Inversion; Ocean surface waves; Significant wave height","Backscattering; Electromagnetic waves; Engineering education; Functions; Inverse problems; Mean square error; Remote sensing; Surface waters; Surface waves; Tracking radar; Water waves; Backscattering coefficients; Inversion; Learning technology; Ocean surface waves; Significant wave height; Deep learning"
"Targeted change detection in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063436334&doi=10.1117%2f12.2523141&partnerID=40&md5=81ee6d2ab7c800e3d247e4110f150921","Recent developments in the remote sensing systems and image processing made it possible to propose a new method of the object classification and detection of the specific changes in the series of satellite Earth images (so called targeted change detection). In this paper we develop a formal problem statement that allows to use effectively the deep learning approach to analyze time-dependent series of remote sensing images. We also introduce a new framework for the development of deep learning models for targeted change detection and demonstrate some cases of business applications it can be used for. Copyright © 2019 SPIE.","change detection; computer vision; deep learning; remote sensing; satellite imagery","Computer vision; Deep learning; Object detection; Satellite imagery; Business applications; Change detection; Learning approach; Learning models; Object classification; Problem statement; Remote sensing images; Remote sensing system; Remote sensing"
"Joint learning of the center points and deep metrics for land-use classification in remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059960676&doi=10.3390%2frs11010076&partnerID=40&md5=0979829fb74a96441b9a1ec2b0d397fc","Deep learning methods, especially convolutional neural networks (CNNs), have shown remarkable ability for remote sensing scene classification. However, the traditional training process of standard CNNs only takes the point-wise penalization of the training samples into consideration, which usually makes the learned CNNs sub-optimal especially for remote sensing scenes with large intra-class variance and low inter-class variance. To address this problem, deep metric learning, which incorporates the metric learning into the deep model, is used to maximize the inter-class variance and minimize the intra-class variance for better representation. This work introduces structured metric learning for remote sensing scene representation, a special deep metric learning which can take full advantage of the training batch. However, the deep metrics only consider the pairwise correlation between the training samples, and ignores the classwise correlation from the class view. To take the classwise penalization into consideration, this work defines the center points of the learned features of each class in the training process to represent the class. Through increasing the variance between different center points and decreasing the variance between the learned features from each class and the corresponding center point, the representational ability can be further improved. Therefore, this work develops a novel center-based structured metric learning to take advantage of both the deep metrics and the center points. Finally, joint supervision of the cross-entropy loss and the center-based structured metric learning is developed for the land-use classification in remote sensing. It can joint learn the center points and the deep metrics to take advantage of the point-wise, the pairwise, and the classwise correlation. Experiments are conducted over three real-world remote sensing scene datasets, namely UC Merced Land-Use dataset, Brazilian Coffee Scene dataset, and Google dataset. The classification performance can achieve 97.30%, 91.24%, and 92.04% with the proposed method over the three datasets which are better than other state-of-the-art methods under the same experimental setups. The results demonstrate that the proposed method can improve the representational ability for the remote sensing scenes. © 2019 by the authors.","Center point; Convolutional Neural Network (CNN); Diversity; Metric learning; Remote sensing scene classification","Classification (of information); Convolution; Deep learning; Land use; Neural networks; Sampling; Center points; Convolutional Neural Networks (CNN); Diversity; Metric learning; Scene classification; Remote sensing"
"Quantification of hydrocarbon abundance in soils using deep learning with dropout and hyperspectral data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071567588&doi=10.3390%2frs11161938&partnerID=40&md5=85e350a84bcc128833058a6e4afc4db9","Terrestrial hydrocarbon spills have the potential to cause significant soil degradation across large areas. Identification and remedial measures taken at an early stage are therefore important. Reflectance spectroscopy is a rapid remote sensing method that has proven capable of characterizing hydrocarbon-contaminated soils. In this paper, we develop a deep learning approach to estimate the amount of Hydrocarbon (HC) mixed with different soil samples using a three-term backpropagation algorithm with dropout. The dropout was used to avoid overfitting and reduce computational complexity. A Hyspex SWIR 384 m camera measured the reflectance of the samples obtained by mixing and homogenizing four different soil types with four different HC substances, respectively. The datasets were fed into the proposed deep learning neural network to quantify the amount of HCs in each dataset. Individual validation of all the dataset shows excellent prediction estimation of the HC content with an average mean square error of ~2.2 × 10-4. The results with remote sensed data captured by an airborne system validate the approach. This demonstrates that a deep learning approach coupled with hyperspectral imaging techniques can be used for rapid identification and estimation of HCs in soils, which could be useful in estimating the quantity of HC spills at an early stage. © 2019 by the authors.","Deep learning; Dropout; Hydrocarbons; Spectral unmixing; Three-term backpropagation","Backpropagation algorithms; Hydrocarbons; Hyperspectral imaging; Mean square error; Reflection; Remote sensing; Soil pollution; Soils; Spectroscopy; Dropout; Hydrocarbon spills; Hyperspectral Data; Learning neural networks; Rapid identification; Reflectance spectroscopy; Spectral unmixing; Three-term; Deep learning"
"A novel data augmentation method for detection of specific aircraft in remote sensing RGB images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067015521&doi=10.1109%2fACCESS.2019.2913191&partnerID=40&md5=11edfba557a224268152dcfeae5543ff","In this paper, we propose a novel data augmentation method for the detection of specific aircraft in remote sensing RGB images. For object detection, the number of training data has great influence on the training of deep learning network. Researchers suggest that the extensive training samples in deep learning are indispensable. The extensive training samples can guarantee the accuracy and robustness of object detection. We refer to military aircraft and helicopter as specific aircraft. Due to the number of remote sensing images of specific aircraft is far less than that of civil aircraft, it is difficult to train an ideal detection model only by using the available specific aircraft images. Deep learning networks have the excellent ability on fault tolerance and generalization and can extract features from simulated aircraft samples. This means that the simulated aircraft samples can partly replace real images to some extent and reduce the need for detection models. Inspired by this, true remote sensing images are combined with specific aircraft three-dimensional models to form simulated images. Compared with previous data augmentation method, such as flipping and rotating, our method brings in new sample information. The experiments based on remote sensing images show the feasibility and effectiveness of our method. Meanwhile, the proposed method has compatibility with other data augmentation methods. © 2013 IEEE.","Aircraft detection; convolutional neural networks; data augmentation; remote sensing images","Deep learning; Fault tolerance; Neural networks; Object detection; Object recognition; Remote sensing; Sampling; Training aircraft; Convolutional neural network; Data augmentation; Detection models; Learning network; Remote sensing images; Sample information; Simulated images; Three-dimensional model; Aircraft detection"
"Psegnet and npsegnet: New neural network architectures for cloud recognition of remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069777117&doi=10.1109%2fACCESS.2019.2925565&partnerID=40&md5=ac2173bb949e90d8dea37ca3a7653164","In recent years, remote sensing images have played an important role in environmental monitoring, weather forecasting, and agricultural planning. However, remote sensing images often contain a large number of cloud layers. These clouds can cover a large amount of surface information. Therefore, an increasing number of cloud recognition methods have been proposed to reduce the impact of cloud cover. There are many difficulties in traditional cloud recognition methods. For example, the threshold method based on spectral features improves the accuracy of cloud detection, but it often leads to omission or misjudgment in cloud detection and depends on prior knowledge. To improve the accuracy and efficiency of cloud recognition, we use deep learning to address cloud recognition problems in remote sensing imagery. We propose a series of methods from the acquisition and production of training datasets to neural network training and cloud recognition applications. This paper describes a realization of cloud recognition of remote sensing imagery based on SegNet architecture. We have proposed two architectures named PSegNet and NPSegNet, which are modified from SegNet. Some parallel structures were also employed into the SegNet architecture to improve the accuracy of cloud recognition. Due to these changes, this paper also discusses the impact of the symmetry network structure on the final accuracy. Our proposed method was compared with the well-known fully convolutional neural network (FCNN) approach. The results have demonstrated the feasibility and practicality of using deep learning approach for cloud recognition in remote sensing images. © 2013 IEEE.","cloud detection; Deep learning; neural network; remote sensing imagery","Computer architecture; Deep learning; Deep neural networks; Image enhancement; Network architecture; Neural networks; Weather forecasting; Agricultural planning; Cloud detection; Convolutional neural network; Environmental Monitoring; Neural network training; Remote sensing imagery; Remote sensing images; Surface information; Remote sensing"
"A Deep-Learning-Based Low-Altitude Remote Sensing Algorithm for Weed Classification in Ecological Irrigation Area","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064043711&doi=10.1007%2f978-981-13-6052-7_39&partnerID=40&md5=29912311ada14b207b890c4f54965eb4","With the development of ecological irrigation area at present, it requires higher detection and control of weeds in irrigation area. In this paper, aiming at the ecological irrigation area, a classification method of weeds based on convolutional neural network (CNN) is proposed. By collecting 3 kinds of weeds and 3 kinds of crops as data sets, through cutting, rotating and so on, data is transported to the CNN. Finally, 6 categories of classifications are implemented. By using the pre-trained AlexNet network for transfer learning, single CPU, single GPU, and double GPUs training experiments are performed in matlab2018(a). The classification results show that the recognition rate of weeds can reach 99.89%. In order to prevent and control specific weeds, a method of detecting single weeds density is also presented in this paper. The accurate monitoring of weeds in irrigation area can be realized through the method proposed in this paper, and there is basis for precise weed control in later stage. © 2019, Springer Nature Singapore Pte Ltd.","Convolutional neural network; Ecological irrigation area; UAV remote sensing; Weed classification","Convolution; Ecology; Irrigation; Neural networks; Program processors; Remote sensing; Weed control; Classification methods; Classification results; Convolutional neural network; Detection and controls; Irrigation area; Remote sensing algorithms; Transfer learning; UAV remote sensing; Deep learning"
"Research on crop classification in Northeast China based on deep learning for Sentinel-2 data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075055891&doi=10.1117%2f12.2527376&partnerID=40&md5=2ea9a2fed4cb20d5f82b6335f5ecbe11","With the development of space satellite technology, a large number of high resolution remote sensing images have emerged. Deep learning has become an effective way to handle big data. Crop classification can estimate crop planting area and structure, and classification result is an important input parameter for crop yield model. Crop classification based on deep learning can further improve the estimation accuracy of production. In this paper, multi-temporal Sentinel-2 data and GF2 data are used as data sources. Sentinel-2 data is used as training data, and GF-2 data is used as validation data. Jilin Province in Northeast of China is selected as the experimental area. The experimental area is classified as rice, towns, corn and soy. Firstly, the multi-temporal Sentinel-2 data and GF-2 data is preprocessed. Then, the Sentinel-2 data is used to classify crops based on convolutional neural network (CNN) and visual geometry group (VGG). The red edge band, multiple indexes including normalization difference vegetation index (NDVI), normalized difference water index (NDWI) and difference vegetation index (DVI) are added respectively to compare with the classification results of original multitemporal Sentinel-2 data. The final classification results using CNN and VGG are compared with the other two machine learning algorithms including support vector machine (SVM) and random forest (RF). The experimental results show that the VGG performs best in crop classification accuracy. The overall classification accuracy of the crop can reach 94.8878%, and the Kappa coefficient can reach 0.9253, which is superior to the two traditional machine learning algorithms. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","CNN; Crop classification; Deep learning; VGG","Classification (of information); Crops; Decision trees; Learning algorithms; Machine learning; Neural networks; Remote sensing; Support vector machines; Vegetation; Classification accuracy; Classification results; Convolutional neural network; Crop classification; High resolution remote sensing images; Kappa coefficient; Normalized difference water index; Vegetation index; Deep learning"
"A Semantic Segmentation Approach Based on DeepLab Network in High-Resolution Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076848765&doi=10.1007%2f978-3-030-34113-8_25&partnerID=40&md5=7f1477ccf1153e0ccf194c945c1e018d","Recently, more and more applications for high-resolution remote sensing image intelligent processing are required. Therefore, the semantic segmentation based on deep learning has successfully attracted people’s attention. In this paper, the improved Deeplabv3 network is used in the application of image semantic segmentation. The problem of segmenting objects of multiple scales of high-resolution remote sensing image is handled, and the Chinese GaoFen NO. 2(GF-2) remote sensing image is taken as the main research object. Firstly, the original image is pre-processed. Next, use data augmentation and expansion for the pre-processed training image to avoid over-fitting. Finally, it is studied the adaptability and accuracy of the model of high-resolution remote sensing images, while is found the appropriate parameters to improve the precise of the result models compared. And explore the effectiveness of the model in the case of a fewer samples. This model is demonstrated that could be achieved the well classification result. © 2019, Springer Nature Switzerland AG.","Deep learning; Remote sensing image classification; Semantic segmentation","Deep learning; Image enhancement; Remote sensing; Semantic Web; Semantics; Classification results; Data augmentation; High resolution remote sensing images; Intelligent processing; Remote sensing image classification; Remote sensing images; Research object; Semantic segmentation; Image segmentation"
"Recognition of low-resolution objects in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078147199&doi=10.1117%2f12.2533315&partnerID=40&md5=56f11a750051f5de2b041ee6aa383e35","Object detection and recognition is one of the important problems in many remote sensing applications such as monitoring, security, rescue mission and other data analysis tasks. So a large number of approaches and techniques have been developed to improve the quality of object detection. Recently, deep learning methods have made significant progress in detecting objects. But when dealing with objects having small size in the image (which are the often case in monitoring or rescue mission), the quality of detection noticeably decreases. To study the performance and the limits of deep learning abilities for object detection in remote sensing imagery with degrading object resolution in the image, a special dataset of aerial images containing objects of interest (human, car) at different resolution has been collected. The dataset consists of images acquired at different distances to the objects of interest, providing representative subsets of object images of various scale. Two state-of-the-art object detection convolutional neural networks (Faster-RCNN and SSD) was evaluated on the collected dataset. The aim of the study was to find out how the object size in the image influences on the detection performance and to estimate the value of object image size at which the performance drops significantly. Also the approaches for improving the small size object recognition were developed and evaluated. First approach uses multimodal image fusion, the second one applies deep learning to increase the resolution of small objects in the image. The performed tests have proved that the developed approaches allow to improve the quality of object recognition when dealing with low resolution object images. © 2019 SPIE.","Convolutional neural network; Deep learning; Detection performance; Image reso-lution; Object detection; Unmanned aerial vehicle","Aircraft detection; Antennas; Convolution; Deep learning; Deep neural networks; Image enhancement; Image fusion; Neural networks; Object recognition; Remote sensing; Unmanned aerial vehicles (UAV); Vehicle performance; Convolutional neural network; Detection performance; Different resolutions; Multimodal image fusions; Object detection and recognition; Remote sensing applications; Remote sensing imagery; Remote sensing images; Object detection"
"Towards a temporal deep learning model to support sustainable agricultural practices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081587220&partnerID=40&md5=67574cddc731d5c1e5fec952db2284d7","The impressive results of deep learning in many different fields, specifically in remote sensing, together with the growing availability of open Earth Observation data creates new opportunities to address global problems. One such global problem is associated with the simplification and intensification of agricultural systems which threatens the worldwide sustainability of crop production. Despite the fact that a plethora of satellite images describe a given location on earth every year, very few deep learning-based solutions have harnessed the temporal and sequential dynamics of land use to map sustainable and unsustainable cropping practices. In this paper, we present the preliminary results of a set of experiments conducted using one-dimensional Convolutional Neural Networks (CNN) for classifying multispectral time series derived from Landsat satellites constellation. The experimental data is related to agricultural practices in Sacramento County, California, United States of America. We discuss the applicability of this approach for mapping sustainable crop rotation-based practices which have been proven to mitigate the environmental impact of agricultural land use dynamics. Copyright © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","AI; Deep Learning; Satellite Images; Sustainable Agriculture","Artificial intelligence; Convolutional neural networks; Crops; Cultivation; Environmental impact; Land use; Remote sensing; Satellites; Sustainable development; Agricultural land use; Agricultural practices; Agricultural system; California , United States Of America; Earth observation data; Satellite images; Sustainable agricultural; Sustainable agriculture; Deep learning"
"Airborne Object Detection Using Hyperspectral Imaging: Deep Learning Review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069201796&doi=10.1007%2f978-3-030-24289-3_23&partnerID=40&md5=6adfd1f46cdbb0b0d9a95cae6014992a","Hyperspectral images have been increasingly important in object detection applications especially in remote sensing scenarios. Machine learning algorithms have become emerging tools for hyperspectral image analysis. The high dimensionality of hyperspectral images and the availability of simulated spectral sample libraries make deep learning an appealing approach. This report reviews recent data processing and object detection methods in the area including hand-crafted and automated feature extraction based on deep learning neural networks. The accuracy performances were compared according to existing reports as well as our own experiments (i.e., re-implementing and testing on new datasets). CNN models provided reliable performance of over 97% detection accuracy across a large set of HSI collections. A wide range of data were used: a rural area (Indian Pines data), an urban area (Pavia University), a wetland region (Botswana), an industrial field (Kennedy Space Center), to a farm site (Salinas). Note that, the Botswana set was not reviewed in recent works, thus high accuracy selected methods were newly compared in this work. A plain CNN model was also found to be able to perform comparably to its more complex variants in target detection applications. © 2019, Springer Nature Switzerland AG.","Classification; Deep learning; Hyperspectral imaging; Remote sensing","Classification (of information); Data handling; Feature extraction; Hyperspectral imaging; Learning algorithms; Machine learning; Object detection; Object recognition; Remote sensing; Space optics; Space platforms; Spectroscopy; Urban growth; Well testing; Automated feature extraction; Detection accuracy; High dimensionality; Industrial fields; Kennedy space centers; Learning neural networks; Object detection method; Reliable performance; Deep learning"
"Optimizing deep learning model selection for angular feature extraction in satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072570189&doi=10.1117%2f12.2519373&partnerID=40&md5=69babfccbc10ae5fe55c3b21b76df800","Deep learning techniques have been leveraged in numerous applications and across different data modalities over the past few decades, more recently in the domain of remotely sensed imagery. Given the complexity and depth of Convolutional Neural Networks (CNNs) architectures, it is difficult to fully evaluate performance, optimize the hyperparameters, and provide robust solutions to a specific machine learning problem that can be easily extended to similar problems, e.g. via transfer learning. Ursa Space Systems Inc. (Ursa) develops novel machine learning approaches to build custom solutions and extract answers from Synthetic Aperture Radar (SAR) satellite data fused with other remote sensing datasets. One application is identifying the orientation with respect to true north of the inlet pipe, which is one common feature located on the top of a cylindrical oil storage tank. In this paper, we propose a two-phase approach for determining this orientation: first an optimized CNN is used to probabilistically determine a coarse orientation of the inlet pipe, followed by a maximum likelihood voting scheme to automatically extract the location of the angular feature within 7.5°. We present a systematic technique to determine the best deep learning CNN architecture for our specific problem and under user-defined optimization and accuracy constraints, by optimizing model hyperparameters (number of layers, size of the input image, and dataset preprocessing) using a manual and grid search approach. The use of this systematic approach for hyperparameter optimization yields increased accuracy for our angular feature extraction algorithm from 86% to 94% and can be extended to similar applications. © 2019 SPIE.",,"Digital storage; Extraction; Feature extraction; Machine learning; Maximum likelihood; Network architecture; Neural networks; Remote sensing; Satellite imagery; Search engines; Space-based radar; Spectroscopy; Synthetic aperture radar; Convolutional neural network; Feature extraction algorithms; Hyper-parameter optimizations; Learning techniques; Machine learning approaches; Machine learning problem; Remotely sensed imagery; Transfer learning; Deep learning"
"A review of convolutional neural networks in remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066020722&doi=10.1145%2f3316615.3316712&partnerID=40&md5=3adc1c3071e00d73131570243bbefaae","Effectively analysis of remote-sensing images is very important in many practical applications, such as urban planning, geospatial object detection, military monitoring, vegetation mapping and precision agriculture. Recently, convolutional neural network based deep learning algorithm has achieved a series of breakthrough research results in the fields of objective detection, image semantic segmentation and image classification, etc. Their powerful feature learning capabilities have attracted more attention and have important research value. In this article, firstly we have summarized the basic structure and several classical convolutional neural network architectures. Secondly, the recent research problems on convolutional neural network are discussed. Later, we summarized the latest research results in convolutional neural network based remote sensing fields. Finally, the conclusion has made on the basis of current issue on convolutional neural networks and the future development direction. © 2019 Association for Computing Machinery.","Convolutional neural network; Deep learning; Remote-sensing images","Application programs; Convolution; Deep learning; Image segmentation; Learning algorithms; Military applications; Military photography; Network architecture; Neural networks; Object detection; Semantics; Convolutional neural network; Development directions; Feature learning; Geo-spatial objects; Recent researches; Remote sensing images; Research results; Vegetation mapping; Remote sensing"
"Unsupervised self-adaptive deep learning classification network based on the optic nerve microsaccade mechanism for unmanned aerial vehicle remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074974444&doi=10.1080%2f10106049.2019.1687593&partnerID=40&md5=3c14383c14ab65abe193611312267d13","Unmanned aerial vehicle remote sensing images need to be precisely and efficiently classified. However, complex ground scenes produced by ultra-high ground resolution, data uniqueness caused by multi-perspective observations, and need for manual labelling make it difficult for current popular deep learning networks to obtain reliable references from heterogeneous samples. To address these problems, this paper proposes an optic nerve microsaccade (ONMS) classification network, developed based on multiple dilated convolution. ONMS first applies a Laplacian of Gaussian filter to find typical features of ground objects and establishes class labels using adaptive clustering. Then, using an image pyramid, multi-scale image data are mapped to the class labels adaptively to generate homologous reliable samples. Finally, an end-to-end multi-scale neural network is applied for classification. Experimental results show that ONMS significantly reduces sample labelling costs while retaining high cognitive performance, classification accuracy, and noise resistance—indicating that it has significant application advantages. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","image classification; Optic nerve microsaccade; self-adaptive learning; unmanned aerial vehicle",
"Deep Learning for Automatic Outlining Agricultural Parcels: Exploiting the Land Parcel Identification System","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078001879&doi=10.1109%2fACCESS.2019.2950371&partnerID=40&md5=e10c163a759dba285475d0821e4fe624","Accurate and up-to-date information on the spatial and geographical characteristics of agricultural areas is an indispensable value for the various activities related to agriculture and research. Most agricultural studies and policies are carried out at the field level, for which precise boundaries are required. Today, high-resolution remote sensing images provide useful spatial information for plot delineation; however, manual processing is time-consuming and prone to human error. The objective of this paper is to explore the potential of deep learning (DL) approach, in particular a convolutional neural network (CNN) model, for the automatic outlining of agricultural plot boundaries from orthophotos over large areas with a heterogeneous landscape. Since DL approaches require a large amount of labeled data to learn, we have exploited the open data from the Land Parcel Identification System (LPIS) from the Chartered Community of Navarre, Spain. The boundaries of the agricultural plots obtained from our methodology were compared with those obtained using a state-of-the-art methodology known as gPb-UCM (global probability of boundary followed by ultrametric contour map) through an error measurement called the boundary displacement error index (BDE). In BDE terms, the results obtained by our method outperform those obtained from the gPb-UCM method. In this regard, CNN models trained with LPIS data are a useful and powerful tool that would reduce intensive manual labor in outlining agricultural plots. © 2013 IEEE.","Convolutional neural network; deep learning; edge extraction; land parcel identification system; parcels delineation","Convolution; Deep learning; Deep neural networks; Errors; Lead compounds; Neural networks; Open Data; Remote sensing; Boundary displacements; Convolutional neural network; Edge extraction; Heterogeneous landscapes; High resolution remote sensing images; Land parcel identification systems; parcels delineation; Spatial informations; Agriculture"
"Classification of forest vegetation types in Jilin Province, China based on deep learning and multi-temporal Sentinel-2 data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075061867&doi=10.1117%2f12.2527392&partnerID=40&md5=432fd48c900471c836da616bd121d675","The classification of forest vegetation types plays an important role in the land management agencies for natural resource inventory information, especially for federally protected national forests in China. The classification results are widely used in the calculation and inversion of parameters such as forest storage volume, biomass and coverage. Forest canopy density response the extent to which the canopy is connected to each other in the forest. It can be used to observe vegetation growth. In recent years, deep learning convolutional neural networks have made significant progress in the task of remote sensing image classification and recognition. Considering that the spectral characteristics of forests in different seasons in Jilin Province of China are quite different, this paper used the optical image data of Sentinel-2A in summer, spring and autumn as the data source to calculate the normalized difference vegetation index (NDVI), bare index (BI), perpendicular vegetation index (PVI) and shadow index (SI). Next use the four vegetation indexes combined with weighted overlay analysis method to calculate forest canopy density. In this paper, the convolutional neural network (CNN) was used as the forest vegetation type classifier. The classification indexes were the spectral data and the spectral data combined with the forest canopy density information, respectively. The experiment shows that the forest canopy density can significantly improve the classification accuracy and the overall accuracy is increased from 85.58% to 90.41%. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Canopy density; CNN; Deep learning; Forest vegetation type; Sentinel-2A","Classification (of information); Convolution; Digital storage; Forestry; Geometrical optics; Information management; Neural networks; Remote sensing; Vegetation; Canopy density; Convolutional neural network; Forest vegetation; Normalized difference vegetation index; Perpendicular vegetation index; Remote sensing image classification; Sentinel-2A; Spectral characteristics; Deep learning"
"Dimensionality reduction using discriminative autoencoders for remote sensing image retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072979174&doi=10.1007%2f978-3-030-30642-7_45&partnerID=40&md5=54183ff3513c30973f1fdf43a30d810b","Advancements in deep learning techniques caused a paradigm shift in feature extraction for image perception from handcrafted methods to deep methods. However, these deep features if learned through unsupervised methods bear large memory footprints and are prone to the curse of dimensionality. Traditional feature reduction schemes involving aggregation of these learned visual descriptors may lead to loss of essential information necessary for their obvious discrimination. Therefore, this research studies various feature reduction techniques for remote sensing image features. We also propose an deep discriminative network with dimensionality reduction (DAE-DR), exploiting stacked autoencoder based solution to abbreviate unsupervised features without significantly affecting their discriminative and regenerative characteristics. It is observed that the spatial dimensions encoded in the feature vector are more important than increasing the number of network filters for efficient image reconstruction. Validation of our approach has been tested for remote sensing image retrieval (RSIR) problem. Results demonstrate that our proposed network achieves 25 times reduction in feature size with only 0.8 times depletion of retrieval score. © 2019, Springer Nature Switzerland AG.","Deep features; Deep learning; Remote sensing image retrieval (RSIR); Unsupervised features","Deep learning; Image analysis; Image reconstruction; Remote sensing; Curse of dimensionality; Deep features; Dimensionality reduction; Discriminative networks; Remote sensing image retrieval; Remote sensing images; Unsupervised features; Unsupervised method; Image retrieval"
"V-RSIR: An Open Access Web-Based Image Annotation Tool for Remote Sensing Image Retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068918167&doi=10.1109%2fACCESS.2019.2924933&partnerID=40&md5=476ba893465fd68fee6ee2d60273ec48","Benchmark datasets play an important role in evaluating remote sensing image retrieval (RSIR) methods. At present, several small-scale benchmark datasets for RSIR are publicly available on the web and are mostly collected through the Google Map API or other desktop tools. Because the Google Map API requires the users to have programming skills and other collection tools are not publicly available, this may limit the possibility for a wide range of volunteers to participate in generating large-scale benchmark datasets. To address this challenge, we develop an open access web-based tool V-RSIR that allows volunteers to easily participate in generating new benchmark datasets for RSIR. This web-based tool not only facilitates the remote sensing image label and cropping, but also provides image editing, review, quantity statistics, spatial distribution, sharing, and so on. To validate this tool, we recruit 32 volunteers to label and crop remote sensing images by using the tool. Finally, a new benchmark dataset that contains 38 classes with at least 1500 images per class is created. Then, the new dataset is validated by five handcrafted low-level feature methods and four deep learning high-level feature methods. The experimental results show that the handcrafted low-level feature methods perform worse than the deep learning methods, in which the precision at top 5 can achieve 94%. The evaluation results are consistent with our theoretical understanding and experimental results on the PatternNet dataset. This indicates that our web-based tool can help users generating valid benchmark datasets with volunteers for the RSIR. © 2013 IEEE.","Annotation tool; benchmark dataset; remote sensing image retrieval; volunteers; web-based tool","Application programming interfaces (API); Deep learning; Image retrieval; Large dataset; Open access; Remote sensing; Websites; Annotation tool; Benchmark datasets; Remote sensing image retrieval; volunteers; Web-based tools; Image annotation"
"Application of agricultural remote sensing image in rice growth grading monitoring","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073412274&partnerID=40&md5=5aaa8fba6ed8f609d45b047b5fe974c0","In this paper, through the grading monitoring of rice growth, developed a special map for rice growth monitoring and grading based on the deep learning and remote sensing images, it makes agricultural technicians can formulate effective field management measures in time. To achieve the goal of production increasing, remote sensing crop growth monitoring has important theoretical and practical significance for accuracy and digitization of management in agricultural production processes. This paper takes a farm as an example, uses satellite remote sensing image to extract rice planting area, and analyzes the growth of rice at heading stage. Use normalized vegetation index to invert leaf area index data, and grade growth of rice according to LAI data, Produce a thematic map of hierarchical remote sensing monitoring at heading stage of rice. Through analysis, it is known that remote sensing monitoring of crop growth can realize simultaneous monitoring and forecasting of rice growth status and growth environment, and further combine remote sensing monitoring technology with crop management knowledge model to realize real-time diagnosis and dynamic regulation of rice growth process. The research shows that the application of agricultural remote sensing images based on deep learning has a good response to rice grading monitoring. In the end, research on rice growth can better assist agronomists and agricultural technology extension workers to obtain information about the growth of the rice region, and providing information and decision support. Solve the management problems that occur during rice growth and improve the yield of rice. © 2019, Universidad del Zulia. All rights reserved.","Deep learning; Remote sensing image; Rice growth",
"Automatic Building Extraction on High-Resolution Remote Sensing Imagery Using Deep Convolutional Encoder-Decoder with Spatial Pyramid Pooling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078267965&doi=10.1109%2fACCESS.2019.2940527&partnerID=40&md5=7a3147ca967c07e7857fd48b1aa51ee0","Automatic extraction of buildings from remote sensing imagery plays a significant role in many applications, such as urban planning and monitoring changes to land cover. Various building segmentation methods have been proposed for visible remote sensing images, especially state-of-the-art methods based on convolutional neural networks (CNNs). However, high-accuracy building segmentation from high-resolution remote sensing imagery is still a challenging task due to the potentially complex texture of buildings in general and image background. Repeated pooling and striding operations used in CNNs reduce feature resolution causing a loss of detailed information. To address this issue, we propose a light-weight deep learning model integrating spatial pyramid pooling with an encoder-decoder structure. The proposed model takes advantage of a spatial pyramid pooling module to capture and aggregate multi-scale contextual information and of the ability of encoder-decoder networks to restore losses of information. The proposed model is evaluated on two publicly available datasets; the Massachusetts roads and buildings dataset and the INRIA Aerial Image Labeling Dataset. The experimental results on these datasets show qualitative and quantitative improvement against established image segmentation models, including SegNet, FCN, U-Net, Tiramisu, and FRRN. For instance, compared to the standard U-Net, the overall accuracy gain is 1.0% (0.913 vs. 0.904) and 3.6% (0.909 vs. 0.877) with a maximal increase of 3.6% in model-training time on these two datasets. These results demonstrate that the proposed model has the potential to deliver automatic building segmentation from high-resolution remote sensing images at an accuracy that makes it a useful tool for practical application scenarios. © 2013 IEEE.","building extraction; Deep learning; encoder-decoder; fully convolutional networks; high-resolution remote sensing imagery","Antennas; Buildings; Convolution; Decoding; Deep learning; Extraction; Image enhancement; Image segmentation; Neural networks; Signal encoding; Textures; Automatic building extraction; Building extraction; Convolutional networks; Convolutional neural network; Encoder-decoder; High resolution remote sensing imagery; High resolution remote sensing images; Image segmentation model; Remote sensing"
"Procedural Synthesis of Remote Sensing Images for Robust Change Detection with Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068618933&doi=10.1007%2f978-3-030-22808-8_37&partnerID=40&md5=6ef800e1b4111cab13b571714259508f","Data-driven methods such as convolutional neural networks (CNNs) are known to deliver state-of-the-art performance on image recognition tasks when the training data are abundant. However, in some instances, such as change detection in remote sensing images, annotated data cannot be obtained in sufficient quantities. In this work, we propose a simple and efficient method for creating realistic targeted synthetic datasets in the remote sensing domain, leveraging the opportunities offered by game development engines. We provide a description of the pipeline for procedural geometry generation and rendering as well as an evaluation of the efficiency of produced datasets in a change detection scenario. Our evaluations demonstrate that our pipeline helps to improve the performance and convergence of deep learning models when the amount of real-world data is severely limited. © Springer Nature Switzerland AG 2019.","Deep learning; Remote sensing; Synthetic imagery","Deep learning; Image recognition; Neural networks; Petroleum reservoir evaluation; Pipelines; Change detection; Convolutional neural network; Data-driven methods; Remote sensing images; Robust change detections; State-of-the-art performance; Synthetic datasets; Synthetic imagery; Remote sensing"
"Combining deep learning and prior knowledge for crop mapping in tropical regions from multitemporal SAR image sequences","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071986851&doi=10.3390%2frs11172029&partnerID=40&md5=d27a88e4751d628a77436d75f17985f3","Accurate crop type identification and crop area estimation from remote sensing data in tropical regions are still considered challenging tasks. The more favorable weather conditions, in comparison to the characteristic conditions of temperate regions, permit higher flexibility in land use, planning, and management, which implies complex crop dynamics. Moreover, the frequent cloud cover prevents the use of optical data during large periods of the year, making SAR data an attractive alternative for crop mapping in tropical regions. This paper evaluates the effectiveness of Deep Learning (DL) techniques for crop recognition from multi-date SAR images from tropical regions. Three DL strategies are investigated: autoencoders, convolutional neural networks, and fully-convolutional networks. The paper further proposes a post-classification technique to enforce prior knowledge about crop dynamics in the target area. Experiments conducted on a Sentinel-1 multitemporal sequence of a tropical region in Brazil reveal the pros and cons of the tested methods. In our experiments, the proposed crop dynamics model was able to correct up to 16.5% of classification errors and managed to improve the performance up to 3.2% and 8.7% in terms of overall accuracy and average F1-score, respectively. © 2019 by the authors.","Crop mapping; Deep learning; Multitemporal image analysis; SAR; Sentinel-1; Tropical agriculture","Convolution; Crops; Dynamics; Land use; Mapping; Neural networks; Radar imaging; Remote sensing; Synthetic aperture radar; Tropical engineering; Tropics; Convolutional networks; Convolutional neural network; Crop area estimations; Crop mapping; Multi-temporal SAR images; Multitemporal image analysis; Sentinel-1; Tropical agriculture; Deep learning"
"Efficient deep feature selection for remote sensing image recognition with fused deep learning architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076853164&doi=10.1007%2fs11227-019-03106-y&partnerID=40&md5=2836ee8e192a327ba255f636c292260c","Convolutional neural networks (CNNs) have recently emerged as a popular topic for machine learning in various academic and industrial fields. It is often an important problem to obtain a dataset with an appropriate size for CNN training. However, the lack of training data in the case of remote image research leads to poor performance due to the overfitting problem. In addition, the back-propagation algorithm used in CNN training is usually very slow and thus requires tuning different hyper-parameters. In order to overcome these drawbacks, a new approach fully based on machine learning algorithm to learn useful CNN features from Alexnet, VGG16, VGG19, GoogleNet, ResNet and SqueezeNet CNN architectures is proposed in the present study. This method performs a fast and accurate classification suitable for recognition systems. Alexnet, VGG16, VGG19, GoogleNet, ResNet and SqueezeNet pretrained architectures were used as feature extractors. The proposed method obtains features from the last fully connected layers of each architecture and applies the ReliefF feature selection algorithm to obtain efficient features. Then, selected features are given to the support vector machine classifier with the CNN-learned features instead of the FC layers of CNN to obtain excellent results. The effectiveness of the proposed method was tested on the UC-Merced dataset. Experimental results demonstrate that the proposed classification method achieved an accuracy rate of 98.76% and 99.29% in 50% and 80% training experiment, respectively. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","CNN; Feature reduction; Image recognition; ReliefF algorithm; UC-Merced dataset","Backpropagation algorithms; Feature extraction; Genetic algorithms; Image recognition; Learning algorithms; Machine learning; Network architecture; Neural networks; Remote sensing; Support vector machines; Classification methods; Convolutional neural network; Feature reduction; Feature selection algorithm; Learning architectures; Relieff algorithms; Support vector machine classifiers; UC-Merced dataset; Deep learning"
"Multi-scale convolutional neural network for road extraction in remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071456875&doi=10.1007%2f978-981-32-9298-7_5&partnerID=40&md5=985b7be350b3b92d3a93caba099f4203","Road is an important semantic region in remote sensing imagery and plays an important role in many applications. Deep learning has obtained a great success in image classification, since it can directly learn from labeled training samples and extract different level image features to encode the input image. In this paper, we propose a multi-scale convolutional neural network (MSCNN) for extracting road from high-resolution remote sensing image, in which road detection can be seen as a regional classification. This core trainable detection engine consists of an encoder-decoder network and a fusion model. Firstly, image was encoded as a feature representation with several stacked convolutional layers. And then guided by the different scale road training data, the pre-trained decoder networks output a series of classification maps. Finally, we investigate the fusion model utilizing different scale classification maps and obtain a final road decision map. To validate the performance of the proposed method, we test our MSCNN based method and other state-of-the-art approaches on two challenging datasets of high-resolution images. Experiments show our method gets the best results both in quantitative and qualitative evaluation. © 2019, Springer Nature Singapore Pte Ltd.","Convolutional neural network; Multi-scale features; Remote sensing imagery; Road extraction","Classification (of information); Convolution; Decoding; Deep learning; Extraction; Feature extraction; Image processing; Neural networks; Roads and streets; Semantics; Convolutional neural network; High resolution remote sensing images; Multi-scale features; Qualitative evaluations; Regional classifications; Remote sensing imagery; Road extraction; State-of-the-art approach; Remote sensing"
"Deep fusion feature based object detection method for high resolution optical remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063737043&doi=10.3390%2fapp9061130&partnerID=40&md5=91cd4c7339037431d1f4e4656762c1bc","With the rapid growth of high-resolution remote sensing image-based applications, one of the fundamental problems in managing the increasing number of remote sensing images is automatic object detection. In this paper, we present a fusion feature-based deep learning approach to detect objects in high-resolution remote sensing images. It employs fine-tuning from ImageNet as a pre-training model to address the challenge of it lacking a large amount of training datasets in remote sensing. Besides, we improve the binarized normed gradients algorithm by multiple weak feature scoring models for candidate window selection and design a deep fusion feature extraction method with the context feature and object feature. Experiments are performed on different sizes of high-resolution optical remote sensing images. The results show that our model is better than regular models, and the average detection accuracy is 8.86% higher than objNet. © 2019 by the authors.","Deep learning; High-resolution; Object detection; Optical remote sensing; Transfer learning",
"A new privacy attack network for remote sensing images classification with small training samples","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066139585&doi=10.3934%2fmbe.2019222&partnerID=40&md5=52eee6d985fe39e44ce5e9ee654974d9","Solving overfitting problems of privacy attacks on small-sample remote sensing data is still a big challenge in practical application. We propose a new privacy attack network, called joint residual network (JRN), for deep learning based privacy objects classification of small-sample remote sensing images in this paper. Unlike the original residual network structure, which add the bottom feature map to top feature map, JRN fuses the bottom feature map with top feature map by matrix joint. It can reduce the possibility that convolution layers extract the noise of training set or consider the inherent attributes of training set as the whole sample attributes. A series benchmark experiments based on GoogleNet model have been enforced and finally, we compare the model process output and the classification accuracy on small-sample data sets. On the UCMLU data set, the GoogleNet-Feat model which is integrated with JRN is 1.66% higher of accuracy than the original GoogleNet model and 1.87% higher than the GoogleNet-R model; on the WHU-RS dataset, GoogleNet-Feat model is 1.04% higher than the GoogleNet model, and is 3.12% higher than the GoogleNet-R model. Compared with the contrast experiments, the classification accuracy of GoogleNet-Feat is the highest when facing the overfitting problems resulting from the small samples. © 2019 American Institute of Mathematical Sciences. All rights reserved.","Convolutional neural network; Deep learning; Overfitting; Residual network","Classification (of information); Computer crime; Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Image classification; Benchmark experiments; Classification accuracy; Contrast experiment; Over fitting problem; Overfitting; Remote sensing data; Remote sensing images; Remote sensing images classification; Remote sensing; article; controlled study; convolutional neural network; deep learning; noise; privacy; remote sensing"
"Research on Optimization Methods of ELM Classification Algorithm for Hyperspectral Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071141436&doi=10.1109%2fACCESS.2019.2932909&partnerID=40&md5=36de5b2c885f3d5c20a7b6753c279aee","In land-use classification of hyperspectral remote sensing (RS) images, traditional classification methods often experience large amount of datasets and low efficiency. To solve these problems, a fast machine-learning method, the extreme learning machine (ELM) algorithm, was introduced. However, basic use of the ELM usually encounters problems of unstable classification results and low classification accuracy. Hence, in this paper, optimization methods for ELM-based RS image classification were mainly discussed and applied to solve the bottleneck problems. From the three perspectives of ensemble learning, making full use of image texture features, and deep learning, three classification optimization methods have been designed and implemented. The results show that: 1) To some extent, all the three methods can achieve a balance between classification accuracy and efficiency, i.e., they can maintain the advantage of ELM algorithm in classification efficiency and speed while have better classification accuracy; 2) The image texture feature optimization method (LBP-KELM) solves the problem of unsatisfactory classification results experienced by the ensemble learning optimization method (Ensemble-ELM) and further improves classification accuracy. However, the classification results are sensitive to the type of dataset; and 3) Fortunately, the optimization method combined with deep learning (CNN-ELM) can meet the application needs of multiple datasets. Furthermore, it can also further improve classification accuracy. © 2013 IEEE.","deep learning; ELM algorithm; ensemble learning; Hyperspectral remote sensing; texture features","Classification (of information); Deep learning; Efficiency; Image enhancement; Image texture; Land use; Large dataset; Learning algorithms; Machine learning; Remote sensing; Textures; Classification accuracy and efficiency; Classification efficiency; ELM algorithms; Ensemble learning; Extreme learning machine; Hyperspectral remote sensing; Hyperspectral Remote Sensing Image; Texture features; Image classification"
"Remote Sensing Semantic Segregation for Water Information Extraction: Optimization of Samples via Training Error Performance","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061753311&doi=10.1109%2fACCESS.2019.2894099&partnerID=40&md5=f3c17ab75d4d9ee1ea2426ea3f9ddaa2","The corrupted information in training samples is an important factor affecting the accuracy and generalizability of the machine learning models. Due to the extremely high memory capacity of deep learning models, the interference of excessive corrupted information makes the model prone to bad generalization behavior. This paper proposes a method of estimating training sample quality using the value calculated by the loss function in the process of gradient descent optimization. The method includes a model accuracy variation degree algorithm and a sample quality analysis algorithm. The model accuracy variation degree algorithm provides a basis for determining the intervention time of the sample quality analysis algorithm by calculating the intensity of the model accuracy variation change. The data error evaluation algorithm analyzes the distribution characteristics of the training error and estimates the error degree of the training samples to control the quality of the input samples. This study includes a water segmentation experiment performed on GF1 remote sensing images, which demonstrates that the optimization method can significantly improve the model accuracy and training stability. © 2019 IEEE.","deep learning (DL); error analysis; generalization accuracy; remote sensing; semantic segmentation; Training error; water information extraction","Artificial intelligence; Data mining; Deep learning; Error analysis; Image enhancement; Image segmentation; Information retrieval; Optimization; Quality control; Remote sensing; Sampling; Semantics; Distribution characteristics; Generalization accuracy; Gradient descent optimization; Machine learning models; Remote sensing images; Semantic segmentation; Training errors; Water informations; Extraction"
"Hyperspectral remote sensing image classification based on auto-encoder","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076338870&doi=10.3788%2fLOP56.192801&partnerID=40&md5=d78f42426009437205279e5d11656bd2","Hypcrspcctral remote sensing image data have characteristics of high dimension, spatial correlation, and feature nonlincarity, based on which a spatial-spectral feature extraction classification method based on deep learning is proposed herein. First, the weight decay is added to a stacked sparse auto-encoder. Next, the principal component analysis method is used to reduce the dimensionality of the image data. Then, neighborhood information is sorted, deleted, reorganized, and stacked according to the difference between the first principal component of all pixels in the principal component image block and the central pixel. Finally, the obtained spatial-spectral information is input into a stacked sparse auto-encoder combined with the SoftMax classifier for classification. The comparison of two sets of experimental data reveals that the proposed classification algorithm improves the classification accuracy of hypcrspcctral images. © 2019 Universitat zu Koln. All rights reserved.","Deep learning; Hypcrspcctral remote sensing image; Remote sensing; Remote sensing image classification; Spatial-spectral feature",
"Numerical investigations on sea states estimation based on the convolution neural networks deep learning technique","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079776387&partnerID=40&md5=85d7e714d466221498c9e562fe879e4c","Maritime X-band radar has become a very popular tool in ocean waves remote sensing. Conventional approach is difficult to extract high-precision wave parameters. In this paper, a method for ocean wave remote sensing by using Convolution Neural Networks (CNN) is proposed. The CNN models are employed to retrieve ocean wave parameters from simulation radar images. Validations are carried out on the basis of the simulated wave radar data. The results of different CNN models are also compared. Numerical results suggest that considerable accuracy is obtained in predicting the wave parameters. The present work provides another feasible way for accurate waves remote sensing. © 2019 by the International Society of Offshore and Polar Engineers (ISOPE).","CNN; Deep learning; Ocean waves remote sensing; Radar images","Arctic engineering; Convolution; Ocean currents; Radar; Radar imaging; Remote sensing; Water waves; Conventional approach; Convolution neural network; High-precision; Learning techniques; Numerical investigations; Numerical results; Wave parameters; X-band Radars; Deep learning"
"Neural network technology to search for targets in remote sensing images of the Earth","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069650384&doi=10.18287%2f1613-0073-2019-2391-180-186&partnerID=40&md5=1c025f8774f7126128c455953c9b1bc3","The paper introduces how multi-class and single-class problems of searching and classifying target objects in remote sensing images of the Earth are solved. To improve the recognition efficiency, the preparation tools for training samples, optimal configuration and use of deep learning neural networks using high-performance computing technologies have been developed. Two types of CNN were used to process ERS images: a convolutional neural network from the nnForge library and a network of the Darknet type. A comparative analysis of the results is obtained. The research showed that the capabilities of convolutional neural networks allow solving simultaneously the problems of searching (localizing) and recognizing objects in ERS images with high accuracy and completeness. © 2019 CEUR-WS. All rights reserved.",,"Convolution; Deep learning; Image processing; Nanotechnology; Neural networks; Comparative analysis; Convolutional neural network; High-performance computing technology; Learning neural networks; Network technologies; Recognition efficiency; Remote sensing images; Training sample; Remote sensing"
"Remote sensing landslide recognition based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073011698&doi=10.1155%2f2019%2f8389368&partnerID=40&md5=cc46e03b6c306696aa8b10a6555ab56b","Landslides are a type of frequent and widespread natural disaster. It is of great significance to extract location information from the landslide in time. At present, most articles still select single band or RGB bands as the feature for landslide recognition. To improve the efficiency of landslide recognition, this study proposed a remote sensing recognition method based on the convolutional neural network of the mixed spectral characteristics. Firstly, this paper tried to add NDVI (normalized difference vegetation index) and NIRS (near-infrared spectroscopy) to enhance the features. Then, remote sensing images (predisaster and postdisaster images) with same spatial information but different time series information regarding landslide are taken directly from GF-1 satellite as input images. By combining the 4 bands (red + green + blue + near-infrared) of the prelandslide remote sensing images with the 4 bands of the postlandslide images and NDVI images, images with 9 bands were obtained, and the band values reflecting the changing characteristics of the landslide were determined. Finally, a deep learning convolutional neural network (CNN) was introduced to solve the problem. The proposed method was tested and verified with remote sensing data from the 2015 large-scale landslide event in Shanxi, China, and 2016 large-scale landslide event in Fujian, China. The results showed that the accuracy of the method was high. Compared with the traditional methods, the recognition efficiency was improved, proving the effectiveness and feasibility of the method. Copyright © 2019 Yu Wang et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.",,"Convolution; Deep learning; Disasters; Efficiency; Infrared devices; Landslides; Near infrared spectroscopy; Neural networks; Convolutional neural network; Large-scale landslides; NIRS (near-infrared spectroscopy); Normalized difference vegetation index; Recognition efficiency; Remote sensing images; Spectral characteristics; Time series informations; Remote sensing"
"Imbalanced remote sensing ship image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068847975&doi=10.23940%2fijpe.19.06.p22.17091715&partnerID=40&md5=cb9caf3ce001d691291587950bbcbc6a","Aiming at the unbalanced classification problem of remote sensing ship image datasets in ship target classification and the problem that the traditional decision tree classification algorithm needs to rely on artificial construction features to realize classification, a weighted deep neural decision forest is proposed. This method combines deep learning with resampling. The results show that the method can achieve a better classification accuracy than the traditional decision tree on unbalanced classification of ship target. © 2019 Totem Publisher, Inc. All rights reserved.","Decision tree; Deep learning; Imbalanced ship classification","Decision trees; Deep learning; Image classification; Remote sensing; Ships; Classification accuracy; Construction features; Decision forest; Decision tree classification; Image datasets; Resampling; Ship classification; Ship targets; Classification (of information)"
"3D Virtual Urban Scene Reconstruction from a Single Optical Remote Sensing Image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067200722&doi=10.1109%2fACCESS.2019.2915932&partnerID=40&md5=059499cea3441b4c3cc233d973ed3e26","This paper presents a low-cost and efficient method for 3D virtual urban scene reconstruction based on multi-source remote sensing big data and deep learning. By integrating maps, satellite optical images, and digital terrain model (DTM), the proposed method achieves a reasonable reconstructed 3D model for complex urban. The method consists of two independent convolutional neural networks (CNN) to process the land cover and the building height extraction. The proposed method is then tested on a 100 km2 scene in San Diego, USA, including about 30 000 buildings. The land cover classification achieves an overall accuracy (OA) of 80.4% for eight types of land as defined in NLCD 2011 datasets. Building height estimation achieves an average error at 1.9 meters on NYC open data, the building footprint. Furthermore, the scene reconstruction including the estimation of both land cover and building height can be finished in 10 min on a single NVidia Titan X GPU. © 2013 IEEE.","convolutional neural networks; Optical image; urban reconstruction","3D modeling; Buildings; Classification (of information); Convolution; Deep learning; Geometrical optics; Neural networks; Open Data; Remote sensing; Three dimensional computer graphics; Building footprint; Convolutional neural network; Digital terrain model; Land cover classification; Optical image; Optical remote sensing; Overall accuracies; Scene reconstruction; Image reconstruction"
"Exploring the Response Mechanism of Remote Sensing Images in Monitoring Fixed Assets Investment Project in Terms of Building Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077799479&doi=10.1109%2fACCESS.2019.2948060&partnerID=40&md5=22b4cda6e732b845a7e0f96dcc19151c","Fixed assets investment is a driving factor in facilitating urbanization and economic growth. The local governments have spent a lot of budgets on the construction of fixed assets. However, such investment lacks a scientific and objective mechanism to supervise the construction process continuously to guarantee on-time delivery. Owing to the development of remote sensing technology, the availability of high spatial resolution images makes it possible to visualize the construction process continuously. By synthesizing the amount of fixed assets investment, we can build a reasonable monitoring system to supervise the fixed assets projects mutually in both terms of visual construction and statistical money spent. However, as far as we know, there is not much work exploring the methods in monitoring fixed assets investment yet. We collected the continuous investment records of fourteen fixed assets projects from the year 2015 to 2017 and the corresponding GaoFen satellite images in nine-time nodes. Semantic segmentation deep learning technology is applied to detect buildings from the high spatial resolution images. The monitoring system is built by regression between the ratio of investment and the ratio of building area at nine-time nodes. Compared with the regression model from the ratio of investment and that of ground truth building area, our model achieves an RMSE of 0.0136 in the test samples. It indicates the strong potential applicability of remote sensing images in supervising the reasonability of the construction process of fixed assets and the investment allocation. © 2019 IEEE.","continuous monitoring; deep learning; Fixed assets investment project; high spatial resolution remote sensing; semantic segmentation; supervise construction process","Budget control; Construction; Deep learning; Image resolution; Image segmentation; Investments; Monitoring; Regression analysis; Semantics; Construction process; Continuous monitoring; High spatial resolution; Investment projects; Semantic segmentation; Remote sensing"
"U-Net: deep learning for cell counting, detection, and morphometry","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058842709&doi=10.1038%2fs41592-018-0261-2&partnerID=40&md5=f1b09895cc1a3f7e5c99598874a1f9f9","U-Net is a generic deep-learning solution for frequently occurring quantification tasks such as cell detection and shape measurements in biomedical image data. We present an ImageJ plugin that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service. The plugin comes with pretrained models for single-cell segmentation and allows for U-Net to be adapted to new tasks on the basis of a few annotated samples. © 2018, The Author(s), under exclusive licence to Springer Nature America, Inc.",,"Article; bright field microscopy; cell counting; cloud computing; deep learning; electron microscopy; learning; microglia; morphometry; phase contrast microscopy; priority journal; remote sensing; single cell analysis; artificial neural network; cell count; software design; Cell Count; Cloud Computing; Deep Learning; Neural Networks (Computer); Software Design"
"Long time series land cover classification in China from 1982 to 2015 based on Bi-LSTM deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071528697&doi=10.3390%2frs11141639&partnerID=40&md5=f218c9f3dba4d8d42082aafbee8d1f67","Land cover classification data have a very important practical application value, and long time series land cover classification datasets are of great significance studying environmental changes, urban changes, land resource surveys, hydrology and ecology. At present, the starting point of continuous land cover classification products for many years is mostly after the year 2000, and there is a lack of long-term continuously annual land cover classification products before 2000. In this study, a long time series classification data extraction model is established using a bidirectional long-term and short-term memory network (Bi-LSTM). In the model, quantitative remote sensing products combined with DEM, nighttime lighting data, and latitude and longitude elevation data were used. We applied this model in China and obtained China's 1982-2017 0.05° land cover classification product. The accuracy assessment results of the test data show that the overall accuracy is 84.2% and that the accuracies of wetland, water, glacier, tundra, city and bare soil reach 92.1%, 92.0%, 94.3%, 94.6% and 92.4%, respectively. For the first time, this study used a variety of long time series data, especially quantitative remote sensing products, for the classification of features. At the same time, it also acquired long time series land cover classification products, including those from the year 2000. This study provides new ideas for the establishment of higher-resolution long time series land cover classification products. © 2019 by the authors.","Bi-LSTM; Land cover classification; Quantitative remote sensing; Time series","Deep learning; Long short-term memory; Remote sensing; Time series; Accuracy assessment; Environmental change; Higher resolution; Land cover classification; Long time series; Overall accuracies; Quantitative remote sensing; Short term memory; Classification (of information)"
"Deep learning on hyperspectral data to obtain water properties and bottom depths","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072697728&doi=10.1117%2f12.2519881&partnerID=40&md5=0a4cea058d7d2d5467fbbd9782a8d46c","Developing accurate methods to determine bathymetry, bottom type, and water column optical properties from hyperspectral imagery is an ongoing scientific problem. Recent advances in deep learning have made convolutional neural networks (CNNs) a popular method for classification and regression on complex datasets. In this paper, we explore the use of CNNs to extract water depth, bottom type, and inherent optical properties (IOPs) from hyperspectral imagery (HSI) of water. We compare the CNN results to other machine learning algorithms: k-nearest-neighbors (KNN), stochastic gradient descent (SGD), random forests (RF), and extremely randomized trees (ET). This work is an inverse problem in which we seek to find the water properties than impact the reflectance and hence the collected HSI. The data includes optically shallow water, in which the bottom can be seen, and optically deep, in which the bottom cannot be seen and does not affect the reflectance. The scalar optical properties we find through regression are chlorophyll (CHL), colored dissolved organic matter (CDOM), total suspended sediments (TSS). For the case of the optically shallow water, we classify the bottom type among 114 different substrates. The results demonstrate that for finding water depth, bottom type, and IOPs in the case of optically shallow water, the CNN has better performance than other machine learning methods. For regression of the IOPs in optically deep water, the extremely randomized trees method outperforms the CNN. We further investigate the mechanisms of these results and discuss hyperparameter tuning strategies that may improve deep learning accuracy. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Convolutional neural network; Deep learning; Hyperspectral imagery; Inherent optical properties; Machine learning; Water bottom depths","Classification (of information); Convolution; Decision trees; Deep learning; Deep neural networks; Forestry; Gradient methods; Inverse problems; Learning algorithms; Learning systems; Nearest neighbor search; Neural networks; Optical properties; Reflection; Regression analysis; Remote sensing; Signal processing; Spectroscopy; Stochastic systems; Suspended sediments; Colored dissolved organic matter; Convolutional neural network; Hyper-spectral imageries; Inherent optical properties; Inherent optical properties (IOPs); Stochastic gradient descent; Total suspended sediments; Water bottom; Machine learning"
"Knowledge models and image processing analysis in remote sensing: Examples of Yakutsk (Russia) and Kaunas (Lithuania)","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067473050&doi=10.5220%2f0007752202820288&partnerID=40&md5=9d1af32e0ad9aeec556c095695e4cfc6","The use of geographic knowledge in remote sensing constitutes one of the fundamental base of the methodologies of image processing. Image processing, image analysis, and oriented-object recognition are based on the geographic knowledge. More specifically, the large panel of supervised classifications methods are one of the main example where geographic knowledge is necessary for both algorithms training and results validation. Recently, with the coming back of the artificial intelligence (AI) wave, it appears that a large spectrum of usually employed methodologies in remote sensing and image processing, are one of the main drivers of AI: machine learning, deep learning are the most effective's examples. As well as many based processing algorithms like the Support Vector Machine (SVM) or the Random Forest (RF). However, despite the constant performances of the methods of calculus; the geographic knowledge's determines the accuracy of recognition and classification in image processing and spatial modelling generated. In regard of the fast seasonal and annual landscape changes in the Arctic climate, and complex urban structures, Yakutsk and Kaunas cities contribute to the reflexion. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Artificial Intelligence; Geographic Knowledge; Geographic Ontologies; Image Processing; Remote Sensing; Simulation; Spatial Modelling; Spectral Databases; Temporal Analysis","Artificial intelligence; Calculations; Decision trees; Deep learning; Geographic information systems; Image analysis; Image processing; Information management; Information systems; Information use; Object recognition; Supervised learning; Support vector machines; System theory; Geographic Knowledge; Geographic ontologies; Simulation; Spatial modelling; Spectral database; Temporal analysis; Remote sensing"
"Change detection for high-resolution satellite images using transfer learning and deep learning network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070914050&doi=10.7848%2fksgpc.2019.37.3.199&partnerID=40&md5=a09fde8f25a541c937b0d400175dbefd","As the number of available satellites increases and technology advances, image information outputs are becoming increasingly diverse and a large amount of data is accumulating. In this study, we propose a change detection method for high-resolution satellite images that uses transfer learning and a deep learning network to overcome the limit caused by insufficient training data via the use of pre-trained information. The deep learning network used in this study comprises convolutional layers to extract the spatial and spectral information and convolutional long-short term memory layers to analyze the time series information. To use the learned information, the two initial convolutional layers of the change detection network are designed to use learned values from 40,000 patches of the ISPRS (International Society for Photogrammertry and Remote Sensing) dataset as initial values. In addition, 2D (2-Dimensional) and 3D (3-dimensional) kernels were used to find the optimized structure for the high-resolution satellite images. The experimental results for the KOMPSAT-3A (KOrean Multi-Purpose SATllite-3A) satellite images show that this change detection method can effectively extract changed/unchanged pixels but is less sensitive to changes due to shadow and relief displacements. In addition, the change detection accuracy of two sites was improved by using 3D kernels. This is because a 3D kernel can consider not only the spatial information but also the spectral information. This study indicates that we can effectively detect changes in high-resolution satellite images using the constructed image information and deep learning network. In future work, a pre-trained change detection network will be applied to newly obtained images to extend the scope of the application. © 2019 Korean Society of Surveying. All rights reserved.","Change detection; Convolutional long short term memory layer; Deep learning; Fully convolutional layer; High-resolution satellite images; Transfer learning","artificial neural network; detection method; learning; satellite data; satellite imagery; spatial resolution; three-dimensional modeling"
"Cloud detection of ZY-3 satellite remote sensing images based on improved fully convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063664045&doi=10.3788%2fLOP56.052801&partnerID=40&md5=bbd367a30ad732462777b355898c0375","A method for cloud detection of ZY-3 satellite remote sensing images is proposed based on improved deep learning fully convolutional neural network. In prc-traincd deep convolutional neural network, full convolution layer is used instead of full connection layer, and dcconvolution method is used to up-sample feature map to optimize and improve network structure, then the Adam gradient descent method is adopted to accelerate convergence. The network is trained by using the resource image database of ZY-3 satellite, and the up-sampled image features are input into the Sigmoid classifier . Experimental results show that the proposed method performs better than the traditional methods in terms of detection accuracy and speed. The accuracy achieves 90. 11%, and detection time can be reduced to 0.46 s. © Universitat zu Koln. All rights reserved.","Cloud detection; Deep learning; Fully convolutional networks; Remote sensing; ZY-3 images",
"Dense semantic labeling with atrous spatial pyramid pooling and decoder for high-resolution remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059961038&doi=10.3390%2frs11010020&partnerID=40&md5=0ca7bb1ad704c086fa7584d4cd8d9bb7","Dense semantic labeling is significant in high-resolution remote sensing imagery research and it has been widely used in land-use analysis and environment protection. With the recent success of fully convolutional networks (FCN), various types of network architectures have largely improved performance. Among them, atrous spatial pyramid pooling (ASPP) and encoder-decoder are two successful ones. The former structure is able to extract multi-scale contextual information and multiple effective field-of-view, while the latter structure can recover the spatial information to obtain sharper object boundaries. In this study, we propose a more efficient fully convolutional network by combining the advantages from both structures. Our model utilizes the deep residual network (ResNet) followed by ASPP as the encoder and combines two scales of high-level features with corresponding low-level features as the decoder at the upsampling stage. We further develop a multi-scale loss function to enhance the learning procedure. In the postprocessing, a novel superpixel-based dense conditional random field is employed to refine the predictions. We evaluate the proposed method on the Potsdam and Vaihingen datasets and the experimental results demonstrate that our method performs better than other machine learning or deep learning methods. Compared with the state-of-the-art DeepLab_v3+ our model gains 0.4% and 0.6% improvements in overall accuracy on these two datasets respectively. © 2018 by the authors.","Atrous spatial pyramid pooling; Dense semantic labeling; Encoder-decoder; Fully convolutional networks; Remote sensing imagery; Superpixel-based DenseCRF","Convolution; Decoding; Deep learning; Image segmentation; Land use; Network architecture; Pixels; Semantics; Signal encoding; Superpixels; Convolutional networks; Encoder-decoder; Remote sensing imagery; Semantic labeling; Spatial pyramids; Remote sensing"
"Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV)","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073625556&doi=10.1371%2fjournal.pone.0223906&partnerID=40&md5=65a6293b4ad9bc22b9bf4e021d145600","The production of banana—one of the highly consumed fruits—is highly affected due to loss of certain number of banana plants in an early phase of vegetation. This affects the ability of farmers to forecast and estimate the production of banana. In this paper, we propose a deep learning (DL) based method to precisely detect and count banana plants on a farm exclusive of other plants, using high resolution RGB aerial images collected from Unmanned Aerial Vehicle (UAV). An attempt to detect the plants on the normal RGB images resulted less than 78.8% recall for our sample images of a commercial banana farm in Thailand. To improve this result, we use three image processing methods—Linear Contrast Stretch, Synthetic Color Transform and Triangular Greenness Index—to enhance the vegetative properties of orthomosaic, generating multiple variants of orthomosaic. Then we separately train a parameter-optimized Convolutional Neural Network (CNN) on manually interpreted banana plant samples seen on each image variants, to produce multiple results of detection on our region of interest. 96.4%, 85.1% and 75.8% of plants were correctly detected on three of our dataset collected from multiple altitude of 40, 50 and 60 meters, of same farm. Further discussion on results obtained from combination of multiple altitude variants are also discussed later in the research, in an attempt to find better altitude combination for data collection from UAV for the detection of banana plants. The results showed that merging the detection results of 40 and 50 meter dataset could detect the plants missed by each other, increasing recall upto 99%. © 2019 Neupane et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"altitude; article; banana; convolutional neural network; deep learning; human; image processing; nonhuman; recall; Thailand; agricultural land; agriculture; growth, development and aging; Musa; procedures; remote sensing; telecommunication; Agriculture; Deep Learning; Farms; Image Processing, Computer-Assisted; Musa; Remote Sensing Technology; Satellite Communications; Thailand"
"Application of deep learning with stratified K-fold for vegetation species discrimation in a protected mountainous region using Sentinel-2 image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077021174&doi=10.1080%2f10106049.2019.1704070&partnerID=40&md5=c6ae4f07776f99f9b857744fca9d415f","Understanding the spatial distribution of vegetation species is essential to gain knowledge on the recovery process of an ecosystem. Few studies have used deep learning and machine learning models for image processing focusing on forest/crop classification. This study, therefore, makes use of a multi-layer perceptron (MLP) deep neural network to discriminate grass species in a mountainous region using Sentinel-2 images. Vegetation indices, Sentinel-1 and ASTER DEM were combined with Sentinel-2 images to improve classification accuracy. Stratified K-fold was used to ensure balanced training and test data. The results, when compared with other commonly used machine learning models, outperformed them all. It produced a better discriminate of the grass species when ASTER DEM was combined with Sentinel-2 images, with overall F1 score of 92%. The results of the species discrimination show a general increase in increaser II species such as Eragrostis curvula and a decrease in decreaser species like Phragmites australis. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","deep learning; grass species; Machine learning; remote sensing; sentinel imagery",
"SAR tomography based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082548411&doi=10.1109%2fIGARSS.2019.8900616&partnerID=40&md5=c6f815a2b0d32a96dabf5a0b9032cb60","In this paper, the potential of a deep learning approach for SAR tomography (TomoSAR) is investigated. TomoSAR is a powerful technique that allows the 3D reconstruction of objects lying on the Earth surface, by separating multiple scatterers with different elevations laying in the same range-azimuth resolution cell. In urban applications, the number of interfering scatterers is typically very small, so that the reconstruction of the elevation reflectivity profile can be faced as a statistical detection problem. Detection performance depends on how well the adopted statistical model fits to the observed scene. For complex urban scenarios this issue can greatly impair achievable accuracy of results. Then, we propose to exploit the neural networks' capabilities to learn the data generative model, in order to face the problem of signal model inaccuracies. In particular, in the assumption of a single scatterer, a neural network can be trained to solve a simple classification problem. Results on simulated and real data are presented. © 2019 IEEE.","Deep Learbing; Detection; Neural Networks; SAR; Tomography","Deep neural networks; Error detection; Neural networks; Remote sensing; Tomography; 3D reconstruction; Deep Learbing; Detection performance; Reflectivity profiles; Sar tomography (SARTom); Statistical detection; Statistical modeling; Urban applications; Deep learning"
"Remote sensing image deblurring algorithm based on WGAN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064861662&doi=10.1007%2f978-3-030-17642-6_10&partnerID=40&md5=915c818670c878b02bc68d4f278e1ba2","Remote sensing images are blurred due to large and wide imaging, long shooting distance, fast scanning speed, interference from external light, etc. At the same time, because of that remote sensing images have the characteristics of diverse and dense shooting objects, deblurring remote sensing images is a major problem in remote sensing research. Therefore, we propose a remote sensing image deblurring algorithm, which based on WGAN. The algorithm is different from the traditional method in estimating the blur kernel of image. What’s more our method does not require an explicit estimation of the blur kernel, and it implements an end-to-end image deblurring process. We use a WGAN-based deblurring model. First, the training images are processed in pairs. Then, in order to increase the generalization ability, a image of 256 * 256 that is a sub-region cropped at the random position in the original image is chosen as the input image. Finally, to achieve a better deblurring effect, a content loss function and a perceptual loss function are added to the loss function to achieve the specific implementation. The remote sensing image deblurring model trained by the proposed method has achieved better results on the remote sensing image dataset. The experimental results show that the proposed algorithm have better performance than the traditional method in filtering out the blur of remote sensing images, which could optimize the overall visual effect subjectively and improve the peak signal-to-noise ratio of the image objectively. © Springer Nature Switzerland AG 2019.","Deblurring; Deep learning; Generative Adversarial Networks; Motion blur; Remote sensing image","Deep learning; Distributed computer systems; Image segmentation; Quality of service; Remote sensing; Signal to noise ratio; Adversarial networks; Deblurring; Generalization ability; Image deblurring; Motion blur; Peak signal to noise ratio; Remote sensing images; Shooting distance; Image enhancement"
"Deep learning for dense labeling of hydrographic regions in very high resolution imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078138217&doi=10.1117%2f12.2533161&partnerID=40&md5=87257f6e7757df3b50cc4b40f797dbfd","Automatic dense labeling of multispectral satellite images facilitates faster map update process. Water objects are essential elements of a geographic map. While modern dense labeling methods perform robust segmentation of such objects like roads, buildings, and vegetation, dense labeling of hydrographic regions remains a challenging problem. Water objects change their surface albedo, color, and reflection in different weather and different seasons. Moreover, rivers and lakes can change their boundaries after floods or droughts. Robust documentation of such seasonal changes is an essential task in the field of analysis of satellite imagery. Due to the high variance in water object appearance, their segmentation is usually performed manually by a human operator. Recent advances in machine learning have made possible robust segmentation of static objects such as buildings and roads. To the best of our knowledge, there is little research in the modern literature regarding dense labeling of water regions. This paper is focused on the development of a deep-learning-based method for dense labeling of hydrographic in aerial and satellite imagery. We use the GeoGAN framework and MobileNetV2 as the starting point for our research. The GeoGAN framework uses an aerial image as an input to generate pixel-level annotations of five object classes: building, low vegetation, high vegetation, road, and car. The GeoGAN framework leverages two deep learning approaches to ensure robust labeling: a generator with skip connections and Generative Adversarial Networks. A generator with skip connections performs image→label translation using feed-forward connections between convolutional and deconvolutional layers of the same depth. A GAN framework consists of two competing networks: a generator and a discriminator. The adversarial loss improves the quality of the resulting dense labeling. We made the following contributions to the GeoGAN framework: (1) new MobileNetV2-based generator, (2) adversarial loss function. We term the resulting framework as HydroGAN. We evaluate our HydroGAN model using a new HydroViews dataset focused on dense labeling of areas that are subject to severe flooding during the spring season. The evaluation results are encouraging and demonstrate that our HydroGAN model competes with the state-of-the-art models for dense labeling of aerial and satellite imagery. The evaluation demonstrates that our model can generalize from the training data to previously unseen samples. The developed HydroGAN model is capable of performing dense labeling of water objects in different seasons. We made our model publicly available. © 2019 SPIE.","Convolutional Neural Networks; Generative Adversarial Networks; Multispectral satellite images; Remote sensing; Semantic segmentation","Antennas; Convolution; Floods; Image segmentation; Neural networks; Remote sensing; Satellite imagery; Semantics; Vegetation; Adversarial networks; Convolutional neural network; Evaluation results; Learning-based methods; Multispectral satellite image; Robust segmentation; Semantic segmentation; Very high resolution; Deep learning"
"Attention-based deep feature fusion for the scene classification of high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071988282&doi=10.3390%2frs11171996&partnerID=40&md5=17f2ec359573dc7b3cdf9ce4d69654fb","Scene classification of high-resolution remote sensing images (HRRSI) is one of the most important means of land-cover classification. Deep learning techniques, especially the convolutional neural network (CNN) have been widely applied to the scene classification of HRRSI due to the advancement of graphic processing units (GPU). However, they tend to extract features from the whole images rather than discriminative regions. The visual attention mechanism can force the CNN to focus on discriminative regions, but it may suffer from the influence of intra-class diversity and repeated texture. Motivated by these problems, we propose an attention-based deep feature fusion (ADFF) framework that constitutes three parts, namely attention maps generated by Gradient-weighted Class Activation Mapping (Grad-CAM), a multiplicative fusion of deep features and the center-based cross-entropy loss function. First of all, we propose to make attention maps generated by Grad-CAM as an explicit input in order to force the network to concentrate on discriminative regions. Then, deep features derived from original images and attention maps are proposed to be fused by multiplicative fusion in order to consider both improved abilities to distinguish scenes of repeated texture and the salient regions. Finally, the center-based cross-entropy loss function that utilizes both the cross-entropy loss and center loss function is proposed to backpropagate fused features so as to reduce the effect of intra-class diversity on feature representations. The proposed ADFF architecture is tested on three benchmark datasets to show its performance in scene classification. The experiments confirm that the proposed method outperforms most competitive scene classification methods with an average overall accuracy of 94% under different training ratios. © 2019 by the authors.","Attention maps; Center loss; Multiplicative fusion of deep feature; Remote sensing; Scene classification","Behavioral research; Benchmarking; Cams; Deep learning; Entropy; Graphics processing unit; Image classification; Image enhancement; Image fusion; Neural networks; Remote sensing; Textures; Convolutional neural network; Feature representation; Graphic processing unit(GPU); High resolution remote sensing images; Land cover classification; Multiplicative fusion of deep feature; Scene classification; Visual attention mechanisms; Classification (of information)"
"Multi-Component Fusion Network for Small Object Detection in Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078354914&doi=10.1109%2fACCESS.2019.2939488&partnerID=40&md5=cb97c3bbcaf861cc5ffd37dc100864e2","Small object detection is a major challenge in the field of object detection. With the development of deep learning, many methods based on deep convolutional neural networks (DCNNs) have greatly improved the speed of detection while ensuring accuracy. However, due to the contradiction between the spatial details and semantic information of DCNNs, previous deep learning methods often meet problems when detecting small objects. The challenge can be more serious in complex scenes involving similar background objects and/or occlusion, such as in remote sensing imagery. In this paper, we propose an end-to-end DCNN called the multi-component fusion network (MCFN) to improve the accuracy of small object detection in such cases. First, we propose a dual pyramid fusion network, which densely concatenates spatial information and semantic information to extract small object features via encoding and decoding operations. Then we use a relative region proposal network to adequately extract the features of small objects samples and parts of objects. Finally, to achieve robustness against background disturbance, we add contextual information to the proposal regions before final detection. Experimental evaluations demonstrate that the proposed method significantly improves the accuracy of object detection in remote sensing images compared with other state-of-the-art methods, especially in complex scenes with the conditions of occlusion. © 2013 IEEE.","complex scene; dual pyramid fusion; multi-component; occlusion; remote sensing; Small object","Complex networks; Deep neural networks; Image enhancement; Neural networks; Object recognition; Remote sensing; Semantics; Complex scenes; Convolutional neural network; Experimental evaluation; Multicomponents; occlusion; Remote sensing imagery; Small objects; State-of-the-art methods; Object detection"
"Deep Learning Prediction of Polycyclic Aromatic Hydrocarbons in the High Arctic","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074897635&doi=10.1021%2facs.est.9b05000&partnerID=40&md5=d8ace9dba2c73b34c1f0482eae14443b","Given the lack of understanding of the complex physiochemical and environmental processes of persistent organic pollutants (POPs) in the Arctic and around the globe, atmospheric models often yield large errors in the predicted atmospheric concentrations of POPs. Here, we developed a recurrent neural network (RNN) method based on nonparametric deep learning algorithms. The RNN model was implemented to predict monthly air concentrations of polycyclic aromatic hydrocarbons (PAHs) at the high Arctic monitoring station Alert. To train the RNN system, we used MODIS satellite remotely sensed forest fire data, air emissions, meteorological data, sea ice cover area, and sampled PAH concentration data from 1996 to 2012. The system was applied to forecast monthly PAH concentrations from 2012 to 2014 at the Alert station. The results were compared with monitored PAHs and an atmospheric transport model (CanMETOP) for POPs. We show that the RNN significantly improved PHE and BaP predictions from 2012 to 2014 by 62.5 and 91.1%, respectively, compared to CanMETOP predictions. The sensitivity analysis using the Shapley value reveals that air emissions determined the magnitude of PAH levels in the high Arctic, whereas forest fires played a significant role in the changes in PAH concentrations in the high Arctic, followed by air temperature and meridional wind fields. © 2019 American Chemical Society.",,"Air pollution; Barium compounds; Deep learning; Deforestation; Fire hazards; Fires; Forecasting; Learning algorithms; Meteorology; Mineral oils; Organic pollutants; Recurrent neural networks; Sea ice; Sensitivity analysis; Atmospheric concentration; Atmospheric transport model; Environmental process; Meteorological data; Monitoring stations; Persistent organic pollutants; Polycyclic aromatic hydrocarbons (PAHS); Recurrent neural network (RNN); Polycyclic aromatic hydrocarbons; polycyclic aromatic hydrocarbon; air temperature; atmospheric transport; concentration (composition); error analysis; forest fire; ice cover; MODIS; PAH; persistent organic pollutant; physicochemical property; sea ice; atmospheric modeling; remote sensing; sensitivity analysis; air pollution; air temperature; algorithm; Arctic; Article; atmospheric transport; concentration (parameter); deep learning; forest fire; meteorological phenomena; model; persistent organic pollutant; prediction; recurrent neural network; sea ice; wind; air pollutant; Arctic; environmental monitoring; biomass; meteorology; prediction; wind speed; Arctic; Alert; Canada; Ellesmere Island; Nunavut; Queen Elizabeth Islands; Air Pollutants; Arctic Regions; Deep Learning; Environmental Monitoring; Polycyclic Aromatic Hydrocarbons"
"A newest data set analysis for remote sensing applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070217988&partnerID=40&md5=e3875d625ece3b28f134775213692d8b","Object detection and classification plays an important role for wide range of remote sensing application. The recent enhancement on application of object detection using deep learning methodology produces accurate results. Datasets is one of the foundations for deep learning. Hence it has been a fundamental but a challenging problem to obtain the datasets to carry out the research. So we are in need of public dataset which can be used for different kind of application. This paper summarize regardingdifferent open data set offered, additionally the performance analysis of the dataset that helps to understand for which application it is well suited. © 2019, Institute of Advanced Scientific Research, Inc.. All rights reserved.","Analysis; Classification; Dataset; Deep Learning; Object Detection; Performance; Public; Remotely Sensed Images",
"A picture tells a thousand…exposures: Opportunities and challenges of deep learning image analyses in exposure science and environmental epidemiology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057047743&doi=10.1016%2fj.envint.2018.11.042&partnerID=40&md5=7e030a88ef98e405e0398c7abeb626f5","Background: Artificial intelligence (AI) is revolutionizing our world, with applications ranging from medicine to engineering. Objectives: Here we discuss the promise, challenges, and probable data sources needed to apply AI in the fields of exposure science and environmental health. In particular, we focus on the use of deep convolutional neural networks to estimate environmental exposures using images and other complementary data sources such as cell phone mobility and social media information. Discussion: Characterizing the health impacts of multiple spatially-correlated exposures remains a challenge in environmental epidemiology. A shift toward integrated measures that simultaneously capture multiple aspects of the urban built environment could improve efficiency and provide important insights into how our collective environments influence population health. The widespread adoption of AI in exposure science is on the frontier. This will likely result in new ways of understanding environmental impacts on health and may allow for analyses to be efficiently scaled for broad coverage. Image-based convolutional neural networks may also offer a cost-effective means of estimating local environmental exposures in low and middle-income countries where monitoring and surveillance infrastructure is limited. However, suitable databases must first be assembled to train and evaluate these models and these novel approaches should be complemented with traditional exposure metrics. Conclusions: The promise of deep learning in environmental health is great and will complement existing measurements for data-rich settings and could enhance the resolution and accuracy of estimates in data poor scenarios. Interdisciplinary partnerships will be needed to fully realize this potential. © 2018 The Authors",,"Convolution; Cost effectiveness; Deep neural networks; mHealth; Mobile phones; Neural networks; Convolutional neural network; Cost-effective means; Deep convolutional neural networks; Environmental epidemiology; Environmental exposure; Environmental health; Low and middle income countries; Social media informations; Environmental impact; artificial intelligence; artificial neural network; developing world; environmental conditions; epidemiology; health impact; image analysis; learning; mobile phone; social media; air pollution; artificial intelligence; artificial neural network; audio recording; correlational study; cost effectiveness analysis; data base; deep learning; environmental exposure; environmental health; environmental impact; environmental monitoring; health impact assessment; human; image analysis; information model; low income country; machine learning; middle income country; population health; priority journal; remote sensing; satellite imagery; social media; urban area; environmental health; image processing; procedures; Deep Learning; Environmental Exposure; Environmental Health; Humans; Image Processing, Computer-Assisted"
"Unraveling low abundance intimate mixtures with deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072560086&doi=10.1117%2f12.2518487&partnerID=40&md5=7c6bb127a90a074639625a0fceb36f84","The high-confidence detection and identification of very low abundance, subpixel quantities of solid materials in nonlinear/intimate mixtures are still significant challenges for hyperspectral imagery (HSI) data analysis. We compare the ability of a traditional, shallow neural network (NN), deep learning with a convolutional neural network (DL/CNN), and a support vector machine (SVM) to analyze spectral signatures of nonlinear mixtures. Traditional mainstay algorithms (e.g., spectral unmixing, the matched filter) are also applied. Using a benchtop shortwave infrared (SWIR) hyperspectral imager, we acquired several microscenes of intimate mixtures of sand and neodymium oxide (Nd2O3). A microscene is a hyperspectral image measured in a laboratory. Several hundred thousand labeled spectra are easily and rapidly generated in one HSI cube of a microscene. Individual Petri dishes of 0, 0.5, 1, 2, 3, 4, and 5 weight-percent (wt. %) Nd2O3 with a silicate sand comprise a suite of microscenes furnishing labeled spectra for analysis. The NN and the DL/CNN both have average validation accuracies of ≥ 98 % (for the low wt. % classes); the SVM yields similar performance. As wt. % Nd2O3 increases, accuracies decrease slightly-perhaps due to the dominance of the Nd2O3 signature in the mixtures, which causes an increasing difficulty in separation. For example, this could affect the 4 and 5 wt. % classes in which the Nd2O3 would be easily detected and identified with traditional, mainstay HSI algorithms. The fact that neural network methods can separate such low quantity classes (e.g., 0, 0.5, and 1 wt. %), though not unexpected, is encouraging and demonstrates the potential of NNs and DL/CNNs for such detailed HSI analysis. © 2019 SPIE.","CNN; Convolutional neural network; Deep learning; DL; HSI; Hyperspectral; Microscene; Neural network; Nonlinear mixture; Support vector machine; SVM","Convolution; Deep learning; Deep neural networks; Infrared radiation; Matched filters; Mixtures; Neural networks; Nonlinear analysis; Remote sensing; Silicates; Spectroscopy; Support vector machines; Convolutional neural network; Detection and identifications; HyperSpectral; Hyperspectral imagers; Hyperspectral imagery; Microscene; Neural network method; Nonlinear mixtures; Neodymium compounds"
"Image classification using deep learning technique","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075024532&partnerID=40&md5=d5f08208d722a83039a80aef07ed94b2","Image classification and object recognition are very essential in automation purposes. Various domains like agriculture, health, food-processing, defence, remote-sensing, virtual-reality, 3Dmodeling, manufacturing industries, robotics, augmented reality, etc are flourishing because of image classification. A misclassification can lead to bulk loss of information and money in large scale applications. Conventional classification methods involve pre-processing, feature extraction and using well trained classifiers to obtain the decision which is very time consuming. For real time decision making, Convolutional Neural networks, particularly Pretrained Convolutional Neural Network performs much effectively. Convolutional Neural Networks are the most preferred and emerging trend today in the field of machine learning, artificial intelligence and computer vision. Our proposed method is to study the concept of image classification by making use of the Pretrained Convolutional Neural Network namely AlexNet. © 2019, Editorial office of Journal of International Pharmaceutical Research. All rights reserved.","AlexNet; Deep Learning","agriculture; article; artificial intelligence; classifier; computer vision; convolutional neural network; decision making; deep learning; feature extraction; health food; manufacturing industry; money; recognition; remote sensing; robotics; virtual reality"
"Deep learning-based extraction of building contours for large-scale 3D urban reconstruction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078127439&doi=10.1117%2f12.2533149&partnerID=40&md5=27996f4310211be79c7c0a9065385c5e","Automated 3D reconstruction of urban models from satellite images remains a challenging research topic, with many interesting outcoming applications, such as telecommunications and urban simulation. To reconstruct 3D city from stereo pairs of satellite images, semi-automatic strategies are typically applied, which are based either on procedural modeling, or on the use of both image processing and machine learning methods to infer scene geometries together with semantics. In both cases, human interaction still plays a key role, in particular for the rooftop buildings extraction. In the last decade, the use of deep learning algorithms, notably convolutional neural networks (CNNs), has shown a remarkable success for automatic image interpretation. We propose an approach using CNN architecture to automatize the procedure of building contour extraction with the final purpose to automatize 3D urban reconstruction chain and improve the quality of the generated city models. The developed algorithm consists of three steps: 1) We apply a mask-based normalization technique to the input image. 2) CNN network is applied to obtain a raster map of buildings. 3) A polygonization algorithm is designed, which processes a raster map of building to output an ensemble of building contours. We have adopted a U-Net neural network for building segmentation task. We compare the use of several U-Net architectures with the purpose to retain the best suited model. To train models, we have built a dataset of high-resolution satellite images over 15 different cities, and the corresponding building masks. The experimental results show that the proposed approach succeeds in predicting building polygons in a short time, and exhibits good generalization properties to be applied on diverse Earth areas. The developed algorithm combined with the existing LuxCarta reconstruction chain improves 3D urban scene modeling results, and thus supplies an important step towards the automatic reconstruction of 3D city scenes. © 2019 SPIE.","Deep Learning; Satellite Images; Semantic Segmentation; U-Net","Buildings; Deep learning; Extraction; Image reconstruction; Image segmentation; Learning algorithms; Network architecture; Neural networks; Rasterization; Remote sensing; Satellites; Semantics; Stereo image processing; Automated 3D-reconstruction; Automatic reconstruction; Convolutional neural network; Generalization properties; High resolution satellite images; Machine learning methods; Satellite images; Semantic segmentation; Three dimensional computer graphics"
"Boundary Loss for Remote Sensing Imagery Semantic Segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068595476&doi=10.1007%2f978-3-030-22808-8_38&partnerID=40&md5=d0e23069294044e282bd5afa4dfd74ca","In response to the growing importance of geospatial data, its analysis including semantic segmentation becomes an increasingly popular task in computer vision today. Convolutional neural networks are powerful visual models that yield hierarchies of features and practitioners widely use them to process remote sensing data. When performing remote sensing image segmentation, multiple instances of one class with precisely defined boundaries are often the case, and it is crucial to extract those boundaries accurately. The accuracy of segments boundaries delineation influences the quality of the whole segmented areas explicitly. However, widely-used segmentation loss functions such as BCE, IoU loss or Dice loss do not penalize misalignment of boundaries sufficiently. In this paper, we propose a novel loss function, namely a differentiable surrogate of a metric accounting accuracy of boundary detection. We can use the loss function with any neural network for binary segmentation. We performed validation of our loss function with various modifications of UNet on a synthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL). Trained with the proposed loss function, models outperform baseline methods in terms of IoU score. © Springer Nature Switzerland AG 2019.","Aerial imagery; Building detection; CNN; Computer vision; Deep learning; Loss function; Semantic segmentation","Aerial photography; Computer vision; Deep learning; Neural networks; Remote sensing; Semantics; Aerial imagery; Building detection; Convolutional neural network; Loss functions; Remote sensing data; Remote sensing imagery; Remote sensing images; Semantic segmentation; Image segmentation"
"A public dataset for ship classification in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078101275&doi=10.1117%2f12.2532741&partnerID=40&md5=dba5c4e7aa4e5ed8aad53bd38d6d60f2","Ship classification in remote sensing images has been rarely studied because of relative scarcity of publicly available datasets. It is well known that datasets have played an important role in object classification research, especially for CNN-based algorithms which have been proved to perform well. In this paper, we introduce a public Dataset for Ship Classification in Remote sensing images (DSCR). We collect 1,951 remote sensing images from DOTA, HRSC2016, NWPU VHR-10 and Google Earth, containing warships and civilian ships of various scales. For object classification, we cut out ships of different categories from the collected images. The whole dataset contains about 20,675 instances which are divided into seven categories, i.e. aircraft carrier, destroyer, assault ship, combat ship, cruiser, other military ship and civilian ship. Each image contains ships of the same category, which is labeled by the category name. Since our dataset contains most models of major warships, it is relatively comprehensive for ship classification. To build a benchmark for ship classification, we evaluated six popular CNN-based object classification algorithms on our dataset, including ResNet, ResNext, VGG, GoogLeNet, DenseNet, and AlexNet. Experiments demonstrates that our dataset can be used for verifying ship classification algorithms and may advance the development of ship classification in remote sensing images. © 2019 SPIE.","Dataset; Deep learning; Remote sensing images; Ship classification","Aircraft carriers; Classification (of information); Deep learning; Image classification; Combat ships; Dataset; Google earths; Military ship; Object classification; Public dataset; Remote sensing images; Ship classification; Remote sensing"
"Fast and robust detection of oil palm trees using high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072572476&doi=10.1117%2f12.2518352&partnerID=40&md5=6c6ba39b48c7da2772eec4d77ef871eb","Oil palm tree detection is of great significance for improving the irrigation, estimating the yield of palm oil, and predicting the expansion trend, etc. Existing tree detection methods include traditional image processing, machine learning methods, and sliding window based deep learning methods. In this paper, we proposed a deep learning based end-To-end method for oil palm detection in large scale. First, we built an oil palm sample dataset from 0.1m-resolution Unmanned Aerial Vehicle (UAV) images. Second, we implemented five state-of-The-Art object detection algorithms (i.e. Faster-RCNN, VGG-SSD, YOLO-v3, RetinaNet and Mobilenet-SSD) and evaluated their performances for detecting the tree crown size and the location of oil palms. Moreover, we designed an overlapping partition method to improve the oil palm detection results of the UAV images in over 40,000 × 40,000 pixels. Experiment results demonstrate that in terms of the detection accuracy, VGG-SSD achieves the best accuracy of 90.91% on the validation dataset, followed by YOLO-v3, RetinaNet, Mobilenet-SSD and Faster RCNN. Meanwhile, we compared the detection time of the five object detection algorithms. Mobilenet-SSD achieves the highest detection speed among five algorithms (12.81ms per image in 500×500 pixels), with the speedup ratios of 17.5×, 10.2×, 4.51×, and 17.33× compared with Faster-RCNN, VGG-SSD, YOLO-v3 and RetinaNet. The results show that our proposed oil palm detection method is of great practical value to the precision agriculture of the oil palm industry. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep learning; High-resolution UAV images; Object detection; Oil palm; Remote sensing","Aircraft detection; Antennas; Automatic target recognition; Deep learning; Forestry; Image enhancement; Object detection; Object recognition; Palm oil; Pixels; Remote sensing; Signal detection; Unmanned aerial vehicles (UAV); Detection accuracy; Detection methods; High resolution; High resolution remote sensing images; Machine learning methods; Object detection algorithms; Oil palm; Sliding window-based; Palmprint recognition"
"Multi-label land cover indices classification of satellite images using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048627614&doi=10.1007%2f978-981-13-0869-7_11&partnerID=40&md5=2a2e45fb9e550d1a1a7aa7f181229e83","Accurate classification of land cover indices is important for diverse disciplines (e.g., ecology, geography, and climatology) because it serves as a basis for various real world applications. For detection and classification of land cover, remote sensing has long been used as an excellent source of data for finding different types of data attribute present in the land cover. A variety of feature extraction and classification methods in machine learning have been used to classify land cover using satellite images. In recent years, deep learning have recently emerged as a dominant paradigm for machine learning in a variety of domains. The objective of this paper presents the multi-labeled land cover indices classification using Google Earth Satellite images with deep convolutional neural network (DCNN). Since the lack of massive labeled land cover dataset, the own created labeled dataset for Ayeyarwaddy Delta is applied and tested with AlexNet. Then the results of land cover classification are compared with Multiclass-SVM using confusion matrices. According to the tested results, 76.6% of building index, 81.5% road index, 91.8% of vegetation index and 93.2% of water index can be correctly classified by using DCNN. The confusion matrix for Multiclass-SVM, 78.9% of building index, 72.7% road index, 94.2% of vegetation index and 98.1% of water index can be correctly classified. © 2019, Springer Nature Singapore Pte Ltd.","DCNN; Multi-labeled land cover indices; Multiclass-SVM","Big data; Data handling; Deep neural networks; Image classification; Image resolution; Matrix algebra; Neural networks; Remote sensing; Roads and streets; Satellites; Vegetation; Confusion matrices; DCNN; Deep convolutional neural networks; Feature extraction and classification; Land cover; Land cover classification; Multiclass SVM; Vegetation index; Classification (of information)"
"Ecosystem controlling using identification of animal based on deep learning and GLCM (Gray Level Co-occurrence Matrix) method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066029231&doi=10.13189%2fujeee.2019.061302&partnerID=40&md5=1b2cd5ac3a52d55dd7a421532c8944d3","Indonesia is the largest country in Southeast Asia. Plantations in Indonesia are still traditionally and use irrigation systems that are housed in reservoirs and ponds. While plantations in West Java generally averaged near springs and near nature preserve locations, which has a benefit impact of the available amount of water. The negative impact is entry on areas by animals and pests, which will damage Plantation grounds and other agricultural land. The animals generally go down at night when the time of the harvest season arrives. Currently, prevention of destructive animals is done by making a trap, but it is very dangerous for animals and farmers. This can lead to more severe death and destruction, especially if the dead are protected animals. One of the safe technologies, which can be used is by installing camera in the plantation area. This system is connected with the computer remotely and can operate automatically. This system is equipped with data on all types of animals that are protected. The data is processed with the Deep Learning Method and GLCM. The system will identify the animal in the category of pests or not by recognizing the pattern, texture and posture of the animal, for example if the system distinguished the wild boar. This system is considered effective and does not interfere the ecosystem and animal habitat. The addition of thermal camera and remote sensing are very effective to monitor the condition of the plant. This system is effective reduce and avoid animals killing and able to control stability the ecosystem. © 2019 by authors.","Deep learning; Ecosystems; GLCM",
"Remote sensing; Image detection based on dense connected networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076084850&doi=10.3788%2fLOP56.222803&partnerID=40&md5=733ba76685405a768138bd73d008cc1f","This study proposes a remote sensing image detection method based on deep learning to solve the issues of human intervention, slow speed, and low accuracy associated with the traditional remote sensing image detection algorithm. A dense connected network is considered to completely use the features extracted from each layer and reduce the network inference time. Further, an expanding block structure with a large perceptive field is adopted, and the low- A nd high-level feature informations of the network arc combined based on the expanding block structure and deconvolution network. Thus, the performance of multiscale object detection for remote sensing images is improved. The experimental results denote that the proposed method exhibits high accuracy and short detection time, especially during the detection of small objects. © 2019 Universitat zu Koln. All rights reserved.","Dense connected structure; Feature fusion; Image processing; Remote sensing image; Small object detection",
"Multimodal scene understanding: Algorithms, applications and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082074527&doi=10.1016%2fC2018-0-01791-0&partnerID=40&md5=f3d9f86eb921eb0cd8b922aa80d36b2c","Multimodal Scene Understanding: Algorithms, Applications and Deep Learning presents recent advances in multi-modal computing, with a focus on computer vision and photogrammetry. It provides the latest algorithms and applications that involve combining multiple sources of information and describes the role and approaches of multi-sensory data and multi-modal deep learning. The book is ideal for researchers from the fields of computer vision, remote sensing, robotics, and photogrammetry, thus helping foster interdisciplinary interaction and collaboration between these realms. Researchers collecting and analyzing multi-sensory data collections - for example, KITTI benchmark (stereo+laser) - from different platforms, such as autonomous vehicles, surveillance cameras, UAVs, planes and satellites will find this book to be very useful. © 2019 Elsevier Inc. All rights reserved.",,
"TQR-Net: Tighter Quadrangle-Based Convolutional Neural Network for Dense Building Instance Localization in Remote Sensing Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076860168&doi=10.1007%2f978-3-030-34113-8_24&partnerID=40&md5=2da605e8309e8884bca0922deb354f68","Building localization in remote sensing imagery (RSI) is widely applied in many geoscience and remote sensing areas. However, many existing methods cannot generate accurate building contours. In this paper, we propose an effective convolutional neural network (CNN) framework, Tighter Quadrangle Network (TQR-Net), to locate buildings with quadrangular contours in RSI. Here, TQR-Net can generate regular contours for each of building targets using a CNN branch which can predict tighter quadrangles in parallel. Then, we train and test TQR-Net on a large building dataset collected from Google Earth, and the experiment results demonstrate that the proposed method can generate high-quality building contours and significantly outperforms other CNN-based detectors. © 2019, Springer Nature Switzerland AG.","Building instance localization; Convolutional neural network; Deep learning; Remote sensing; Tighter quadrangle","Buildings; Convolution; Deep learning; Deep neural networks; Large dataset; Neural networks; Statistical tests; Convolutional neural network; Geosciences; Google earths; High quality; Large buildings; Remote sensing imagery; Tighter quadrangle; Remote sensing"
"Spear and Shield: Attack and Detection for CNN-Based High Spatial Resolution Remote Sensing Images Identification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073902766&doi=10.1109%2fACCESS.2019.2927376&partnerID=40&md5=2d7f4317754ca2906bfebc35023a4041","High spatial resolution remote sensing (HSRRS) images classification and identification is an important technology to acquire land surface information for land resource management, geographical situation monitoring, and global climate change. As the hottest deep learning method, convolutional neural network (CNN) has been successfully applied in HSRRS image classification and identification due to its powerful information extraction capability. However, adversarial perturbations caused by radiation transfer process or artificial or other unpredictable disturbances often deteriorate the stability of CNN. Under this background, we propose a robust architecture for adversarial attack and detection to classify and identify HSRRS images. First of all, two white-box attacks [i.e., large Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) and fast gradient sign method (FGSM)] are adopted respectively to generate adversarial images to confuse the model, and to assess the robustness of the HSRRS image classifier. Second, adversarial detection models based on support vector machine (SVM) with single or fused two level features are proposed to improve the detection accuracy. The features extracted from the testing CNN full connected layers contain adversarial perturbations and real information, from which SVM classifier and discriminate the real and the adversarial images. The adversarial attack model is evaluated in terms of overall accuracy (OA) and kappa coefficient (kc). The simulation results show that the OA decreases from 96.4% to 44.4% and 33.3% for L-BFGS and FGSM attacked classifier model, respectively. The adversarial detection is evaluated via OA, detection probability PD, false alarm probability PFA, and miss probability PM. The simulation results indicate that the fused model with two different level features based on SVM can obtain the best OA (94.5%), PD (0.933), PFA (0.040), and PM (0.067) among the detectors if the classifier is attacked by the FGSM. Meanwhile, when facing the L-BFGS attack, the fused model presents similar performance if the best single level features are utilized. © 2013 IEEE.","attack detection; Convolutional neural network; fast gradient sign method (FGSM); large Broyden-Fletcher-Goldfarb-Shanno (L-BFGS); white-box attack","Classification (of information); Climate change; Convolution; Deep learning; Feature extraction; Image resolution; Information management; Neural networks; Nonlinear programming; Probability; Remote sensing; Support vector machines; Attack detection; Broyden-Fletcher-Goldfarb-Shanno; Convolutional neural network; fast gradient sign method (FGSM); White box; Image classification"
"An improved perceptual hash algorithm based on U-net for the authentication of high-resolution remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070675245&doi=10.3390%2fapp9152972&partnerID=40&md5=daedb7885b1ec754aea5715b08d0dd80","Data security technology is of great significance for the effective use of high-resolution remote sensing (HRRS) images in GIS field. Integrity authentication technology is an important technology to ensure the security of HRRS images. Traditional authentication technologies perform binary level authentication of the data and cannot meet the authentication requirements for HRRS images, while perceptual hashing can achieve perceptual content-based authentication. Compared with traditional algorithms, the existing edge-feature-based perceptual hash algorithms have already achieved high tampering authentication accuracy for the authentication of HRRS images. However, because of the traditional feature extraction methods they adopt, they lack autonomous learning ability, and their robustness still exists and needs to be improved. In this paper, we propose an improved perceptual hash scheme based on deep learning (DL) for the authentication of HRRS images. The proposed method consists of a modified U-net model to extract robust feature and a principal component analysis (PCA)-based encoder to generate perceptual hash values for HRRS images. In the training stage, a training sample generation method combining artificial processing and Canny operator is proposed to generate robust edge features samples. Moreover, to improve the performance of the network, exponential linear unit (ELU) and batch normalization (BN) are applied to extract more robust and accurate edge feature. The experiments have shown that the proposed algorithm has almost 100% robustness to format conversion between TIFF and BMP, LSB watermark embedding and lossless compression. Compared with the existing algorithms, the robustness of the proposed algorithm to lossy compression has been improved, with an average increase of 10%. What is more, the algorithm has good sensitivity to detect local subtle tampering to meet the high-accuracy requirements of authentication for HRRS images. © 2019 by the authors.","Deep learning; High-resolution remote sensing image; Integrity authentication; Perceptual hash; U-net",
"A deep learning framework for road marking extraction, classification and completion from mobile laser scanning point clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057073429&doi=10.1016%2fj.isprsjprs.2018.10.007&partnerID=40&md5=a0f8bfd3a1b591f4d2020fac23ea5edf","Road markings play a critical role in road traffic safety and are one of the most important elements for guiding autonomous vehicles (AVs). High-Definition (HD) maps with accurate road marking information are very useful for many applications ranging from road maintenance, improving navigation, and prediction of upcoming road situations within AVs. This paper presents a deep learning-based framework for road marking extraction, classification and completion from three-dimensional (3D) mobile laser scanning (MLS) point clouds. Compared with existing road marking extraction methods, which are mostly based on intensity thresholds, our method is less sensitive to data quality. We added the step of road marking completion to further optimize the results. At the extraction stage, a modified U-net model was used to segment road marking pixels to overcome the intensity variation, low contrast and other issues. At the classification stage, a hierarchical classification method by integrating multi-scale clustering with Convolutional Neural Networks (CNN) was developed to classify different types of road markings with considerable differences. At the completion stage, a method based on a Generative Adversarial Network (GAN) was developed to complete small-size road markings first, then followed by completing broken lane lines and adding missing markings using a context-based method. In addition, we built a point cloud road marking dataset to train the deep network model and evaluate our method. The dataset contains urban road and highway MLS data and underground parking lot data acquired by our own assembled backpacked laser scanning system. Our experimental results obtained using the point clouds of different scenes demonstrated that our method is very promising for road marking extraction, classification and completion. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Classification; Completion; Deep learning; Extraction; Point cloud; Road marking","Classification (of information); Deep learning; Digital television; Extraction; Highway markings; Laser applications; Neural networks; Roads and streets; Scanning; Well completion; Convolutional Neural Networks (CNN); Hierarchical classification; Laser scanning point clouds; Laser scanning systems; Point cloud; Road marking; Threedimensional (3-d); Underground parking lots; Road and street markings; algorithm; artificial neural network; data set; experimental study; image classification; laser method; numerical method; numerical model; prediction; road traffic; safety"
"Multiscale features supported deeplabv3+ optimization scheme for accurate water semantic segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074730152&doi=10.1109%2fACCESS.2019.2949635&partnerID=40&md5=1c14da2b115b6c1f2beef2239f074fbb","In the task of using deep learning semantic segmentation model to extract water from high-resolution remote sensing images, multiscale feature sensing and extraction have become critical factors that affect the accuracy of image classification tasks. A single-scale training mode will cause one-sided extraction results, which can lead to 'reverse' errors and imprecise detail expression. Therefore, fusing multiscale features for pixel-level classification is the key to achieving accurate image segmentation. Based on this concept, this paper proposes a deep learning scheme to achieve fine extraction of image water bodies. The process includes multiscale feature perception splitting of images, a restructured deep learning network model, multiscale joint prediction, and postprocessing optimization performed by a fully connected conditional random field (CRF). According to the scale space concept of remote sensing, we apply hierarchical multiscale splitting processing to images. Then, we improve the structure of the image semantic segmentation model DeepLabV3+, an advanced image semantic segmentation model, and adjust the feature output layer of the model to multiscale features after weighted fusion. At the back end of the deep learning model, the water boundary details are optimized with the fully connected CRF. The proposed multiscale training method is well adapted to feature extraction for the different scale images in the model. In the multiscale output fusion, assigning different weights to the output features of each scale controls the influence of the various scale features on the water extraction results. We carried out a large number of water extraction experiments on GF1 remote sensing images. The results show that the method significantly improves the accuracy of water extraction and demonstrates the effectiveness of the method. © 2013 IEEE.","deep learning; DeepLabV 3+; multi-scales; Remote sensing; semantic segmentation; water information extraction","Deep learning; Extraction; Image enhancement; Random processes; Remote sensing; Scales (weighing instruments); Semantics; Space optics; Conditional random field; DeepLabV 3; High resolution remote sensing images; Multi-scale features; Multi-scales; Remote sensing images; Semantic segmentation; Water informations; Image segmentation"
"Three-dimensional densely connected convolutional network for hyperspectral remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062600898&doi=10.1117%2f1.JRS.13.016519&partnerID=40&md5=312b78800c9437784c70a706772d0dad","Hyperspectral remote sensing images (HSIs) are rich in spatial and spectral information, thus they help to enhance the ability to distinguish geographic objects. In recent years, great progress have been made in image classification using deep learning (such as 2D-CNN and 3D-CNN). Compared with traditional machine learning methods, deep learning methods can automatically extract the abstract features from low to high levels and convert the images into more easily recognizable features. Most HSI classification tasks focus on spectral information but often ignore the rich spatial structures in HSIs, leading to a low classification accuracy. Moreover, most supervised learning methods use shallow structures in HSI classifications and hence exhibit weak performance in finding sparse geographic objects. We proposed to use the three-dimensional (3-D) structure to extract spectral-spatial information to build a deep neural network for HSI classifications. Based on DenseNet, the 3D densely connected convolutional network was improved to learn spectral-spatial features of HSIs. The densely connected structure can enhance feature transmission, support feature reuse, improve information flow in the network, and make deeper networks easier to train. The 3D-DenseNet has a deeper structure than 3D-CNN, thus it can learn more robust spectral-spatial features from HSIs. In fact, the deeper network structure has a regularized effect, which can effectively reduce overfitting on small sample datasets. The network uses HSIs instead of feature engineering as input data and is trained in an end-to-end manner. The experimental results of this model on the Indian Pines datasets and the Pavia University datasets show that deeper neural networks further improve the classification of complex objects, especially in the areas where geographic objects are sparse. It effectively improves the classification accuracy of HSIs. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","DenseNet; feature fusion; hyperspectral remote sensing image classification; spectral-spatial features; Three-dimensional-DenseNet","Convolution; Deep neural networks; Image classification; Image enhancement; Machine learning; Remote sensing; Classification accuracy; DenseNet; Feature fusion; Hyperspectral Remote Sensing Image; Machine learning methods; Spatial features; Supervised learning methods; Three dimensional (3D) structures; Classification (of information)"
"Chimney and condensing tower detection based on FPN in high resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078096586&doi=10.1117%2f12.2532376&partnerID=40&md5=5643d4456432c700595941a354ea5ab4","The frequent hazy weather in North China has drawn people's attention. The anthropogenic emission by fossil fuel power plants is one of the main pollution resource, so the environmental protection administration need to monitor power plants. Thus, a power plant detection system is needed to locate power plants and judge their working status. In this paper, we propose a power plant monitoring framework based on Feature Pyramid Network (FPN) to automatically detect the chimneys and condensing towers of the power plants and judge their working status in high resolution remote sensing images (RSIs). We improve the original FPN by changing the number of layers and scales of feature pyramid to get better performance. Experimental results show that our improved FPN framework can effectively detect the chimneys and condensing towers of fossil-fuel power plants and judge their working status with mean average precision up-to 0.8591, showing good potential for power plant monitoring. © 2019 SPIE.","Chimney and condensing tower detection; Deep learning; Feature Pyramid Network; Remote sensing image","Chimneys; Deep learning; Feature extraction; Fossil fuel deposits; Fossil fuel power plants; Fossil fuels; Image processing; Towers; Anthropogenic emissions; Environmental protection administrations; Feature pyramid; High resolution remote sensing images; Number of layers; Plant detections; Plant monitoring; Remote sensing images; Remote sensing"
"Severe convective weather classification in remote sensing images by semantic segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072866120&doi=10.1007%2f978-3-030-30508-6_12&partnerID=40&md5=e7d25aa25f7a15495fd5598e2d5cae1b","Severe convective weather is a catastrophic weather that can cause great harm to the public. One of the key studies for meteorological practitioners is how to recognize severe convection weather accurately and effectively, and it is also an important issue in government climate risk management. However, most existing methods extract features from satellite data by classifying individual pixels instead of using tightly integrated spatial information, ignoring the fact the clouds are highly dynamic. In this paper, we propose a new classification model, which is based on image segmentation of deep learning. And it uses U-net architecture as the technology platform to identify all weather conditions in the datasets accurately. As heavy rainfall is one of the most frequent and widespread server weather hazards, when the storms come ashore with high speed of wind, it makes the precipitation time longer and causes serious damage in turn. Therefore, we suggest a new evaluation metric to evaluate the performance of detecting heavy rainfall. Compared with existing methods, the model based on Himawari-8 dataset has a better performance. Further, we explore the representations learned by our model in order to better understand this important dataset. The results play a crucial role in the prediction of climate change risks and the formulation of government policies on climate change. © Springer Nature Switzerland AG 2019.","Evaluation metric; Segmentation; Severe convective weather","Climate change; Deep learning; Image classification; Image segmentation; Meteorological problems; Neural networks; Rain; Remote sensing; Risk management; Semantics; Classification models; Climate change risks; Convective weather; Evaluation metrics; Remote sensing images; Semantic segmentation; Spatial informations; Technology platforms; Classification (of information)"
"Road and railway detection in SAR images using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077326036&doi=10.1117%2f12.2532803&partnerID=40&md5=9111a0386b6d13476be823d8727728a3","Detection and segmentation of motorways, railroads and other roads with similar features are significant for comprehension of both low and high resolution synthetic aperture radar (SAR) imagery. Separation of transportation network from other fields or features is important to understand area contained in SAR image (i.e. the road density can inform about characteristic of that area). Standard image processing methods are inadequate to detect multiple linear targets correctly where computer vision, especially deep learning, provides more insight about features for different type of roads which help better discrimination of multiple linear features like roads and railroads. State-of-art deep learning algorithms are proposed as solutions for understanding road characteristics and extraction of multiple roads. In this paper, a method which uses deep convolutional neural network (DeepLabv3+) backbone architecture is proposed to detect road and railways concurrently. Semantic segmentation of roads using SAR imagery is challenging since these images differ as ground sample distance changes with sensor types which creates a setback for establishing dataset for all sensors. Training set contains 3 classes (road, railway, other) with collected signatures from TerraSAR-X Spotlight images for classification. Proposed method shows robust performance when applied to other sensor and results are presented. © 2019 SPIE.","Railway detection; Road detection; Synthetic aperture radar; TerraSAR-X imagery","Classification (of information); Deep neural networks; Feature extraction; Image segmentation; Learning algorithms; Neural networks; Processing; Railroads; Remote sensing; Roads and streets; Satellites; Semantics; Synthetic aperture radar; Tracking radar; Convolutional neural network; Ground sample distances; High resolution synthetic aperture radar; Road detection; Robust performance; Semantic segmentation; TerraSAR-X; Transportation network; Radar imaging"
"Fast detection of ship targets for large-scale remote sensing image based on a cascade convolutional neural network [基于级联卷积神经网络的大场景遥感图像舰船目标快速检测方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072342364&doi=10.12000%2fJR19041&partnerID=40&md5=4d49a1c64da993730076b85bf9d4cb43","For the fast detection of ships in large-scale remote sensing images, a cascade convolutional neural network is proposed, which is a cascade combination of two Fully Convolutional Neural networks (FCNs), the target FCN for Prescreening (P-FCN), and the target FCN for Detection (D-FCN). The P-FCN is a lightweight image classification network that is responsible for the rapid pre-screening of possible ship areas in large-scale images. The region proposals generated by the P-FCN have less redundancy, which can reduce the computational burden of the D-FCN. The D-FCN is an improved U-Net that can accurately detect arbitrary-oriented ships by adding target masks and ship orientation estimation layers to the traditional U-Net structure for multitask learning. In our experiment, TerraSAR-X remote sensing images and the optical remote sensing images obtained from the 91 satellite map software and the DOTA dataset were used to test the network. The results show that the detection accuracy of our method was 0.928 and 0.926 for synthetic aperture radar images and optical images, respectively, which were close to the performance of the traditional sliding window method. However, the running time of the proposed method was only about 1/3 of that of the sliding window method. Therefore, the cascade convolutional neural network can significantly improve the target detection efficiency while maintaining the detection accuracy and can realize the rapid detection of ship targets in large-scale remote sensing images. © 2019 Institute of Electronics Chinese Academy of Sciences. All rights reserved.","Deep learning; Fast detection; Fully Convolutional Neural network(FCN); Large scale remote sensing image; Ship detection",
"Semantic classification of DSM using convolutional neural network based deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078122145&doi=10.7848%2fksgpc.2019.37.6.435&partnerID=40&md5=3303b83d2f6b8fc72d25474add261f6c","Recently, DL (Deep Learning) has been rapidly applied in various fields. In particular, classification and object recognition from images are major tasks in computer vision. Most of the DL utilizing imagery is primarily based on the CNN (Convolutional Neural Network) and improving performance of the DL model is main issue. While most CNNs are involve with images for training data, this paper aims to classify and recognize objects using DSM (Digital Surface Model), and slope and aspect information derived from the DSM instead of images. The DSM data sets used in the experiment were established by DGPF (German Society for Photogrammetry, Remote Sensing and Geoinformatics) and provided by ISPRS (International Society for Photogrammetry and Remote Sensing). The CNN-based SegNet model, that is evaluated as having excellent efficiency and performance, was used to train the data sets. In addition, this paper proposed a scheme for training data generation efficiently from the limited number of data. The results demonstrated DSM and derived data could be feasible for semantic classification with desirable accuracy using DL. © 2019 Korean Society of Surveying. All rights reserved.","CNN; DL Model; DSM Classification; Training and Validation Data","accuracy assessment; artificial neural network; image classification; modeling; pattern recognition"
"Unsupervised feature learning in remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075748078&doi=10.1117%2f12.2529791&partnerID=40&md5=5c650fe98a60e43d9f6970a58da3f720","The need for labeled data is among the most common and well-known practical obstacles to deploying deep learning algorithms to solve real-world problems. The current generation of learning algorithms requires a large volume of data labeled according to a static and pre-defined schema. Conversely, humans can quickly learn generalizations based on large quantities of unlabeled data, and turn these generalizations into classifications using spontaneous labels, often including labels not seen before. We apply a state-of-The-Art unsupervised learning algorithm to the noisy and extremely imbalanced xView data set to train a feature extractor that adapts to several tasks: visual similarity search that performs well on both common and rare classes; identifying outliers within a labeled data set; and learning a natural class hierarchy automatically. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Anomaly detection; Classification; Deep learning; Hierarchy discovery; remote sensing; Similarity search; Unsupervised learning","Anomaly detection; Classification (of information); Deep learning; Machine learning; Remote sensing; Unsupervised learning; Class hierarchies; Current generation; Feature extractor; Hierarchy discovery; Real-world problem; Similarity search; Unsupervised feature learning; Visual similarity; Learning algorithms"
"Comparative analysis of generalized intersection over union and error matrix for vegetation cover classification assessment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076690771&doi=10.18494%2fSAM.2019.2584&partnerID=40&md5=5197d63652a566143692de1a01ced75a","The result of vegetation cover classification greatly depends on the classification methods. Accuracy analysis is mostly performed using the error matrix in remote sensing. In recent remote sensing, image classification has been carried out on the basis of deep learning. In the field of image processing in computer science, Intersection over Union (IoU) is mainly used for accuracy analysis. In this study, the error matrix, which is frequently used in remote sensing, and IoU, which is mainly used for deep learning images, were compared and reviewed to analyze their accuracy levels for the results of vegetation index calculation. The results of vegetation index calculation were applied to the comparison of the accuracy levels of IoU and the error matrix. According to the results of accuracy analysis using the error matrix, which is based on random points, the accuracy of the normalized difference vegetation index (NDVI) was shown to be 82.4% and that of deep learning was shown to be 93.7%, with a difference of about 11.3%. © MYU K.K.","Error matrix; Intersection over Union (IoU); Normalized difference vegetation index (NDVI); Remote sensing","Deep learning; Errors; Image analysis; Vegetation; Accuracy analysis; Classification methods; Comparative analysis; Error matrices; Generalized intersection; Normalized difference vegetation index; Vegetation cover; Vegetation index calculations; Remote sensing"
"Hyper-spectral Images Classification Based on 3D Convolution Neural Networks for Remote Sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066915221&doi=10.1007%2f978-981-13-5937-8_21&partnerID=40&md5=7a6851b302f90f4fd6624b5cf46f15da","With the rapid development of hyper-spectral imaging techniques, hyper-spectral image classification has been applied to many tasks such as monitoring, astronomy and substance exploration. Hyper-spectral Images with rich spatial and spectral content is more difficult to be classified than common images with RGB channels. Many deep learning methods have ignored the context between spectral features when extracting spectral-spatial features of hyper-spectral images. So we implemented a 3D Convolution Neural Network model to extract correlated and effective features and improve the performance for Hyper-spectral Images classification. The hyper-spectral data set we use is the University of Pavia which has less training samples. So we exploited dropout and cross validation methods in the training process to avoid over fitting and we have extended the training samples by some transformation. The results of our experiments have shown that our model can generally get better results than some of the state-of-the-art methods. © 2019, Springer Nature Singapore Pte Ltd.","Convolution Neural Network; Deep learning; Hyper-spectral image classification; Remote sensing","Convolution; Deep learning; Image enhancement; Imaging techniques; Information services; Remote sensing; Sampling; Space optics; Spectroscopy; Convolution neural network; Cross-validation methods; Hyper-spectral images; Hyperspectral Data; Learning methods; Spatial features; Spectral feature; State-of-the-art methods; Image classification"
"Convolutional Autoencoder-Based Multispectral Image Fusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063928679&doi=10.1109%2fACCESS.2019.2905511&partnerID=40&md5=f6d1dc17c5508ded619cc2911c0fd8f8","This paper presents a deep learning-based pansharpening method for fusion of panchromatic and multispectral images in remote sensing applications. This method can be categorized as a component substitution method in which a convolutional autoencoder network is trained to generate original panchromatic images from their spatially degraded versions. Low resolution multispectral images are then fed into the trained convolutional autoencoder network to generate estimated high resolution multispectral images. The fusion is achieved by injecting the detail map of each spectral band into the corresponding estimated high resolution multispectral bands. Full reference and no-reference metrics are computed for the images of three satellite datasets. These measures are compared with the existing fusion methods whose codes are publicly available. The results obtained indicate the effectiveness of the developed deep learning-based method for multispectral image fusion. © 2013 IEEE.","convolutional autoencoder-based pansharpening; fusion of panchromatic and multispectral images in remote sensing; Multispectral image fusion by convolutional autoencoder","Convolution; Deep learning; Remote sensing; Auto encoders; Component substitution; Learning-based methods; Low resolution multispectral images; Multi-spectral image fusions; Multispectral images; Pan-sharpening; Remote sensing applications; Image fusion"
"AI-based ocean information mining from large satellite remote sensing data set","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079143769&partnerID=40&md5=3006640365a57ca3694a8699d0198376","Artificial intelligent (AI) technology has been utilized widely in many fields since the training of deep learning (DL) neural network (NN) became possible in 2006. Compared with human brain, DL has many advantages in dealing with multi-dimensional data sources that are usually required to address information mining for scientific problems. In this study, we took advantage the large remote sensing database offered by government agencies, i.e., NASA, NOAA and European Space Agency (ESA) and implemented the DL models for coastal inundation mapping and Sea Surface Temperature (SST) forecast in the equatorial Pacific. Case studies show that this approach worked very efficiently in identifying coastal inundation area after the passage of hurricanes and the forecasting the tropical instability wave propagation in the equator Pacific. Copyright © 2019 by the International Astronautical Federation (IAF). All rights reserved.","Artificial intelligent; Remote Sensing; Sea surface temperature",
"Road Detection of Remote Sensing Image Based on Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076895342&doi=10.1007%2f978-3-030-34110-7_10&partnerID=40&md5=201c9f536889fb270d36265eda5352f2","We propose an end-to-end framework for the road detection in satellite imagery with convolutional neural networks (CNNs). Firstly, we analyze the limitations of patch-based network and full convolution neural network and propose a new method. In our approach, CNNs are directly trained to produce classification maps out of the input images without losing much edge information. The method, called M-FCN, extends FCN by using a new activation function instead of the traditional activation function. We then address the issue of imperfect training data through a traditional approach: labeled images manually to form a new dataset. Finally, we show that such a network can be trained on remote sensing images with a composite loss function. At the same time, we validate the effect of label accuracy in dataset on the model. To ensure the accuracy of our method, we apply different methods to train in the same dataset. A series of experiments show that our networks consider a large amount of context to provide fine-grained classification maps, M-FCN outperforms SVM, patch-based network and full convolution network. © 2019, Springer Nature Switzerland AG.","Convolutional neural networks (CNNs); Deep learning; Road detection; Satellite images","Chemical activation; Classification (of information); Convolution; Deep learning; Deep neural networks; Neural networks; Roads and streets; Satellite imagery; Support vector machines; Activation functions; Convolution neural network; Convolutional neural network; New activation functions; Remote sensing images; Road detection; Satellite images; Traditional approaches; Remote sensing"
"Learnable gated convolutional neural network for semantic segmentation in remote-sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071595417&doi=10.3390%2frs11161922&partnerID=40&md5=832396545302f62d3c2e997e2321f1f8","Semantic segmentation in high-resolution remote-sensing (RS) images is a fundamental task for RS-based urban understanding and planning. However, various types of artificial objects in urban areas make this task quite challenging. Recently, the use of Deep Convolutional Neural Networks (DCNNs) with multiscale information fusion has demonstrated great potential in enhancing performance. Technically, however, existing fusions are usually implemented by summing or concatenating feature maps in a straightforward way. Seldom do works consider the spatial importance for global-to-local context-information aggregation. This paper proposes a Learnable-Gated CNN (L-GCNN) to address this issue. Methodologically, the Taylor expression of the information-entropy function is first parameterized to design the gate function, which is employed to generate pixelwise weights for coarse-to-fine refinement in the L-GCNN. Accordingly, a Parameterized Gate Module (PGM) was designed to achieve this goal. Then, the single PGM and its densely connected extension were embedded into different levels of the encoder in the L-GCNN to help identify the discriminative feature maps at different scales. With the above designs, the L-GCNN is finally organized as a self-cascaded end-to-end architecture that is able to sequentially aggregate context information for fine segmentation. The proposed model was evaluated on two public challenging benchmarks, the ISPRS 2Dsemantic segmentation challenge Potsdam dataset and the Massachusetts building dataset. The experiment results demonstrate that the proposed method exhibited significant improvement compared with several related segmentation networks, including the FCN, SegNet, RefineNet, PSPNet, DeepLab and GSN.For example, on the Potsdam dataset, our method achieved a 93.65% F1 score and 88.06% IoU score for the segmentation of tiny cars in high-resolution RS images. As a conclusion, the proposed model showed potential for object segmentation from the RS images of buildings, impervious surfaces, low vegetation, trees and cars in urban settings, which largely varies in size and have confusing appearances. © 2019 by the authors.","CNN; Deep learning; Gate function; Multiscale feature fusion; Remote sensing; Semantic segmentation","Aggregates; Convolution; Deep learning; Deep neural networks; Image enhancement; Neural networks; Remote sensing; Semantic Web; Semantics; Urban planning; Convolutional neural network; Discriminative features; High resolution remote sensing; High-resolution RS images; Multi-scale features; Multi-scale informations; Remote sensing images; Semantic segmentation; Image segmentation"
"A data augmentation strategy based on simulated samples for ship detection in RGB remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068595465&doi=10.3390%2fijgi8060276&partnerID=40&md5=4b226faaab96ea3f777ba2ac3bf42a5c","In this paper, we propose a data augmentation method for ship detection. Inshore ship detection using optical remote sensing imaging is a challenging task owing to an insufficient number of training samples. Although the multilayered neural network method has achieved excellent results in recent research, a large number of training samples is indispensable to guarantee the accuracy and robustness of ship detection. The majority of researchers adopt such strategies as clipping, scaling, color transformation, and flipping to enhance the samples. Nevertheless, these methods do not essentially increase the quality of the dataset. A novel data augmentation strategy was thus proposed in this study by using simulated remote sensing ship images to augment the positive training samples. The simulated images are generated by true background images and three-dimensional models on the same scale as real ships. A faster region-based convolutional neural network (Faster R-CNN) based on Res101netwok was trained by the dataset, which is composed of both simulated and true images. A series of experiments is designed under small sample conditions; the experimental results show that better detection is obtained with our data augmentation strategy. © 2019 by the authors.","Data augmentation; Deep learning; Optical remote sensing image; Ship detection; Simulated samples",
"PolSAR Marine Aquaculture Detection Based on Nonlocal Stacked Sparse Autoencoder","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068614320&doi=10.1007%2f978-3-030-22808-8_46&partnerID=40&md5=aa73ff9b9763422774698912198dc3bd","Marine aquaculture plays an important role in marine economic, which distributes widely around the coast. Using satellite remote sensing monitoring, it can achieve large scale dynamic monitoring. As a classic model of deep learning, stacked sparse autoencoder (SSAE) has the advantages of simple model and self-learning of features. Nonlocal spatial information is utilized to assist SSAE construct NSSAE to improve the precision in this paper. Experimental results demonstrate the superiority of nonlocal SSAE methods on marine target recognition. © Springer Nature Switzerland AG 2019.","Classification; Nonlocal spatial information; Polarimetric SAR; Remote sensing images; Stacked sparse autoencoder","Aquaculture; Classification (of information); Deep learning; Auto encoders; Large-scale dynamics; Marine aquaculture; Polarimetric SAR; Remote sensing images; Satellite remote sensing; Spatial informations; Target recognition; Remote sensing"
"Oil slick extraction from hyperspectral images using a modified stacked auto-encoder network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072604658&doi=10.1117%2f12.2539664&partnerID=40&md5=9e6eb3b04df8971c152f26e24eb5b643","Hyperspectral remote sensing provides an outstanding tool in oil slick detection and classification, for its advantages in abundant spectral information. Many classification methods have been proposed and tested for oil spill extraction using hyperspectral images. However, the deep learning method were hardly researched to classify oil slicks using hyperspectral images. In this work, we proposed a spatial-spectral jointed Stacked Auto-encoder (SSAE) to extract and classify oil slicks on the sea surface. The traditional machine learning methods, Support Vector Machine (SVM), Back Propagation Neural network (BPNN) and Stacked Auto-encoder (SAE), were also adopted. The experimental results reveal that our proposed SSAE model can remarkably outperform the other models, especially for the thick oil films. The results of this work could provide an alternative method to extract oil slicks on hyperspectral remote sensing images. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Hyperspectral remote sensing; Oil spill; Stacked auto-encoder","Classification (of information); Deep learning; Extraction; Network coding; Neural networks; Oil spills; Remote sensing; Spectroscopy; Support vector machines; Surface waters; Auto encoders; Back-propagation neural networks; Classification methods; Hyperspectral remote sensing; Hyperspectral Remote Sensing Image; Machine learning methods; Oil-slick detection; Spectral information; Image processing"
"Small manhole cover detection in remote sensing imagery with deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061390535&doi=10.3390%2fijgi8010049&partnerID=40&md5=c71700135b523e28b0272f0bfc413327","With the development of remote sensing technology and the advent of high-resolution images, obtaining data has become increasingly convenient. However, the acquisition of small manhole cover information still has shortcomings including low efficiency of manual surveying and high leakage rate. Recently, deep learning models, especially deep convolutional neural networks (DCNNs), have proven to be effective at object detection. However, several challenges limit the applications of DCNN in manhole cover object detection using remote sensing imagery: (1) Manhole cover objects often appear at different scales in remotely sensed images and DCNNs' fixed receptive field cannot match the scale variability of such objects; (2) Manhole cover objects in large-scale remotely-sensed images are relatively small in size and densely packed, while DCNNs have poor localization performance when applied to such objects. To address these problems, we propose an effective method for detecting manhole cover objects in remotely-sensed images. First, we redesign the feature extractor by adopting the visual geometry group (VGG), which can increase the variety of receptive field size. Then, detection is performed using two sub-networks: a multi-scale output network (MON) for manhole cover object-like edge generation from several intermediate layers whose receptive fields match different object scales and a multi-level convolution matching network (M-CMN) for object detection based on fused feature maps, which combines several feature maps that enable small and densely packed manhole cover objects to produce a stronger response. The results show that our method is more accurate than existing methods at detecting manhole covers in remotely-sensed images. © 2019 by the authors.","Deep convolutional neural networks; Manhole cover; Object detection; Remote sensing images",
"Unsupervised Deep Feature Learning with Iteratively Refined Pseudo Classes for Scene Representation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073896146&doi=10.1109%2fACCESS.2019.2928328&partnerID=40&md5=f965dc74b422e51e6a81810c926f5ace","Recently, more and more attention has been focused on the remote sensing scenes since they contain plentiful spectral and spatial information. In order to obtain good performance for scene representation, a proper model for feature extraction and large amounts of labeled training samples are required. However, in real-world applications, it usually cannot provide enough labeled samples since labeling is always time-consuming. To overcome this problem, this work develops a novel unsupervised deep feature learning framework with iteratively refined pseudo-classes for remote sensing scene representation. First, we introduce the center points to construct the pseudo-classes and assign the pseudo labels to the training samples. Then, a pseudo-center loss is developed by decreasing the intra-class variance between the learned features of the samples and the corresponding center points to iteratively refine the pseudo classes with the training samples in the training process. Moreover, to increase the inter-class variance between different pseudo classes and further improve the performance of the unsupervised learning, this work imposes the diversity-promoting priors over the center points. Finally, the unsupervised learning framework is developed by joint learning of the diversified pseudo-center loss and pseudo-class-based softmax loss where the pseudo-class-based softmax loss is to update the convolutional neural network (CNN) with the pseudo-classes and the diversified pseudo-center loss is to iteratively refine the pseudo-classes with the features learned from the CNN. Experiments are conducted over three real-world remote sensing scene datasets to validate the effectiveness of the proposed method and the experimental results show the superiority of the method when compared with other state-of-the-art methods. © 2013 IEEE.","CNN; diversity; pseudo class; scene classification; Unsupervised deep learning","Iterative methods; Machine learning; Neural networks; Remote sensing; Sampling; Unsupervised learning; Convolutional neural network; Deep feature learning; diversity; pseudo class; Scene classification; Scene representation; Spatial informations; State-of-the-art methods; Deep learning"
"Semi-automatic ice floe detection for drift evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070823567&partnerID=40&md5=2244619ca5bb08093682539fa7cdd171","Automatic ice drift detection from remote sensing data is a straight forward process in the arctic where revisit time by polar orbiting satellites is quite small and it is possible to rely on one satellite to define change detection algorithms and extract drift data. But if we consider subArctic regions intervals of about three days minimum between images of the same satellite makes automated drift detection obsolete for majority of cases due to changes in wind regime and intermittent drift events. Therefore, multi-platform remote sensing data needs to be used to reduce the gaps. In this project we exploit deep learning capabilities in order to estimate ice drift between image from different sensors, by learning their similarities with Siamese Neural Network. © 2019, Lulea University of Technology. All rights reserved.","Computer Vision; Deep Learning; Ice Charting Operations; Ice Drift detection; Ice Floe Recognition; Remote Sensing","Arctic engineering; Computer vision; Deep learning; Ocean engineering; Orbits; Sea ice; Small satellites; Change detection algorithms; Ice drift; Ice floes; Learning capabilities; Polar-orbiting satellites; Remote sensing data; Semi-automatics; Subarctic regions; Remote sensing"
"Clouds classification from Sentinel-2 imagery with deep residual learning and semantic image segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060691480&doi=10.3390%2frs11020119&partnerID=40&md5=afc23c6257fa27fa27d30e7becc9fb7e","Detecting changes in land use and land cover (LULC) from space has long been the main goal of satellite remote sensing (RS), yet the existing and available algorithms for cloud classification are not reliable enough to attain this goal in an automated fashion. Clouds are very strong optical signals that dominate the results of change detection if they are not removed completely from imagery. As various architectures of deep learning (DL) have been proposed and advanced quickly, their potential in perceptual tasks has been widely accepted and successfully applied to many fields. A comprehensive survey of DL in RS has been reviewed, and the RS community has been suggested to be leading researchers in DL. Based on deep residual learning, semantic image segmentation, and the concept of atrous convolution, we propose a new DL architecture, named CloudNet, with an enhanced capability of feature extraction for classifying cloud and haze from Sentinel-2 imagery, with the intention of supporting automatic change detection in LULC. To ensure the quality of the training dataset, scene classification maps of Taiwan processed by Sen2cor were visually examined and edited, resulting in a total of 12,769 sub-images with a standard size of 224 × 224 pixels, cut from the Sen2cor-corrected images and compiled in a trainset. The data augmentation technique enabled CloudNet to have stable cirrus identification capability without extensive training data. Compared to the traditional method and other DL methods, CloudNet had higher accuracy in cloud and haze classification, as well as better performance in cirrus cloud recognition. CloudNet will be incorporated into the Open Access Satellite Image Service to facilitate change detection by using Sentinel-2 imagery on a regular and automatic basis. © 2019 by the authors.","Atrous convolution; Change detection; Cloud classification; CloudNet; Deep learning; Deep residual learning; Land use and land cover; Semantic image segmentation; Sentinel-2","Classification (of information); Clouds; Convolution; Feature extraction; Image classification; Image enhancement; Image segmentation; Land use; Open access; Remote sensing; Satellite imagery; Semantics; Change detection; Cloud classification; CloudNet; Deep residual learning; Land use and land cover; Semantic image segmentations; Sentinel-2; Deep learning"
"Deep learning in photogrammetry, remote sensing and geospatial information processing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064566160&partnerID=40&md5=57b655c1b7a6546c8f607b5b4f2311dc",[No abstract available],,
"Moderate Resolution Imaging Spectroradiometer Products Classification Using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076148374&doi=10.1007%2f978-3-030-33229-7_6&partnerID=40&md5=84818ab854809af1b41f124631b9e8eb","During the last years, the algorithms based on Artificial Intelligence have increased their popularity thanks to their application in multiple areas of knowledge. Nowadays with the increase of storage capacities and computing power, as well as the incorporation of new technologies for massively parallel processing (GPUs and TPUs) and Cloud Computing, it is increasingly common to incorporate this kind of algorithms and technology in tasks with a deep social and technological impact. In the present work a new Convolutional Neural Network specialized in the automatic classification of Moderate Resolution Imaging Spectroradiometer satellite products is proposed. The proposed architecture has shown a high-generalization by classifying more than 250,000 images with 99.99% accuracy. The methodology designed also can be extended, with other types of images, to make detection of Sargassum, oil spills, red tide, etc. © 2019, Springer Nature Switzerland AG.","Data science; Deep learning; Machine learning; MODIS; Remote sensing","Data Science; Digital storage; Economic and social effects; Learning systems; Neural networks; Oil spills; Program processors; Radiometers; Remote sensing; Spectrometers; Automatic classification; Convolutional neural network; Massively parallel processing; Moderate resolution imaging spectroradiometer; Moderate resolution imaging spectroradiometer satellites; MODIS; Proposed architectures; Storage capacity; Deep learning"
"Deep learning for remote sensed target classification in maritime satellite radar images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068520232&doi=10.1117%2f12.2519577&partnerID=40&md5=67f0914566bbb8d8376ef865e9777461","Detecting drifting icebergs is an important task to avoid threats to navigation and offshore activities. Government and companies use aerial reconnaissance and shore-based observation platforms to detect these icebergs. However, in some areas with harsh weather conditions only satellite imagery can be used to monitor this risk. In this work, we propose the use of deep Convolutional Neural Networks to detect and classify these small remotely sensed targets as ships or icebergs. In this work, we use satellite radar imagery composed of two bands. The image patches have a resolution below 6K pixels and are noisy. To solve this challenge, we developed a deep convolutional network architecture and optimized its hyperparameters for this classification. The obtained results show that the proposed deep convolutional network achieves a very interesting accuracy for the classification of icebergs vs. ships with radar satellite images. Copyright © 2019 SPIE.","CNN; Deep learning; Icebergs; Maritime surveillance; Radar satellite imagery; Ships","Antennas; Convolution; Deep learning; Deep neural networks; Image classification; Network architecture; Neural networks; Offshore oil well production; Remote sensing; Satellite imagery; Sea ice; Ships; Space-based radar; Tracking radar; Aerial reconnaissance; Convolutional networks; Convolutional neural network; Icebergs; Maritime surveillance; Offshore activity; Radar satellites; Target Classification; Radar imaging"
"Deep learning network integrated multi-spectral data and interferometric imaging radar altimeter data of Tiangong-2 for land use classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058545956&doi=10.1007%2f978-981-13-3501-3_22&partnerID=40&md5=d5069eb84f3fdc30fb2f2d61739ec0e9","Multispectral data and radar data contain abundant spectral and texture features, which are widely used in land use classification. High precision registration and information mining are the two major difficulties faced by the classification of these two data. In this paper, visible and near infrared spectrum range of Wide-band Imaging Spectrometer (VNI) and Interferometric Imaging Radar Altimeterdata (InIRA) are used as raw data, and data registration and combination is carried out (the combination data is called for VNI_InIRA). Then the information of each pixel before and after combination is extracted and transformed into gray-scale image which is easy to be input by convolution neural network (CNN). Finally, the classification of land use is carried out by trained CNN models, and the accuracy is verified. The results show that the classification results of VNI_InIRA data are better than those of VNI data. The overall classification accuracy is 94.29% and 91.83% respectively. Therefore, the CNN which synthesizes the spectral characteristics of VNI data and the texture topographic features of InIRA data is a feasible method to obtain accurate land use classification information, and to provide a reference for the study of land use classification extraction in coastal areas. © Springer Nature Singapore Pte Ltd. 2019.","Convolutional Neural Network; Deep learning; Interferometric Imaging Radar Altimeter data; Land use classification; Multi-spectral data; Tiangong-2","Aneroid altimeters; Convolution; Data mining; Deep learning; Indium compounds; Infrared devices; Interferometry; Land use; Meteorological instruments; Near infrared spectroscopy; Neural networks; Radar; Radar equipment; Radar imaging; Radio altimeters; Remote sensing; Altimeter data; Convolutional neural network; Landuse classifications; Multi-spectral data; Tiangong-2; Classification (of information)"
"Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056707759&doi=10.1016%2fj.isprsjprs.2018.11.011&partnerID=40&md5=199ea1fe62b8ef56006aa9407572d686","As an important branch of deep learning, convolutional neural network has largely improved the performance of building detection. For further accelerating the development of building detection toward automatic mapping, a benchmark dataset bears significance in fair comparisons. However, several problems still remain in the current public datasets that address this task. First, although building detection is generally considered equivalent to extracting roof outlines, most datasets directly provide building footprints as ground truths for testing and evaluation; the challenges of these benchmarks are more complicated than roof segmentation, as relief displacement leads to varying degrees of misalignment between roof outlines and footprints. On the other hand, an image dataset should feature a large quantity and high spatial resolution to effectively train a high-performance deep learning model for accurate mapping of buildings. Unfortunately, the remote sensing community still lacks proper benchmark datasets that can simultaneously satisfy these requirements. In this paper, we present a new large-scale benchmark dataset termed Aerial Imagery for Roof Segmentation (AIRS). This dataset provides a wide coverage of aerial imagery with 7.5 cm resolution and contains over 220,000 buildings. The task posed for AIRS is defined as roof segmentation. We implement several state-of-the-art deep learning methods of semantic segmentation for performance evaluation and analysis of the proposed dataset. The results can serve as the baseline for future work. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Automatic mapping; Building detection; Deep learning; Large-scale dataset; Roof segmentation","Aerial photography; Antennas; Buildings; Infrared devices; Mapping; Neural networks; Remote sensing; Roofs; Semantics; Automatic mapping; Building detection; Convolutional neural network; High spatial resolution; Large-scale dataset; Performance evaluations; Performance of buildings; Testing and evaluation; Deep learning; AIRS; algorithm; artificial neural network; building; data set; detection method; mapping method; remote sensing; satellite imagery; segmentation; spatial resolution"
"On generalization of deep learning recognizers in overhead imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072554342&doi=10.1117%2f12.2519064&partnerID=40&md5=0a8fae0d9039c44731df87cc22ca9ade","In many applications, access to large quantities of labeled data is prohibitive due to its cost or lack of access to classes of interest. This problem is exacerbated in the context of specific subclasses and data types that are not easily accessible, such as remotes sensing data. The problem of limited data for specific classes of data is referred to as the low-shot or few-shot problem. Typically in the low-shot problem, there is a wealth of data from a source domain that is leveraged to train a convolutional feature extractor that is then applied to a target domain in innovative ways. In this work we apply this framework to the low-shot and fully sampled problem, in which the convolutional neural network is used as a feature extractor and paired with an alternate classifier. We evaluate the benefits of this approach in two contexts, a baseline problem, and limited training data. Additionally, we investigate the impact of loss function selection and sequestering of low-shot data on the classification performance of this approach. We present an applications of these techniques on the recent public xView dataset. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Classiffication; Few-shot; Low-shot; Machine Learning; Recognition; xView","Automatic target recognition; Convolution; Learning systems; Neural networks; Classiffication; Few-shot; Low-shot; Recognition; xView; Deep learning"
"Dense-HSGP: Dense hierarchical spatial Gaussian-based pooling for very high-resolution building extraction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078191926&doi=10.1117%2f12.2532489&partnerID=40&md5=58b6afb6d7dedbee2bf43d269b060528","In the remote sensing area, how to automatically and accurately extract buildings from images is a hot and challenging topic in these years. With the rapid development of sensor and computer hardware technologies, it gets easier to gain remote sensing images with very high-resolution and extract buildings from them by the popular deep learning models such as Fully Convolutional Networks (FCN). However, current FCN based models always lead to blurred building boundaries and have poor abilities on extracting small buildings. Therefore, in this paper, we propose the Gaussian Dilate Convolution, which is a cascade of a trainable Gaussian Filter and an dilate convolution with proper hyperparameter initializations. Also, we carefully design a hierarchical dense feature fusion structure following the dense connection manners. Finally, we embed the Gaussian Dilate Convolution into the hierarchical dense fusion structure and name it as Dense Hierarchical Spatial Gaussian Pool (Dense-HSGP). More specifically, the Gaussian Dilate Convolution has the advantages of the original dilate convolution but preserves much more context information, while the hierarchical dense connection structure of Dense-HSGP provides more abundant receptive fields and higher feature reused abilities within the model. We execute the experiments on the widely used Inrial Labelling Dataset to verify the efficiency of the proposed model. The experimental results show that the proposed model achieves 96.45 % average accuracy and 77.17% IoU respectively, which are distinct improvements rather than several recent state-of-the-art building extraction models. © 2019 SPIE.","Building Extraction; Deep Learning; Dense Connection; Gaussian Pooling; High-Resolution Remote Sensing Imagery","Buildings; Computer hardware; Convolution; Deep learning; Extraction; Gaussian distribution; Image processing; Building extraction; Connection structures; Convolutional networks; Dense Connection; Gaussians; High resolution remote sensing imagery; Remote sensing images; Very high resolution; Remote sensing"
"IRSNET: An Inception-Resnet Feature Reconstruction Model for Building Segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078445301&doi=10.1007%2f978-3-030-36802-9_5&partnerID=40&md5=aa9a8932f022b5e27d2823f86b46ff3d","Effective analysis of remote sensing images remains a challenging topic in deep learning research field. This paper proposes a semantic segmentation approach based on the inception architecture. Specifically, the combination of residual network and Inception improves the feature extraction capability of encoder network. In addition, this paper discusses the problem of unbalanced information allocation caused by concatenated operation in the up-sampling process of network models such as Unet, and the RCSE module is proposed to complete feature reconstruction to solve this problem. The approach ensures accurate assignment of semantic labels to the buildings in the aerial images. Experiments based on the dataset proposed by CrowdAi certify the effectiveness of our approach with a 3.7% IoU improvement compared to Unet. © Springer Nature Switzerland AG 2019.","Aerial image; Building extraction; Convolution network; Semantic segmentation","Antennas; Deep learning; Extraction; Remote sensing; Semantics; Aerial images; Building extraction; Effective analysis; Extraction capability; Feature reconstruction; Remote sensing images; Research fields; Semantic segmentation; Image segmentation"
"Conditional generative adversarial networks for data augmentation and adaptation in remotely sensed imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075761560&doi=10.1117%2f12.2529586&partnerID=40&md5=2ba1b48d95dcb4e7c01da1a6b14c9827","The difficulty in obtaining labeled data relevant to a given task is among the most common and well-known practical obstacles to applying deep learning techniques to new or even slightly modified domains. The data volumes required by the current generation of supervised learning algorithms typically far exceed what a human needs to learn and complete a given task. We investigate ways to expand a given labeled corpus of remote sensed imagery into a larger corpus using Generative Adversarial Networks (GANs). We then measure how these additional synthetic data affect supervised machine learning performance on an object detection task. Our data driven strategy is to train GANs to (1) generate synthetic segmentation masks and (2) generate plausible synthetic remote sensing imagery corresponding to these segmentation masks. Run sequentially, these GANs allow the generation of synthetic remote sensing imagery complete with segmentation labels. We apply this strategy to the data set from ISPRS' 2D Semantic Labeling Contest-Potsdam, with a follow on vehicle detection task. We find that in scenarios with limited training data, augmenting the available data with such synthetically generated data can improve detector performance. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep Learning; Generative Adversarial Net-works; Object Detection; Remote sensing; Synthetic Data","Deep learning; Learning algorithms; Machine learning; Object detection; Object recognition; Semantics; Supervised learning; Adversarial networks; Detector performance; Limited training data; Remote sensed imagery; Remote sensing imagery; Remotely sensed imagery; Supervised machine learning; Synthetic data; Remote sensing"
"Ship classification in high-resolution SAR images via transfer learning with small training dataset","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059227497&doi=10.3390%2fs19010063&partnerID=40&md5=b25a874921b45cfdca5e87cb22fa8373","Synthetic aperture radar (SAR) as an all-weather method of the remote sensing, now it has been an important tool in oceanographic observations, object tracking, etc. Due to advances in neural networks (NN), researchers started to study SAR ship classification problems with deep learning (DL) in recent years. However, the limited labeled SAR ship data become a bottleneck to train a neural network. In this paper, convolutional neural networks (CNNs) are applied to ship classification by using SAR images with the small datasets. To solve the problem of over-fitting which often appeared in training small dataset, we proposed a new method of data augmentation and combined it with transfer learning. Based on experiments and tests, the performance is evaluated. The results show that the types of the ships can be classified in high accuracies and reveal the effectiveness of our proposed method. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks (CNNs); Deep learning (DL); Ship classification; Synthetic aperture radar (SAR)","Classification (of information); Convolution; Deep learning; Image classification; Neural networks; Radar imaging; Remote sensing; Ships; Convolutional neural network; Data augmentation; High-resolution SAR; Neural network (nn); Object Tracking; Ship classification; Small data set; Transfer learning; Synthetic aperture radar"
"Research on object detection algorithm based on PVANet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053257736&doi=10.1007%2f978-981-13-0341-8_13&partnerID=40&md5=91ac078e40b5eb10e2f6b7c538c6d2e2","Based on the research and development of remote sensing image target extraction technology, the deep learning framework of cascade principal component analysis network is used to study the sea surface vessel detection algorithm. The visible image of the sea surface vessel is the input, the suspected target area is determined by the significance test, the PVANet model is extracted from the suspected target area, and the result is input into the support vector machine to obtain the final classification result. The experimental results show that the designed algorithm can successfully output the results of the detection of the sea area in the airspace, and verify the efficiency and accuracy of the PVANet model by comparing with the CNN algorithm. It proves the superiority of the PVANet model in feature extraction. © 2019, Springer Nature Singapore Pte Ltd.","Deep but lightweight neural networks; Deep learning; Significance detection; Vessel detection","Extraction; Object detection; Principal component analysis; Remote sensing; Signal detection; Surface waters; Classification results; Learning frameworks; Object detection algorithms; Remote sensing images; Research and development; Significance test; Target extraction; Vessel detection; Deep learning"
"Image and Signal Processing for Remote Sensing XXV","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078131307&partnerID=40&md5=7428f396d9a4ad45df142449eb2997e3","The proceedings contain 70 papers. The topics discussed include: finer scale mapping with super resolved GF-4 satellite images; super-resolution imaging design for CX6-02 micro-satellite; optimization of unsupervised affinity propagation clustering method; road and railway detection in SAR images using deep learning; remote sensing and machine learning for tree detection and classi?cation in forestry applications; on-line learning tracking-by-segmentation of spacecraft observed by satellite-based imaging system; and GWENN-SS: a simple semi-supervised nearest-neighbor density-based classi?cation method with application to hyperspectral images.",,
"Object detectionin of remote sensing images based on convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063691294&doi=10.3788%2fLOP56.051002&partnerID=40&md5=e308f86f962742ed5293bcffbdd14767","Aiming at the problem of object detection in remote sensing images, the Fastcr-Rcnn network based on the convolutional neural network models is used to extract the features of the object area. An object detection datasct containing three kinds of common targets in remote sensing images is made to train this network. In addition, in order to solve the problem of large rotation angle of remote sensing images, a target detection model with a rotation invariancc self-learning ability is proposed, which integrates the spatial transformation network into the Faster R-CNN framework. By the analysis and comparison with the traditional object detection methods, the true effects of object detection in remote sensing images by different methods arc explored. The features extracted by the convolutional neural networks based on the spatial transformation networks possess stronger orientation robustness than those by the traditional methods, which makes it possible to obtain a high detection precision. © Universitat zu Koln. All rights reserved.","Convolutional neural networks; Deep learning; Image processing; Object detection; Spatial transformation networks",
"Broad area target search system for ship detection via deep convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071987892&doi=10.3390%2frs11171965&partnerID=40&md5=e5e5c6a5a30b1f32519ac699d4585153","High-resolution optical remote sensing data can be utilized to investigate the human behavior and the activities of artificial targets, for example ship detection on the sea. Recently, the deep convolutional neural network (DCNN) in the field of deep learning is widely used in image processing, especially in target detection tasks. Therefore, a complete processing system called the broad area target search (BATS) is proposed based on DCNN in this paper, which contains data import, processing and storage steps. In this system, aiming at the problem of onshore false alarms, a method named as Mask-Faster R-CNN is proposed to differentiate the target and non-target areas by introducing a semantic segmentation sub network into the Faster R-CNN. In addition, we propose a DCNN framework named as Saliency-Faster R-CNN to deal with the problem of multi-scale ships detection, which solves the problem of missing detection caused by the inconsistency between large-scale targets and training samples. Based on these DCNN-based methods, the BATS system is tested to verify that our system can integrate different ship detection methods to effectively solve the problems that existed in the ship detection task. Furthermore, our system provides an interface for users, as a data-driven learning, to optimize the DCNN-based methods. © 2019 by the authors.","Convolutional neural network; Deep learning; Optical remote sensing imagery; Ship detection","Behavioral research; Convolution; Data handling; Deep learning; Digital storage; Image processing; Neural networks; Optical data processing; Remote sensing; Search engines; Semantics; Ships; Artificial targets; Convolutional neural network; Optical remote sensing data; Optical remote-sensing imagery; Processing systems; Semantic segmentation; Ship detection; Target and non targets; Deep neural networks"
"Geospatial object detection using deep networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075035411&doi=10.1117%2f12.2530027&partnerID=40&md5=98e5bcfb7c528b6765e8ea76ea809fa7","In the last decade, deep learning has been drawing a huge interest due to the developments in the computational hardware and novel machine learning techniques. This progress also significantly effects satellite image analysis for various objectives, such as disaster and crisis management, forest cover, road mapping, city planning and even military purposes. For all these applications, detection of geospatial objects has crucial importance and some recent object detection techniques are still unexplored to be applied for satellite imagery. In this study, aircraft, building, and ship detection in 4-band remote sensing images by using convolutional neural networks based on popular YOLO network is examined and the accuracy comparison between 4-band and 3-band images are tested. Based on simulation results, it can be concluded that state-of-the-art object detectors can be utilized for geospatial objection detection purposes. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Convolutional Neural Network; Multiband Satellite Images; Object Detection; Remote Sensing; YOLO","Aircraft accidents; Aircraft detection; Convolution; Deep learning; Mapping; Military photography; Neural networks; Object recognition; Remote sensing; Satellite imagery; Accuracy comparisons; Convolutional neural network; Geo-spatial objects; Machine learning techniques; Remote sensing images; Satellite image analysis; Satellite images; YOLO; Object detection"
"Mapping plastic mulched farmland for high resolution images of unmanned aerial vehicle using deep semantic segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071987069&doi=10.3390%2frs11172008&partnerID=40&md5=c59d65a12310ce887c39ee5ec4233bb2","With increasing consumption, plastic mulch benefits agriculture by promoting crop quality and yield, but the environmental and soil pollution is becoming increasingly serious. Therefore, research on the monitoring of plastic mulched farmland (PMF) has received increasing attention. Plastic mulched farmland in unmanned aerial vehicle (UAV) remote images due to the high resolution, shows a prominent spatial pattern, which brings difficulties to the task of monitoring PMF. In this paper, through a comparison between two deep semantic segmentation methods, SegNet and fully convolutional networks (FCN), and a traditional classification method, Support Vector Machine (SVM), we propose an end-to-end deep-learning method aimed at accurately recognizing PMF for UAV remote sensing images from Hetao Irrigation District, Inner Mongolia, China. After experiments with single-band, three-band and six-band image data, we found that deep semantic segmentation models built via single-band data which only use the texture pattern of PMF can identify it well; for example, SegNet reaching the highest accuracy of 88.68% in a 900 nm band. Furthermore, with three visual bands and six-band data (3 visible bands and 3 near-infrared bands), deep semantic segmentation models combining the texture and spectral features further improve the accuracy of PMF identification, whereas six-band data obtains an optimal performance for FCN and SegNet. In addition, deep semantic segmentation methods, FCN and SegNet, due to their strong feature extraction capability and direct pixel classification, clearly outperform the traditional SVM method in precision and speed. Among three classification methods, SegNet model built on three-band and six-band data obtains the optimal average accuracy of 89.62% and 90.6%, respectively. Therefore, the proposed deep semantic segmentation model, when tested against the traditional classification method, provides a promising path for mapping PMF in UAV remote sensing images. © 2019 by the authors.","Deep semantic segmentation; Fully convolutional networks; Plastic mulched farmland; Unmanned aerial vehicle remote sensing image","Antennas; Convolution; Deep learning; Farms; Infrared devices; Mapping; Remote sensing; Semantics; Soil pollution; Space optics; Support vector machines; Textures; Unmanned aerial vehicles (UAV); Classification methods; Convolutional networks; Extraction capability; Hetao irrigation districts; High resolution image; Pixel classification; Remote sensing images; Semantic segmentation; Image segmentation"
"Empowering change vector analysis with autoencoding in bi-temporal hyperspectral images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073874953&partnerID=40&md5=76b22f4a7441657c7738e36140ad8efe","Modern sensor technologies are capable of covering large surfaces of the Earth with exceptional spatial, spectral, and temporal resolutions. With the significant development of hyperspectral imaging technology, change detection, which discovers change knowledge about Earth surface, has emerged as a hot topic in remote sensing. In this paper, we propose a deep learning enhanced change detection methodology that leverages the power of traditional change vector analysis techniques by gaining in accuracy with autoencoding neural networks and clustering. Preliminary experiments performed evaluating the proposed methodology with bechmark data provide encouraging results, also when compared to recent state-of-the-art change vector analysis competitors. Copyright © 2019 for this paper by its authors.",,"Hyperspectral imaging; Machine learning; Observatories; Remote sensing; Spectroscopy; Change detection; Change vector analysis; Earth surface; Hyperspectral imaging technologies; Large surfaces; Recent state; Sensor technologies; Temporal resolution; Deep learning"
"High-Performance SAR Automatic Target Recognition under Limited Data Condition Based on a Deep Feature Fusion Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077627793&doi=10.1109%2fACCESS.2019.2952928&partnerID=40&md5=8d4712adcddfcf3ee226e4afa5224089","Deep learning algorithms have been widely applied to the recognition of remote-sensing images due to their excellent performance on various recognition problems with sufficient data. However, limited data on synthetic aperture radar (SAR) images degrade the performance of neural networks for SAR automatic target recognition (ATR). To address this problem, this paper presents a new deep feature fusion framework by combining the Gabor features and information of raw SAR images and fusing the feature vectors extracted from different layers in our proposed neural network. Gabor features improve the richness of SAR image features. The number of free parameters of neural networks is largely reduced by utilizing large-scale convolutional kernel factorization and global average pooling. Moreover, the fusion of feature vectors from different layers helps improve the recognition performance of neural networks. Experimental results on the MSTAR dataset demonstrate the effectiveness of our proposed method. The proposed neural network can achieve an average accuracy of 99% on the classification of ten-class targets and even achieve a high recognition accuracy on limited data and noisy data. © 2013 IEEE.","Automatic target recognition; deep convolutional networks; deep feature fusion; Gabor features; synthetic aperture radar","Automatic target recognition; Convolution; Deep learning; Image enhancement; Multilayer neural networks; Radar imaging; Remote sensing; Synthetic aperture radar; Convolutional kernel; Convolutional networks; Feature fusion; Fusion of features; Gabor feature; Recognition accuracy; Remote sensing images; Synthetic aperture radar (SAR) images; Radar target recognition"
"Assessment of sustainable development goals achieving with use of NEXUS approach in the framework of GEOEssential ERA-PLANET project","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051822777&doi=10.1007%2f978-3-319-97885-7_15&partnerID=40&md5=3255ec703a0487524617b402d0b7d7f7","In this paper, we propose methodology for calculating indicators of sustainable development goals within the GEOEssential project, that is a part of ERA-PLANET Horizon 2020 project. We consider indicators 15.1.1 Forest area as proportion of total land area, 15.3.1 Proportion of land that is degraded over total land area, and 2.4.1. Proportion of agricultural area under productive and sustainable agriculture. For this, we used remote sensing data, weather and climatic models’ data and in-situ data. Accurate land cover maps are important for precisely land cover changes assessment. To improve the resolution and quality of existing global land cover maps, we proposed our own deep learning methodology for country level land cover providing. For calculating essential variables, that are vital for achieving indicators, NEXUS approach based on idea of fusion food, energy, and water was applied. Long-term land cover change maps connected with land productivity maps are essential for determining environment changes and estimation of consequences of anthropogenic activity. © 2019, Springer Nature Switzerland AG.","Classification maps; ERA-PLANET; Essential variables; GEOEssential","Agriculture; Deep learning; Information analysis; Planning; Remote sensing; Anthropogenic activity; Classification maps; Essential variables; GEOEssential; Indicators of sustainable development; Land productivities; Remote sensing data; Sustainable agriculture; Sustainable development"
"Semantic segmentation of multisensor remote sensing imagery with deep ConvNets and higher-order conditional random fields","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062614142&doi=10.1117%2f1.JRS.13.016501&partnerID=40&md5=8b5e80ccb8bdf5400d901a5b92ed5c60","Aerial images acquired by multiple sensors provide comprehensive and diverse information of materials and objects within a surveyed area. The current use of pretrained deep convolutional neural networks (DCNNs) is usually constrained to three-band images (i.e., RGB) obtained from a single optical sensor. Additional spectral bands from a multiple sensor setup introduce challenges for the use of DCNN. We fuse the RGB feature information obtained from a deep learning framework with light detection and ranging (LiDAR) features to obtain semantic labeling. Specifically, we propose a decision-level multisensor fusion technique for semantic labeling of the very-high-resolution optical imagery and LiDAR data. Our approach first obtains initial probabilistic predictions from two different sources: one from a pretrained neural network fine-tuned on a three-band optical image, and another from a probabilistic classifier trained on LiDAR data. These two predictions are then combined as the unary potential using a higher-order conditional random field (CRF) framework, which resolves fusion ambiguities by exploiting the spatialcontextual information. We utilize graph cut to efficiently infer the final semantic labeling for our proposed higher-order CRF framework. Experiments performed on three benchmarking multisensor datasets demonstrate the performance advantages of our proposed method. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Conditional random fields; Deep convolutional neural networks; Light detection and ranging; Multisensor remote sensing; Semantic segmentation","Antennas; Benchmarking; Convolution; Deep neural networks; Geometrical optics; Graphic methods; Image segmentation; Neural networks; Random processes; Remote sensing; Semantics; Conditional random field; Convolutional neural network; Light detection and ranging; Multisensor remote sensing; Semantic segmentation; Optical radar"
"Pixel level smoke detection model with deep neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078104872&doi=10.1117%2f12.2532562&partnerID=40&md5=bc411dfb19bb128c333ab75868fedd42","Traditional analysis of smoke extent from satellite imagery relies largely on spectral analysis using multispectral data thereby requiring large data volumes or subjective and time-consuming evaluation. These methods are not scalable to observing capabilities of the new generation of remote sensing platforms. We propose an automated, deep learning based detection model capable of identifying smoke plumes from shortwave reflectance for the Geostationary Operational Environmental Satellite R series of satellites. Hand-labelled, past instances of smoke plumes from the NOAA Hazard Mapping System, quality controlled for spatiotemporal accuracy by a subject matter expert, comprises the reference truth dataset. The detection pipeline comprises of pre-process, detection, and post-process stages. A Convolutional Neural Network (CNN), trained on smoke events with varying optical thicknesses and sun-satellite viewing geometry is used to predict the probability score for a given pixel containing smoke. The model is able to detect smoke over both low and high reflectance surfaces and discriminate smoke from clouds though challenges remain in identifying optically thin smoke. Finally, we discuss a web-based interface to visualize daily smoke prediction and analyze the predictions over time. © 2019 SPIE.","Convolutional neural network; Deep learning; Hazard; Machine learning; Remote sensing; Smoke detection; Smoke event","Convolution; Deep learning; Deep neural networks; Forecasting; Geostationary satellites; Hazards; Learning systems; Multimedia systems; Neural networks; Pixels; Quality control; Reflection; Remote sensing; Satellite imagery; Smoke detectors; Spectrum analysis; Convolutional neural network; Geostationary Operational Environmental Satellite R; Hazard mapping system; Remote sensing platforms; Smoke detection; Spatio-temporal accuracy; Subject matter experts; Web-based interface; Smoke"
"LiDAR Data Classification Using Spatial Transformation and CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054497035&doi=10.1109%2fLGRS.2018.2868378&partnerID=40&md5=ad2171b8c0f904a24a44356265873643","Light detection and ranging (LiDAR) is a useful data acquisition technique, which is widely used in a variety of practical applications. The classification of LiDAR-derived rasterized digital surface model (LiDAR-DSM) is a fundamental technique in LiDAR data processing. In recent years, deep learning methods, especially convolutional neural networks (CNNs), have shown their capability in remote sensing areas, including LiDAR data processing. Traditional deep models empirically use a fixed neighborhood system as input to the network. Therefore, the weight and height of the input rectangle may not be optimal. In order to modify such handcrafted setting, a spatial transformation network is used here to identify optimal inputs. The transformed inputs are fed into a well-designed CNN to obtain the final classification results. Furthermore, morphological profiles are combined with spatial transformation CNN to further improve the classification accuracy. The proposed frameworks are tested on two LiDAR-DSMs (i.e., the Recology and Houston data sets). The experimental results show that the proposed models provide competitive results compared to the state-of-the-art methods. Furthermore, the proposed optimal input identification approach can also be found beneficial for other remote sensing applications. © 2004-2012 IEEE.","Convolutional neural networks (CNNs); deep learning; feature extraction; light detection and ranging (LiDAR); morphological profile (MP); spatial transformation network (STN)","Convolution; Data acquisition; Data handling; Data mining; Deep learning; Extraction; Feature extraction; Gas generators; Metadata; Neural networks; Remote sensing; Tracking radar; Convolutional neural network; Light detection and ranging; Morphological profile; Shape; Spatial database; Spatial transformation; Optical radar; algorithm; artificial neural network; data acquisition; data processing; lidar; numerical method; remote sensing; satellite data; spatial analysis"
"AECNN: Autoencoder with Convolutional Neural Network for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076707161&doi=10.1007%2f978-981-15-1387-9_10&partnerID=40&md5=9d6fd5b3b922b11ee5af575eb326db78","This paper addresses an approach for classification of hyperspectral imagery (HSI). In remote sensing, the HSI sensor acquires hundreds of images with very narrow but continuous spectral width in visible and near-infrared regions of the electromagnetic (EM) spectrum. Due to the nature of data acquisition with contiguous bands, the HS images are very useful in classification and/or the identification of materials present in the captured geographical area. However, the low spatial resolution and large volume of HS images make the classification of those images as a challenging task. In the proposed approach, we use an autoencoder with convolutional neural network (AECNN) for classification of HS image. Pre-processing procedure with autoencoder leads to obtain optimized weights in the initial layer of CNN model. Moreover, features are enhanced in the HS images by utilizing the autoencoder. The CNN is used for efficient extraction of the features and same is also utilised for the classification of HS data. The potential of the proposed approach has been verified by conducting the experiments on three recent datasets. The experimental results are compared with the results obtained in the geoscience and remote sensing society (GRSS) Image Fusion Contest-2018 held at IEEE International Geoscience and Remote Sensing Symposium (IGARSS)-2018 and other existing frameworks for HSI classification. The testing accuracy of classification for the proposed approach is better than that of the other existing deep learning based methods. © Springer Nature Singapore Pte Ltd, 2019.","Autoencoder; CNN; Feature extraction; Hyperspectral classification","Classification (of information); Computer vision; Convolution; Data acquisition; Deep learning; Extraction; Feature extraction; Geology; Image enhancement; Image fusion; Infrared devices; Neural networks; Remote sensing; Spectroscopy; Auto encoders; Convolutional neural network; Geographical area; Hyper-spectral classification; Hyperspectral imagery; Learning-based methods; Spatial resolution; Visible and near infrared; Image classification"
"ERISNet: Deep neural network for sargassum detection along the coastline of the mexican caribbean","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074449574&doi=10.7717%2fpeerj.6842&partnerID=40&md5=093592f6e5c1f213f0ef128a3f910d4e","Recently, Caribbean coasts have experienced atypical massive arrivals of pelagic Sargassum with negative consequences both ecologically and economically. Based on deep learning techniques, this study proposes a novel algorithm for floating and accumulated pelagic Sargassum detection along the coastline of Quintana Roo, Mexico. Using convolutional and recurrent neural networks architectures, a deep neural network (named ERISNet) was designed specifically to detect these macroalgae along the coastline through remote sensing support. A new dataset which includes pixel values with and without Sargassum was built to train and test ERISNet. Aqua-MODIS imagery was used to build the dataset. After the learning process, the designed algorithm achieves a 90% of probability in its classification skills. ERISNet provides a novel insight to detect accurately algal blooms arrivals. © Copyright 2019 Arellano-Verdejo et al.","Algal blooms; Deep learning; Mexico; Modis; Neural networks; Remote sensing; Sargassum","algal bloom; algorithm; article; Caribbean; deep learning; deep neural network; human; human experiment; macroalga; nonhuman; probability; Quintana Roo; recurrent neural network; Sargassum; satellite imagery; skill"
"Unsupervised change-detection based on convolutional-autoencoder feature extraction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078109258&doi=10.1117%2f12.2533812&partnerID=40&md5=c3f4194f31b6dff611223b141c872b46","Rapid identification of areas affected by changes is a challenging task in many remote sensing applications. Sentinel-1 (S1) images provided by the European Space Agency (ESA) can be used to monitor such situations due to its high temporal and spatial resolution and indifference to weather. Though a number of deep learning based methods have been proposed in the literature for change detection (CD) in multi-temporal SAR images, most of them require labeled training data. Collecting sufficient labeled multi-temporal data is not trivial, however S1 provides abundant unlabeled data. To this end, we propose a solution for CD in multi-temporal S1 images based on unsupervised training of deep neural networks (DNNs). Unlabeled single-time image patches are used to train a multilayer convolutional-autoencoder (CAE) in unsupervised fashion by minimizing the reconstruction error between the reconstructed output and the input. The trained multilayer CAE is used to extract multi-scale features from both the pre and post change images that are analyzed for CD. The multi-scale features are fused according to a detail-preserving scale-driven approach that allows us to generate change maps by preserving details. The experiments conducted on a S1 dataset from Brumadinho, Brazil confirms the effectiveness of the proposed method. © 2019 SPIE.","Autoencoder; Change-detection; Convolutional-autoencoder; Convolutional-Neural-Network; Deep learning; Unsupervised change-detection; Unsupervised Learning","Convolution; Deep learning; Deep neural networks; Feature extraction; Multilayers; Neural networks; Remote sensing; Space optics; Synthetic aperture radar; Unsupervised learning; Auto encoders; Change detection; Convolutional neural network; European Space Agency; Learning-based methods; Multi-temporal SAR images; Remote sensing applications; Unsupervised change detection; Image processing"
"Super-resolution restoration of spaceborne HD videos using the UCL MAGiGAN system","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078198191&doi=10.1117%2f12.2532889&partnerID=40&md5=828817e6907aad33d8482ac8726058cf","We developed a novel SRR system, called Multi-Angle Gotcha image restoration with Generative Adversarial Network (MAGiGAN), to produce resolution enhancement of 3-5 times from multi-pass EO images. The MAGiGAN SRR system uses a combination of photogrammetric and machine vision approaches including image segmentation and shadow labelling, feature matching and densification, estimation of an image degradation model, and deep learning approaches, to retrieve image information from distorted features and training networks. We have tested the MAGiGAN SRR using the NVIDIA® Jetson TX-2 GPU card for onboard processing within a smart-satellite capturing high definition satellite videos, which will enable many innovative remote-sensing applications to be implemented in the future. In this paper, we show SRR processing results from a Planet® SkySat HD 70cm spaceborne video using a GPU version of the MAGiGAN system. Image quality and effective resolution enhancement are measured and discussed. © 2019 SPIE.","Earth Observation; Generative Adversarial Network; MAGiGAN; Multi-angle; Planet® SkySat® HD Video; Super-Resolution Restoration","Deep learning; Digital television; Earth (planet); Graphics processing unit; Image enhancement; Image segmentation; Optical resolving power; Remote sensing; Restoration; Video signal processing; Adversarial networks; Earth observations; HD videos; MAGiGAN; Multi angle; Super-resolution restoration; Image reconstruction"
"SmokeNet: Satellite smoke scene detection using convolutional neural network with spatial and channel-wise attention","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071533983&doi=10.3390%2frs11141702&partnerID=40&md5=6e62534e3b96befa9c91d94dcd1bf069","A variety of environmental analysis applications have been advanced by the use of satellite remote sensing. Smoke detection based on satellite imagery is imperative for wildfire detection and monitoring. However, the commonly used smoke detection methods mainly focus on smoke discrimination from a few specific classes, which reduces their applicability in different regions of various classes. To this end, in this paper, we present a new large-scale satellite imagery smoke detection benchmark based on Moderate Resolution Imaging Spectroradiometer (MODIS) data, namely USTC_SmokeRS, consisting of 6225 satellite images from six classes (i.e., cloud, dust, haze, land, seaside, and smoke) and covering various areas/regions over the world. To build a baseline for smoke detection in satellite imagery, we evaluate several state-of-the-art deep learning-based image classification models. Moreover, we propose a new convolution neural network (CNN) model, SmokeNet, which incorporates spatial and channel-wise attention in CNN to enhance feature representation for scene classification. The experimental results of our method using different proportions (16%, 32%, 48%, and 64%) of training images reveal that our model outperforms other approaches with higher accuracy and Kappa coefficient. Specifically, the proposed SmokeNet model trained with 64% training images achieves the best accuracy of 92.75% and Kappa coefficient of 0.9130. The model trained with 16% training images can also improve the classification accuracy and Kappa coefficient by at least 4.99% and 0.06, respectively, over the state-of-the-art models. © 2019 by the authors.","Attention; CNN; Scene classification; Smoke detection; Wildfire","Convolution; Deep learning; Fires; Image enhancement; Neural networks; Radiometers; Remote sensing; Satellite imagery; Smoke detectors; Attention; Convolution neural network; Convolutional neural network; Moderate resolution imaging spectroradiometer datum; Satellite remote sensing; Scene classification; Smoke detection; Wildfire; Smoke"
"Fully Convolutional Networks for Multisource Building Extraction from an Open Aerial and Satellite Imagery Data Set","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052689796&doi=10.1109%2fTGRS.2018.2858817&partnerID=40&md5=8efd8be57d6b1c5942a7aa90a85dd2c6","The application of the convolutional neural network has shown to greatly improve the accuracy of building extraction from remote sensing imagery. In this paper, we created and made open a high-quality multisource data set for building detection, evaluated the accuracy obtained in most recent studies on the data set, demonstrated the use of our data set, and proposed a Siamese fully convolutional network model that obtained better segmentation accuracy. The building data set that we created contains not only aerial images but also satellite images covering 1000 km 2 with both raster labels and vector maps. The accuracy of applying the same methodology to our aerial data set outperformed several other open building data sets. On the aerial data set, we gave a thorough evaluation and comparison of most recent deep learning-based methods, and proposed a Siamese U-Net with shared weights in two branches, and original images and their down-sampled counterparts as inputs, which significantly improves the segmentation accuracy, especially for large buildings. For multisource building extraction, the generalization ability is further evaluated and extended by applying a radiometric augmentation strategy to transfer pretrained models on the aerial data set to the satellite data set. The designed experiments indicate our data set is accurate and can serve multiple purposes including building instance segmentation and change detection; our result shows the Siamese U-Net outperforms current building extraction methods and could provide valuable reference. © 1980-2012 IEEE.","Building extraction; deep learning; full convolutional network; remote sensing building data set","Antennas; Buildings; Convolution; Deep learning; Extraction; Image enhancement; Image resolution; Image segmentation; Neural networks; Quality control; Remote sensing; Satellite imagery; Satellites; Building extraction; Convolutional networks; Convolutional neural network; Data set; Generalization ability; Learning-based methods; Satellite broadcasting; Satellite imagery data; Data mining; accuracy assessment; aerial survey; artificial neural network; building; comparative study; data set; machine learning; radiometric method; remote sensing; satellite altimetry; satellite data; satellite imagery; segmentation"
"SAR-to-optical image translation based on conditional generative adversarial networks-optimization, opportunities and limits","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071977712&doi=10.3390%2frs11172067&partnerID=40&md5=8171f22fbc5bc8c3c9cab6b5ac7ecf26","Due to its all time capability, synthetic aperture radar (SAR) remote sensing plays an important role in Earth observation. The ability to interpret the data is limited, even for experts, as the human eye is not familiar to the impact of distance-dependent imaging, signal intensities detected in the radar spectrum as well as image characteristics related to speckle or steps of post-processing. This paper is concerned with machine learning for SAR-to-optical image-to-image translation in order to support the interpretation and analysis of original data. A conditional adversarial network is adopted and optimized in order to generate alternative SAR image representations based on the combination of SAR images (starting point) and optical images (reference) for training. Following this strategy, the focus is set on the value of empirical knowledge for initialization, the impact of results on follow-up applications, and the discussion of opportunities/drawbacks related to this application of deep learning. Case study results are shown for high resolution (SAR: TerraSAR-X, optical: ALOS PRISM) and low resolution (Sentinel-1 and -2) data. The properties of the alternative image representation are evaluated based on feedback from experts in SAR remote sensing and the impact on road extraction as an example for follow-up applications. The results provide the basis to explain fundamental limitations affecting the SAR-to-optical image translation idea but also indicate benefits from alternative SAR image representations. © 2019 by the authors.","Deep learning; Generative adversarial networks; Interpretation; Synthetic aperture radar (SAR)","Deep learning; Feature extraction; Geometrical optics; Image processing; Remote sensing; Synthetic aperture radar; Adversarial networks; Earth observations; Empirical knowledge; Fundamental limitations; Image characteristics; Image representations; Interpretation; Signal intensities; Radar imaging"
"Arctic vegetation mapping using unsupervised training datasets and convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059940275&doi=10.3390%2frs11010069&partnerID=40&md5=8aa8db8305643771f64e90394f440c0f","Land cover datasets are essential for modeling and analysis of Arctic ecosystem structure and function and for understanding land-atmosphere interactions at high spatial resolutions. However, most Arctic land cover products are generated at a coarse resolution, often limited due to cloud cover, polar darkness, and poor availability of high-resolution imagery. A multi-sensor remote sensing-based deep learning approach was developed for generating high-resolution (5 m) vegetation maps for the western Alaskan Arctic on the Seward Peninsula, Alaska. The fusion of hyperspectral, multispectral, and terrain datasets was performed using unsupervised and supervised classification techniques over a ~343 km 2 area, and a high-resolution (5 m) vegetation classification map was generated. An unsupervised technique was developed to classify high-dimensional remote sensing datasets into cohesive clusters. We employed a quantitative method to add supervision to the unlabeled clusters, producing a fully labeled vegetation map. We then developed convolutional neural networks (CNNs) using the multi-sensor fusion datasets to map vegetation distributions using the original classes and the classes produced by the unsupervised classification method. To validate the resulting CNN maps, vegetation observations were collected at 30 field plots during the summer of 2016, and the resulting vegetation products developed were evaluated against them for accuracy. Our analysis indicates the CNN models based on the labels produced by the unsupervised classification method provided the most accurate mapping of vegetation types, increasing the validation score (i.e., precision) from 0.53 to 0.83 when evaluated against field vegetation observations. © 2019 by the authors.","Arctic; Convolutional neural network; Field-scale mapping; Hyperspectral; Vegetation classification","Convolution; Deep learning; Mapping; Neural networks; Remote sensing; Vegetation; Arctic; Convolutional neural network; Field scale; HyperSpectral; Vegetation classification; Classification (of information)"
"Spectral-Spatial Hyperspectral Unmixing Using Multitask Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077694484&doi=10.1109%2fACCESS.2019.2944072&partnerID=40&md5=68dd5b5bafb1b88ff0183f282da95d52","Hyperspectral unmixing is an important and challenging task in the field of remote sensing which arises when the spatial resolution of sensors is insufficient for the separation of spectrally distinct materials. Hyperspectral images, like other natural images, have highly correlated pixels and it is very desirable to make use of this spatial information. In this paper, a deep learning based method for blind hyperspectral unmixing is presented. The method uses multitask learning through multiple parallel autoencoders to unmix a neighborhood of pixels simultaneously. Operating on image patches instead of single pixels enables the method to take advantage of spatial information in the hyperspectral image. The method is the first in its class to directly utilize the spatial structure of hyperspectral images (HSIs) for the estimation of the spectral signatures of endmembers in the data cube. We evaluate the proposed method using two real HSIs and compare it to seven state-of-The-Art methods that either rely only on spectral or both on spectral and spatial information in the HSIs. The proposed method outperforms all the baseline unmixing methods in experiments. © 2013 IEEE.","autoencoder; deep learning; Hyperspectral; multitask learning; unmixing","Pixels; Remote sensing; Spectroscopy; Auto encoders; HyperSpectral; Hyperspectral unmixing; Learning-based methods; Multitask learning; Spatial informations; State-of-the-art methods; Unmixing; Deep learning"
"Evaluation of transfer learning techniques with convolutional neural networks (CNNs) to detect the existence of roads in high-resolution aerial imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076276032&doi=10.1007%2f978-3-030-32475-9_14&partnerID=40&md5=61f0a9ddfe53fb2cd4bd3006dc2bba2a","Infrastructure detection and monitoring traditionally required manual identification of geospatial objects in aerial imagery but advances in deep learning and computer vision enabled the researchers in the field of remote sensing to successfully apply transfer learning from pretrained models on large-scale datasets for the task of geospatial object detection. However, they mostly focused on objects with clearly defined boundaries that are independent of the background (e.g. airports, airplanes, buildings, ships, etc.). What happens when we have to deal with more complicated, continuous objects like roads? In this paper we will review four of the best-known CNN architectures (VGGNet, Inception-V3, Xception, Inception-ResNet) and apply feature extraction and fine-tuning techniques to detect the existence of roads in aerial orthoimages divided in tiles of 256 × 256 pixels in size. We will evaluate each model´s performance on unseen test data using the accuracy metric and compare the results with those obtained by a CNN especially built for this purpose. © Springer Nature Switzerland AG 2019.","Convolutional neural networks; Remote sensing; Road detection; Transfer learning","Aerial photography; Antennas; Building materials; Convolution; Deep learning; Feature extraction; Large dataset; Neural networks; Remote sensing; Continuous objects; Convolutional neural network; Geo-spatial objects; High resolution aerial imagery; Large-scale datasets; Manual identification; Road detection; Transfer learning; Object detection"
"Classification of land cover images using modified water wave Optimization-based hybrid classifier","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071984102&doi=10.1080%2f1206212X.2019.1660835&partnerID=40&md5=819b2f932f5948913a0260055ffe0b82","The inadequate land cover data have predominantly limited the significance and impacts of land cover. Even though remote sensing or satellite imaging has been deployed in mapping several temporal and spatial scales, but its full effort was not yet recognized. Hence, this paper intends to implement a novel land cover classification model through optimal deep learning architecture. Here, it includes three main phases namely, (i) Segmentation (ii) Feature Extraction and (iii) Classification. Initially, the land cover image is segmented, and subjected for feature extraction process. For feature extraction, Vegetation Indexes (VI), such as Simple Ratio (SR), Normalized Difference Vegetation Index (NDVI) and Kauth–Thomas Tasseled Cap, are extracted. Furthermore, these features are classified using Neural Network (NN) and Convolutional Neural Network (CNN). In both the classifiers, the number of hidden neuron is optimized by modified Water wave Optimization (WWO) called Modified Propagation Update based WWO (MPU-WWO). The optimization of hidden neurons is done in such a way that the classification accuracy should be high, which is taken as the main objective. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","convolutional neural network; Land cover image; neural network; vegetation indexes; WWO algorithm","Backpropagation; Convolution; Deep learning; Extraction; Feature extraction; Image classification; Remote sensing; Vegetation; Water waves; Classification accuracy; Convolutional neural network; Land cover; Land cover classification; Normalized difference vegetation index; Number of hidden neurons; Temporal and spatial scale; Vegetation index; Neural networks"
"Monitoring spatial and temporal variation of water quality parameters using time series of open multispectral data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073901815&doi=10.1117%2f12.2533708&partnerID=40&md5=8f45dfaf3772635cc4dd53231342b506","Water bodies are among most sensitive ecological environments. In order to ensure good water quality and establish framework for their protection European Parliament was adopted the Water Framework Directive (WFD) (Directive 2000/60/EC). The biological, hydro morphological and physic chemical quality parameters which are relevant for assessment of ecological status of water body are defined in Annex V of WFD. Traditionally, quality of surface water bodies are monitored by in situ measurements resulting in low spatial and temporal resolution of historical data. Remote sensing has great potential for monitoring and identification of water bodies over large scale regions in a more effective and efficient manner. In order to provide reliable monitoring of water quality, surface reflection derived by multispectral sensors need to be integrated with in situ measurements. Relationship between remote sensing and in situ data is usually modeled by using empirical, machine learning or deep learning algorithms. In this study, a 4-year (2013-2016) result of in situ monitoring of surface water bodies in Serbia are used for calibration and validation of algorithm for water quality monitoring based on Landsat 8 satellite image. The Turbidity, Suspending Sediments, Total Phosphorus and Total Nitrogen (physic chemical parameters) in region of Vojvodina, Republic of Serbia are monitored. The Neuron Networks and Supported Vector Machine are used to analyzing correlation between in situ measurements and Landsat 8 atmospherically corrected satellite images. Feature more, capabilities of Landsat 8 are compared with Sentinel 2 images (2-years, 2015-2016). In situ data are provided by Agency for environment protection of Serbia. © 2019 SPIE. Downloading of the abstract is permitted for personal use only.","Landsat 8; Neural Network; Sentienl-2; Supported Vector Machine; water quality parameters","Deep learning; Ecology; Environmental regulations; Learning algorithms; Machine learning; Neural networks; Open Data; Remote sensing; Water conservation; Water quality; Calibration and validations; LANDSAT; Sentienl-2; Spatial and temporal resolutions; Spatial and temporal variation; Supported vector machines; Water Framework Directives; Water quality parameters; Surface waters"
"Detection of forest disaster using satellite images with semantic segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078138610&doi=10.1117%2f12.2532990&partnerID=40&md5=5dfd5baca30a68852aabb7947b439e07","In recent 10 years, forest damage caused by forest fires in Korea has increased significantly compared to previous years. Therefore, interest and concern about damage caused by forest fires are very important in terms of environmental and ecosystem. According to various domestic and international research results, forests perform functions such as reporting of life resources, prevention of desertification, and adjustment of micro climate. There are many studies to extract the damage areas based on hyper spectral aerial image, high resolution satellite image, vegetation index and factors affecting the forest environment. However, there are limitations that the indexes have different threshold values depending on the region and season, and the threshold value must be continuously adjusted in order to detect the concentration of the damage areas. In this study, we detected forest disaster damaged areas through satellite image data and deep learning. We collected image data on Landsat satellite and applied to the detection of damaged area using U-net [1] and SegNet [2] models. We tried to verify the applicability of semantic segmentation for remote sensing, compare and evaluate each model, and build an optimal forest disaster detection model. © 2019 SPIE.","Forest disaster; Remote sensing; Satellite images; Semantic segmentation","Antennas; Deep learning; Deforestation; Disasters; Fire hazards; Fires; Remote sensing; Satellites; Semantics; Detection models; Forest environments; High resolution satellite images; International researches; Landsat satellite; Satellite image datas; Satellite images; Semantic segmentation; Image segmentation"
"Improving transfer learning performance: An application in the classification of remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064810967&doi=10.5220%2f0007372201740183&partnerID=40&md5=ac57db8a878f76fc2446ce7b4774c409","The present paper aims to train and analyze Convolutional Neural Networks (CNN or ConvNets) capable of classifying plant species of a certain region for applications in an environmental monitoring system. In order to achieve this for a limited training dataset, the samples were expanded with the use of a data generator algorithm. Next, transfer learning and fine tuning methods were applied with pre-trained networks. With the purpose of choosing the best layers to be transferred, a statistical dispersion method was proposed. Through a distributed training method, the training speed and performance for the CNN in CPUs was improved. After tuning the parameters of interest in the resulting network by the cross-validation method, the learning capacity of the network was verified. The obtained results indicate an accuracy of about 97%, which was acquired transferring the pre-trained first seven convolutional layers of the VGG-16 network to a new sixteen-layer convolutional network in which the final training was performed. This represents an improvement over the state of the art, which had an accuracy of 91% on the same dataset. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Convolutional Neural Networks; Cross Validation; Data Augmentation; Deep Learning; Distributed Learning; Fine Tuning; Remote Sensing; Transfer Learning; Vegetation Monitoring","Convolution; Deep learning; Deep neural networks; Neural networks; Program processors; Remote sensing; Convolutional neural network; Cross validation; Data augmentation; Distributed learning; Fine tuning; Transfer learning; Vegetation monitoring; Network layers"
"Application research of there-dimensional liDAR in unmannedVehicle environment perception","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069481601&doi=10.3788%2fLOP56.130001&partnerID=40&md5=71ff4c928685fc4124ac5ea236a28202","The environmental perception of unmanned vehicles is a vital technology for automatic driving. The usage of three-dimensional (3D) LiDAR for obstacle detection becomes a popular research topic. In this paper, we first introduce the classification of obstacle detection methods for an unmanned vehicle according to different sensors. The basic principle of obstacle detection based on 3D LiDAR is then introduced in detail along with an analysis of the traditional method of obstacle detection using 3D LiDAR. Deep learning is an important method for two-dimensional object detection and classification. We analyze the characteristics of the 3D LiDAR point clouds and the challenges of deep learning for point clouds. Finally, we analyze the research status and development trend of deep learning in the point cloud obstacle detection application and introduce relevant datasets in the field of automatic driving, such as KITTI and ApolloScape. © 2019 Universitat zu Koln. All rights reserved.","Automatic driving; Deep learning; Grid network; Obstacle detection; Remote sensing; Three-dimensional lidar",
"Retrieval of precipitation based on microwave sensor of satellite using deep learning and blending grid-based multi-satellite precipitation using EBMA","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078192861&doi=10.1117%2f12.2533138&partnerID=40&md5=146a5ccce7c43187cfb6d9084e689464","In this study, deep neural network is utilized as another approach for improving accuracy of the precipitation based on microwave-sensor. And The ensemble Bayesian model averaging(EBMA), which employs a weighting scheme for each member using posterior probability, in order to produce a more improved blending of precipitation from multi-satellite and to evaluate the effect of accuracy improvement. Experiments to improve rain rate were carried out based on data obtained from Global Precipitation Measurement (GPM) Microwave Imager (GMI). Input data for the DNN model include 7 brightness temperatures (Tb), ice water path (IWP), convective rain rate, scattering index (SI) and land sea mask is used. The experiment for blending of precipitation product was performed using rain rate product of three satellites and sensors, namely GMI of GPM core observatory, special sensor microwave imager/sounder (SSMI/S) of the Defense Meteorological Satellite Program (DMSP) F16 and microwave humidity sounder (MHS) of NOAA-18. In both experiments, precipitation product of the Dual-frequency Precipitation Radar (DPR) of CO was used as reference data. The probability density function(PDF) of gamma distribution combined with logistic regression is used to estimate the probability and quantity of precipitation for considering the characteristics of precipitation. And then, the exponent for these two functions and the percentile threshold of the cumulative density function were set by optimizing simulations. After that, the validation statistics of the blending precipitation through comparison with precipitation obtained from DPR is carried out. © 2019 SPIE.","Blending; Deep neural network; Ensemble Bayesian model averaging(EBMA); Microwave; Precipitation","Bayesian networks; Blending; Deep neural networks; Image enhancement; Microwaves; Precipitation (chemical); Probability density function; Probability distributions; Rain; Remote sensing; Satellites; Bayesian model averaging; Cumulative density functions; Defense Meteorological Satellite Programs; Dual-frequency precipitation radars; Global precipitation measurements; Probability density function (pdf); Satellite precipitation; Special sensor microwave imagers; Microwave sensors"
"Semantic enrichment of point cloud by automatic extraction and enhancement of 360° panoramas.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078123528&doi=10.5194%2fisprs-archives-XLII-2-W17-355-2019&partnerID=40&md5=0869afa9360afe907dc548462530a689","The raw nature of point clouds is an important challenge for their direct exploitation in architecture, engineering and construction applications. Particularly, their lack of semantics hinders their utility for automatic workflows (Poux, 2019). In addition, the volume and the irregularity of the structure of point clouds makes it difficult to directly and automatically classify datasets efficiently, especially when compared to the state-of-the art 2D raster classification. Recently, with the advances in deep learning models such as convolutional neural networks (CNNs), the performance of image-based classification of remote sensing scenes has improved considerably (Chen et al., 2018; Cheng et al., 2017). In this research, we examine a simple and innovative approach that represent large 3D point clouds through multiple 2D projections to leverage learning approaches based on 2D images. In other words, the approach in this study proposes an automatic process for extracting 360° panoramas, enhancing these to be able to leverage raster data to obtain domain-base semantic enrichment possibilities. Indeed, it is very important to obtain a rigorous characterization for use in the classification of a point cloud. Especially because there is a very large variety of 3D point cloud domain applications. In order to test the adequacy of the method and its potential for generalization, several tests were performed on different datasets. The developed semantic augmentation algorithm uses only the attributes X, Y, Z and camera positions as inputs. © Authors 2019.","3D Point cloud; Deep learning; Feature extraction; Image recognition; Point cloud representation; Semantic information","Classification (of information); Deep learning; Extraction; Feature extraction; Image recognition; Neural networks; Rasterization; Remote sensing; Semantics; 3D point cloud; Architecture , engineering and constructions; Convolutional neural network; Image-based classification; Innovative approaches; Point cloud; Semantic augmentations; Semantic information; Image enhancement"
"Detection of disaster affected regions based on change detection using deep architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063010149&partnerID=40&md5=03866a6e2f97ea77d8dfb26c3e3b6601","Natural disasters pose a serious threat to national economy, human lives and can disturb the social fabric of the society, although we can not entirely prevent these natural disasters from happening but with the advancements in the satellite imagery, remote sensing and machine learning it has become possible to minimize the damage caused by them. Satellite images are very useful because they can give you a huge amount of information from a single picture. Since it is becoming easy to get these satellite images the climate and environmental detection systems are in high demand. In this paper, we propose a post disaster system which we have named, Automatic Disaster Detection System (ADDS) which is designed to detect the disaster affected areas and help in the relief operations. The existing methods for detection of disaster affected regions are mostly dependent on manpower where people use the drone technology to see which area is affected by flying that drone over a large area which takes a lot of time. A new approach of Convolution Neural Network towards detection of disaster affected areas through their satellite images is examined in this paper which is comparatively better than previous image processing techniques. This method is based on deep learning which has been a widely popular technique for image processing in recent past. This technique can help save lives by reducing the response time and increasing the efficiency of the relief operations. © BEIESP.","Convolution neural network; Deep learning; Flood detection; Image processing; Machine learning; Remote sensing; Satellite imagery",
"Automatic Cloud Segmentation Based on Fused Fully Convolutional Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070695356&doi=10.1007%2f978-3-030-26763-6_50&partnerID=40&md5=021783ffbf95bcaacd7b900eefea8dc6","Cloud detection and segmentation of remote sensing images is a pivotal task in the area of weather forecast. Many meteorologic applications such as precipitation forecast, extreme weather forecast, etc., depend on the results of the cloud detection. In this paper, based on the satellite remote sensing image dataset, we propose an image segmentation model to address the cloud detection problem. Our model is derived from the fully convolutional neural network, which achieves pixel-level cloud segmentation results on high resolution, large scale, multi-channel satellite images. We introduce Deep Feature Aggregation and Model Fuse strategies to improve the cloud segmentation results. Compared with the traditional methods, our proposed algorithm has the advantages that is independent of the expert knowledge, totally data motivated, and more robust in hard cases. The testing results show that the proposed model can satisfy the requirements of the weather forecast, thus has a strong potential to be put into business usage. © 2019, Springer Nature Switzerland AG.","Cloud detection; Deep learning; FCN; Remote sensing images; Semantic segmentation","Computation theory; Convolution; Deep learning; Image segmentation; Intelligent computing; Neural networks; Precipitation (meteorology); Remote sensing; Semantics; Cloud detection; Convolutional networks; Convolutional neural network; Image segmentation model; Precipitation forecast; Remote sensing images; Satellite remote sensing; Semantic segmentation; Weather forecasting"
"Web-Net: A novel nest networks with ultra-hierarchical sampling for building extraction from aerial imageries","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071573053&doi=10.3390%2frs11161897&partnerID=40&md5=fd6125d6b2a669bb84625f392475b045","Abstract: How to efficiently utilize vast amounts of easily accessed aerial imageries is a critical challenge for researchers with the proliferation of high-resolution remote sensing sensors and platforms. Recently, the rapid development of deep neural networks (DNN) has been a focus in remote sensing, and the networks have achieved remarkable progress in image classification and segmentation tasks. However, the current DNN models inevitably lose the local cues during the downsampling operation. Additionally, even with skip connections, the upsampling methods cannot properly recover the structural information, such as the edge intersections, parallelism, and symmetry. In this paper, we propose the Web-Net, which is a nested network architecture with hierarchical dense connections, to handle these issues. We design the Ultra-Hierarchical Sampling (UHS) block to absorb and fuse the inter-level feature maps to propagate the feature maps among different levels. The position-wise downsampling/upsampling methods in the UHS iteratively change the shape of the inputs while preserving the number of their parameters, so that the low-level local cues and high-level semantic cues are properly preserved. We verify the effectiveness of the proposedWeb-Net in the Inria Aerial Dataset and WHU Dataset. The results of the proposedWeb-Net achieve an overall accuracy of 96.97% and an IoU (Intersection over Union) of 80.10% on the Inria Aerial Dataset, which surpasses the state-of-the-art SegNet 1.8% and 9.96%, respectively; the results on the WHU Dataset also support the effectiveness of the proposedWeb-Net. Additionally, benefitting from the nested network architecture and the UHS block, the extracted buildings on the prediction maps are obviously sharper and more accurately identified, and even the building areas that are covered by shadows can also be correctly extracted. The verified results indicate that the proposedWeb-Net is both effective and efficient for building extraction from high-resolution remote sensing images. © 2019 by the authors.","Building extraction; Deep learning; Remote sensing; Ultra-hierarchical sampling; Web-net","Aerial photography; Antennas; Buildings; Deep learning; Deep neural networks; Extraction; Image segmentation; Iterative methods; Object recognition; Remote sensing; Semantics; Signal sampling; Building extraction; Critical challenges; Hierarchical sampling; High level semantics; High resolution remote sensing; High resolution remote sensing images; Structural information; Web-net; Network architecture"
"A Novel Semi-Supervised Learning Method Based on Fast Search and Density Peaks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062348489&doi=10.1155%2f2019%2f6876173&partnerID=40&md5=c4429409eab1fd98c2b1cdca3eb92c9b","Radar image recognition is a hotspot in the field of remote sensing. Under the condition of sufficiently labeled samples, recognition algorithms can achieve good classification results. However, labeled samples are scarce and costly to obtain. Our major interest in this paper is how to use these unlabeled samples to improve the performance of a recognition algorithm in the case of limited labeled samples. This is a semi-supervised learning problem. However, unlike the existing semi-supervised learning methods, we do not use unlabeled samples directly and, instead, look for safe and reliable unlabeled samples before using them. In this paper, two new semi-supervised learning methods are proposed: a semi-supervised learning method based on fast search and density peaks (S2DP) and an iterative S2DP method (IS2DP). When the labeled samples satisfy a certain requirement, S2DP uses fast search and a density peak clustering method to detect reliable unlabeled samples based on the weighted kernel Fisher discriminant analysis (WKFDA). Then, a labeling method based on clustering information (LCI) is designed to label the unlabeled samples. When the labeled samples are insufficient, IS2DP is used to iteratively search for reliable unlabeled samples for semi-supervision. Then, these samples are added to the labeled samples to improve the recognition performance of S2DP. In the experiments, real radar images are used to verify the performance of our proposed algorithm in dealing with the scarcity of the labeled samples. In addition, our algorithm is compared against several semi-supervised deep learning methods with similar structures. Experimental results demonstrate that the proposed algorithm has better stability than these methods. © 2019 Fei Gao et al.",,"Cluster analysis; Deep learning; Discriminant analysis; Fisher information matrix; Image recognition; Iterative methods; Learning algorithms; Machine learning; Radar; Radar imaging; Remote sensing; Classification results; Clustering information; Clustering methods; Kernel fisher discriminant analysis; Recognition algorithm; Semi supervisions; Semi- supervised learning; Semi-supervised learning methods; Supervised learning"
"Hyperspectral image classification using similarity measurements-based deep recurrent neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060737559&doi=10.3390%2frs11020194&partnerID=40&md5=7ba141604d63c42faaea0db9d712b9ab","Classification is a common objective when analyzing hyperspectral images, where each pixel is assigned to a predefined label. Deep learning-based algorithms have been introduced in the remote-sensing community successfully in the past decade and have achieved significant performance improvements compared with conventional models. However, research on the extraction of sequential features utilizing a single image, instead of multi-temporal images still needs to be further investigated. In this paper, a novel strategy for constructing sequential features from a single image in long short-term memory (LSTM) is proposed. Two pixel-wise-based similarity measurements, including pixel-matching (PM) and block-matching (BM), are employed for the selection of sequence candidates from the whole image. Then, the sequential structure of a given pixel can be constructed as the input of LSTM by utilizing the first several matching pixels with high similarities. The resulting PM-based LSTM and BM-based LSTM are appealing, as all pixels in the whole image are taken into consideration when calculating the similarity. In addition, BM-based LSTM also utilizes local spectral-spatial information that has already shown its effectiveness in hyperspectral image classification. Two common distance measures, Euclidean distance and spectral angle mapping, are also investigated in this paper. Experiments with two benchmark hyperspectral images demonstrate that the proposed methods achieve marked improvements in classification performance relative to the other state-of-the-art methods considered. For instance, the highest overall accuracy achieved on the Pavia University image is 96.20% (using both BM-based LSTM and spectral angle mapping), which is an improvement compared with 84.45% overall accuracy generated by 1D convolutional neural networks. © 2019 by the authors.","Block matching; Deep learning; Hyperspectral image classification; Pixel matching; Recurrent neural network; Spatial similarity measurements","Benchmarking; Classification (of information); Deep learning; Deep neural networks; Hyperspectral imaging; Image classification; Image enhancement; Motion compensation; Photomapping; Pixels; Recurrent neural networks; Remote sensing; Spectroscopy; Block Matching; Classification performance; Convolutional neural network; Learning-based algorithms; Pixel matching; Similarity measurements; Spatial similarity; State-of-the-art methods; Long short-term memory"
"Designing deep CNN models based on sparse coding for aerial imagery: a deep-features reduction approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062424629&doi=10.1080%2f22797254.2019.1581582&partnerID=40&md5=b6a255f6b0d92c6e5d6d39ab53ba17c3","Traditional methods focus on low-level handcrafted features representations and it is difficult to design a comprehensive classification algorithm for remote sensing scene classification problems. Recently, convolutional neural networks (CNNs) have obtained remarkable performance outcomes, setting several remote sensing benchmarks. Furthermore, direct applications of UAV remote sensing images that use deep convolutional networks are extremely challenging given high input data dimensionality with relatively small amounts of available labelled data. We, therefore, propose a CNN approach to scene classification that architecturally incorporates sparse coding (SC) technique for dimension reduction to minimize overfitting. Outcomes were compared with principal component analysis (PCA) and global average pooling (GAP) alternatives that use fully connected layer(s) in pre-trained CNN architecture(s) to minimize overfitting. SC was used to encode deep features extracted from the last convolutional layer of pre-trained CNN models by using different features maps in which deep features had been converted into low-dimensional SC features. These same sparse-coded features were concatenated by means of different pooling techniques to obtain global image features for scene classification. The proposed algorithm outperformed current state-of-the-art algorithms based on handcrafted features. When using our own UAV-based dataset and existing datasets, it was also exceptionally efficient computationally when learning data representations, producing a 93.64% accuracy rate. © 2019, © 2019 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Deep Learning algorithms; feature extraction; Sparse Coding; Unmanned aerial vehicle (UAV)","Aerial photography; Antennas; Benchmarking; Codes (symbols); Convolution; Feature extraction; Learning algorithms; Neural networks; Principal component analysis; Remote sensing; Unmanned aerial vehicles (UAV); Classification algorithm; Convolutional networks; Convolutional neural network; Dimension reduction; Features reductions; Performance outcome; Scene classification; Sparse coding; Deep learning; artificial neural network; design method; image analysis; numerical method; principal component analysis; remote sensing; software; unmanned vehicle"
"Tiangong-2 Remote Sensing Application Conference on Technology, Method and Application","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058519321&partnerID=40&md5=4ff9d4af882e30a02ebd53b9b9337744","The proceedings contain 31 papers. The special focus in this conference is on Technology, Method and Application. The topics include: Image geometric correction parallelization of the Multi-Azimuth UV imager based on GPU; Cross-calibration of S-NPP/VIIRS and Tiangong-2/MAI visible channels using the SNO method; on-orbit polarization calibration of the multi-angle polarization imager based on sunglint over the ocean; a management and service approach for mass remote sensing data of Tiangong-2 space laboratory; downscaling of Tiangong-2 land surface temperature; Analysis of slope and NDVI effects on land surface temperature retrieval accuracy in mountain area based on WIS data of Tiangong-2; Radiometric cross-calibration between the thermal infrared data of MODIS and Tiangong-2 WIS for monitoring sea surface temperature; Cross-comparison of ocean color products derived from Tiangong-2/WIS and GOCI in the Yangtze estuary, China; design, performance and in-orbit evaluation results of Tiangong-2 wide-band imaging spectrometer; Sharpening the VNIR-SWIR-TIR bands of the WIS of Tiangong-2 for mapping land use and land cover; object-based classification from Tiangong-2 using support vector machine optimized with evolutionary algorithm; deep learning network integrated multi-spectral data and interferometric imaging radar altimeter data of Tiangong-2 for land use classification; comparison of land cover types classification methods using Tiangong-2 multispectral image; land use change monitoring in Angkor Wat based on Tiangong-2 wide band imaging data; identification of cotton using random forest based on wide-band imaging spectrometer data of Tiangong-2; drought monitoring using Tiangong-2 wide-band spectrometer data; temporal and spatial changes of the yellow river delta wetland based on multi-source data during 30 years.",,
"Esfnet: Efficient network for building extraction from high-resolution aerial images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065468716&doi=10.1109%2fACCESS.2019.2912822&partnerID=40&md5=f858e932a57bf09dc92254b9ea9177da","Building footprint extraction from high-resolution aerial images is always an essential part of urban dynamic monitoring, planning, and management. It has also been a challenging task in remote sensing research. In recent years, deep neural networks have made great achievement in improving the accuracy of building extraction from remote sensing imagery. However, most of the existing approaches usually require a large amount of parameters and floating point operations for high accuracy, it leads to high memory consumption and low inference speed which are harmful to research. In this paper, we proposed a novel efficient network named ESFNet which employs separable factorized residual block and utilizes the dilated convolutions, aiming to preserve slight accuracy loss with low computational cost and memory consumption. Our ESFNet obtains a better trade-off between accuracy and efficiency, it can run at over 100 FPS on single Tesla V100, requires 6x fewer FLOPs and has 18x fewer parameters than state-of-The-Art real-Time architecture ERFNet while preserving similar accuracy without any additional context module, post-processing and pre-Trained scheme. We evaluated our networks on WHU building dataset and compared it with other state-of-The-Art architectures. The result and comprehensive analysis show that our networks are benefit for efficient remote sensing researches, and the idea can be further extended to other areas. The code is publicly available at: https://github.com/mrluin/ESFNet-Pytorch © 2013 IEEE.","Building extraction; deep learning; efficient neural networks; remote sensing; semantic segmentation","Antennas; Buildings; Deep learning; Deep neural networks; Digital arithmetic; Economic and social effects; Extraction; Image enhancement; Memory architecture; Network architecture; Semantics; Building extraction; Comprehensive analysis; Computational costs; Floating point operations; High-resolution aerial images; Real-time architecture; Remote sensing imagery; Semantic segmentation; Remote sensing"
"Comparing deep neural networks, ensemble classifiers, and support vector machine algorithms for object-based urban land use/land cover classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071588103&doi=10.3390%2frs11141713&partnerID=40&md5=b91631048a5b7940590841733b43114a","With the advent of high-spatial resolution (HSR) satellite imagery, urban land use/land cover (LULC) mapping has become one of the most popular applications in remote sensing. Due to the importance of context information (e.g., size/shape/texture) for classifying urban LULC features, Geographic Object-Based Image Analysis (GEOBIA) techniques are commonly employed for mapping urban areas. Regardless of adopting a pixel- or object-based framework, the selection of a suitable classifier is of critical importance for urban mapping. The popularity of deep learning (DL) (or deep neural networks (DNNs)) for image classification has recently skyrocketed, but it is still arguable if, or to what extent, DL methods can outperform other state-of-the art ensemble and/or Support Vector Machines (SVM) algorithms in the context of urban LULC classification using GEOBIA. In this study, we carried out an experimental comparison among different architectures of DNNs (i.e., regular deep multilayer perceptron (MLP), regular autoencoder (RAE), sparse, autoencoder (SAE), variational autoencoder (AE), convolutional neural networks (CNN)), common ensemble algorithms (Random Forests (RF), Bagging Trees (BT), Gradient Boosting Trees (GB), and Extreme Gradient Boosting (XGB)), and SVM to investigate their potential for urban mapping using a GEOBIA approach. We tested the classifiers on two RS images (with spatial resolutions of 30 cm and 50 cm). Based on our experiments, we drew three main conclusions: First, we found that the MLP model was the most accurate classifier. Second, unsupervised pretraining with the use of autoencoders led to no improvement in the classification result. In addition, the small difference in the classification accuracies of MLP from those of other models like SVM, GB, and XGB classifiers demonstrated that other state-of-the-art machine learning classifiers are still versatile enough to handle mapping of complex landscapes. Finally, the experiments showed that the integration of CNN and GEOBIA could not lead to more accurate results than the other classifiers applied. © 2019 by the authors.","Deep learning; GEOBIA; High-spatial resolution imagery; Land use/cover classification; Remote sensing","Classification (of information); Decision trees; Deep learning; Deep neural networks; Forestry; Image resolution; Land use; Machine learning; Mapping; Multilayer neural networks; Remote sensing; Satellite imagery; Support vector machines; Convolutional neural network; Experimental comparison; GEOBIA; Geographic object-based image analysis; High spatial resolution imagery; Land use/cover; Support vector machine algorithm; Support vector machines algorithms; Image classification"
"Deep belief network for spectral–spatial classification of hyperspectral remote sensor data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059796976&doi=10.3390%2fs19010204&partnerID=40&md5=91a23be3231d6ec1b205b465590be6f8","With the development of high-resolution optical sensors, the classification of ground objects combined with multivariate optical sensors is a hot topic at present. Deep learning methods, such as convolutional neural networks, are applied to feature extraction and classification. In this work, a novel deep belief network (DBN) hyperspectral image classification method based on multivariate optical sensors and stacked by restricted Boltzmann machines is proposed. We introduced the DBN framework to classify spatial hyperspectral sensor data on the basis of DBN. Then, the improved method (combination of spectral and spatial information) was verified. After unsupervised pretraining and supervised fine-tuning, the DBN model could successfully learn features. Additionally, we added a logistic regression layer that could classify the hyperspectral images. Moreover, the proposed training method, which fuses spectral and spatial information, was tested over the Indian Pines and Pavia University datasets. The advantages of this method over traditional methods are as follows: (1) the network has deep structure and the ability of feature extraction is stronger than traditional classifiers; (2) experimental results indicate that our method outperforms traditional classification and other deep learning approaches. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Classification; Deep learning; Feature extraction; Hyperspectral image; Multi-sensor fusion; Remote sensors","Deep learning; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Neural networks; Optical sensors; Remote sensing; Spectroscopy; Convolutional neural network; Deep belief network (DBN); Feature extraction and classification; Hyperspectral sensors; Multi-sensor fusion; Remote sensors; Restricted boltzmann machine; Spatial classification; Classification (of information)"
"Unsupervised feature extraction based on improved Wasserstein generative adversarial network for hyperspectral classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072633405&doi=10.1117%2f12.2527466&partnerID=40&md5=b3c6e9c0bb97da686af10d3a04b7c048","Accurate classification is one of the most important prerequisites for hyperspectral applications and feature extraction is the key step of classification. Recently, deep learning models have been successfully used to extract the spectral-spatial features in hyperspectral images (HSIs). However, most deep learning-based classification methods are supervised and require sufficient samples to guarantee their performance. And the labeled samples in HSI are limited. To solve this problem, unsupervised feature extraction based on improved Wasserstein generative adversarial network (WGAN) is proposed in this paper. Further, in order to fully explore the spectral-spatial features in HSIs, a three-dimensional (3D) model is designed. Considering that its difficult for GANs to generate high dimension data, the dimension of HSI is reduced firstly by principle component analysis (PCA). Then, an improved WGAN is trained unsupervised with HSIs after dimensionality reduction to achieve the stable status. Finally, the discriminator of the trained improved WGAN is used as a feature extractor and followed by a classifier, which can be used to classify the HSIs. In order to evaluate the performance of the proposed method, PCA-based methods and GAN-based methods are compared in the experiment of a real-world HSI. Experimental results have shown that our proposed method gained more promising results which has great potential prospects in HSI classification. © 2019 SPIE.","Classification; Convolutional neural network (CNN); Deep learning; Feature extraction; Generative adversarial network (GAN); Remote sensing","3D modeling; Classification (of information); Deep learning; Deep neural networks; Extraction; Feature extraction; Neural networks; Remote sensing; Spectroscopy; Adversarial networks; Classification methods; Convolutional neural network; Dimensionality reduction; High-dimension data; Hyper-spectral classification; Principle component analysis; Three dimensional (3-D) modeling; Principal component analysis"
"Using transfer learning technique for SAR automatic target recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078169428&doi=10.1117%2f12.2538012&partnerID=40&md5=5621af619e940be812ad3843f5a5d643","In this paper, a deep learning approach for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) is proposed. The novelty of the proposed framework stems from the fact that it is based on a transfer learning scheme, where a pre-trained Convolutional Neural Network (CNN) is employed to extract learned features in combination with a classical Support Vector Machine (SVM) for classification. The efficiency of the presented approach is validated on the MSTAR dataset, where ten target classes are used. A classification accuracy of 99.27% is achieved. Copyright © 2019 SPIE.","Automatic Target Recognition; Deep learning; Remote sensing; Synthetic Aperture Radar; Transfer learning","Automatic target recognition; Deep learning; Neural networks; Remote sensing; Support vector machines; Synthetic aperture radar; Classification accuracy; Convolutional neural network; Learning approach; Target class; Transfer learning; Radar target recognition"
"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066493298&partnerID=40&md5=019865d51889fac13feaf980207cd41c","The proceedings contain 39 papers. The topics discussed include: spectral-spatial classification of hyperspectral imagery using neural network algorithm and hierarchical segmentation; investigation of filtering and objects detection algorithms for a multizone image sequence; face recognition with low false positive error rate; the procedures of visual analysis for multidimensional data volumes; real-time tracking in satellite videos via joint discrimination and pose estimation; automated extraction of liver outlines from computed tomography scan images using a CUDA-based segmentation method; ray-based segmentation algorithm for medical imaging; dermatological image denoising using adaptive HENLM method; and the study of activation functions in deep learning for pedestrian detection and tracking.",,
"Ship detection and classification with terrestrial hyperspectral data based on convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078117389&doi=10.1117%2f12.2533090&partnerID=40&md5=5d94f08c8ab2e34bb38f51cb1d41c910","The detection and classification of maritime objects in a harbour environment or coastal areas using a terrestrial hyperspectral system in combination with a high-resolution RGB sensor is a challenging task since the large number of spectral channels requires a robust analytical method. Recently, deep learning methods have shown a good performance in many computer vision applications. In this paper, we present a general analysis workflow for terrestrial ship detection and classification based on fused RGB and hyperspectral images, which employs a deep learning network for the localization of ships in the high-resolution images and a following convolutional neural network based multi-input model for the classification of each detected object. During a measurement campaign, images of various ship types were collected under distinct weather conditions for the training and evaluation of the neural network model. In the first part of the workflow, ship candidates were located using the Mask R-CNN framework based on the RGB images. For the following classification process, which was trained to separate different ship type classes, we developed a multi-input convolutional neural network using the RGB and the hyperspectral images as data samples. For the pre-processing procedure of the hyperspectral data a principal component analysis was applied to reduce the number of input channels for the network while still maintaining a large fraction of the initial information. For the architecture of the RGB classification branch, the structure and the weights of a pre-trained model was integrated and fine-tuned. Since only limited training data was available, regularization methods and data augmentation were employed. The detection and multi-input classification network was finally evaluated and showed that the classification performance can be increased when integrating additional information from a hyperspectral sensor. © 2019 SPIE.","Classification; Convolutional Neural Networks; Hyperspectral Data; Maritime Environment; Object Detection; Principal Component Analysis","Convolution; Deep learning; Neural networks; Object detection; Principal component analysis; Remote sensing; Ships; Spectroscopy; Classification networks; Classification performance; Classification process; Computer vision applications; Convolutional neural network; Hyperspectral Data; Maritime environment; Robust analytical methods; Classification (of information)"
"Sheared multi-scale weight sharing for multi-spectral superresolution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072564498&doi=10.1117%2f12.2519982&partnerID=40&md5=7559c81f60ee7cebaf51c33a194aa781","Deep learning approaches to single-image superresolution typically use convolutional neural networks. Convolutional layers introduce translation invariance to neural networks. However, other spatial invariants appear in imaging data. Two such invariances are scale invariance, similar features at multiple spacial scales, and shearing invariance. We investigate these invariances by using weight sharing between dilated and sheared convolutional kernels in the context of multi-spectral imaging data. Traditional pooling methods can extract features at coarse spacial levels. Our approach explores a finer range of scales. Additionally, our approach offers improved storage efficiency because dilated and sheared convolutions allows single trainable kernels to extract information at multiple spacial scales and shears without the costs of training and storing many filters, especially in multi-spectral imaging where data representations are complex. © 2019 SPIE.",,"Convolution; Deep learning; Digital storage; Multilayer neural networks; Optical resolving power; Remote sensing; Spectroscopy; Convolutional kernel; Convolutional neural network; Data representations; Extract informations; Learning approach; Multispectral imaging; Storage efficiency; Translation invariance; Scales (weighing instruments)"
"Single image super-resolution for optical satellite scenes using deep deconvolutional network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072981508&doi=10.1007%2f978-3-030-30642-7_37&partnerID=40&md5=1930830da23bfc4fa3c3531ac7476d01","In this paper, we deal with the problem of super-resolution (SR) imaging and propose a deep deconvolutional network based model for the same. In principle, the SR problem considers the construction of the high-resolution (HR) version of a scene given a number of so-called low-level image instances of the respective scene. Moreover, if there is a single low-resolution (LR) image available, the problem becomes even difficult and ill-posed. We deal with such a scenario and show how the popular deconvolutional network can effectively reconstruct the HR image by learning the functional mapping at the patch level. We evaluate the proposed model on a number of optical remote sensing (RS) images obtained from the UC-Merced dataset. Experimental results suggest that the proposed model consistently outperforms the existing deep and shallow models for single image SR for the RS images. © 2019, Springer Nature Switzerland AG.","Deconvolutional neural networks; Deep learning; Image super resolution; Satellite imaging","Deep learning; Deep neural networks; Optical resolving power; Remote sensing; Functional mapping; Image super resolutions; Low resolution images; Network-based modeling; Optical remote sensing; Optical satellites; Satellite imaging; Super resolution; Image analysis"
"Initial investigation into the effect of image degradation on the performance of a 3-category classifier using transfer learning and data augmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072585053&doi=10.1117%2f12.2519019&partnerID=40&md5=a1209816785a383fc9dd45041488efce","This paper documents an initial investigation into the effect of image degradation on the performance of transfer learning (TL) as the number of retrained layers is varied, using a well-documented, commonly-used, and well-performing deep learning classifier (VGG16). Degradations were performed on a publicly-available data set to simulate the effects of noise and varying optical resolution by electro-optical (EO/IR) imaging sensors. Performance measurements were gathered on TL performance on the base image-set as well as modified image-sets with different numbers of retrained layers, with and without data augmentation. It is shown that TL mitigates against corrupt data, and improves classifier performance with increased numbers of retrained layers. Data augmentation also improves performance. At the same time, the phenomenal performance of TL cannot overcome the lack of feature information in severely degraded images. This experiment provides a qualitative sense of when transfer learning cannot be expected to improve classification results. © 2019 SPIE.","Classifier; Noise; Remote sensing; SNR; Transfer learning; VGG16","Classifiers; Deep learning; Image classification; Remote sensing; Signal to noise ratio; Spectroscopy; Classification results; Classifier performance; Feature information; Learning classifiers; Noise; Performance measurements; Transfer learning; VGG16; Classification (of information)"
"A CNN-based pansharpening method with perceptual loss","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077856020&doi=10.1109%2fIGARSS.2019.8900390&partnerID=40&md5=2cfef72d3c7352fde748d6c01325a0d6","Pansharpening is a classical data fusion task that is often necessary when dealing with data sensed through multiresolution acquisition systems. These systems, in fact, provide a single panchromatic band at full spatial resolution coupled with a multispectral lower resolution image of the same scene, which must be fused (pansharpened) to generate a full spatial-spectral resolution datacube. In the last few years, there has been a methodological shift in pansharpening towards the deep learning (DL) paradigm. Most DL solutions proposed thus far use self-supervised learning. Training is carried out on data at downgraded resolution, where ground truth data are also available. Then, the trained network is applied to perform pansharpening on native resolution data. As a consequence, such solutions show good results on low-resolution datasets, but less convincing results on full-resolution data, due to limited generalization ability. In this work, to address this problem, we enrich the training loss function with a perceptual term computed on full-resolution data, obtaining promising experimental results. © 2019 IEEE.","Convolutional neural network; Data fusion; Deep learning; Super-resolution","Convolutional neural networks; Data fusion; Deep neural networks; Remote sensing; Full resolution datum; Generalization ability; Ground truth data; Lower resolution; Multiresolution acquisitions; Panchromatic bands; Spatial resolution; Super resolution; Deep learning"
"A Generative Discriminatory Classified Network for Change Detection in Multispectral Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059449220&doi=10.1109%2fJSTARS.2018.2887108&partnerID=40&md5=fff6fc5380de5e6c6f863588252413a5","Multispectral image change detection based on deep learning generally needs a large amount of training data. However, it is difficult and expensive to mark a large amount of labeled data. To deal with this problem, we propose a generative discriminatory classified network (GDCN) for multispectral image change detection, in which labeled data, unlabeled data, and new fake data generated by generative adversarial networks are used. The GDCN consists of a discriminatory classified network (DCN) and a generator. The DCN divides the input data into changed class, unchanged class, and extra class, i.e., fake class. The generator recovers the real data from input noises to provide additional training samples so as to boost the performance of the DCN. Finally, the bitemporal multispectral images are input to the DCN to get the final change map. Experimental results on the real multispectral imagery datasets demonstrate that the proposed GDCN trained by unlabeled data and a small amount of labeled data can achieve competitive performance compared with existing methods. © 2008-2012 IEEE.","Change detection; deep learning; generative adversarial networks (GANs); multispectral imagery","Deep learning; Image classification; Remote sensing; Adversarial networks; Change detection; Competitive performance; Large amounts; Multi-spectral imagery; Multispectral images; Training sample; Unlabeled data; Discriminators; data set; detection method; image analysis; image classification; machine learning; spatiotemporal analysis; spectral analysis"
"Land use land cover classification from satellite imagery using mUnet: A modified UNET architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068218665&doi=10.5220%2f0007370603590365&partnerID=40&md5=0ddd51b9eab47f982e7800923c27b459","Land-use-land-cover classification(LULC) is used to automate the process of providing labels, describing the physical land type to represent how a land area is being used. Many sectors such as telecom, utility, hydrology etc need land use and land cover information from remote sensing images. This information provides an insight into the type of geographical distribution of a region with providing low level features such as amount of vegetation, building area, and geometry etc as well as higher level concepts such as land use classes. This information is particularly useful for resource-starved rapidly developing cities for urban planning and resource management. LULC also provides historical changes in land-use patterns over a period of time. In this paper, we analyze patterns of land use in urban and rural neighborhoods using high resolution satellite imagery, utilizing a state of the art deep convolutional neural network. The proposed LULC network, termed as mUnet is based on an encoder-decoder convolutional architecture for pixel-level semantic segmentation. We test our approach on 3 band, FCC satellite imagery covering 225 km2 area of Karachi. Experimental results show the superiority of our proposed network architecture vis-à-vis other state of the art networks. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved","Convolutional Networks; Deep Learning; Land-use-classification; Remote Sensing; Satellite Imagery","Computer graphics; Computer vision; Convolution; Deep learning; Deep neural networks; Geographical distribution; Image classification; Image segmentation; Information management; Information use; Network architecture; Neural networks; Remote sensing; Satellite imagery; Semantics; Convolutional networks; Convolutional neural network; High resolution satellite imagery; Land use and land cover; Land use/ land covers; Landuse classifications; Remote sensing images; Semantic segmentation; Land use"
"Optimizing CNN-Based Hyperspectral Image Classification on FPGAs","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064554507&doi=10.1007%2f978-3-030-17227-5_2&partnerID=40&md5=93a25ce3025c853d32fa978081dc0a07","Hyperspectral image (HSI) classification has been widely adopted in remote sensing imagery analysis applications which require high classification accuracy and real-time processing speed. Convolutional neural networks (CNNs)-based methods have been proven to achieve state-of-the-art accuracy in classifying HSIs. However, CNN models are often too computationally intensive to achieve real-time response due to the high dimensional nature of HSI, compared to traditional methods such as Support Vector Machines (SVMs). Besides, previous CNN models used in HSI are not specially designed for efficient implementation on embedded devices such as FPGAs. This paper proposes a novel CNN-based algorithm for HSI classification which takes into account hardware efficiency and thus is more hardware friendly compared to prior CNN models. An optimized and customized architecture which maps the proposed algorithm on FPGA is then proposed to support real-time on-board classification with low power consumption. Implementation results show that our proposed accelerator on a Xilinx Zynq 706 FPGA board achieves more than 70 &#x0024;&#x0024;\times &#x0024;&#x0024; faster than an Intel 8-core Xeon CPU and 3 &#x0024;&#x0024;\times &#x0024;&#x0024; faster than an NVIDIA GeForce 1080 GPU. Compared to previous SVM-based FPGA accelerators, we achieve comparable processing speed but provide a much higher classification accuracy. © 2019, Springer Nature Switzerland AG.","Convolution neural network; Deep learning; Field-programmable gate array; Hyperspectral image classification","Computer hardware; Convolution; Data mining; Deep learning; Field programmable gate arrays (FPGA); Hyperspectral imaging; Neural networks; Reconfigurable architectures; Remote sensing; Spectroscopy; Support vector machines; Classification accuracy; Convolution neural network; Convolutional neural network; Efficient implementation; Low-power consumption; Real-time processing speed; Remote sensing imagery; Support vector machine (SVMs); Image classification"
"Analysis of land cover change detection using satellite images in Patheingyi Township","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048623609&doi=10.1007%2f978-981-13-0869-7_41&partnerID=40&md5=ae31fc129994522edb0ad3c4d2938449","Patheingyi Township is located in the eastern part of Mandalay, Myanmar. Patheingyi Township is an area that experienced a fast increase of urban population in the recent decades in Mandalay region. Rapid urbanization has significant effect on resources and urban environment. The remote sensing technique using satellite imagery has been recognized as an effective and powerful tool in monitoring and detecting land use and land cover changes. The aim of the system is to quantify changes in urban area of Patheingyi Township using satellite images. In this study, Landsat 7 and Landsat 8 images are used to study land cover change in study area. Land cover maps of 2000, 2005, 2010 and 2015 are produced using one of the supervised classification techniques. In this system, Random Forest Classifier model is used to detect the growth of urban area. The system is classified three major land cover classes: built-up, water and other. And then, ArcGIS tool is used to classify built-up area into five classes: urban built-up, suburban built-up, rural built-up, urbanized open land and rural open land. A total of 295 ground check points are used to calculate the accuracy assessment. According to the tested results, built-up area was increased by 285 ha from 2010 to 2015 in Patheingyi Township. © 2019, Springer Nature Singapore Pte Ltd.","ArcGIS; Google earth engine; Patheingyi Township; Random Forest; Remote sensing","Data handling; Decision trees; Deep learning; Geographic information systems; Information analysis; Land use; Remote sensing; Rural areas; Satellite imagery; Urban growth; ArcGIS; Google earths; Land use and land cover change; Patheingyi Township; Random forest classifier; Random forests; Remote sensing techniques; Supervised classification; Big data"
"Machine learning models for predicting lettuce health using UAV imageries","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072638232&doi=10.1117%2f12.2519157&partnerID=40&md5=fc0b29fd9ee83f44313f2ba67a852012","This paper presents the development and validation of machine learning models for the prediction of water and nitrogen stresses in lettuce. Linear regression and deep learning neural networks, mainly convolutional neural networks (CNNs), are used to train the machine learning models. The data used for the training include both airborne and proximal sensor data. The airborne data used are digital images collected from unmanned aerial vehicles (UAVs) and the normalized difference vegetation index (NDVI) obtained from airborne multispectral images. Chlorophyll meter, water potential meter, and spectroradiometer are the proximal sensors used. Also used for the training are agronomic measurements such as leaf count and plant height. For the validation of the developed models, two sets of tests were performed. The first test used a set of data similar to the training data, but different from the training data. The second test used aerial images of various random lettuce plots at farms obtained from Google Maps. The second test evaluates the models' portability and performance in an unknown environment using the data that was not collected from the experimental plot. The goal of the machine learning algorithms is to provide precise detection of nitrogen and water stresses on a plant level basis using just the digital images collected from UAVs. This will help reduce the cost associated with precision agriculture. © 2019 SPIE. Downloading of the abstract is permitted for personal use only.","Deep learning; Machine learning; Precision Agriculture; Remote Sensing; UAV","Aircraft detection; Antennas; Deep learning; Image processing; Learning algorithms; Learning systems; Neural networks; Nitrogen; Precision agriculture; Remote sensing; Unmanned aerial vehicles (UAV); Chlorophyll meters; Convolutional neural network; Learning neural networks; Machine learning models; Multispectral images; Normalized difference vegetation index; Spectro-radiometers; Unknown environments; Machine learning"
"Machine learning for evaluating the social impact of engineered products: A framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076361437&doi=10.1115%2fDETC2019-98412&partnerID=40&md5=4f0fa7a857f492accf68e606ec08d025","Evaluating the social impact indicators of engineered products is crucial to better understanding how products affect individuals’ lives and discover how to design for positive social impact. Most existing methods for evaluating social impact indicators require direct human interaction with users of a product, such as one-on-one interviews. These interactions produce high-fidelity data that are rich in information but provide only a single snapshot in time of the product’s impacts and are less frequently collected due to the significant human resources and cost associated with obtaining them. A framework is proposed that describes how low-fidelity data passively obtained using remote sensors, satellites, and digital technology can be collected and correlated with high-fidelity, low-frequency data using machine learning. Using this framework provides an inexpensive way to continuously monitor the social impact indicators of products by augmenting high-fidelity, low-frequency data with low-fidelity, continuously-collected data using machine learning. We illustrate an application of this framework by demonstrating how it can be used to examine the gender-related social impact indicators of water pumps in Uganda. The provided example uses a deep learning model to correlate pump handle movement (measured via an integrated motion unit) with user type (man, woman, or child) of 1,200 hand pump users. Copyright © 2019 ASME.",,"Computer aided design; Deep learning; Engineering education; Machine learning; Product design; Pumps; Remote sensing; Digital technologies; Engineered products; Human interactions; Learning models; Low fidelities; Low-frequency data; Remote sensors; Single snapshots; Economic and social effects"
"Pre-trained classification of hyperspectral images using denoising autoencoders and joint features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066866683&doi=10.1145%2f3318236.3318241&partnerID=40&md5=3b29d83c9abc1b5f19527a386504fa3a","Hyperspectral image classification in remote sensing discipline aims to analyze scene properties of the environment captured from earth observing satellites of aircrafts. To comprehend this aim common linear methods like principal component analysis and linear discriminant analysis fail to model the nonlinear structures of data. To learn feature representations on large-scale high-dimensional data, deep learning methods have been applied successfully. We utilize a deep neural network for both feature extraction and then classification based on unsupervised pre-training using stacked denoising autoencoder method and supervised fine-tuning using logistic regression on top. This paper both exploit joint representation, namely spectral-spatial information of hyperspectral images to pre-train classification capturing the most salient features. Besides that, since extracting sparse features might improve the discrimination, rectified linear unit (ReLU) is used as activation function in encoders to extract high-level sparse features. The results show in our experiments that this model achieves the higher classification accuracy than other evaluation methods, and excels classical classifiers namely support vector machines and random forests. © 2019 Association for Computing Machinery","Denoising autoencoders (DAEs); Hyperspectral images; Image classification; Spectral -spatial information; Stacked autoencoders","Clustering algorithms; Decision trees; Deep neural networks; Discriminant analysis; Hyperspectral imaging; Image denoising; Principal component analysis; Remote sensing; Spectroscopy; Autoencoders; Classification accuracy; Earth observing satellite; Feature representation; High dimensional data; Linear discriminant analysis; Logistic regressions; Spatial informations; Image classification"
"Landslide detection with ALOS-2/PALSAR-2 data using convolutional neural networks: A case study of 2018 Hokkaido Eastern Iburi earthquake","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076887599&doi=10.1117%2f12.2531695&partnerID=40&md5=0b5c8c03a018794baa76b494360b234c","Landslide events are triggered by heavy rain or earthquake each year around the world. The remote sensing technique is an effective tool for disaster mapping, monitoring, and early warning. In particular, synthetic aperture radar (SAR) has great potentials due to its all-weather with day and night time imaging capabilities. Thus, rapid damage detection of landslide events using SAR data is expected. However, the technique for landslide detection has not been established. In recent years, convolutional neural networks (CNNs) have been widely developed in semantic segmentation. The CNNs based on the U-Net, in particular, has a capability of fewer training images for semantic segmentation. In this study, we demonstrate a landslide detection using the CNNs based on the U-Net with SAR data. The landslides induced by the 2018 Hokkaido Eastern Iburi earthquake was selected for a case study. ALOS-2/PALSAR-2 data were acquired on 29 March 2018 and 13 September 2018 in ascending orbit and were acquired on 23 August 2018 and 6 September 2018 in descending orbit. For pre-processing, radiometric calibration and ortho-rectification were performed in each PALSAR-2 data. Firstly, we performed a conventional method, which is the backscattering coefficient difference (BCD) for landslide detection. The highest value of the F-measure (31.9%) was obtained within a 7 x 7 window size with the slope angle in the ascending orbit dataset. Secondly, we performed a CNNs method based on the U-Net for landslide detection. The highest value of the F-measure (79.9%) was obtained using the CNNs method in the ascending orbit dataset. In conclusion, the proposed CNNs method is more effective than the threshold method for landslide detection using pre- A nd post-event ALOS-2/PALSAR-2 data. © 2019 SPIE.","CNNs; deep learning; SAR; U-Net","Backscattering; Convolution; Damage detection; Deep learning; Earthquakes; Environmental engineering; Image segmentation; Landslides; Neural networks; Semantics; Synthetic aperture radar; Backscattering coefficients; CNNs; Conventional methods; Convolutional neural network; Imaging capabilities; Radiometric calibrations; Remote sensing techniques; Semantic segmentation; Remote sensing"
"Super resolution of DS-2 satellite imagery using deep convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078179533&doi=10.1117%2f12.2533116&partnerID=40&md5=2b905ccf0cac78140cfb7b3b35dcf426","Nowadays, Satellite images are used for various analysis, including building detection and road extraction, which are directly beneficial to governmental applications, such as urbanization and monitoring the environment. Spatial resolution is an element of crucial impact on the usage of remote sensing imagery. High spatial resolution means satellite images provide more detailed information. To improve the spatial resolution at the sensor level, many factors are ought to be taken into consideration, such as the manufacturing process. Moreover, once the satellite is launched, no further action can be taken from the perspective of hardware. Therefore, a more practical solution to improve the resolution of a satellite image is to use Single Image Super Resolution (SISR) techniques. This research proposal deals with the re-design, implementation, and evaluation of SISR technique using Deep Convolutional Neural Network with Skip Connections and Network in Network (DCSCN) for enlarging multispectral remote sensing images captured by DubaiSat-2 (DS-2) and estimating the missing high frequency details. The goal is to achieve high performance in terms of quality, and to test whether training the network using luminance channel only, which is extracted from YCbCr domain, can achieve high quality results. For this purpose, DCSCN is trained, evaluated, and tested using a dataset collected from DS-2. A single low resolution DS-2 image is used to construct its high resolution version by training the model from scratch and fine-tuning its hyper-parameters to produce optimal results. The performance is evaluated using various quality indices, such as Structural Similarity Index Measurement (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Wavelet domain Signal-to-Noise Ratio (WSNR). The performance is compared to other state-of-the-art methods, such as Bil-inear, Bi-cubic, and Lanczos interpolation. © 2019 SPIE.","Deep Learning; Image Reconstruction; Multispectral; Peak Signal to Noise Ratio (PSNR); Structure Similarity Index Measurement (SSIM).; Super Resolution; Wavelet domain Signal to Noise Ratio (WSNR)","Convolution; Deep learning; Deep neural networks; Feature extraction; Image enhancement; Image reconstruction; Image resolution; Neural networks; Optical resolving power; Quality control; Remote sensing; Satellite imagery; Multi-spectral; Peak signal to noise ratio; Structure similarity; Super resolution; Wavelet domain; Signal to noise ratio"
"Training lightweight network from scratch for efficient object detection in aerial images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078104236&doi=10.1117%2f12.2535479&partnerID=40&md5=ebc49eadec1a8bec8473b5e4cf6f5fcd","Object detection in aerial images plays an important role for a wide range of applications. Although many efforts have been done in the last decade, it is still an active and challenging problem because of the highly complex backgrounds and the large variations in the visual appearance of objects caused by viewpoint variation, occlusion, illumination, etc. Recently, many object detectors based on deep learning demonstrate the great advantages for significantly improving the detection performance in aerial images. However, the most accuracy neural networks usually have hundreds of layers and thousands of channels, thus requiring huge computation and memory consumption. Besides, the state-of-the-art object detectors are usually fined-tuned from the models pretrained on classification dataset ImageNet, which limits the modification of network architecture and also leads to learning bias because of the different domains. In this paper we trained a lightweight convolutional neural network from scratch to perform object detection in aerial images. When designing the lightweight network, Concatenated Rectified Linear Units (CReLU) and depthwise separable convolution operation were employed to reduce the computation cost and model size. When training the lightweight network from scratch, we employ Group Normalization (GN) in each convolution layer, which makes smoother optimization landscape and has more stable gradients. A serial of ablation experiments is conducted on the recently published large-scale Dataset for Object detection in Aerial images (DOTA), and the results show that the proposed object detection methods with lightweight network trained from scratch achieves competitive performance but has smaller model size and lower computation cost. © 2019 SPIE.","Aerial Images; Concatenated Rectified Linear Units; Depthwise Separable Convolution; Group Normalization; Lightweight Network; Object Detection; Training from Scratch","Antennas; Classification (of information); Convolution; Deep learning; Image enhancement; Large dataset; Multilayer neural networks; Network architecture; Object recognition; Remote sensing; Aerial images; Competitive performance; Convolutional neural network; Detection performance; Efficient object detections; Group Normalization; Linear units; Object detection method; Object detection"
"Land cover segmentation of aerial imagery using SegNet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075817870&doi=10.1117%2f12.2532983&partnerID=40&md5=58a937eba864de056ab5595820ea2d63","Land cover relates to the biophysical characteristics of the Earth's surface, identifying vegetation, water, bare soil or artificial infrastructure. Land cover mapping is essential for planning and managing natural resource, and for understanding distribution of habitats. Land cover classification for land cover mapping has been developed in a variety of ways. Among them, there are many attempts to classification land cover using deep learning techniques such as Convolutional Neural Network(CNN). CNN has been developed in many models, and semantic segmentation techniques that combining segmentation are also being announced. Among the Semantic Segmentation models developed until recently, SegNet has high accuracy and learning efficiency. We analyzed the availability of SegNet in the Land Cover classification. The study area was conducted in parts of South Chungcheongnam-do in South Korea. For the learning of the model, 2,000 data were constructed with the same size using the aerial image, and the constructed data was divided into training and validation data by 8 to 2. To solve the problem of class imbalance, which causes problems such as overfitting due to the difference in area per class, the weight value of each class was calculated using medium frequency balancing method. In order to calculate the hyper parameter optimization, the batch size was changed from 1 to 5 and the iteration was changed from 0 to 100,000 times Our experiments show that an overall accuracy (OA) of up to 85%, which confirmed the positive possibility of the semantic segmentation technique in the study of land cover classification. © 2019 SPIE.","Aerial imagery; Classification; Land cover; Segmentation; Segnet","Aerial photography; Antennas; Classification (of information); Deep learning; Image segmentation; Iterative methods; Mapping; Neural networks; Semantics; Aerial imagery; Biophysical characteristics; Convolutional neural network; Hyper-parameter optimizations; Land cover; Land cover classification; Segnet; Semantic segmentation; Remote sensing"
"Automatic Target Recognition XXIX","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072572089&partnerID=40&md5=5a1c768ad81c1e12524d552ec63faeed","The proceedings contain 30 papers. The topics discussed include: simple linear regression model based data clustering; fast and robust detection of oil palm trees using high-resolution remote sensing images; semantic segmentation based large-scale oil palm plantation detection using high-resolution satellite images; design of adversarial targets: fooling deep ATR systems; comparing classifiers that exploit random subspaces; radar target recognition using wavelet-based features extracted from compressively sensed signatures; on generalization of deep learning recognizers in overhead imagery; automatic machine learning for target recognition; and multisource deep learning for situation awareness.",,
"Performance evaluation of convolutional neural network at hyperspectral and multispectral resolution for classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078146455&doi=10.1117%2f12.2533077&partnerID=40&md5=8728a2730a6bdcdc1fa05c324a48544b","Convolutional Neural Network (CNN) has established as an effective deep learning model for hyperspectral image classification by considering both spectral and spatial information. In this study, the performance of two-dimensional (2D) CNN architecture is evaluated at hyperspectral and multispectral resolution. Two types of multispectral data are analyzed viz., original and transformed multispectral data. Hyperspectral bands are transformed to spectral resolution of multispectral bands by averaging the reflectances of specific hyperspectral narrow bands which are falling within the spectral ranges of multispectral bands. The well-known Pavia University dataset and a new dataset of Pear orchard are investigated in this study. In case of Pear orchard dataset, classification is performed with both types of multispectral data. All the experiments are carried out with the same 2D CNN architecture. In case of Pavia University dataset, hyperspectral and transformed multispectral data achieve OA(%) of 94.29±1.28 and 94.27±2.01 respectively considering 20% samples as training. In case of Pear orchard dataset, hyperspectral, multispectral and transformed multispectral data achieve OA(%) of 91.59±0.89, 88.65±1.35, and 93.24±0.16 respectively considering 20% samples as training. It is evident that transformed multispectral data, which comprises of inherent hyperspectral information, provides similar or better performance compared to hyperspectral data. Further, with the use of 3D CNN architecture, classification performance improves in case of Pavia University dataset, whereas it remains statistically similar in case of Pear orchard dataset. The present promising results illustrates the performance of CNN even in small dataset which is comparable to several published state-of-the art results on the same dataset. © 2019 SPIE.","2D CNN; 3D CNN; Convolutional neural network; Deep learning; Hyperspectral data; Multispectral data; Spectral-spatial classification","Convolution; Deep learning; Deep neural networks; Fruits; Image classification; Network architecture; Neural networks; Orchards; Remote sensing; Spectroscopy; 2D CNN; 3D CNN; Convolutional neural network; Hyperspectral Data; Multi-spectral data; Spectral-spatial classification; Classification (of information)"
"Capsule and convolutional neural network-based SAR ship classification in Sentinel-1 data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076886411&doi=10.1117%2f12.2532551&partnerID=40&md5=57aa0d0733091b006646c50bd216600f","Synthetic Aperture Radar (SAR) constitutes a fundamental asset for wide-areas monitoring with high-resolution requirements. The first SAR sensors have given rise to coarse coastal and maritime monitoring applications, including oil spill, ship and ice floes detection. With the upgrade to very high-resolution sensors in the recent years, with relatively new SAR missions such as Sentinel-1, a great deal of data providing a stronger information content has been released, enabling more refined studies on general targets features and thus permitting complex classifications, as for ship classification, which has become increasingly relevant given the growing need for coastal surveillance in commercial and military segments. In the last decade, several works focused on this topic have been presented, generally based on radiometric features processing; furthermore, in the very recent years a significant amount of research works have focused on emerging deep learning techniques, in particular on Convolutional Neural Networks (CNN). Recently Capsule Neural Networks (CapsNets) have been presented, demonstrating a notable improvement in capturing the properties of given entities, improving the use of spatial informations, in particular of spatial dependence between features, a severely lacking feature in CNNs. In fact, CNNs pooling operations have been criticized for losing spatial relations, thus special capsules, along with a new iterative routing-by-agreement mechanism, have been proposed. In this work a comparison between Capsule and CNNs potential in the ship classification application domain is shown, by leveraging the OpenSARShip, a SAR Sentinel-1 ship chips dataset; in particular, a performance comparison between capsule and various convolutional architectures is built, demonstrating better performances of CapsNet in classifying ships within a small dataset. © 2019 SPIE.","Capsule; Classication; Convolutional; Deep Learning; Detection; Generative Adversarial; Neural Networks; SAR; Sentinel; Ship","Convolution; Deep learning; Deep neural networks; Environmental engineering; Error detection; Iterative methods; Neural networks; Oil spills; Radar target recognition; Remote sensing; Sea ice; Ships; Synthetic aperture radar; Capsule; Classication; Convolutional; Generative Adversarial; Sentinel; Classification (of information)"
"Convolutional Neural Network Based Models for Improving Super-Resolution Imaging","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064869864&doi=10.1109%2fACCESS.2019.2908501&partnerID=40&md5=46419bf4674ab8c29943290a9a2cacf3","Many fields, such as remote sensing, medical imaging, and biological detection, pose a technical challenge for achieving super-resolution imaging. Convolutional neural networks (CNNs) are considered one of the potential solutions to realize the super-resolution. In this paper three-layer, CNN-based models are proposed to reconstruct the super-resolution images using four optimization algorithms, i.e., stochastic gradient descent, adaptive gradient (AdaGrad), root mean square prop (RMSprop), and adaptive moment estimation (ADAM). Among these four optimizations, ADAM is considered to have the best performance. To further verify the impact of the number of convolution layers on performance, a selection of CNN-based models with four convolutional layers is then proposed, each of which is named with the convolution parameters. All the four-layer models are optimized with ADAM, and the experimental results indicate that the 9-3-3-5 model achieves the best performance in the super-resolution reconstruction task. © 2013 IEEE.","adaptive moment estimation; convolutional neural networks; deep learning; Super-resolution imaging","Convolution; Deep learning; Neural networks; Optical resolving power; Optimization; Remote sensing; Stochastic models; Stochastic systems; Biological detection; Convolutional neural network; Moment estimation; Optimization algorithms; Stochastic gradient descent; Super resolution imaging; Super resolution reconstruction; Technical challenges; Medical imaging"
"Counting the Uncountable: Deep Semantic Density Estimation from Space","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063508014&doi=10.1007%2f978-3-030-12939-2_24&partnerID=40&md5=259edfa2a009915caa47eb072eb8adca","We propose a new method to count objects of specific categories that are significantly smaller than the ground sampling distance of a satellite image. This task is hard due to the cluttered nature of scenes where different object categories occur. Target objects can be partially occluded, vary in appearance within the same class and look alike to different categories. Since traditional object detection is infeasible due to the small size of objects with respect to the pixel size, we cast object counting as a density estimation problem. To distinguish objects of different classes, our approach combines density estimation with semantic segmentation in an end-to-end learnable convolutional neural network (CNN). Experiments show that deep semantic density estimation can robustly count objects of various classes in cluttered scenes. Experiments also suggest that we need specific CNN architectures in remote sensing instead of blindly applying existing ones from computer vision. © 2019, Springer Nature Switzerland AG.","Computer vision; Deep learning; Density estimation; Remote sensing","Computer vision; Deep learning; Image segmentation; Neural networks; Remote sensing; Semantics; Space optics; Cluttered scenes; Convolutional neural network; Density estimation; Ground sampling distances; Object categories; Satellite images; Semantic density; Semantic segmentation; Object detection"
"Color segmentation based on human perception using fuzzy logic","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048630340&doi=10.1007%2f978-981-13-0869-7_37&partnerID=40&md5=c5d92221f5e6abfd5afb5657e94cb812","Color segmentation is important in the field of remote sensing and Geographic Information System (GIS). Most of the color vision systems need to classify pixel color in a given image. Human perception-based approach to pixel color segmentation is done by fuzzy logic. Fuzzy sets are defined on the H, S and V components of the HSV color space. Three values (H, S and V), the fuzzy logic model has three antecedent variables (Hue, Saturation and Value) and one consequent variable, which is a color class ID are fuzzified with Triangular Fuzzy Numbering Method. Fuzzy Rules are constructed according to the linguistic fuzzy sets. One of Discrete Defuzzification method based on zero-order takagi-Sugeno model is used for color segmentation. To define the output color value, Fuzzy reasoning with zero order Takagi-Sugeno model is used for assigning the color of the given. There are sixteen output colors: Black, White, Red, Orange, Yellow, Dark Gray, Light Gray, Pink, Light Brown, Dark Brown, Aqua, Blue, Olive, Light Green, Dark Green and Purple. © 2019, Springer Nature Singapore Pte Ltd.","Color segmentation; Fuzzy logic; Takagi-Sugeno model","Big data; Color; Color vision; Computer circuits; Data handling; Deep learning; Fuzzy inference; Fuzzy sets; Information analysis; Modal analysis; Pixels; Remote sensing; Antecedent variables; Color segmentation; Color vision system; Defuzzification method; Fuzzy logic model; HSV color spaces; Human perception; Takagi-Sugeno modeling; Fuzzy logic"
"Relationship prior and adaptive knowledge mimic based compressed deep network for aerial scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077808816&doi=10.1109%2fACCESS.2019.2932229&partnerID=40&md5=833fed3ec58d6f6e4105f937f5889d7f","The aerial scene classification is one of the major tasks in the remote sensing community that automatically labels the corresponding semantic categories of aerial images. Recently, a lot of methods based on deep neural networks have been proposed, in which hierarchical internal feature are extracted for representations. However, these presented methods often have complex structures and require large volume of memory, and a large number of labeled aerial scene images are difficult to obtain, hindering their implementation in practical applications. In this paper, we present the between-class similarity priori and adaptive knowledge mimic (BPKM) method for aerial scene classification. First, the method extracts the efficient prior relationship information of the scene images from large-scale network. Then, a compressed network is generated through learning the output and the intermediate representations of the large-scale network, and the compressed network achieves better feature description ability; in addition, an improved cross-entropy method with an adaptive threshold is applied to reduce the training time consumption. A large-scale data set (AID) and UC-Merced data set are considered for performance evaluation, and the experimental results indicate that the proposed method is about 24× parameters saving compared to popular networks, e.g., AlexNet, and has outstanding classification performance in classes with similar features. © 2019 IEEE.","Aerial scene classification; Deep learning; Knowledge transfer; Model compression; Priori knowledge","Classification (of information); Deep learning; Deep neural networks; Knowledge management; Remote sensing; Semantics; Classification performance; Cross-entropy method; Intermediate representations; Knowledge transfer; Large scale data sets; Model compression; Priori knowledge; Scene classification; Antennas"
"GETNET: A General End-To-End 2-D CNN Framework for Hyperspectral Image Change Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050503570&doi=10.1109%2fTGRS.2018.2849692&partnerID=40&md5=be5f7c47604902fa521732b050181206","Change detection (CD) is an important application of remote sensing, which provides timely change information about large-scale Earth surface. With the emergence of hyperspectral imagery, CD technology has been greatly promoted, as hyperspectral data with high spectral resolution are capable of detecting finer changes than using the traditional multispectral imagery. Nevertheless, the high dimension of the hyperspectral data makes it difficult to implement traditional CD algorithms. Besides, endmember abundance information at subpixel level is often not fully utilized. In order to better handle high-dimension problem and explore abundance information, this paper presents a general end-To-end 2-D convolutional neural network (CNN) framework for hyperspectral image CD (HSI-CD). The main contributions of this paper are threefold: 1) mixed-Affinity matrix that integrates subpixel representation is introduced to mine more cross-channel gradient features and fuse multisource information; 2) 2-D CNN is designed to learn the discriminative features effectively from the multisource data at a higher level and enhance the generalization ability of the proposed CD algorithm; and 3) the new HSI-CD data set is designed for objective comparison of different methods. Experimental results on real hyperspectral data sets demonstrate that the proposed method outperforms most of the state of the arts. © 1980-2012 IEEE.","2-D convolutional neural network (CNN); change detection (CD); deep learning; hyperspectral image (HSI); mixed-Affinity matrix; spectral unmixing","Convolution; Deep learning; Job analysis; Learning systems; Matrix algebra; Neural networks; Pixels; Principal component analysis; Remote sensing; Spectral resolution; Spectroscopy; Affinity matrix; Change detection; Convolutional Neural Networks (CNN); Spectral unmixing; Task analysis; Hyperspectral imaging; algorithm; artificial neural network; data set; image analysis; imagery; machine learning; pixel; remote sensing; spectral resolution"
"Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059949711&doi=10.3390%2frs11010011&partnerID=40&md5=7d39994d89098681d79bef29ed19c151","Being an important economic crop that contributes 35% of the total consumption of vegetable oil, remote sensing-based quantitative detection of oil palm trees has long been a key research direction for both agriculture and environmental purposes. While existing methods already demonstrate satisfactory effectiveness for small regions, performing the detection for a large region with satisfactory accuracy is still challenging. In this study, we proposed a two-stage convolutional neural network (TS-CNN)-based oil palm detection method using high-resolution satellite images (i.e. Quickbird) in a large-scale study area of Malaysia. The TS-CNN consists of one CNN for land cover classification and one CNN for object classification. The two CNNs were trained and optimized independently based on 20,000 samples collected through human interpretation. For the large-scale oil palm detection for an area of 55 km 2 , we proposed an effective workflow that consists of an overlapping partitioning method for large-scale image division, a multi-scale sliding window method for oil palm coordinate prediction, and a minimum distance filter method for post-processing. Our proposed approach achieves a much higher average F1-score of 94.99% in our study area compared with existing oil palm detection methods (87.95%, 81.80%, 80.61%, and 78.35% for single-stage CNN, Support Vector Machine (SVM), Random Forest (RF), and Artificial Neural Network (ANN), respectively), and much fewer confusions with other vegetation and buildings in the whole image detection results. © 2018 by the authors.","Convolutional neural networks; Deep learning; High-resolution satellite imagery; Object detection; Oil palm tree","Convolution; Decision trees; Deep learning; Forestry; Neural networks; Object detection; Palm oil; Remote sensing; Satellite imagery; Support vector machines; Convolutional neural network; High resolution satellite imagery; High resolution satellite images; Land cover classification; Object classification; Oil palm tree; Quantitative detection; Sliding window methods; Palmprint recognition"
"Building damage detection from post-event aerial imagery using single shot multibox detector","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063751436&doi=10.3390%2fapp9061128&partnerID=40&md5=d8ba8874c99d0512c6ece713c5ecdc58","Using aerial cameras, satellite remote sensing or unmanned aerial vehicles (UAV) equipped with cameras can facilitate search and rescue tasks after disasters. The traditional manual interpretation of huge aerial images is inefficient and could be replaced by machine learning-based methods combined with image processing techniques. Given the development of machine learning, researchers find that convolutional neural networks can effectively extract features from images. Some target detection methods based on deep learning, such as the single-shot multibox detector (SSD) algorithm, can achieve better results than traditional methods. However, the impressive performance of machine learning-based methods results from the numerous labeled samples. Given the complexity of post-disaster scenarios, obtaining many samples in the aftermath of disasters is difficult. To address this issue, a damaged building assessment method using SSD with pretraining and data augmentation is proposed in the current study and highlights the following aspects. (1) Objects can be detected and classified into undamaged buildings, damaged buildings, and ruins. (2) A convolution auto-encoder (CAE) that consists of VGG16 is constructed and trained using unlabeled post-disaster images. As a transfer learning strategy, the weights of the SSD model are initialized using the weights of the CAE counterpart. (3) Data augmentation strategies, such as image mirroring, rotation, Gaussian blur, and Gaussian noise processing, are utilized to augment the training data set. As a case study, aerial images of Hurricane Sandy in 2012 were maximized to validate the proposed method's effectiveness. Experiments show that the pretraining strategy can improve of 10% in terms of overall accuracy compared with the SSD trained from scratch. These experiments also demonstrate that using data augmentation strategies can improve mAP and mF1 by 72% and 20%, respectively. Finally, the experiment is further verified by another dataset of Hurricane Irma, and it is concluded that the paper method is feasible. © 2019 by the authors.","Building damage assessment; Convolutional autoencoder; Deep learning; Post-event; SSD",
"Mixed model estimation of rice yield based on NDVI and GNDVI using a satellite image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078146622&doi=10.1117%2f12.2532108&partnerID=40&md5=5d105953c1e7949a4baa097c833674b6","In Japan, a sustainable supply of rice should be guaranteed since it is the staple food. Therefore it is important to stabilize rice farmers' incomes by giving them appropriate compensation even at the time of adverse weather, e.g. cold summer. In the case of compensating farmers' incomes based on a comparison between the usual yield and estimated yield, an accurate and quick estimation of rice yield is significant to help farmers at the time of disaster. However, a manual survey requires a huge amount of effort and time to investigate all the fields where damage has been declared, especially when a largescale cool summer occurs; therefore, we propose a yield estimation system using satellite images and part of the yield data. Our system provides an accurate and quick yield estimation for the vast extent of the fields at a reasonable cost. Many rice yield estimation methodologies utilizing satellite images have been studied over the years. For example, a crop growth model-based method and deep learning based method that utilize many satellite images taken multiple times at different times have been proposed. Although we can get much information about fields from many satellite images, systems using plural images cost much and need exhaustive calibration before the estimation. Therefore, in this study we propose a yield estimation method using a single image that is taken just before the harvest time. First, we extracted the spectral values from the satellite image using field GIS data and then used a mixed model to perform rice yield estimation. Mixed model is expanded linear regression model and able to take the difference between rice varieties, such as Yumepirika and Nanatsuboshi, into accounts. In addition, we introduced two vegetation indexes, normalized difference vegetation index (NDVI) and green normalized difference vegetation index (GNDVI), into our model as feature values. Generally, NDVI and GNDVI have a positive correlation with the volume of the plant on the field and are used for yield estimation. Of course, we could use machine learning methods, for example random forest and support vector regression. However, we adopted a mixed model considering the explainability of the results and tha fact that the number of input feature values is small. The area of interest is Asahikawa-City, Hokkaido Province, Japan. We demonstrated our method on two datasets and evaluated the performance of our model based on mean absolute error (MAE) using 10-fold cross-validation. One dataset was damaged field data acquired in 2018 (2170 fields). The other was undamaged field data acquired in 2017 (1358 fields). We used RapidEye and SPOT-6 satellite images in 2017 and 2018, respectively. Our experimental results show that our model reduces the MAE of the estimated yield by over 2.5% percent compared to conventional regression methods in each damaged field and undamaged field case. © 2019 SPIE.","Mixed model; NDVI and GNDVI; RapidEye; Rice; SPOT-6; Statistical model; Yield estimation","Agriculture; Decision trees; Deep learning; Ecosystems; Hydrology; Regression analysis; Remote sensing; Satellites; Vegetation; Mixed modeling; NDVI and GNDVI; Rapideye; Rice; SPOT-6; Statistical modeling; Yield estimation; Cost estimating"
"Multisource hyperspectral and LiDAR data fusion for urban land-use mapping based on a modified two-branch convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061125914&doi=10.3390%2fijgi8010028&partnerID=40&md5=7f18539ce81ce49a9e6da5f124cdb1aa","Accurate urban land-use mapping is a challenging task in the remote-sensing field. With the availability of diverse remote sensors, synthetic use and integration of multisource data provides an opportunity for improving urban land-use classification accuracy. Neural networks for Deep Learning have achieved very promising results in computer-vision tasks, such as image classification and object detection. However, the problem of designing an effective deep-learning model for the fusion of multisource remote-sensing data still remains. To tackle this issue, this paper proposes a modified two-branch convolutional neural network for the adaptive fusion of hyperspectral imagery (HSI) and Light Detection and Ranging (LiDAR) data. Specifically, the proposed model consists of a HSI branch and a LiDAR branch, sharing the same network structure to reduce the time cost of network design. A residual block is utilized in each branch to extract hierarchical, parallel, and multiscale features. An adaptive-feature fusion module is proposed to integrate HSI and LiDAR features in a more reasonable and natural way (based on ""Squeeze-and-Excitation Networks""). Experiments indicate that the proposed two-branch network shows good performance, with an overall accuracy of almost 92%. Compared with single-source data, the introduction of multisource data improves accuracy by at least 8%. The adaptive fusion model can also increase classification accuracy by more than 3% when compared with the feature-stacking method (simple concatenation). The results demonstrate that the proposed network can effectively extract and fuse features for a better urban land-use mapping accuracy. © 2019 by the authors.","Convolutional neural networks; Feature fusion; Multisource data; Urban land-use mapping",
"Target detection based on classification in shadow region of hyperspectral image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072648116&doi=10.1117%2f12.2527578&partnerID=40&md5=8279e20ef5e39dd17b3a531ab32e8015","In the field of hyperspectral image (HSI) processing, shadow regions in HSIs are often ignored or simply treated as a category because of their low reflectivity and complex information. There have been some studies on shadow regions of HSIs but there are still few effective algorithms to detect the real substances under the shadow regions. In view of this problem and the good performance of convolution neural network (CNN) in HSI classification and target detection, this paper improved target detection method in shadow regions in HSI which combines the CNN and the adaptive coherence/cosine estimator (ACE) of the spectral derivative image. This method includes three main steps: firstly, shadow region would been determined by CNN model whose main parameters have been adjusted to optimize the network performance; secondly, the derivative data of hyperspectral image would be obtained by deriving the shadow region of hyperspectral image; finally, due to the prominent performance of ACE algorithm in target detection of HSI, this algorithm could be applied to detect the substances contained in the shadow regions. To assess the performance of the proposed method, one widely used HSI dataset is used in the experiments. The numbers of experiment results show that the proposed method can detect the substances under the shadow regions in HSIs and it also has promising prospect in the field of HSI data processing. © 2019 SPIE.","ACE; Convolution neural network; Deep learning; Derivative image; Optimal parameter; Remote sensing image","Convolution; Data handling; Deep learning; Deep neural networks; Image classification; Image enhancement; Remote sensing; Spectroscopy; ACE algorithm; Complex information; Convolution neural network; Effective algorithms; Main parameters; Optimal parameter; Remote sensing images; Shadow regions; Radar target recognition"
"Lung Nodule Segmentation Based on Convolutional Neural Networks Using Multi-orientation and Patchwise Mechanisms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073160280&doi=10.1007%2f978-3-030-32040-9_30&partnerID=40&md5=7e44eff0310eb0660bbad60962fded57","Image segmentation is used in several knowledge domains, such as medicine, biology, remote sensing, industrial automation, surveillance and security. More specifically, image segmentation plays a crucial role in various medical imaging applications, as an important part of clinical diagnosis. Deep learning techniques have recently benefited medical image segmentation and classification tasks. In this work, we have explored the use of Convolutional Neural Networks (CNN) for lung nodule segmentation using multi-orientation and patchwise mechanisms. Experiments conducted on the public LIDC-IRI dataset demonstrate that our results were able to reduce the number of false negatives, which is important in this task. High segmentation rates were achieved when compared to medical specialists. © Springer Nature Switzerland AG 2019.",,
"Unobtrusive Sensing Technologies for the Lifecare Solution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069794883&doi=10.1155%2f2019%2f7597190&partnerID=40&md5=20a9841cd35d865c7ff9afa6b5ec224b",[No abstract available],,"breathing rate; core temperature; deep learning; Editorial; freezing of gait; health care; heart rate; human; medical technology; Parkinson disease; physical activity; signal processing; skin surface; wellbeing; algorithm; ambulatory monitoring; biomedical engineering; devices; electronic device; health promotion; medical informatics; medical technology; procedures; remote sensing; Algorithms; Biomedical Engineering; Biomedical Technology; Health Promotion; Humans; Medical Informatics; Monitoring, Ambulatory; Remote Sensing Technology; Signal Processing, Computer-Assisted; Wearable Electronic Devices"
"Editorial for the Special Issue ""Frontiers in spectral imaging and 3D technologies for geospatial solutions""","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071529952&doi=10.3390%2frs11141714&partnerID=40&md5=eefaea7ac07c55cb355ae2539b1d3e06","This Special Issue hosts papers on the integrated use of spectral imaging and 3D technologies in remote sensing, including novel sensors, evolving machine learning technologies for data analysis, and the utilization of these technologies in a variety of geospatial applications. The presented results showed improved results when multimodal data was used in object analysis. © 2019 by the authors.","Classification; Data fusion; Deep learning; Estimation; Hyperspectral imaging; Machine learning; Object detection; Point cloud; Point cloud filtering; Semantic segmentation; Sensor integration",
"2nd International Workshop on High Performance Computing for Advanced Modeling and Simulation in Nuclear Energy and Environmental Science, HPCMS 2018, and 1st International Workshop on HPC Supported Data Analytics for Edge Computing, HiDEC 2018 held at the 32nd ACM International Conference on Supercomputing, ACM ICS 2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072873702&partnerID=40&md5=b914130bb83df4f8dee5bcf7eb72ae02","The proceedings contain 20 papers. The special focus in this conference is on High Performance Computing for Advanced Modeling and Simulation in Nuclear Energy and Environmental Science. The topics include: Prospects for CVR-0: A prototype of china virtual reactor; Diffusion mechanism of small helium-vacancy clusters in BCC Fe and Fe-10%cr alloy; Effect of Ni and Mn on the interaction of an edge dislocation with cu-rich precipitates in BCC fe; The performance test and optimization of crystal-MD program on Tianhe-2; research on large scale parallel hydrological simulation; The study of parallelization of SWAT hydrology cycle; research on shared resource contention of cloud data center; a sequence anomaly detection approach based on isolation forest algorithm for time-series; Hourly day-ahead power forecasting for pv plant based on bidirectional LSTM; a reliability task scheduling for high-throughput computing platform; an edge computing platform for intelligent internet data center operational monitoring; topology layout technology of energy internet; big data analytics for water resources sustainability evaluation; exploring water resource changes of artificial reservoir using time-series remote sensing images from landsat sensors and in situ data; a deep learning based objection detection method for high resolution remote sensing image; a case study for user rating prediction on automobile recommendation system using mapreduce; diverse demands estimation and ranking based on user behaviors; energy aware edge computing: a survey.",,
"Automatic ship detection from high resolution satellite images based on a Deep Convolutional Neural Network (DCNN) model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079123930&partnerID=40&md5=34ae47058edc9ac255cf6902ac458a30","Ship detection and monitoring has been playing a vital role in managing marine traffic and activities that might have a security or civilian importance. For instance, it promotes the maritime rescue, port management and cargo transportation. Furthermore, ship detection enables governments to encounter piracy and illegal fishing. Although remote sensing has been introduced for a long time ago as an alternative to the traditional ship detection methods that requires manual observation and consumes manpower and resources, it is still full of challenges. These challenges are due to the existence of complex backgrounds in the satellite images such as clouds and fog as well as various ships structures with different poses, shapes and scales. More and more attention has been paid to the artificial intelligence technologies to efficiently detect objects based on remote sensing images. The aim of this paper is to design, implement and experimentally assess a novel Deep Convolutional Neural Network (DCNN) based framework for detecting ships from high resolution satellite images. The dataset used in this study consists of 50 RGB sample images collected over the Dubai Coastline of the United Arab Emirates (UAE). The experimental results revealed that all ships are accurately detected with bounding boxes using sliding window detection mechanism. Copyright © 2019 by the International Astronautical Federation (IAF). All rights reserved.","Deep Learning; Neural Network; Satellite images; Ships Detection",
"20th International Conference on Image Analysis and Processing, ICIAP 2019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072955985&partnerID=40&md5=5b3decc96092fb86105a948c42c78256","The proceedings contain 117 papers. The special focus in this conference is on Image Analysis and Processing. The topics include: 3D shape segmentation with geometric deep learning; Multiple organs segmentation in abdomen CT scans using a cascade of CNNs; performance evaluation of learned 3D features; variational autoencoder inspired by brain’s convergence–divergence zones for autonomous driving application; hyperspectral image classification via convolutional neural network based on dilation layers; estimation of speed and distance of surrounding vehicles from a single camera; a convolutional neural network for virtual screening of molecular fingerprints; Evaluation of continuous image features learned by ODE nets; deep motion model for pedestrian tracking in 360 degrees videos; within-network ensemble for face attributes classification; visual and textual sentiment analysis of daily news social media images by deep learning; 3DMM for accurate reconstruction of depth data; The effects of data sources: A baseline evaluation of the MoCA dataset; take a ramble into solution spaces for classification problems in neural networks; supervised two-stage transfer learning on imbalanced dataset for sport classification; single image super-resolution for optical satellite scenes using deep deconvolutional network; genuine personality recognition from highly constrained face images; Deep compact person re-identification with distractor synthesis via guided DC-GANs; dimensionality reduction using discriminative autoencoders for remote sensing image retrieval; video-based convolutional attention for person re-identification; a low-cost computer vision system for real-time tennis analysis; Virtual crowds: An LSTM-based framework for crowd simulation; worldly eyes on video: Learnt vs. reactive deployment of attention to dynamic stimuli; low-complexity scene understanding network; preface.",,
"The future of hyperspectral imaging","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075485969&doi=10.3390%2fjimaging5110084&partnerID=40&md5=28acf28bc8aab76bf7e329969b6a7d4d","The Special Issue on hyperspectral imaging (HSI), entitled “The Future of Hyperspectral Imaging”, has published 12 papers. Nine papers are related to specific current research and three more are review contributions: In both cases, the request is to propose those methods or instruments so as to show the future trends of HSI. Some contributions also update specific methodological or mathematical tools. In particular, the review papers address deep learning methods for HSI analysis, while HSI data compression is reviewed by using liquid crystals spectral multiplexing as well as DMD-based Raman spectroscopy. Specific topics explored by using data obtained by HSI include alert on the sprouting of potato tubers, the investigation on the stability of painting samples, the prediction of healing diabetic foot ulcers, and age determination of blood-stained fingerprints. Papers showing advances on more general topics include video approach for HSI dynamic scenes, localization of plant diseases, new methods for the lossless compression of HSI data, the fusing of multiple multiband images, and mixed modes of laser HSI imaging for sorting and quality controls. © 2019 by the author.","Compression; Fluorescence hyperspectral imaging; HSI for biology; Hyperspectral data mining; Hyperspectral data mining and compression; Hyperspectral imaging; Hyperspectral microscopy; Infrared hyperspectral imaging; Medical imaging by HSI; Raman hyperspectral imaging; Remote sensing; Statistical methods for HSI; Statistical methods for HSI",
"Source separation in ecoacoustics: A roadmap towards versatile soundscape information retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076905199&doi=10.1002%2frse2.141&partnerID=40&md5=0bf4be1f46619120250d373c3d2ff5a5","A comprehensive assessment of ecosystem dynamics requires the monitoring of biological, physical and social changes. Changes that cannot be observed visually may be trackable acoustically through soundscape analysis. Soundscapes vary greatly depending on geophysical events, biodiversity and human activities. However, retrieving source-specific information from geophony, biophony and anthropophony remains a challenging task, due to interference by simultaneous sound sources. Audio source separation is a technique that aims to recover individual sound sources when only mixtures are accessible. Here, we review techniques of monoaural audio source separation with the fundamental theories and assumptions behind them. Depending on the availability of prior information about the source signals, the task can be approached as a blind source separation or a model-based source separation. Most blind source separation techniques depend on assumptions about the behaviour of the source signals, and their performance may deteriorate when the assumptions fail. Model-based techniques generally do not require specific assumptions, and the models are directly learned from labelled data. With the recent advances of deep learning, the model-based techniques can yield state-of-the-art separation performance, accordingly facilitate content-based audio information retrieval. Source separation techniques have been adopted in several ecoacoustic applications to evaluate the contributions from biodiversity and anthropogenic disturbance to soundscape dynamics. They can also be employed as nonlinear filters to improve the recognition of bioacoustic signals. To effectively retrieve ecological information from soundscapes, source separation is a crucial tool. We believe that the future integrations of ecological hypotheses and deep learning can realize a high-performance source separation for ecoacoustics, and accordingly improve soundscape-based ecosystem monitoring. Therefore, we outline a roadmap for applying source separation to assist in soundscape information retrieval and hope to promote cross-disciplinary collaboration. © 2019 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London.","acoustic habitat; biodiversity; ecosystem dynamics; machine learning; passive acoustics; signal processing",
"Vehicle engine classification using normalized tone-pitch indexing and neural computing on short remote vibration sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051661111&doi=10.1016%2fj.eswa.2018.07.073&partnerID=40&md5=74a88b4ae698cb6bb467281e8f5f38a9","As a non-invasive and remote sensor, a Laser Doppler Vibrometer (LDV) has found a broad spectrum of applications. It is a remote, non-line-of-sight sensor to detect threats more reliably and provide increased security protection, which is of utmost importance to military and law enforcement applications. However, the use of the LDV in situation surveillance, especially in vehicle classification, lacks systematic investigations as to its phenomenological and statistical properties. In this work, we aim to identify vehicles by their engine types within a very short period of time to yield a practical expert and intelligent system to classify vehicle engines remotely using laser sensors. Based on our preliminary success on the use of tone-pitch indexes (TPI) over these data, a new normalized tone-pitch indexing (nTPI) scheme is developed to capture engine periodic vibrations by various engine types with vibration data over a much shorter period (from 1.25 to 0.2 s), which makes it possible to monitor slowly moving vehicles around 15 miles per hour. We also exploit the learning power of neural computing, including artificial neural network (ANN), Deep Belief nets (DBN), Stacked Auto-Encoder (SAE), and Convolutional Neural Networks (CNN). To apply a CNN, a two-dimensional array is formulated by stacking nTPI data in an overlapping manner, which is termed as 2DonTPI. The classification results using the proposed nTPI and 2DonTPI over a standard LDV dataset are promising: with encoding duration significantly smaller than that required by the original TPI, consistently high performance is attained for all four neural computing methods. The new vibration data representation combined with neural computing approaches gives rise to a powerful expert and intelligent system for vehicle engine classification, which can find a great array of applications for civil, law enforcement, and military agencies for Intelligence, Surveillance and Reconnaissance purposes that are of crucial importance to national and international security. © 2018 Elsevier Ltd","Band-pass filters; Classification algorithms; Fourier transforms; Indexing; Machine learning; Neural networks; Remote sensing","Bandpass filters; Classification (of information); Deep learning; Engines; Fourier transforms; Indexing (of information); Intelligent systems; Laser Doppler velocimeters; Law enforcement; Learning systems; Military applications; Military vehicles; National security; Neural networks; Remote sensing; Signal encoding; Classification algorithm; Convolutional Neural Networks (CNN); Intelligence , surveillance and reconnaissances; International security; Laser Doppler vibrometers; Statistical properties; Two-dimensional arrays; Vehicle classification; Vibrations (mechanical)"
"Semantic segmentation based large-scale oil palm plantation detection using high-resolution satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072561967&doi=10.1117%2f12.2514438&partnerID=40&md5=c6257ca61ae5a57b753aedddf657064d","Detecting oil palm plantation from high-resolution satellite images can provide the necessary information for palm oil production estimation and oil palm plantation layout planning, etc. In this paper, we proposed a novel semantic segmentation based approach for large-scale oil palm plantation detection using QuickBird images and Google Earth Images (in 0.6-m spatial resolution) in Malaysia. We manually labeled a dataset for pixel-wise semantic segmentation into four categories: oil palm plantation, other vegetation, impervious/cloud, and the others (e.g. water and uncertain pixels). We presented an end-To-end deep convolutional neural network (DCNN) for semantic segmentation followed by fully connected conditional random fields (CRF) and applied an ensemble learning method to improve the localization of boundaries. The overall accuracy and mean IoU of our proposed approach in test regions are 95.27% and 88.46%, which are greatly better than the results of the other three common semantic segmentation methods and patch-based CNN method. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep learning; Oil palm; Remote sensing; Satellite images; Semantic segmentation","Automatic target recognition; Deep learning; Deep neural networks; Neural networks; Palm oil; Palmprint recognition; Pixels; Random processes; Remote sensing; Satellites; Semantics; Conditional random field; Convolutional neural network; High resolution satellite images; Oil palm; Oil palm plantations; Overall accuracies; Satellite images; Semantic segmentation; Image segmentation"
"Unsupervised deep domain adaptation for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079342626&doi=10.1109%2fIGARSS40859.2019.8976372&partnerID=40&md5=b47f4af059bd0e2d21999a1b85bf75b9","Deep neural networks have been proven to be a promising way for hyperspectral image (HSI) classification. Their success depends on a premise that source domain (i.e., training) and target domain (i.e., test) samples are identically distributed. However, due to various imaging environments, in practice obvious distribution discrepancy often exists between these two domains, which can dramatically reduce the capacity of the classifier trained in source domain generalizing to target domain. To mitigate this problem, we present a novel deep unsupervised domain adaptation framework for HSI classification, which can simultaneously align the distributions of two domains and learn a classifier in source domain. Firstly, we employ two auto-encoder networks to separately project the samples from two domains into two low-dimensional feature spaces. Then, a multi-level maximum mean discrepancy (MMD) loss is imposed on the feature space to reduce the distribution discrepancy between two domains. Given the resultant features, a classification subnet is further learned to classify the labeled samples in source domain. Since the classifier is trained based on the domain-invariant features, it can well generalize to the target domain. Experimental results on one benchmark cross-domain HSI datasets prove the superior performance of the proposed method. © 2019 IEEE.","Deep learning; Doman adaptation; Hyperspectral image classification","Benchmarking; Deep learning; Deep neural networks; Hyperspectral imaging; Remote sensing; Spectroscopy; Auto encoders; Cross-domain; Domain adaptation; Doman adaptation; Feature space; Invariant features; Low dimensional; Target domain; Image classification"
"Detecting ineligible features in agricultural fields on multitemporal high resolution satellite images using a two-stage DNN architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078159665&doi=10.1117%2f12.2533042&partnerID=40&md5=de75e57e11fe1234b0a28325ffda70f9","Agricultural fields are monitored for the purpose of EU subsidy eligibility checks. A precondition to make automatic monitoring of fields for this purpose possible is that the object geometric boundary is correct. This precondition can be addressed to some extent by performing image time series analysis to identify changes. Accurate object change detection in agricultural fields on satellite images requires separating object class changes such as new ditches, buildings, or roads from other changes, such as crop development, crop management practices, seasonal variation, or shadows from adjacent objects. In this paper we present an approach to identify unchanged agricultural fields using Deep Neural Networks. We propose a combination of CNNs for semantic segmentation and ConvLSTMs for change detection, applied to multitemporal satellite image time-series of arbitrary length. The neural networks were trained on images acquired over the Netherlands in 2017 by the TripleSat and PlanetScope constellations (0.8 and 3.5m resolution respectively) with RGB and NIR bands. We introduce techniques to create artificial change training data, reducing the need for real training data. The results demonstrate that (1) a neural network is not required to be deep to achieve usable semantic segmentation performance for satellite images for this application, that (2) ConvLSTMs can to some extent compensate imperfect image alignment and pixel misclassification, that (3) longer time series significantly increase the performance of the change detection and that (4) expanding and densifying the time series with lower resolution imagery does not improve accuracy in this particular configuration. © 2019 SPIE.","Change detection; CNN; ConvLSTM; Deep learning","Crops; Deep learning; Deep neural networks; Image segmentation; Object detection; Remote sensing; Satellite imagery; Semantics; Time series analysis; Automatic monitoring; Change detection; ConvLSTM; Crop management practices; Geometric boundaries; High resolution satellite images; Multi-temporal satellite images; Semantic segmentation; Image enhancement"
"Multisensor image fusion based on generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078141442&doi=10.1117%2f12.2533098&partnerID=40&md5=f38c3486368ebb972b244f944ea32e65","The paper addresses the further advance in our complex research in the field of multisensory image fusion based on generative adversarial models [1-2] and their application to such practical tasks as visual representation of fused images, acquired in different spectral ranges (e.g. TV and IR), and changes detection on images, acquired in different conditions (e.g. season-varying images). A developed architecture of a neural network based on pix2pix model is presented, which can solve the both tasks mentioned above. A technique for generating training and test datasets including data augmentation process is described. The results are demonstrated on real-world images. © 2019 SPIE.","Deep learning; Generative adversarial networks; Image fusion; Image processing; Multispectral images; Neural networks","Deep learning; Deep neural networks; Image acquisition; Image processing; Neural networks; Remote sensing; Adversarial networks; Changes detections; Data augmentation; Multisensor image fusion; Multisensory image fusions; Multispectral images; Real-world image; Visual representations; Image fusion"
"ClusterNet: Unsupervised generic feature learning for fast interactive satellite image segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078099077&doi=10.1117%2f12.2532796&partnerID=40&md5=21882a5c1bdf497e98af60be8c55e1e0","Semantic segmentation on satellite images is used to automatically detect and classify objects of interest over very large areas. Training a neural network for this task generally requires a lot of human-made ground truth classification masks for each object class of interest. We aim to reduce the time spent by humans in the whole process of image segmentation by learning generic features in an unsupervised manner. Those features are then used to leverage sparse human annotations to compute a dense segmentation of the image. This is achieved by essentially labeling groups of semantically similar pixels at once, instead of labeling each pixel almost individually using strokes. While we apply this method to satellite images, our approach is generic and can be applied to any image and to any class of objects on that image. © 2019 SPIE.","Clustering; Deep Learning; Interactive; Satellite Image; Segmentation; Unsupervised","Deep learning; Machine learning; Object detection; Pixels; Remote sensing; Satellites; Semantics; Clustering; Generic features; Human annotations; Interactive; Satellite images; Semantic segmentation; Unsupervised; Whole process; Image segmentation"
"Improvement of land classification in airports using 3D information","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076911996&doi=10.1117%2f12.2532975&partnerID=40&md5=bfad8d47aa922d822cfd455dfa459f5f","We describe in this paper how we mixed 3D information, i.e., a DSM (Digital Surface Model), for segmentation tasks in airport environments. The segmentation output classes were set to asphalt, concrete, and building classes because these are informative for distinguishing airport functionality. DSM is very informative for extracting buildings because airports are usually located on flat fields; however, high resolution DSMs are not provided for free. Therefore, we gathered adequate numbers of very-high-resolution satellite images and generated DSMs through stereo processing by ourselves. At the same time, we trained a modified U-NET for the initial segmentation. By leveraging the results of the segmentation, we identified ground pixels, i.e., asphalt or concrete, and calculated the ground height. Then, we applied an adaptive threshold algorithm to the DSMs by using the ground height and extracted building masks. Finally, we concatenated probability maps from the modified U-NET and building masks that represented the building class with a high precision in the flat airport fields. Consequently, we obtained better performance than the initial segmentation results, especially in the case of the building class. In experiments, we confirmed that the modified U-NET could detect asphalt and concrete with a high precision and that it was possible to identify ground pixels and extract building masks. The performance of our approach was improved by 20%, especially in detecting the building class. For future work, we will improve the quality of stereo processing and combine size specific detectors to achieve more accurate detection. © 2019 SPIE.","CNN; deep learning; land classification; MVS; semantic segmentation; U-NET; VHR satellite images","Airports; Asphalt; Buildings; Classification (of information); Concretes; Deep learning; Image segmentation; Pixels; Remote sensing; Semantics; Urban planning; Adaptive threshold algorithm; Airport environment; Digital surface models; Extracting buildings; Initial segmentation; Satellite images; Semantic segmentation; Very high resolution satellite images; Stereo image processing"
"Initial investigations into using an ensemble of deep neural networks for building facąde image semantic segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076910657&doi=10.1117%2f12.2532828&partnerID=40&md5=caf84b3b5a733a064bb300868c0e483c","Due to now outdated construction technology, houses which have not been retrofitted since construction typically fail to meet modern energy performance levels. However, identifying at a city scale which houses could benefit the most from retrofit solutions is currently a labour intensive process. In this paper, a system that uses a vehicle mounted camera to capture pictures of residential buildings and then performs semantic segmentation to differentiate components of captured buildings is presented. An ensemble of U-Net semantic segmentation models are trained to identify walls, roofs, chimneys, windows and doors from building facąde images and differentiate between window and door instances which are partially visible or obscured. Results show that the ensemble of U-Net models achieved high accuracy in identifying walls, roofs and chimneys, moderate accuracy in identifying windows and low accuracy in identifying doors and instances of windows and doors which were partially visible or obscured. When U-Net models were retrained to identify doors or windows, irrespective of partially visible and obscured instances, a significant rise in door and window identification accuracy was observed. It is believed that a larger training dataset would produce significantly improved results across all classes. The results presented here prove the operational feasibility in the first part of a process to combine this model with high-resolution thermography and GPS for automating building retrofitting evaluations. © 2019 SPIE.","building retrofit; deep learning; environmental modelling; image segmentation; U-Net","Chimneys; Deep learning; Deep neural networks; Doors; Remote sensing; Retrofitting; Roofs; Semantic Web; Semantics; Urban planning; Walls (structural partitions); Building retrofits; Building retrofitting; Construction technologies; Environmental modelling; Identification accuracy; Operational feasibility; Residential building; Semantic segmentation; Image segmentation"
"Species distribution mapping of grass clover leys using images for targeted nitrogen fertilization","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073756565&doi=10.3920%2f978-90-8686-888-9_79&partnerID=40&md5=ff53f2b13640fbb1f8721f111f9ae5f5","Targeted nitrogen fertilization of grass clover mixtures, based on the local distribution of grass and clover, can potentially increase yield, improve yield quality, and reduce nitrate leaching. In this work, the grass and clover distributions of 150 ha were mapped based on image recognition in colour images. With a high-speed down-facing image acquisition platform mounted on an ATV, the fields were systematically traversed in October 2018 to capture 17,759 geotagged and spatially distributed images. Using a trained deep convolutional neural network, the images were automatically analysed to classify every pixel and estimate the species distributions in each image. Interpolation of the clover distribution into maps showed a high variation of clover content between the fields, and frequently within them. Site-specific nitrogen fertilization strategies were then produced based on the maps. © Wageningen Academic Publishers 2019","Deep learning; Plant classification; Remote sensing; Semantic segmentation; Species mapping","Deep learning; Deep neural networks; Image recognition; Neural networks; Nitrogen; Precision agriculture; Remote sensing; Semantics; Convolutional neural network; Local distributions; Nitrate leaching; Nitrogen fertilization; Plant classification; Semantic segmentation; Species distributions; Species mapping; Population distribution"
"Estimation of Forest Carbon from Aerial Photogrammetry","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068340662&doi=10.1007%2f978-3-030-21077-9_10&partnerID=40&md5=b21abf6dad6ba0a5e8255fcf9053cb71","Quantifying tree biomass is a critical process for carbon stock estimation at the stand, landscape, and national levels. A major challenge for forest managers is the amount of effort involved to document carbon storage levels, especially in terms of human labor. In this paper, we propose a method to quantify the amount of carbon in forest stands. In our approach, we obtain aerial images from where we build 3D reconstructions of the terrain. Using the resulting orthomosaics, we identify individual trees and process their point clouds to extract information to estimate tree the height and to infer the diameter, which we employ in allometric equations to compute carbon content. We compare our results with carbon estimates obtained from allometric equations applied to manual tree diameter and height measurements. © 2019, Springer Nature Switzerland AG.","Carbon estimation; Deep learning; Remote sensing; Tree detection","Antennas; Deep learning; Pattern recognition; Photogrammetry; Remote sensing; 3D reconstruction; Aerial photogrammetry; Allometric equations; Carbon estimations; Extract informations; Forest managers; Tree detections; Tree diameter and heights; Forestry"
"Global-Local Attention Network for Aerial Scene Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067195795&doi=10.1109%2fACCESS.2019.2918732&partnerID=40&md5=d4ae93b28240c6f440ced43496bcd8e4","The classification performance of aerial scenes relies heavily on the discriminative power of feature representation from high-spatial resolution remotely sensed imagery. The convolutional neural networks (CNNs) have recently been applied to adaptively learn image features at different levels of abstraction rather than requiring handcrafted features and achieved state-of-the-art performance. However, most of these networks focus on multi-stage global feature learning yet neglect the local information, which plays an important role in scene recognition. To address this issue, a novel end-to-end global-local attention network (GLANet) is proposed to capture both global and local information for aerial scene classification. FC layers in the VGGNet are replaced by the global attention (GA) branch and local attention (LA) branch, one of which learns the global information while the other learns the local semantic information via attention mechanisms. During each training, the labels of input images can be predicted by the local, global, and their concatenated features using softmax. According to different predicted labels, two auxiliary loss functions are further computed and imposed on the proposed network to enhance the supervision for network learning. The experimental results on three challenging large-scale scene datasets demonstrate the effectiveness of the proposed global-local attention network. © 2013 IEEE.","deep learning; global-local attention network; remote sensing; Scene classification","Antennas; Deep learning; Large dataset; Neural networks; Remote sensing; Semantics; Classification performance; Convolutional neural network; Global and local informations; Global-local; High spatial resolution; Remotely sensed imagery; Scene classification; State-of-the-art performance; Classification (of information)"
"State of the art of machine learning models in energy systems, a systematic review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065016714&doi=10.3390%2fen12071301&partnerID=40&md5=4a12e6969d000c09f96b650a115b65bb","Machine learning (ML) models have been widely used in the modeling, design and prediction in energy systems. During the past two decades, there has been a dramatic increase in the advancement and application of various types of ML models for energy systems. This paper presents the state of the art of ML models used in energy systems along with a novel taxonomy of models and applications. Through a novel methodology, ML models are identified and further classified according to the ML modeling technique, energy type, and application area. Furthermore, a comprehensive review of the literature leads to an assessment and performance evaluation of the ML models and their applications, and a discussion of the major challenges and opportunities for prospective research. This paper further concludes that there is an outstanding rise in the accuracy, robustness, precision and generalization ability of the ML models in energy systems using hybrid ML models. Hybridization is reported to be effective in the advancement of prediction models, particularly for renewable energy systems, e.g., solar energy, wind energy, and biofuels. Moreover, the energy demand prediction using hybrid models of ML have highly contributed to the energy efficiency and therefore energy governance and sustainability. © 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).","ANFIS; Artificial neural networks (ANN); Big data; Blockchain; Decision tree (DT); Deep learning; Energy demand; Energy informatics; Energy systems; Ensemble; Forecasting; Hybrid models; Internet of things (IoT); Machine learning; Neuro-fuzzy; Prediction; Remote sensing; Renewable energy systems; Smart sensors; Support vector machines (SVM); Wavelet neural network (WNN)","Big data; Blockchain; Decision trees; Energy efficiency; Energy management; Forecasting; Fuzzy inference; Fuzzy neural networks; Internet of things; Learning systems; Machine learning; Neural networks; Remote sensing; Smart sensors; Solar energy; Support vector machines; Trees (mathematics); Wind power; ANFIS; Energy demands; Energy informatics; Energy systems; Ensemble; Hybrid model; Internet of Things (IOT); Neuro-Fuzzy; Renewable energy systems; Wavelet neural networks; Deep learning"
"11th International Conference on Advanced Computing and Communication Technologies, ICACCT 2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049881161&partnerID=40&md5=db62c5e6d5eeaf415b93395600dd9319","The proceedings contain 20 papers. The special focus in this conference is on Advanced Computing and Communication Technologies. The topics include: An enhanced approach for detecting helmet on motorcyclists using image processing and machine learning techniques; Affinity-aware synchronization in work stealing run-times for NUMA multi-core processors; back-propagation neural network versus logistic regression in heart disease classification; wormhole attack in wireless sensor networks: A critical review; a general framework for spectrum assignment in cognitive radio networks; novel schemes for energy-efficient IoT; minimizing the total range with two power levels in wireless sensor networks; comparative study of rectangular and e-shaped microstrip patch antenna array for x-band applications; an archimedean spiral-shaped frequency-selective defected structure for narrowband high Q applications; gathering in the discrete domain: State of the art; Challenge-response pair (CRP) generator using schmitt trigger physical unclonable function; Modified SA algorithm for bi-objective robust stochastic cellular facility layout in cellular manufacturing systems; fuzzy time series forecasting method using probabilistic fuzzy sets; Ohmic–Viscous dissipation and heat generation/absorption effects on MHD nanofluid flow over a stretching cylinder with suction/injection; interlocking nodes for structural analysis in social networking; a novel construction method of intuitionistic fuzzy set from fuzzy set and its application in multi-criteria decision-making problem; design of an audio repository for blind and visually impaired: A case study; a futuristic deep learning framework approach for land use-land cover classification using remote sensing imagery.",,
"Automatic localization of phoenix by satellite image analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068708312&doi=10.22268%2fAJPP-037.2.083088&partnerID=40&md5=fb970e441d0ea07aacc729289adfd938","The Red palm weevil (RPW) Rhynchophorus ferrugineus is becoming one of the deadliest pests of the palms in the world. In order to effectively implement a RPW control programme to achieve rapid regression of this pest, it is necessary to have GPS coordinates of each palm present on the control perimeter. This location makes it possible to establish maps and databases which are essential for organizing, at the local and national level, the implementation and permanent monitoring of control measures. It is difficult, time-consuming and expensive to locate palms by visually exploring the entire perimeter from the ground. In the zone of regular plantations, this work can be processed but it becomes extremely heavy in the traditional oasis like in urban environment where the distribution of the palms is very irregular. With advances in satellite imagery, it is possible to acquire high quality images at very short intervals of time from a standard format for a large part of the earth. Combined with the progress of machine learning, particularly deep learning, this amount of data is able to feed a robust model. It would allow to automate the detection of palms at large scale and monitor their evolution at very short intervals, which in the fight against RPW is valuable information. This first work wants to test the interest in this solution. We build and train a convolution neural network in order to find two species of palms Phoenix canariensis and Phoenix dactylifera (C&amp;D) in a very heterogeneous area of 100 km2. Our model evaluation shows that 1/5 of the objects found are false positive and more than 2/3 of C&amp;D are perfectly localized. These first results could be improved greatly by implementing a more robust algorithm using more data and using larger colour spectrum (as near infra-red). The question of the infested palms detection using satellite imagery and machine learning stays open. © 2019 Arab Society for Plant Protection","Convolution neural network; Machine learning; Remote sensing; Rhynchophorus ferrugineus; Vision",
"2nd International Conference on Recent Trends in Image Processing and Pattern Recognition, RTIP2R 2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070237970&partnerID=40&md5=07b878e3aed7c3f45fee12cb9c7a8aab","The proceedings contain 173 papers. The special focus in this conference is on Recent Trends in Image Processing and Pattern Recognition. The topics include: Execution and Performance Evaluation of Cognitive and Expressive Event on a robotic Arm; multiple Decompositions-Based Blind Watermarking Scheme for Color Images; skeleton Joint Difference Maps for 3D Action Recognition with Convolutional Neural Networks; Novel Quality Metric for Image Super Resolution Algorithms - Super Resolution Entropy Metric (SREM); shape Descriptor Based on Centroid with Chord Lengths for Image Retrieval; learning Deep Feature Representation for Face Spoofing; ExNET: Deep Neural Network for Exercise Pose Detection; background Subtraction and Kalman Filter Algorithm for Object Tracking; A Blind Color Image Watermarking Using BRISK Features and Contourlet Transform; let Vehicles Talk with Images for Smart Vehicular Communication System; gray Level Face Recognition Using Spatial Features; application of Gabor Filter for Monitoring Wear of Single Point Cutting Tool; performance Analysis of Log-Gabor Based Multimodal Systems and Its Fusion Techniques; Temporal Super-Pixel Based Convolutional Neural Network (TS-CNN) for Human Activity Recognition in Unconstrained Videos; Content Based Video Retrieval Using SURF, BRISK and HARRIS Features for Query-by-image; An Empirical Study: ELM in Face Matching; optimal Selection of Bands for Hyperspectral Images Using Spectral Clustering; Classification of Natural Flower Videos Through Sequential Keyframe Selection Using SIFT and DCNN; Hyperspectral Remote Sensing Image Analysis with SMACC and PPI Algorithms for Endmember Extraction; real Time Hand Gesture Recognition for Differently-Abled Using Deep Learning; a Novel Foreground Segmentation Method Using Convolutional Neural Network; image Fusion Technique Using Gaussian Pyramid.",,
"11th International Conference on Computational Collective Intelligence, ICCCI 2019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072860943&partnerID=40&md5=ff5f0894ec15550b2eaf0b5047e60b0e","The proceedings contain 117 papers. The special focus in this conference is on Computational Collective Intelligence. The topics include: Fusion of LBP and hu-moments with fisher vectors in remote sensing imagery; an enhanced deep learning framework for skin lesions segmentation; Fully convolutional encoder-decoder architecture (FCEDA) for skin lesions segmentation; decision support system for assignment of conference papers to reviewers; co-evolution dynamics between individual strategy and gaming environment under the feedback control; fuzzy inference model for punishing a perpetrator in a judicial process; research methodology trending in evolutionary computing; visualizing a linguistic ontology with ling-graph; dynamic airspace configuration: A short review of computational approaches; comparison of heuristic algorithms for path planning in 3D printing with multistage experimentation system; an integer programming model for the capacitated vehicle routing problem with drones; current trends in the population-based optimization; fuzzy shapley value-based solution for communication network; new optimization algorithm based on free dynamic schema; VNS-based multi-agent approach to the dynamic vehicle routing problem; refining the imprecise meaning of non-determinism in the web by strategic games; Imprecise data handling with MTE; the relationship between collective intelligence and one model of general collective intelligence; the stable model semantics of normal fuzzy linguistic logic programs; an approach to imbalanced data classification based on instance selection and over-sampling; gender-based insights into the fundamental diagram of pedestrian dynamics; active redundancy allocation in complex systems by using different optimization methods; mining stock market time series and modeling stock price crash using a pretopological framework; preface; modelling stereotyping in cooperation systems.",,
"2nd CCF International Conference on Artificial Intelligence, CCF-ICAI 2019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071427394&partnerID=40&md5=f9c44bc71bfc3d21222240e88ad35922","The proceedings contain 23 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Performance evaluation of visual object detection for moving vehicle; large scale name disambiguation using rule-based post processing combined with Aminer; hybrid recommendation algorithm based on weighted bipartite graph and logistic regression; academic paper recommendation based on clustering and pattern matching; The methods for reducing the number of OOVs in Chinese-Uyghur NMT system; joint kernel low-rank graph construction and subspace learning; defending network traffic attack with distributed multi-agent reinforcement learning; ensemble regression kernel extreme learning machines for multi-instance multi-label learning; a variable precision reduction type for information systems; sub-pixel upsampling decode network for semantic segmentation; quantum theories with hypergestures: A new approach to musical semantics; development of automatic grounding wire working robot for substation; intelligent control system for substation automatic grounding wire operation robot; application of support vector machine model based on an improved elephant herding optimization algorithm in network intrusion detection; tracking algorithm based on dual residual network and kernel correlation filters; image recognition of peanut leaf diseases based on capsule networks; multi-scale convolutional neural network for road extraction in remote sensing imagery; TCPModel: A short-term traffic congestion prediction model based on deep learning; 3D level set method via local structure similarity factor for automatic neurosensory retinal detachment segmentation in retinal SD-OCT images; real-time scale adaptive visual tracking with context information and correlation filters; an improved word spotting method for printed Uyghur document image retrieval.",,
"28th International Conference on Artificial Neural Networks, ICANN 2019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072861154&partnerID=40&md5=92a931e23904e00ba401c24621e1c527","The proceedings contain 320 papers. The special focus in this conference is on Artificial Neural Networks. The topics include: Comparison between U-net and U-renet models in OCR tasks; severe convective weather classification in remote sensing images by semantic segmentation; action recognition based on divide-and-conquer; an adaptive feature channel weighting scheme for correlation tracking; in-silico staining from bright-field and fluorescent images using deep learning; a lightweight neural network for hard exudate segmentation of fundus image; attentional residual dense factorized network for real-time semantic segmentation; random drop loss for tiny object segmentation: application to lesion segmentation in fundus images; flow2seg: Motion-aided semantic segmentation; distortion estimation through explicit modeling of the refractive surface; COCO_TS dataset: Pixel–level annotations based on weak supervision for scene text segmentation; learning deep structured multi-scale features for crisp and object occlusion edge detection; graph-boosted attentive network for semantic body parsing; a global-local architecture constrained by multiple attributes for person re-identification; recurrent connections aid occluded object recognition by discounting occluders; learning relational-structural networks for robust face alignment; An efficient 3D-NAS method for video-based gesture recognition; Robustness of deep LSTM networks in freehand gesture recognition; delving into the impact of saliency detector: A gemininet for accurate saliency detection; FCN salient object detection using region cropping; eye movement-based analysis on methodologies and efficiency in the process of image noise evaluation; object-level salience detection by progressively enhanced network; action unit assisted facial expression recognition; discriminative feature learning using two-stage training strategy for facial expression recognition; Action units classification using clusWISARD; constraint-based visual generation.",,
"Deep Learning Approach for Rock Outcrops Identification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061826424&doi=10.1109%2fEORSA.2018.8598577&partnerID=40&md5=cfa6766ba97e46df87ccae92ae9ff9c5","The steep natural terrain in Hong Kong is susceptible to shallow, small to medium-sized landslides induced by rainfall. These landslides usually involve failures occurring within the top one to two meters of the surface mantle. Terrain features manifested on the ground surface, such as historical landslides, rock outcrops, tension cracks, depressions could affect the terrain's susceptibility to landsliding in future. Identification of these features by conventional means involves substantial resources for Aerial Photo Interpretation (API) and field mapping. A pilot study was carried out to explore the potential of using deep learning to enhance the efficiency in identifying rock outcrops at a territory-wide scale with a view to improving the landslide susceptibility analysis. A methodology of combining Convolutional Neural Network and remote sensing techniques has been developed to derive a very high-resolution map of rock outcrops exposed on the natural terrain throughout the entire Hong Kong territory. The developed algorithm considers the spatial relationship and texture of the surrounding pixels as well as the spectral signature of the pixel of remote sensing imageries. The identification of rock outcrops has been conducted using the orthophotos acquired in years 2012 and 2015, with the aid of SPOT satellite images acquired in year 2015 and airborne LiDAR data acquired in year 2010, and resulting in a five-meter spatial resolution rock outcrops map. The results were promising when validated with the results mapped by engineering geologists using API on the same sets of orthophotos. The developed algorithm provides an alternative for leveraging the balance between spectral and spatial resolution, and for mapping the natural surface features and enhancing the landslide susceptibility analysis. © 2018 IEEE.","Aerial Images; Convolutional Neural Network; Deep Learning; Land Cover Mapping; Rock Outcrops","Antennas; Convolution; Image resolution; Landforms; Landslides; Neural networks; Observatories; Photomapping; Pixels; Remote sensing; Rocks; Textures; Aerial images; Aerial photo interpretation; Convolutional neural network; Engineering geologists; Land cover mapping; Landslide susceptibility; Remote sensing imagery; Remote sensing techniques; Deep learning"
"CapsNet and Triple-GANs Towards Hyperspectral Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061799275&doi=10.1109%2fEORSA.2018.8598574&partnerID=40&md5=0ff3766f42a29eadfee8ea30cc4da5f2","Hyperspectral processing technology becomes one of the most focused issues in remote sensing field. In the hyperspectral classification, significant improvements have been achieved by various deep learning methods. In general, deep learning algorithms adopt a cascade of layers to extract the hierarchical features. However, the deep hierarchical property will cause some defects such as overfitting and gradient vanishing. In this paper, a hybrid method based on CapsNet and Triple-GANs has been explored to avoid overfitting and extract the effective features. Unlike ordinary CNN, the CapsNet is the consist of a group of capsules with vectorizing the activation output which could consider not only spectral deep features but also the relative locations of these features. The Triple-GANs is a game system with three players: a generator, a classifier and a discriminator. When the Triple-GANs converges to balance, the credible labelled samples could been obtained by the generator which boost the CapsNet in the classification task. The main contents are as follows: 1)By introducing the CapsNet in the hyperspectral feature extraction, the 2D convolution operations are replaced by 1D to adapt the pixel-wised spectral features. 2) Use the Triple-GANs and CapsNet to do the hyperspectral classification on small training dataset. Experimental results show that this algorithm can obviously improve the performance of classification compared with the traditional methods. © 2018 IEEE.","CapsNet; hyperspectral classification; Triple-GANs","Deep learning; Learning algorithms; Observatories; Remote sensing; CapsNet; Classification tasks; Hierarchical features; Hyper-spectral classification; Processing technologies; Relative location; Spectral feature; Triple-GANs; Classification (of information)"
"Improving high-resolution satellite images retrieval using Linear SVM classifier and data augmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061797855&doi=10.1109%2fPAIS.2018.8598536&partnerID=40&md5=5c26db3a185b71eb3f3aa8f5400436ab","The availability of huge remote sensing image dataset imposes recourse to powerful techniques of content-based image retrieval for archiving and mining. This paper propose descriptors based on the SIFT (Scale invariant features) combined with SVM linear classification. To build a powerful image classifier using very little training data, image augmentation is usually required to boost the performance of the classification. For this reason, an augmentation data is used to increase the training data for the SVM (Support vector Machine) classifier. The creation of the training data is done using several techniques of augmentation: anisotropic filter. We report a first evaluation of the CBIR (Content based image retrieval) and the second evaluation of the system aims to compare the deep learning with the boosted SVM classification. © 2018 IEEE.","CBIR; Data augmentation; Descriptors; SIFT; SVM classifier","Content based retrieval; Deep learning; Filtration; Image classification; Image enhancement; Intelligent systems; Remote sensing; Search engines; Support vector machines; CBIR; Data augmentation; Descriptors; SIFT; SVM classifiers; Classification (of information)"
"Hyperspectral Image Classification Based on Long Short Term Memory Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061777973&doi=10.1109%2fEORSA.2018.8598645&partnerID=40&md5=b396ab40284ebb29d0dcd6c6ef55ae0a","In the task of hyperspectral image classification, how to learn features of hyperspectral image is the important and difficulty issue which may directly affect the classification results. Inspired by the idea of natural language processing, in this paper, we propose a local space long short-term memory network based hyperspectral image classification, which constructs sequential features in the local area of hyperspectral images. This method is based on the integration features of two traditional low-level features, and from these integration features to extract sequential features of the center sample in the local space, then use the long short-term memory network to learn high-level semantic features, finally use them to classify image. This method can not only obtain more representative and discriminative high-level semantic features, and through constructing the local space sequence to enhance positive impact of the useful pixels, inhibit negative effects of useless pixels, it improves the classification accuracy. © 2018 IEEE.","deep learning; hyperspectral image; long short term memory network; semantic features","Brain; Classification (of information); Deep learning; Hyperspectral imaging; Long short-term memory; Natural language processing systems; Observatories; Pixels; Remote sensing; Semantics; Space optics; Spectroscopy; Classification accuracy; Classification results; High-level semantic features; Local spaces; Low-level features; NAtural language processing; Semantic features; Short term memory; Image classification"
"Multi-Losses Function Based Convolution Neural Network for Single Hyperspectral Image Super-Resolution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061774012&doi=10.1109%2fEORSA.2018.8598551&partnerID=40&md5=3dc503ea8f303fdaa42ea1c6040c710b","Recently deep convolutional neural network (CNN) has made significant achievement in Single Image Super-Resolution (SISR). Most CNN-based SISR methods used the default L2 norm of the error. However, for Hyperspectral Image (HSI), this loss function may bring spectral inconsistencies. The main reason is that most methods did not pay much attention to spectral loss. To HSI, the loss function should capture not only spatial information but also spectral consistency. In this paper, a Multi-Losses Function Network (MLFN) simultaneously considering spatial and spectral information is proposed, and is composed of two parts: one is Concatenate Dense Residual Network (CDRN), and the other is Loss Network (LN). CDRN is an image reconstruction network which can utilize the hierarchical features extracted from the low-resolution image. LN includes pixel-wise spatial loss and spectral loss which drive the learning of the entire reconstruction model. The experimental results prove that the proposed MLFN can enhance spatial resolution with the consistency of the spectrum of HSI preserved. © 2018 IEEE.","Deep Learning; Hyperspectral Image; Spectral Consistency; Super-Resolution","Convolution; Deep learning; Deep neural networks; Hyperspectral imaging; Neural networks; Observatories; Optical resolving power; Remote sensing; Spectroscopy; Convolution neural network; Convolutional neural network; Hierarchical features; Image super resolutions; Low resolution images; Reconstruction networks; Spectral Consistency; Super resolution; Image reconstruction"
"Estimating Regional Ground-Level PM 2.5 Directly From Satellite Top-Of-Atmosphere Reflectance Using Deep Belief Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058942588&doi=10.1029%2f2018JD028759&partnerID=40&md5=cea45b3e5e243e11cbee986c4cb9dd86","Almost all remote sensing atmospheric PM 2.5 estimation methods need satellite aerosol optical depth (AOD) products, which are often retrieved from top-of-atmosphere (TOA) reflectance via an atmospheric radiative transfer model. Then, is it possible to estimate ground-level PM 2.5 directly from satellite TOA reflectance without a physical model? In this study, this challenging work was achieved based on a machine learning model. Specifically, we established the relationship between PM 2.5 , satellite TOA reflectance, observation angles, and meteorological factors in a deep learning architecture (denoted as Ref-PM modeling). This relationship was trained with station PM 2.5 measurements, and then the PM 2.5 values of those locations without stations could be retrieved. Taking the Wuhan Urban Agglomeration as a case study, the results demonstrate that, compared with AOD-PM modeling, the Ref-PM modeling obtains a competitive performance, with sample-based cross-validated R 2 and root-mean-square error values of 0.87 and 9.89 μg/m 3 , respectively. Also, the TOA-reflectance-derived PM 2.5 has a finer resolution and a larger spatial coverage than the AOD-derived PM 2.5 . This work provides an alternative technique to estimate ground-level PM 2.5 , and may have the potential to promote the application in atmospheric environmental monitoring. ©2018. American Geophysical Union. All Rights Reserved.","deep learning; PM2.5; satellite remote sensing; TOA reflectance","aerosol composition; atmospheric modeling; machine learning; optical depth; particle size; particulate matter; radiative transfer; remote sensing; satellite altimetry; surface reflectance; urban atmosphere; China; Hubei"
"A scalable machine learning system for pre-season agriculture yield forecast","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061403101&doi=10.1109%2feScience.2018.00131&partnerID=40&md5=17be4f6dec815b949c4339788d28664a","Yield forecast is essential to agriculture stakeholders and can be obtained with the use of machine learning models and data coming from multiple sources. Most solutions for yield forecast rely on NDVI (Normalized Difference Vegetation Index) data which, besides being time-consuming to acquire and process, only allows forecasting once crop season has already started. To bring scalability for yield forecast, in the present paper we describe a system that incorporates satellite-derived precipitation and soil properties datasets, seasonal climate forecasting data from physical models and other sources to produce a pre-season prediction of soybean/maize yield-with no need of NDVI data. This system provides significantly useful results by the exempting the need for high-resolution remote-sensing data and allowing farmers to prepare for adverse climate influence on the crop cycle. In our studies, we forecast the soybean and maize yields for Brazil and USA, which corresponded to 44% of the world's grain production in 2016. Results show the error metrics for soybean and maize yield forecasts are comparable to similar systems that only provide yield forecast information in the first weeks to months of the crop cycle. © 2018 IEEE.","Agriculture; Crop yield forecasting; Deep learning; LSTM; Machine learning; Maize; RNN; Soybean","Agriculture; Climate models; Crops; Deep learning; Learning systems; Long short-term memory; Machine learning; Remote sensing; Crop yield forecasting; High resolution remote sensing; LSTM; Machine learning models; Maize; Normalized difference vegetation index; Scalable machine learning; Soybean; Forecasting"
"Remote Sensing Images Recognition by Deep Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061030151&doi=10.1109%2fICRAE.2018.8586678&partnerID=40&md5=471d8f45acc4e296243d12177a9ce8dd","Remote sensing image recognition is widely applied, such as agriculture, forest monitoring and meteorology. The effect of remote sensing application analysis greatly depends on the accuracy of image recognition. In this paper, we proposed an image recognition method for remote sensing images using deep convolutional neural network. The neural network learned the feature via training data automatically. After training, the network can recognize query images in a relative high accuracy. According to the experimental results, the proposed method shows a good performance on a given remote sensing image dataset. © 2018 IEEE.","convolutional neural networks; deep learning; neural networks; remote sensing image recognition","Convolution; Deep learning; Deep neural networks; Image recognition; Neural networks; Robotics; Convolutional neural network; Forest monitoring; High-accuracy; Query images; Recognition methods; Remote sensing applications; Remote sensing images; Training data; Remote sensing"
"Low-Altitude UAV Imagery Based Cross-Section Geological Feature Recognition via Deep Transfer Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061030878&doi=10.1109%2fICRAE.2018.8586733&partnerID=40&md5=5746580d320a0b366899d57848791eac","Most remote-sensing systems obtain terrain surface images from vertical observation by satellites and high-altitude aircrafts. However, this can be inadequate for subsurface geological image acquisition. Indeed, subsurface or undersurface geological information is more essential for geological and civil engineering. In this work, we proposed a subsurface geological feature recognition approach: first, cross-section geological images are captured horizontally by low-altitude UAV. Later, deep transfer learning model is trained to classify the type of geological mass exposed on the cross-section. We also examined different deep transfer learning models regarding their classification performances. The result suggests that Inception-ResNet-V2 based model outperformed others by reaching an overall accuracy of 99.72%. © 2018 IEEE.","cross-section feature; deep transfer learning; remote-sensing; UAV","Geology; Remote sensing; Robotics; Unmanned aerial vehicles (UAV); Classification performance; cross-section feature; Geological features; Geological information; High-altitude aircrafts; Overall accuracies; Remote sensing system; Transfer learning; Deep learning"
"Space target motion salient classification using polarimetric retina vision sensing principles","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060698832&doi=10.1109%2fIST.2018.8577175&partnerID=40&md5=aa86425d67fc71f34a91f7c12addc249","A new remote sensing retina vision system aimed at classifying rapid moving objects, such as Space debris, based on their motion patterns, is presented. The purpose of this study is to investigate how different types of target complex motion patterns can be detected and discriminated with high accuracy. The remote retina vision sensing system consists of an asynchronous event-based neuromorphic camera coupled with polarization filters enabling improved detection, tracking, and discrimination, with high contrast and dynamic range; a spinning light modulating wheel, operating at varying angular frequency, is placed in front of a static target. The outcome of this study indicates that deep learning combined with Polarimetric Dynamic Vision Sensor p(DVS) principles is well suited to accurately classify targets based on distinct salient features, such as motion patterns, rapidly, at low operational bandwidth, low-power consumption, and storage. © 2018 IEEE.","complex motion patterns; deep learning; detection and classification of targets; detection and discrimination of non-cooperative Space target; neuromorphic imaging; Polarimetric Dynamic Vision Sensor p(DVS); Retina vision sensors; salient features detection; Space debris","Deep learning; Imaging systems; Motion analysis; Polarimeters; Remote sensing; Time and motion study; Voltage scaling; Complex motion; Dynamic vision sensors; Neuromorphic; Non-cooperative space targets; Salient features; Vision sensors; Space debris"
"Uncertainty gated network for land cover segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058620868&doi=10.1109%2fCVPRW.2018.00052&partnerID=40&md5=3c9778427e98d476458310383674e80a","The production of thematic maps depicting land cover is one of the most common applications of remote sensing. To this end, several semantic segmentation approaches, based on deep learning, have been proposed in the literature, but land cover segmentation is still considered an open problem due to some specific problems related to remote sensing imaging. In this paper we propose a novel approach to deal with the problem of modelling multiscale contexts surrounding pixels of different land cover categories. The approach leverages the computation of a heteroscedastic measure of uncertainty when classifying individual pixels in an image. This classification uncertainty measure is used to define a set of memory gates between layers that allow a principled method to select the optimal decision for each pixel. © 2018 IEEE.",,"Computer vision; Deep learning; Maps; Pixels; Semantics; Heteroscedastic; Measure of uncertainty; Optimal decisions; Remote sensing imaging; Semantic segmentation; Specific problems; Thematic maps; Uncertainty measures; Remote sensing"
"Automatic large-scale 3D building shape refinement using conditional generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058891213&doi=10.1109%2fCVPRW.2018.00249&partnerID=40&md5=2dd1891edf9e51a0e375c27f805608f8","Three-dimensional realistic representations of buildings in urban environments have been increasingly applied as data sources in a growing number of remote sensing fields such as urban planning and city management, navigation, environmental simulation (i.e. flood, earthquake, air pollution), 3D change detection after events like natural disasters or conflicts, etc. With recent technological developments, it becomes possible to acquire high-quality 3D input data. There are two main ways to obtain elevation information: from active remote sensing systems, such as light detection and ranging (LIDAR), and from passive remote sensing systems, such as optical images, which allow the acquisition of stereo images for automatic digital surface models (DSMs) generation. Although airborne laser scanning provides very accurate DSMs, it is a costly method. On the other hand, the DSMs from stereo satellite imagery show a large coverage and lower costs. However, they are not as accurate as LIDAR DSMs. With respect to automatic 3D information extraction, the availability of accurate and detailed DSMs is a crucial issue for automatic 3D building model reconstruction. We present a novel methodology for generating a better-quality stereo DSM with refined buildings shapes using a deep learning framework. To this end, a conditional generative adversarial network (cGAN) is trained to generate accurate LIDAR DSM-like height images from noisy stereo DSMs. © 2018 IEEE.",,"Air navigation; Computer vision; Deep learning; Disasters; Geometrical optics; Optical radar; Remote sensing; Satellite imagery; Three dimensional computer graphics; 3d information extractions; Active remote sensing systems; Airborne Laser scanning; Digital surface models; Environmental simulation; Light detection and ranging; Passive remote sensing; Technological development; Stereo image processing"
"Proceedings - 2018 15th Conference on Computer and Robot Vision, CRV 2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060544136&partnerID=40&md5=930c1faa276dd18805d70abf21b02540","The proceedings contain 50 papers. The topics discussed include: real-time deep hair matting on mobile devices; occluded leaf matching with full leaf databases using explicit occlusion modeling; deep autoencoders with aggregated residual transformations for urban reconstruction from remote sensing data; real-time end-to-end action detection with two-stream networks; generalized Hadamard-product fusion operators for visual question answering; on the robustness of deep learning models to universal adversarial attack; real-time 3D face verification with a consumer depth camera; hierarchical feature map characterization in fashion interpretation; and a hierarchical deep architecture and mini-batch selection method for joint traffic sign and light detection.",,
"A CNN-Based Approach for Automatic Building Detection and Recognition of Roof Types Using a Single Aerial Image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060161759&doi=10.1007%2fs41064-018-0060-5&partnerID=40&md5=2069d55eee9ca4f17c9caf097fb16d4c","Automatic detection and reconstruction of buildings have become essential in many remote sensing and computer vision applications. In this paper, the capability of Convolutional Neural Networks (CNNs) is investigated for building detection as well as recognition of roof shapes using a single image. The major steps are including training dataset generation, model training, image segmentation, building detection and roof shape recognition. First, a CNN is trained for extracting urban objects such as trees, roads and buildings. Next, classification of different roof types into flat, gable and hip shapes is performed using the second trained CNN. The assessment results prove effectiveness of the proposed method with approximately 97% and 92% of quality rates in detection and recognition steps, respectively. © 2019, Deutsche Gesellschaft für Photogrammetrie, Fernerkundung und Geoinformation (DGPF) e.V.","3D modelling; Convolutional neural network (CNN); Deep learning (DL); Fine-tuning; Pattern recognition; Selective search",
"Scene-Coupled Intelligent Multi-Task Detection Algorithm for Air-to-Ground Remote Sensing Image [场景耦合的空对地多任务遥感影像智能检测算法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062024398&doi=10.3788%2fAOS201838.1215008&partnerID=40&md5=04636f8eed567880501aa2c1fc3e0b36","In air-to-ground remote sensing detection, the object has the characteristics of small field of view and single viewing angle, which is susceptible to background interference. At the same time, the height of the field of view varies greatly, which brings challenges to the traditional deep learning detection algorithm. To solve the problem, a scene-coupled multi-task object detection algorithm is proposed. First, a new scene-coupled object detection network structure is designed, which mirrors and fuses the scene classification feature map and the object detection feature map on the same scale to enrich the fine-grain of the feature description. Second, a differentiated activation module is designed to realize the importance screening of feature channels. Then, the optimization function of multi-task coupling is derived, which can simultaneously optimize the scene classification loss and object detection loss. Finally, an air-to-ground detection multi-task dataset is established to verify the effectiveness of proposed method. The experimental results show that the proposed algorithm effectively improves the accuracy and robustness of air-to-ground small object detection, and can adapt to different heights to identify multi-task requirements, which provides a new idea and method for space-based unmanned platform intelligent detection. © 2018, Chinese Lasers Press. All right reserved.","Deep learning; Machine vision; Multi-task coupling; Object detection; Scene understanding; Unmanned aerial vehicle","Aircraft detection; Computer vision; Deep learning; Object recognition; Remote sensing; Signal detection; Space optics; Unmanned aerial vehicles (UAV); Feature description; Intelligent detection; Object detection algorithms; Optimization function; Remote sensing images; Scene classification; Scene understanding; Small object detection; Object detection"
"Hyperspectral Image Classification Based on Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060296645&doi=10.1109%2fICCSS.2018.8572435&partnerID=40&md5=c17d067979591558f87cda72059edec0","The deep learning is of great interest recently. MIT Technology Review ranked deep learning as the top 10 technolog y breakthrough in 2013. The multi-layered structure of Deep Lear ning Network enables it to learn deeper features of data and impr ove the accuracy of data identification and classification. Hypers pectral image classification technology is the most important rese arch direction in hyperspectral remote sensing, and has been wid ely used in military and civil fields. It has become a trend to appl y deep learning to hyperspectral image classification. But deep Ie arning is data-driven and requires a lot of data, however, the hyp erspectral data we deal with is a small data set. In the classificatio n of small data sets, compared to the traditional classification met hods, classification of the deep learning is more effective. In this p aper, we study the convolution neural network which is excellent in the field of image processing, and compare the effect of hypers pectral image classification with the traditional Classification met hod. © 2018 IEEE.","classification; Convolutional neural network (CNN); Hyperspectral image; Small sample; style","Classification (of information); Convolution; Deep learning; Engineering education; Hyperspectral imaging; Military photography; Neural networks; Remote sensing; Spectroscopy; Classification technology; Convolution neural network; Convolutional neural network; Convolutional Neural Networks (CNN); Hyperspectral remote sensing; Multi-layered structure; Small samples; style; Image classification"
"Fully residual convolutional neural networks for aerial image segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059957245&doi=10.1145%2f3287921.3287970&partnerID=40&md5=735179faa065f4b39513caa4a6a72e83","Semantic segmentation from aerial imagery is one of the most essential tasks in the field of remote sensing with various potential applications ranging from map creation to intelligence service. One of the most challenging factors of these tasks is the very heterogeneous appearance of artificial objects like buildings, cars and natural entities such as trees, low vegetation in very high-resolution digital images. In this paper, we propose an efficient deep learning approach to aerial image segmentation. Our approach utilizes the architecture of fully convolutional network (FCN) based on the backbone ResNet101 with additional upsampling skip connections. Besides typical color channels, we also use DSM and normalized DSM (nDSM) as the input data of our models. We achieve overall accuracy of 91%, which is in top 4 among 140 submissions from all over the world on the well-known Vaihingen dataset from ISPRS 2D Semantic Labeling Contest. Especially, our approach yields better results then all state-of-the-art methods in segmentation of car objects. © 2018 Association for Computing Machinery.","Deep learning; Fully convolutional neural network; Residual learning; Semantic image segmentation","Aerial photography; Antennas; Convolution; Deep learning; Neural networks; Remote sensing; Semantics; Convolutional networks; Convolutional neural network; Intelligence services; Residual learning; Semantic image segmentations; Semantic segmentation; State-of-the-art methods; Very high resolution; Image segmentation"
"Remotely Sensed Big Data Era and Intelligent Information Extraction [遥感大数据时代与智能信息提取]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061848415&doi=10.13203%2fj.whugis20180172&partnerID=40&md5=0bb35b74acb8b48d2067b0c2af36ca30","In recent years, the rapid development of the earth observation capability and the intelligent computing technology has provided opportunities for the advancement and even revolution of remote sensing information technology. Remote sensing data processing technology has experienced the Digi-tal Signal Processing Era from 60s to 80s of last century, which utilizes the Statistical Model as the core, and the Quantitative Remote Sensing Era from 90s marked by the Physical Model. Recently, it is developing towards Remotely Sensed Big Data Era which relies on Data Model by data-driven intelligent analysis. This paper summarizes the history of remote sensing information technology and presents the concept of remotely sensed big data and the characteristics of intelligent information extraction era. Firstly, from the view of remotely sensed big data, this paper discusses the construction of object-based remote sensing knowledge dataset and analyzes the data-driven intelligent information extraction strategy combined the knowledge of remote sensing and deep learning algorithm. Then the current status and development of intelligent algorithms represented by deep learning are introduced by typical applications on object detection, fine classification and parameter inversion based on remote sensing data. Consequently, the application potential of deep learning on intelligent information extraction in Remotely Sensed Big Data Era is discussed. © 2018, Research and Development Office of Wuhan University. All right reserved.","Big data; Deep learning; Intelligent information extraction; Neural network; Remotely sensed","Big data; Data handling; Deep learning; Information retrieval; Intelligent computing; Large dataset; Learning algorithms; Neural networks; Object detection; Remote sensing; Intelligent Algorithms; Intelligent analysis; Intelligent information; Quantitative remote sensing; Remote sensing data; Remote sensing information; Remotely sensed; Statistical modeling; Data mining; algorithm; artificial neural network; data processing; data set; information technology; remote sensing"
"Mapping Impervious Surface with 2 m Using Multi-source High Resolution Remote Sensing Images [基于多源高分辨率遥感影像的2 m不透水面一张图提取]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061857899&doi=10.13203%2fj.whugis20180196&partnerID=40&md5=74ac8e5e5b7928b7dcbc8c33e00eb638","Impervious rate is an important indicator to evaluate the urban ecological environment. Currently, there is only 1 km and 30 m resolution of impervious surface thematic information in the global scope, which cannot meet the needs of urban scale hydrological modeling, sponge city planning and construction. In this paper, an impervious surface extraction model incorporated spectral and texture information is proposed, and a new method based on deep learning is implemented to estimate imper-vious surface information. In addition the software for extracting and monitoring of impervious surface is also developed. Based on multi-source high spatial resolution imagery, impervious surface map with 2 m spatial resolution in mainland China including 31 provinces (municipalities, autonomous regions) is accomplished, just supports the high resolution data to research and monitor sponge and ecological cities. © 2018, Research and Development Office of Wuhan University. All right reserved.","Deep learning; High spatial resolution imagery; Impervious surface extraction model; Land cover datasets","Ecology; Extraction; Image resolution; Remote sensing; Textures; Ecological environments; High resolution data; High resolution remote sensing images; High spatial resolution imagery; Hydrological modeling; Impervious surface; Land cover datasets; Thematic information; Deep learning; data set; image resolution; land cover; learning; mapping; remote sensing; China"
"Oil Tank Extraction in High-Resolution Remote Sensing Images Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059942673&doi=10.1109%2fGEOINFORMATICS.2018.8557161&partnerID=40&md5=95eaee6f97690db65a19b55dc461a656","The general methods of circular target extraction include Hough transform, circle fitting method, template circle detection method, etc. However, due to the abundance of information in high resolution remote sensing images, the result of the extraction is disturbed by the background, resulting in poor results. In order to solve this problem, this paper proposes an oil tank extraction method in high-resolution remote sensing image based on deep learning. Our experiment uses the RSOD-Dataset shared by Wuhan University. Firstly, it uses the Selective Search algorithm for target recognition, then trains the CaffeNet network model under the deep learning Caffe framework as a feature extraction classifier, and finally marks the oil tank in the image. Experiments show that the method proposed in this paper can effectively carry out oil tank extraction. The proposed method is robust in different complex backgrounds which has high detection rate and low false alarm rate. © 2018 IEEE.","Caffe; Deep learning; Oil tank extraction; Selective search; Target extraction","Extraction; Feature extraction; Hough transforms; Image processing; Oil tanks; Remote sensing; Caffe; Complex background; Extraction method; High detection rate; High resolution remote sensing images; Selective search; Target extraction; Target recognition; Deep learning"
"A Research on Extracting Road Network from High Resolution Remote Sensing Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059952780&doi=10.1109%2fGEOINFORMATICS.2018.8557042&partnerID=40&md5=56b7033fb6b27feffef144b5f44bc683","The road network plays an important role for traffic management, GPS navigation and many other applications. Extracting the road from a high remote sensing (RS) imagery has been a hot research topic in recent years. The road structure always changing as the terrain, thus, how to extract the features of road network and identify the roads from RS imagery efficiently still a challenging. In this paper, we propose a road extraction method for RS imagery using the deep convolutional neural network, which is designed based on the deep residual networks and take full advantages of the U-net. Road network data form Las Vegas, America, are used to validate the method, and experiments show that the proposed model of deep convolutional neural network can extract road network accurately and effectively. © 2018 IEEE.","Convolutional neural network; Deep learning; Remote sensing imagery; Road network extraction","Convolution; Deep learning; Deep neural networks; Extraction; Feature extraction; Neural networks; Remote sensing; Roads and streets; Traffic control; Convolutional neural network; Deep convolutional neural networks; High resolution remote sensing imagery; Hot research topics; Remote sensing imagery; Road extraction method; Road network extraction; Traffic management; Data mining"
"Research on fast detection method of aircraft in remote sensing image based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068381244&doi=10.1109%2fITOEC.2018.8740585&partnerID=40&md5=4c04150ff5ecb28da8d13cb24b4efd30","The detection and identification of aircraft in high-resolution remote sensing image has important military and civilian value. Aiming at the difficulty of aircraft detection in remote sensing image, this paper proposes a fast detection method based on deep learning. This method avoids the problem of designing manual features in the traditional aircraft detection. It provides a more general and concise solution to solve the problem of aircraft detection in remote sensing image. The experimental results show that the detection accuracy of the method is 84.58%, and the recall rate is 89.3%, The detection speed reaches 50f/s. This method is reasonable and feasible. This paper also analyzes the errors in the detection and provides a reference for the next optimization. © 2018 IEEE.","Aircraft detection; Deep Learning; Remote sensing","Aircraft; Deep learning; Military photography; Remote sensing; Detection accuracy; Detection and identifications; Detection speed; Fast detections; High resolution remote sensing images; Recall rate; Remote sensing images; Aircraft detection"
"Automatic mapping of thermokarst landforms from remote sensing images using deep learning: A case study in the northeastern Tibetan Plateau","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058902662&doi=10.3390%2frs10122067&partnerID=40&md5=bbf7157a76403e1175fdda7bd62dcceb","Thawing of ice-rich permafrost causes thermokarst landforms on the ground surface. Obtaining the distribution of thermokarst landforms is a prerequisite for understanding permafrost degradation and carbon exchange at local and regional scales. However, because of their diverse types and characteristics, it is challenging to map thermokarst landforms from remote sensing images. We conducted a case study towards automatically mapping a type of thermokarst landforms (i.e., thermo-erosion gullies) in a local area in the northeastern Tibetan Plateau from high-resolution images by the use of deep learning. In particular, we applied the DeepLab algorithm (based on Convolutional Neural Networks) to a 0.15-m-resolution Digital Orthophoto Map (created using aerial photographs taken by an Unmanned Aerial Vehicle). Here, we document the detailed processing flow with key steps including preparing training data, fine-tuning, inference, and post-processing. Validating against the field measurements and manual digitizing results, we obtained an F1 score of 0.74 (precision is 0.59 and recall is 1.0), showing that the proposed method can effectively map small and irregular thermokarst landforms. It is potentially viable to apply the designed method to mapping diverse thermokarst landforms in a larger area where high-resolution images and training data are available. © 2018 by the authors.","DeepLab; Permafrost degradation; Semantic segmentation; Thermo-erosion gullies; Thermokarst landforms; Tibetan Plateau; Unmanned aerial vehicle images","Antennas; Erosion; Geomorphology; Image segmentation; Landforms; Mapping; Neural networks; Permafrost; Remote sensing; Semantics; Unmanned aerial vehicles (UAV); DeepLab; Permafrost degradation; Semantic segmentation; Thermo-erosion gullies; Tibetan Plateau; Deep learning"
"Accelerating the image processing by the optimization strategy for deep learning algorithm DBN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054126476&doi=10.1186%2fs13638-018-1255-6&partnerID=40&md5=5b73bb5e8db6676518dc22e39109694f","In recent years, image processing especially for remote sensing technology has developed rapidly. In the field of remote sensing, the efficiency of processing remote sensing images has been a research hotspot in this field. However, the remote sensing data has some problems when processing by a distributed framework, such as Spark, and the key problems to improve execution efficiency are data skew and data reused. Therefore, in this paper, a parallel acceleration strategy based on a typical deep learning algorithm, deep belief network (DBN), is proposed to improve the execution efficiency of the DBN algorithm in Spark. First, the re-partition algorithm based on the tag set is proposed to the relief data skew problem. Second, the cache replacement algorithm on the basis of characteristics is proposed to automatic cache the frequently used resilient distributed dataset (RDD). By caching RDD, the re-computation time of frequently reused RDD is reduced, which lead to the decrease of total computation time of the job. The numerical and analysis verify the effectiveness of the strategy. © 2018, The Author(s).","Acceleration strategy; Data skew; DBN; Deep learning; RDD cache","Efficiency; Image processing; Learning algorithms; Remote sensing; Acceleration strategies; Cache replacement algorithm; Data skew; Deep belief network (DBN); RDD cache; Remote sensing technology; Resilient distributed dataset; Total computation time; Deep learning"
"Opium poppy detection using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058903305&doi=10.3390%2frs10121886&partnerID=40&md5=09f0730fde75ea917e75ef12e1aa608e","Opium poppies are a major source of traditional drugs, which are not only harmful to physical and mental health, but also threaten the economy and society. Monitoring poppy cultivation in key regions through remote sensing is therefore a crucial task; the location coordinates of poppy parcels represent particularly important information for their eradication by local governments. We propose a new methodology based on deep learning target detection to identify the location of poppy parcels and map their spatial distribution. We first make six training datasets with different band combinations and slide window sizes using two ZiYuan3 (ZY3) remote sensing images and separately train the single shot multibox detector (SSD) model. Then, we choose the best model and test its performance using 225 km 2 verification images from Lao People's Democratic Republic (Lao PDR), which exhibits a precision of 95% for a recall of 85%. The speed of our method is 4.5 km 2 /s on 1080TI Graphics Processing Unit (GPU). This study is the first attempt to monitor opium poppies with the deep learning method and achieve a high recognition rate. Our method does not require manual feature extraction and provides an alternative way to rapidly obtain the exact location coordinates of opium poppy cultivation patches. © 2018 by the authors.","Deep learning; Lao PDR; Object detection; Opium poppy; Remote sensing; Single shot multibox detector (SSD)","Computer graphics; Computer graphics equipment; Graphics processing unit; Location; Object detection; Object recognition; Program processors; Remote sensing; Band combinations; Economy and society; Graphics Processing Unit (GPU); Lao PDR; Opium poppy; Remote sensing images; Single shots; Training data sets; Deep learning"
"A Ship rotation detection model in remote sensing images based on Feature Fusion Pyramid Network and deep reinforcement learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058872324&doi=10.3390%2frs10121922&partnerID=40&md5=29e5348b825e13bc82835aca860c56b7","Ship detection plays an important role in automatic remote sensing image interpretation. The scale difference, large aspect ratio of ship, complex remote sensing image background and ship dense parking scene make the detection task difficult. To handle the challenging problems above, we propose a ship rotation detection model based on a Feature Fusion Pyramid Network and deep reinforcement learning (FFPN-RL) in this paper. The detection network can efficiently generate the inclined rectangular box for ship. First, we propose the Feature Fusion Pyramid Network (FFPN) that strengthens the reuse of different scales features, and FFPN can extract the low level location and high level semantic information that has an important impact on multi-scale ship detection and precise location of dense parking ships. Second, in order to get accurate ship angle information, we apply deep reinforcement learning to the inclined ship detection task for the first time. In addition, we put forward prior policy guidance and a long-term training method to train an angle prediction agent constructed through a dueling structure Q network, which is able to iteratively and accurately obtain the ship angle. In addition, we design soft rotation non-maximum suppression to reduce the missed ship detection while suppressing the redundant detection boxes. We carry out detailed experiments on the remote sensing ship image dataset, and the experiments validate that our FFPN-RL ship detection model has efficient detection performance. © 2018 by the authors.","Convolution neural network; Deep reinforcement learning; Feature map fusion; Ship detection","Aspect ratio; Feature extraction; Iterative methods; Reinforcement learning; Remote sensing; Semantics; Ships; Convolution neural network; Efficient detection; Feature map; High level semantics; Non-maximum suppression; Remote sensing image interpretations; Remote sensing images; Ship detection; Deep learning"
"M 3 Fusion: A Deep Learning Architecture for Multiscale Multimodal Multitemporal Satellite Data Fusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055877123&doi=10.1109%2fJSTARS.2018.2876357&partnerID=40&md5=521586202aa1a2448e5914b587ba04fa","Modern Earth Observation systems provide remote sensing data at different temporal and spatial resolutions. Among all the available spatial mission, today the Sentinel-2 program supplies high temporal (every five days) and high spatial resolution (HSR) (10 m) images that can be useful to monitor land cover dynamics. On the other hand, very HSR (VHSR) imagery is still essential to figure out land cover mapping characterized by fine spatial patterns. Understanding how to jointly leverage these complementary sources in an efficient way when dealing with land cover mapping is a current challenge in remote sensing. With the aim of providing land cover mapping through the fusion of multitemporal HSR and VHSR satellite images, we propose a suitable end-To-end deep learning framework, namely M3text{Fusion}, which is able to simultaneously leverage the temporal knowledge contained in time series data as well as the fine spatial information available in VHSR images. Experiments carried out on the Reunion Island study area confirm the quality of our proposal considering both quantitative and qualitative aspects. © 2008-2012 IEEE.","Data fusion; deep learning; land cover mapping; satellite image time series; sentinel-2; very high spatial resolution (VHSR)","Data fusion; Deep learning; Feature extraction; Image fusion; Image resolution; Knowledge management; Remote sensing; Satellite imagery; Satellites; Time series analysis; Land cover mapping; Satellite images; sentinel-2; Spatial resolution; Very high spatial resolutions; Mapping; data quality; land cover; mapping; satellite data; satellite imagery; Sentinel; spatial resolution; time series; Mascarene Islands; Reunion"
"WSF-NET: Weakly supervised feature-fusion network for binary segmentation in remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058876913&doi=10.3390%2frs10121970&partnerID=40&md5=14fe6bdac4eb308508d52802f32507c6","Binary segmentation in remote sensing aims to obtain binary prediction mask classifying each pixel in the given image. Deep learning methods have shown outstanding performance in this task. These existing methods in fully supervised manner need massive high-quality datasets with manual pixel-level annotations. However, the annotations are generally expensive and sometimes unreliable. Recently, using only image-level annotations, weakly supervised methods have proven to be effective in natural imagery, which significantly reduce the dependence on manual fine labeling. In this paper, we review existing methods and propose a novel weakly supervised binary segmentation framework, which is capable of addressing the issue of class imbalance via a balanced binary training strategy. Besides, a weakly supervised feature-fusion network (WSF-Net) is introduced to adapt to the unique characteristics of objects in remote sensing image. The experiments were implemented on two challenging remote sensing datasets: Water dataset and Cloud dataset. Water dataset is acquired by Google Earth with a resolution of 0.5 m, and Cloud dataset is acquired by Gaofen-1 satellite with a resolution of 16 m. The results demonstrate that using only image-level annotations, our method can achieve comparable results to fully supervised methods. © 2018 by the authors.","Deep learning; Localization; Remote sensing image; Weakly supervised binary segmentation","Deep learning; Image fusion; Image segmentation; Pixels; Supervised learning; Binary segmentation; Binary training; Class imbalance; Feature fusion; Learning methods; Localization; Remote sensing images; Supervised methods; Remote sensing"
"Deep networks under scene-level supervision for multi-class geospatial object detection from remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053749687&doi=10.1016%2fj.isprsjprs.2018.09.014&partnerID=40&md5=3b506c1228b61425afe3f9ac3fe69da0","Due to its many applications, multi-class geospatial object detection has attracted increasing research interest in recent years. In the literature, existing methods highly depend on costly bounding box annotations. Based on the observation that scene-level tags provide important cues for the presence of objects, this paper proposes a weakly supervised deep learning (WSDL) method for multi-class geospatial object detection using scene-level tags only. Compared to existing WSDL methods which take scenes as isolated ones and ignore the mutual cues between scene pairs when optimizing deep networks, this paper exploits both the separate scene category information and mutual cues between scene pairs to sufficiently train deep networks for pursuing the superior object detection performance. In the first stage of our training method, we leverage pair-wise scene-level similarity to learn discriminative convolutional weights by exploiting the mutual information between scene pairs. The second stage utilizes point-wise scene-level tags to learn class-specific activation weights. While considering that the testing remote sensing image generally covers a large region and may contain a large number of objects from multiple categories with large size variations, a multi-scale scene-sliding-voting strategy is developed to calculate the class-specific activation maps (CAM) based on the aforementioned weights. Finally, objects can be detected by segmenting the CAM. The deep networks are trained on a seemingly unrelated remote sensing image scene classification dataset. Additionally, the testing phase is conducted on a publicly open multi-class geospatial object detection dataset. The experimental results demonstrate that the proposed deep networks dramatically outperform the state-of-the-art methods. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Class-specific activation weights; Deep networks; Discriminative convolutional weights; Multi-class geospatial object detection; Scene-level supervision","Cams; Chemical activation; Classification (of information); Convolution; Deep learning; Image segmentation; Object recognition; Remote sensing; Scales (weighing instruments); Statistical tests; Stereo vision; Class-specific activation weights; Deep networks; Discriminative convolutional weights; Geo-spatial objects; Scene-level supervision; Object detection; artificial neural network; comparative study; data set; detection method; experimental study; geostatistics; image analysis; methodology; optimization; remote sensing; satellite altimetry; satellite imagery; segmentation"
"Landslide susceptibility assessment using integrated deep learning algorithm along the china-nepal highway","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058619741&doi=10.3390%2fs18124436&partnerID=40&md5=3b51ab022ef6d78b480de8a1e5e03183","The China-Nepal Highway is a vital land route in the Kush-Himalayan region. The occurrence of mountain hazards in this area is a matter of serious concern. Thus, it is of great importance to perform hazard assessments in a more accurate and real-time way. Based on temporal and spatial sensor data, this study tries to use data-driven algorithms to predict landslide susceptibility. Ten landslide instability factors were prepared, including elevation, slope angle, slope aspect, plan curvature, vegetation index, built-up index, stream power, lithology, precipitation intensity, and cumulative precipitation index. Four machine learning algorithms, namely decision tree (DT), support vector machines (SVM), Back Propagation neural network (BPNN), and Long Short Term Memory (LSTM) are implemented, and their final prediction accuracies are compared. The experimental results showed that the prediction accuracies of BPNN, SVM, DT, and LSTM in the test areas are 62.0%, 72.9%, 60.4%, and 81.2%, respectively. LSTM outperformed the other three models due to its capability to learn time series with long temporal dependencies. It indicates that the dynamic change course of geological and geographic parameters is an important indicator in reflecting landslide susceptibility. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","China-nepal highway; Landslide susceptibility; LSTM; Machine learning; Remote sensing images","Backpropagation algorithms; Data mining; Decision trees; Deep learning; Forecasting; Hazards; Landslides; Learning systems; Lithology; Long short-term memory; Remote sensing; Support vector machines; Back-propagation neural networks; China-nepal highway; Cumulative precipitation; Landslide susceptibility; Landslide susceptibility assessments; LSTM; Precipitation intensity; Remote sensing images; Learning algorithms"
"Open-source, machine and deep learning-based automated algorithm for gestational age estimation through smartphone lens imaging","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057855822&doi=10.1364%2fBOE.9.006038&partnerID=40&md5=31f31cbcec355fccc4b4a37d905089cf","Gestational age estimation at time of birth is critical for determining the degree of prematurity of the infant and for administering appropriate postnatal treatment. We present a fully automated algorithm for estimating gestational age of premature infants through smartphone lens imaging of the anterior lens capsule vasculature (ALCV). Our algorithm uses a fully convolutional network and blind image quality analyzers to segment usable anterior capsule regions. Then, it extracts ALCV features using a residual neural network architecture and trains on these features using a support vector machine-based classifier. The classification algorithm is validated using leave-one-out cross-validation on videos captured from 124 neonates. The algorithm is expected to be an influential tool for remote and point-of-care gestational age estimation of premature neonates in low-income countries. To this end, we have made the software open source. © 2018 Optical Society of America.",,"Image segmentation; Network architecture; Open source software; Open systems; Smartphones; Statistical methods; Automated algorithms; Classification algorithm; Convolutional networks; Fully automated; Leave-one-out cross validations; Low income countries; Premature infants; Premature neonates; Deep learning; algorithm; anterior lens capsule; Article; artificial neural network; capillary density; classification algorithm; correlational study; deep learning; gestational age; human; image analysis; image processing; image quality; image segmentation; information processing; learning; machine learning; measurement accuracy; measurement precision; methodology; prematurity; remote sensing; sensitivity and specificity; support vector machine; training; validation process; vascularization"
"Scene classification based on multiscale convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050645425&doi=10.1109%2fTGRS.2018.2848473&partnerID=40&md5=2e3ce3ad49d550cb745773dc2b5b1eb3","With the large amount of high-spatial resolution images now available, scene classification aimed at obtaining high-level semantic concepts has drawn great attention. The convolutional neural networks (CNNs), which are typical deep learning methods, have widely been studied to automatically learn features for the images for scene classification. However, scene classification based on CNNs is still difficult due to the scale variation of the objects in remote sensing imagery. In this paper, a multiscale CNN (MCNN) framework is proposed to solve the problem. In MCNN, a network structure containing dual branches of a fixed-scale net (F-net) and a varied-scale net (V-net) is constructed and the parameters are shared by the F-net and V-net. The images and their rescaled images are fed into the F-net and V-net, respectively, allowing us to simultaneously train the shared network weights on multiscale images. Furthermore, to ensure that the features extracted from MCNN are scale invariant, a similarity measure layer is added to MCNN, which forces the two feature vectors extracted from the image and its corresponding rescaled image to be as close as possible in the training phase. To demonstrate the effectiveness of the proposed method, we compared the results obtained using three widely used remote sensing data sets: The UC Merced data set, the aerial image data set, and the google data set of SIRI-WHU. The results confirm that the proposed method performs significantly better than the other state-of-the-art scene classification methods. © 2018 IEEE.","Convolutional neural networks (CNNs); multiscale; scene classification; similarity measure","Antennas; Convolution; Deep learning; Neural networks; Semantics; Convolutional neural network; High level semantics; High spatial resolution images; Multi-scale; Remote sensing data; Remote sensing imagery; Scene classification; Similarity measure; Remote sensing; artificial neural network; data set; image classification; learning; remote sensing; satellite data; satellite imagery; spatial resolution"
"MsRi-CCF: Multi-scale and rotation-insensitive convolutional channel features for geospatial object detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058901498&doi=10.3390%2frs10121990&partnerID=40&md5=077f45f2b5ec8076b98f7c545e3860a1","Geospatial object detection is a fundamental but challenging problem in the remote sensing community. Although deep learning has shown its power in extracting discriminative features, there is still room for improvement in its detection performance, particularly for objects with large ranges of variations in scale and direction. To this end, a novel approach, entitled multi-scale and rotation-insensitive convolutional channel features (MsRi-CCF), is proposed for geospatial object detection by integrating robust low-level feature generation, classifier generation with outlier removal, and detection with a power law. The low-level feature generation step consists of rotation-insensitive and multi-scale convolutional channel features, which were obtained by learning a regularized convolutional neural network (CNN) and integrating multi-scaled convolutional feature maps, followed by the fine-tuning of high-level connections in the CNN, respectively. Then, these generated features were fed into AdaBoost (chosen due to its lower computation and storage costs) with outlier removal to construct an object detection framework that facilitates robust classifier training. In the test phase, we adopted a log-space sampling approach instead of fine-scale sampling by using the fast feature pyramid strategy based on a computable power law. Extensive experimental results demonstrate that compared with several state-of-the-art baselines, the proposed MsRi-CCF approach yields better detection results, with 90.19% precision with the satellite dataset and 81.44% average precision with the NWPU VHR-10 datasets. Importantly, MsRi-CCF incurs no additional computational cost, which is only 0.92 s and 0.7 s per test image on the two datasets. Furthermore, we determined that most previous methods fail to gain an acceptable detection performance, particularly when they face several obstacles, such as deformations in objects (e.g., rotation, illumination, and scaling). Yet, these factors are effectively addressed by MsRi-CCF, yielding a robust geospatial object detection method. © 2018 by the authors.","AdaBoost; Deep learning; Multi-scale aggregation; Object detection; Optical remote sensing imagery; Outlier removal; Rotation-insensitive","Adaptive boosting; Convolution; Deep learning; Feature extraction; Law enforcement; Neural networks; Object recognition; Remote sensing; Rotation; Space optics; Statistics; Classifier training; Convolutional Neural Networks (CNN); Detection performance; Discriminative features; Multi-scale; Optical remote-sensing imagery; Outlier removals; Ranges of variations; Object detection"
"Road segmentation in SAR satellite images with deep fully convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052674413&doi=10.1109%2fLGRS.2018.2864342&partnerID=40&md5=a47b3188b854edb330a9cdcb7c4af948","Remote sensing is extensively used in cartography. As transportation networks grow and change, extracting roads automatically from satellite images is crucial to keep maps up-to-date. Synthetic aperture radar (SAR) satellites can provide high-resolution topographical maps. However, roads are difficult to identify in these data as they look visually similar to targets, such as rivers and railways. Most road extraction methods on SAR images still rely on a prior segmentation performed by the classical computer vision algorithms. Few works study the potential of deep learning techniques, despite their successful applications to optical imagery. This letter presents an evaluation of fully convolutional neural networks (FCNNs) for road segmentation in SAR images. We study the relative performance of early and state-of-the-art networks after carefully enhancing their sensitivity toward thin objects by adding the spatial tolerance rules. Our models show promising results, successfully extracting most of the roads in our test data set. This shows that although FCNNs natively lack efficiency for road segmentation, they are capable of good results if properly tuned. As the segmentation quality does not scale well with the increasing depth of the networks, the design of specialized architectures for roads extraction should yield better performances. © 2018 IEEE.","Deep learning; high-resolution synthetic aperture radar (SAR) data; road extraction; SAR; semantic segmentation; TerraSAR-X","Computer vision; Convolution; Data visualization; Deep learning; Deep neural networks; Extraction; Feature extraction; Image segmentation; Job analysis; Maps; Neural networks; Remote sensing; Roads and streets; Satellites; Semantics; Space-based radar; Statistical tests; Synthetic aperture radar; High resolution synthetic aperture radar; Road extraction; Roads; Semantic segmentation; Task analysis; TerraSAR-X; Urban areas; Radar imaging; algorithm; artificial neural network; computer vision; detection method; satellite data; satellite imagery; segmentation; synthetic aperture radar; TerraSAR-X"
"DSM-to-LoD2: Spaceborne stereo digital surface model refinement","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058873455&doi=10.3390%2frs10121926&partnerID=40&md5=cf5f2747c86413742836895228bd3a38","A digital surface model (DSM) provides the geometry and structure of an urban environment with buildings being the most prominent objects in it. Built-up areas change with time due to the rapid expansion of cities. New buildings are being built, existing ones are expanded, and old buildings are torn down. As a result, 3D surface models can increase the understanding and explanation of complex urban scenarios. They are very useful in numerous fields of remote sensing applications, in tasks related to 3D reconstruction and city modeling, planning, visualization, disaster management, navigation, and decision-making, among others. DSMs are typically derived from various acquisition techniques, like photogrammetry, laser scanning, or synthetic aperture radar (SAR). The generation of DSMs from very high resolution optical stereo satellite imagery leads to high resolution DSMs which often suffer from mismatches, missing values, or blunders, resulting in coarse building shape representation. To overcome these problems, we propose a method for 3D surface model generation with refined building shapes to level of detail (LoD) 2 from stereo half-meter resolution satellite DSMs using deep learning techniques. Mainly, we train a conditional generative adversarial network (cGAN) with an objective function based on least square residuals to generate an accurate LoD2-like DSM with enhanced 3D object shapes directly from the noisy stereo DSM input. In addition, to achieve close to LoD2 shapes of buildings, we introduce a new approach to generate an artificial DSM with accurate and realistic building geometries from city geography markup language (CityGML) data, on which we later perform a training of the proposed cGAN architecture. The experimental results demonstrate the strong potential to create large-scale remote sensing elevation models where the buildings exhibit better-quality shapes and roof forms than just using the matching process. Moreover, the developed model is successfully applied to a different city that is unseen during the training to show its generalization capacity. © 2018 by the authors.","3D building shape; 3D scene refinement; Conditional generative adversarial networks; Digital surface model; Urban region","Buildings; Decision making; Deep learning; Disaster prevention; Disasters; Markup languages; Remote sensing; Satellite imagery; Space-based radar; Stereo image processing; Synthetic aperture radar; 3d buildings; 3D scenes; Adversarial networks; Digital surface models; Urban regions; Three dimensional computer graphics"
"A new method for region-based majority voting CNNs for very high resolution image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058871481&doi=10.3390%2frs10121946&partnerID=40&md5=b35bf2f6de3a636cd619353da646bab0","Conventional geographic object-based image analysis (GEOBIA) land cover classification methods by using very high resolution images are hardly applicable due to their complex ground truth and manually selected features, while convolutional neural networks (CNNs) with many hidden layers provide the possibility of extracting deep features from very high resolution images. Compared with pixel-based CNNs, superpixel-based CNN classification, carrying on the idea of GEOBIA, is more efficient. However, superpixel-based CNNs are still problematic in terms of their processing units and accuracies. Firstly, the limitations of salt and pepper errors and low boundary adherence caused by superpixel segmentation still exist; secondly, this method uses the central point of the superpixel as the classification benchmark in identifying the category of the superpixel, which does not allow classification accuracy to be ensured. To solve such problems, this paper proposes a region-based majority voting CNN which combines the idea of GEOBIA and the deep learning technique. Firstly, training data was manually labeled and trained; secondly, images were segmented under multiresolution and the segmented regions were taken as basic processing units; then, point voters were generated within each segmented region and the perceptive fields of points voters were put into the multi-scale CNN to determine their categories. Eventually, the final category of each region was determined in the region majority voting system. The experiments and analyses indicate the following: 1. region-based majority voting CNNs can fully utilize their exclusive nature to extract abstract deep features from images; 2. compared with the pixel-based CNN and superpixel-based CNN, the region-based majority voting CNN is not only efficient but also capable of keeping better segmentation accuracy and boundary fit; 3. to a certain extent, region-based majority voting CNNs reduce the impact of the scale effect upon large objects; and 4. multi-scales containing small scales are more applicable for very high resolution image classification than the single scale. © 2018 by the authors.","CNN; GEOBIA; Region-based classification; Remote sensing; Very high resolution image","Deep learning; Neural networks; Pixels; Remote sensing; Superpixels; Voting machines; Classification accuracy; Convolutional neural network; GEOBIA; Geographic object-based image analysis; Land cover classification; Region-based; Superpixel segmentations; Very high resolution (VHR) image; Image classification"
"Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054157072&doi=10.1016%2fj.isprsjprs.2018.09.018&partnerID=40&md5=c68a033c3a355a6eba4e50a52d6efaad","The Sentinel-2 satellite mission delivers multi-spectral imagery with 13 spectral bands, acquired at three different spatial resolutions. The aim of this research is to super-resolve the lower-resolution (20 m and 60 m Ground Sampling Distance – GSD) bands to 10 m GSD, so as to obtain a complete data cube at the maximal sensor resolution. We employ a state-of-the-art convolutional neural network (CNN) to perform end-to-end upsampling, which is trained with data at lower resolution, i.e., from 40 → 20 m, respectively 360 → 60 m GSD. In this way, one has access to a virtually infinite amount of training data, by downsampling real Sentinel-2 images. We use data sampled globally over a wide range of geographical locations, to obtain a network that generalises across different climate zones and land-cover types, and can super-resolve arbitrary Sentinel-2 images without the need of retraining. In quantitative evaluations (at lower scale, where ground truth is available), our network, which we call DSen2, outperforms the best competing approach by almost 50% in RMSE, while better preserving the spectral characteristics. It also delivers visually convincing results at the full 10 m GSD. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural network; Deep learning; Sentinel-2; Sharpening of bands; Super-resolution","Convolution; Deep learning; Neural networks; Optical resolving power; Satellite imagery; Signal sampling; Spectroscopy; Convolutional neural network; Convolutional Neural Networks (CNN); Ground sampling distances; Quantitative evaluation; Sentinel-2; Sharpening of bands; Spectral characteristics; Super resolution; Deep neural networks; artificial neural network; image resolution; learning; sampling; satellite imagery; satellite mission; Sentinel"
"Spatial mapping with Gaussian processes and nonstationary Fourier features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046165427&doi=10.1016%2fj.spasta.2018.02.002&partnerID=40&md5=5937d37630a5fa8918a0fec9567adeda","The use of covariance kernels is ubiquitous in the field of spatial statistics. Kernels allow data to be mapped into high-dimensional feature spaces and can thus extend simple linear additive methods to nonlinear methods with higher order interactions. However, until recently, there has been a strong reliance on a limited class of stationary kernels such as the Matérn or squared exponential, limiting the expressiveness of these modelling approaches. Recent machine learning research has focused on spectral representations to model arbitrary stationary kernels and introduced more general representations that include classes of nonstationary kernels. In this paper, we exploit the connections between Fourier feature representations, Gaussian processes and neural networks to generalise previous approaches and develop a simple and efficient framework to learn arbitrarily complex nonstationary kernel functions directly from the data, while taking care to avoid overfitting using state-of-the-art methods from deep learning. We highlight the very broad array of kernel classes that could be created within this framework. We apply this to a time series dataset and a remote sensing problem involving land surface temperature in Eastern Africa. We show that without increasing the computational or storage complexity, nonstationary kernels can be used to improve generalisation performance and provide more interpretable results. © 2018 The Authors","Gaussian process; Nonstationary; Random Fourier features; Spatial statistics",
"Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060702279&doi=10.3390%2fdrones2040039&partnerID=40&md5=f91de18c3c6b6a071e10a74241b3912c","Remote sensing is important to precision agriculture and the spatial resolution provided by Unmanned Aerial Vehicles (UAVs) is revolutionizing precision agriculture workflows for measurement crop condition and yields over the growing season, for identifying and monitoring weeds and other applications. Monitoring of individual trees for growth, fruit production and pest and disease occurrence remains a high research priority and the delineation of each tree using automated means as an alternative to manual delineation would be useful for long-term farm management. In this paper, we detected citrus and other crop trees from UAV images using a simple convolutional neural network (CNN) algorithm, followed by a classification refinement using superpixels derived from a Simple Linear Iterative Clustering (SLIC) algorithm. The workflow performed well in a relatively complex agricultural environment (multiple targets, multiple size trees and ages, etc.) achieving high accuracy (overall accuracy = 96.24%, Precision (positive predictive value) = 94.59%, Recall (sensitivity) = 97.94%). To our knowledge, this is the first time a CNN has been used with UAV multi-spectral imagery to focus on citrus trees. More of these individual cases are needed to develop standard automated workflows to help agricultural managers better incorporate large volumes of high resolution UAV imagery into agricultural management operations. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Citrus; CNN; Deep learning; Feature extraction; Precision agriculture; Superpixels; Tree identification; UAS",
"Classification of High Resolution Remote Sensing Images using Deep Learning Techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060010966&doi=10.1109%2fICACCI.2018.8554605&partnerID=40&md5=7951b7c05e073bb167e640675b583c32","High Resolution Satellite Images are widely used in many applications. Since such images are useful to provide more useful information about the details about the every regions around the world. In this work, transfer learning is used efficiently for the feature extraction from a pretrained Convolutional Neural Network(CNN) model which is used for training in the classification task. Using transfer learning the classification yielded a better accurate results. The experiments are carried out on two high resolution remote sensing satellite images such as UC Merced LandUse and SceneSat Datasets. The pre-trained CNN used here is VGG-16 which is trained on millions of Image-Net Dataset. The proposed method yielded a classification accuracy of 93% in UC Merced LandUse Dataset and in SceneSat Dataset it is about 84%. This proposed method yielded a better precision of 0.93 and 0.86 in UC Merced LandUse Dataset and in SceneSat Dataset respectively. © 2018 IEEE.","Classification; CNN; Transfer Learning","Classification (of information); Image classification; Neural networks; Remote sensing; Classification accuracy; Classification tasks; Convolutional Neural Networks (CNN); High resolution remote sensing; High resolution remote sensing images; High resolution satellite images; Learning techniques; Transfer learning; Deep learning"
"Weed classification of remote sensing by UAV in ecological irrigation areas based on deep learning [基于深度学习的无人机遥感生态灌区杂草分类]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064501301&doi=10.3969%2fj.issn.1674-8530.18.1131&partnerID=40&md5=b9c924782e55d267d6abd29798ef7c29","In order to better promote the construction of ecological irrigation areas, it is required to detect and control weeds in the areas. Thus, a method was proposed based on convolutional neural network(CNN) for weed classification and density estimation in ecological irrigation areas.The images were taken by an UAV at low-altitude for three kinds of weeds namely chenopodium album, humulusscandens and xanthium sibiricum, as well as 3 sorts of crops such as wheat, peanut seeding and maize, and then 17 115 training samples and 750 test samples were harvested through trimming, gray scale and rotation. Finally, the training sets were input into the CNN, and the classification of 6 types of plants was conducted by means of Softmax regression.In order to reduce the network parameters, the effect of 100×100 and 300×300 resolution images on recognition accuracy was also clarified.The results show that the highest recognition rate of 300×300 resolution can reach as high as 95.6%accuracy.In order to prevent and control specific weeds, a method of detecting single weeds density is pre-sented, too.Through this method, accurate monitoring of various weeds in irrigation areas can be achieved. This method can provide a basis for precise applying pesticide, and has important significance and theoretical and practical values for realizing efficient, green, and safe modern agriculture. © 2018, Editorial Department of Journal of Drainage and Irrigation Machinery Engineering. All right reserved.","Convolutional neural network; Ecological irrigation area; Unmanned aerial vehicle(UAV); Weed classification",
"A Fast Target Detection Method for SAR Image Based on Electromagnetic Characteristics","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060049777&doi=10.1109%2fSARS.2018.8552037&partnerID=40&md5=b8594bebeb2ce5041ed617a1f83624b1","Target detection for remote sensing images which contain optical images and radar images has attracted lots of relative researchers. With the development of deep learning, target detection for optical images has been developing towards high accuracy and real-time detection. High resolution optical images reflect geometric features of the object. Unlike optical images, SAR images reflect the electromagnetic characteristics of the target, so the SAR image detection which uses optical image detection algorithm will lead to weak detection performance. This paper studies a fast target detection algorithm for SAR images which fused electromagnetic characteristics and geometric features through support vector machine. The algorithm is based on the Faster R-CNN framework enabling nearly cost-free target detection. © 2018 IEEE.","electromagnetic characteristics; Faster R-CNN; real-time detection; target detection","Chemical detection; Deep learning; Geometrical optics; Remote sensing; Signal detection; Synthetic aperture radar; Target tracking; Detection performance; Electromagnetic characteristic; Faster R-CNN; Geometric feature; High-accuracy; High-resolution optical images; Real-time detection; Remote sensing images; Radar imaging"
"A cloud detection algorithm for remote sensing images using fully convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059981748&doi=10.1109%2fMMSP.2018.8547095&partnerID=40&md5=bb6a62657467488f67b73fbee370bf31","This paper presents a deep-learning based framework for addressing the problem of accurate cloud detection in remote sensing images. This framework benefits from a Fully Convolutional Neural Network (FCN), which is capable of pixel-level labeling of cloud regions in a Landsat 8 image. Also, a gradient-based identification approach is proposed to identify and exclude regions of snow/ice in the ground truths of the training set. We show that using the hybrid of the two methods (threshold-based and deep-learning) improves the performance of the cloud identification process without the need to manually correct automatically generated ground truths. In average the Jaccard index and recall measure are improved by 4.36% and 3.62%, respectively. © 2018 IEEE.","Cloud detection; CNN; deep-learning; FCN; image segmentation; Landsat 8; remote sensing; U-Net","Convolution; Deep learning; Image segmentation; Multimedia signal processing; Neural networks; Signal detection; Automatically generated; Cloud detection; Cloud detection algorithms; Cloud identification; Convolutional neural network; Identification approach; LANDSAT; Remote sensing images; Remote sensing"
"Lifting Scheme Based Deep Network Model for Remote Sensing Imagery Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059778186&doi=10.1109%2fICPR.2018.8545600&partnerID=40&md5=d7078f13e7feed6bd9474a6998262a01","Deep Learning has shown great success in many fields, however, transferring this potential to remote sensing imagery interpretation is still a challenging task due to the special data properties, e.g., low Signal-to-Noise Ratio (SNR), high variation, etc. In this work, a lifting scheme based deep model is presented for remote sensing imagery classification. The main idea underlying this scheme is that, an innovative strategy is adopted to decompose the input image into two compact and low-resolution components, and these components are then fed into a standard Convolutional Neural Network (CNN) for classification task. More precisely, (1) one decomposed component is devoted to enhancing the latent patterns and simultaneously attenuating the random variation in the input, and (2) the other component is used to capture the local structural information in the input. The experimental results show that the lifting deep model is computationally efficient and has promising potential, improving the classification accuracy by about 5.7% and obtaining 2.69 x speed-up compared with the counterpart CNN. © 2018 IEEE.",,"Deep learning; Image classification; Neural networks; Signal to noise ratio; Wavelet analysis; Classification accuracy; Classification tasks; Computationally efficient; Convolutional Neural Networks (CNN); Innovative strategies; Low signal-to-noise ratio; Remote sensing imagery; Structural information; Remote sensing"
"Enhancing Pix2Pix for Remote Sensing Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059779460&doi=10.1109%2fICPR.2018.8545870&partnerID=40&md5=608e7efb57bdebb4007aad32be2f4a40","Remote sensing image classification is challenging due to low separation between different classes and difficulty in learning discriminative features. GAN (Generative Adversarial Model) is promising for this task due to the generator in reproducing samples and the discriminator for improving the generator. Among GANs variants for image translation and image classification tasks, Pix2Pix performs best. However, Pix2Pix is limited in explicitly capturing the relationship between the source domain and the reconstructed ones from the target domain. To address the above problem, an improved Pix2Pix is proposed in this paper, where a controller is added to Pix2Pix whose role is to improve classification performance and enhance training stability. Experiments demonstrate the effectiveness and advantages of the proposed approach. © 2018 IEEE.","Deep Learning; GAN; Pix2Pix; Remote Sensing Image Classification","Deep learning; Image classification; Remote sensing; Classification performance; Different class; Discriminative features; Image translation; Pix2Pix; Remote sensing image classification; Target domain; Image enhancement"
"Recurrent Neural Networks based on LSTM for Predicting Geomagnetic Field","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061821594&doi=10.1109%2fICARES.2018.8547087&partnerID=40&md5=cd72376978fd3f7aca2b74acaa1f8682","The predicting accuracy of geomagnetic field is a major factor influencing magnetic anomaly detection, geomagnetic navigation and geomagnetism. The limitations of current methods consist of complex model, a large number of parameters, method of solving parameters with high complexity and low forecast accuracy during geomagnetic disturbed days. In this paper we explore a deep learning method for forecasting geomagnetic field that adopts structure of recurrent neural networks (RNN) based on long-short term memory (LSTM). This method of LSTM RNN includes analyzing the characteristics of geomagnetic field and training the data set of geomagnetic data with simple and robust mathematical model. Compared with current methods, the high-precision prediction of geomagnetic field based on LSTM RNN is achieved during both geomagnetic quiet and disturbed days. Furthermore, it could be found that the average error and maximum error of LSTM RNN are far smaller than those of the other methods. © 2018 IEEE.","Geomagnetic Field; High-precision Prediction; Long-Short Term Memory; Recurrent Neural Networks","Anomaly detection; Brain; Complex networks; Deep learning; Forecasting; Geomagnetism; Recurrent neural networks; Remote sensing; Forecast accuracy; Geomagnetic data; Geomagnetic fields; Geomagnetic navigation; High-precision; Learning methods; Magnetic anomaly detection; Recurrent neural network (RNN); Long short-term memory"
"Spectral-spatial classification of hyperspectral remote sensing images using variational autoencoder and convolution neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057599194&partnerID=40&md5=a37181bd0fe4a35d3643fea9ac9ed5fc","In this paper, we propose a spectral-spatial feature extraction framework based on deep learning (DL) for hyperspectral image (HSI) classification. In this framework, the variational autoencoder (VAE) is used for extraction of spectral features from two widely used hyperspectral datasets- Kennedy Space Centre, Florida and University of Pavia, Italy. Additionally, a convolutional neural network (CNN) is utilized to obtain spatial features. The spatial and spectral feature vectors are then stacked together to form a joint feature vector. Finally, the joint feature vector is trained using multinomial logistic regression (softmax regression) for prediction of class labels. The classification performance analysis is done through generation of the confusion matrix. The confusion matrix is then used to calculate Cohen’s Kappa (K) to get a quantitative measure of classification performance. The results show that the K value is higher than 0.99 for both HSI datasets. © 2018 International Society for Photogrammetry and Remote Sensing. All Rights Reserved.","Classification; Deep learning; Feature extraction; Hyperspectral; Spectral channels","Classification (of information); Convolution; Deep learning; Extraction; Feature extraction; Image classification; Image resolution; Neural networks; Spectroscopy; Classification performance; Convolution neural network; Convolutional Neural Networks (CNN); HyperSpectral; Hyperspectral Remote Sensing Image; Multinomial logistic regression; Spectral channels; Spectral-spatial classification; Remote sensing"
"The Recognition of Rice Area Images by UAV Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057447685&doi=10.1051%2fmatecconf%2f201823202057&partnerID=40&md5=ea496dcd30ab6513c7d71acbb3eae974","Aiming at the target detection of remote sensing rice field of uav, the image of large-size uav is firstly segmented, and the type of each image is manually identified, and the image training set and verification set are made. Then, the training model of convolutional neural network is realized by python programming. The advantage and disadvantage of the two-layer convolutional neural network and ResNet50 are compared, and it is found that the training set is less and the picture feature complexity is not high in practical application. In the end, the feature recognition of rice field is realized, which has certain application value. © The Authors, published by EDP Sciences, 2018.",,"Aircraft detection; Convolution; Network layers; Neural networks; Remote sensing; Unmanned aerial vehicles (UAV); Convolutional neural network; Feature recognition; Image training; Python programming; Rice fields; Training model; Training sets; Two-layer; Deep learning"
"Dual-satellite integrated intelligent reconnaissance autonomous decision-making model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057436171&doi=10.1051%2fmatecconf%2f201823204052&partnerID=40&md5=1c2ba4b6ffeab80e80a686d412d30e88","Aiming at aerospace remote sensing technology, this paper proposes a dualsatellite integrated intelligent reconnaissance decision-making model, and establishes a doublesatellite system. One satellite is used for ""general investigation"" and one satellite is used for ""detailed investigation"". The target is carried out by general investigation load. Track identification provides guidance for detailed inspection loads, thereby improving the accuracy, real-time, flexibility and intelligence of space remote sensing. The focus is on three aspects: investigation model design, key algorithm research and semi-physical simulation verification. The reconnaissance model design completes the conceptual design and process demonstration of the model. The key algorithm research part completes the research of the fast detection based on deep learning and the key algorithm of load imaging strategy decision in the system. The satellite semi-physical simulation verification is carried out by constructing the semi-physical simulation verification system. © The Authors, published by EDP Sciences, 2018.",,"Conceptual design; Deep learning; Remote sensing; Satellites; Space optics; Aerospace remote sensing; Algorithm researches; Autonomous decision; Decision making models; Fast detections; Semi-physical simulations; Space remote sensing; Strategy decision; Decision making"
"Comparitive study of tree counting algorithms in dense and sparse vegetative regions","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057608327&partnerID=40&md5=62d48600e54a6c028b6e7550c9773680","Tree counting can be a challenging and time consuming task, especially if done manually. This study proposes and compares three different approaches for automatic detection and counting of trees in different vegetative regions. First approach is to mark extended minima’s, extended maxima’s along with morphological reconstruction operations on an image for delineation and tree crown segmentation. To separate two touching crowns, a marker controlled watershed algorithm is used. For second approach, the color segmentation method for tree identification is used. Starting with the conversion of an RGB image to HSV color space then filtering, enhancing and thresholding to isolate trees from non-trees elements followed by watershed algorithm to separate touching tree crowns. Third approach involves deep learning method for classification of tree and non-tree, using approximately 2268 positive and 1172 negative samples each. Each segment of an image is then classified and sliding window algorithm is used to locate each tree crown. Experimentation shows that the first approach is well suited for classification of trees is dense vegetation, whereas the second approach is more suitable for detecting trees in sparse vegetation. Deep learning classification accuracy lies in between these two approaches and gave an accuracy of 92% on validation data. The study shows that deep learning can be used as a quick and effective tool to ascertain the count of trees from airborne optical imagery. © 2018 International Society for Photogrammetry and Remote Sensing. All Rights Reserved.","Color space; Convolution Neural Network; Morphological operators; Tree Crown Delineation; Watershed algorithm","Color; Deep learning; Forestry; Image enhancement; Image segmentation; Vegetation; Watersheds; Color space; Convolution neural network; Morphological operator; Tree crown delineation; Water-shed algorithm; Trees (mathematics)"
"OrthoSeg: A deep multimodal convolutonal neural network architecture for semantic segmentation of orthoimagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057596125&partnerID=40&md5=cc5ad46c291e102d62a9021339b1187a","This paper addresses the task of semantic segmentation of orthoimagery using multimodal data e.g. optical RGB, infrared and digital surface model. We propose a deep convolutional neural network architecture termed OrthoSeg for semantic segmentation using multimodal, orthorectified and coregistered data. We also propose a training procedure for supervised training of OrthoSeg. The training procedure complements the inherent architectural characteristics of OrthoSeg for preventing complex co-adaptations of learned features, which may arise due to probable high dimensionality and spatial correlation in multimodal and/or multispectral coregistered data. OrthoSeg consists of parallel encoding networks for independent encoding of multimodal feature maps and a decoder designed for efficiently fusing independently encoded multimodal feature maps. A softmax layer at the end of the network uses the features generated by the decoder for pixel-wise classification. The decoder fuses feature maps from the parallel encoders locally as well as contextually at multiple scales to generate per-pixel feature maps for final pixel-wise classification resulting in segmented output. We experimentally show the merits of OrthoSeg by demonstrating state-of-the-art accuracy on the ISPRS Potsdam 2D Semantic Segmentation dataset. Adaptability is one of the key motivations behind OrthoSeg so that it serves as a useful architectural option for a wide range of problems involving the task of semantic segmentation of coregistered multimodal and/or multispectral imagery. Hence, OrthoSeg is designed to enable independent scaling of parallel encoder networks and decoder network to better match application requirements, such as the number of input channels, the effective field-of-view, and model capacity. © 2018 International Society for Photogrammetry and Remote Sensing. All Rights Reserved.","Deep Learning; Residual Networks; Supervised Image Segmentation","Channel coding; Classification (of information); Decoding; Deep learning; Deep neural networks; Encoding (symbols); Image segmentation; Network coding; Neural networks; Pixels; Semantic Web; Semantics; Application requirements; Deep convolutional neural networks; Digital surface models; High dimensionality; Multi-spectral imagery; Semantic segmentation; Spatial correlations; Supervised trainings; Network architecture"
"A remote sensing image key target recognition system design based on faster R-CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059866138&doi=10.1109%2fICMCCE.2018.00040&partnerID=40&md5=2cc3358637089cb06594da445bfbf73b","Aiming at the problem of traditional low-level recognition of key targets in remote sensing images, a method for target detection and recognition based on Faster R-CNN is proposed. Firstly, the open source remote sensing image data set NWPU VHR-10 dataset is converted into VOC 2007 format as the training sets and test sets. Secondly, according to the training set category information, the hyper-parameters of the neural network are refined, and then the training set is trained using the Faster R-CNN neural network to generate a model. Finally, this model is used to detect unknown remote sensing images and identify important targets. The simulation results show that the method has high recognition accuracy and speed, and can provide reference for recognition of the key targets of remote sensing images. © 2018 IEEE.","Convolution neural network; Deep learning; Faster R-CNN; Key target recognition; Remote sensing image detection","Deep learning; Statistical tests; Convolution neural network; Faster R-CNN; Hyper-parameter; Recognition accuracy; Remote sensing images; Target detection and recognition; Target recognition; Training sets; Remote sensing"
"Detection and analysis of wheat spikes using Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056653363&doi=10.1186%2fs13007-018-0366-8&partnerID=40&md5=17440cb0cdde1cccae4fb663fd03565a","Background: Field phenotyping by remote sensing has received increased interest in recent years with the possibility of achieving high-throughput analysis of crop fields. Along with the various technological developments, the application of machine learning methods for image analysis has enhanced the potential for quantitative assessment of a multitude of crop traits. For wheat breeding purposes, assessing the production of wheat spikes, as the grain-bearing organ, is a useful proxy measure of grain production. Thus, being able to detect and characterize spikes from images of wheat fields is an essential component in a wheat breeding pipeline for the selection of high yielding varieties. Results: We have applied a deep learning approach to accurately detect, count and analyze wheat spikes for yield estimation. We have tested the approach on a set of images of wheat field trial comprising 10 varieties subjected to three fertilizer treatments. The images have been captured over one season, using high definition RGB cameras mounted on a land-based imaging platform, and viewing the wheat plots from an oblique angle. A subset of in-field images has been accurately labeled by manually annotating all the spike regions. This annotated dataset, called SPIKE, is then used to train four region-based Convolutional Neural Networks (R-CNN) which take, as input, images of wheat plots, and accurately detect and count spike regions in each plot. The CNNs also output the spike density and a classification probability for each plot. Using the same R-CNN architecture, four different models were generated based on four different datasets of training and testing images captured at various growth stages. Despite the challenging field imaging conditions, e.g., variable illumination conditions, high spike occlusion, and complex background, the four R-CNN models achieve an average detection accuracy ranging from 88 to $$94\%$$ 94 % across different sets of test images. The most robust R-CNN model, which achieved the highest accuracy, is then selected to study the variation in spike production over 10 wheat varieties and three treatments. The SPIKE dataset and the trained CNN are the main contributions of this paper. Conclusion: With the availability of good training datasets such us the SPIKE dataset proposed in this article, deep learning techniques can achieve high accuracy in detecting and counting spikes from complex wheat field images. The proposed robust R-CNN model, which has been trained on spike images captured during different growth stages, is optimized for application to a wider variety of field scenarios. It accurately quantifies the differences in yield produced by the 10 varieties we have studied, and their respective responses to fertilizer treatment. We have also observed that the other R-CNN models exhibit more specialized performances. The data set and the R-CNN model, which we make publicly available, have the potential to greatly benefit plant breeders by facilitating the high throughput selection of high yielding varieties. © 2018 The Author(s).","Deep learning; Field imaging; Plant phenotyping; Spike detection; Statistical analysis",
"An unsupervised augmentation framework for deep learning based geospatial object detection: A summary of results","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058620637&doi=10.1145%2f3274895.3274901&partnerID=40&md5=549a9b2f8acf5ac8ba0b4c775f1725bc","Given remote sensing datasets in a spatial domain, we aim to detect geospatial objects with minimum bounding rectangles (i.e., angle-aware) leveraging deep learning frameworks. Geospatial objects (e.g., buildings, vehicles, farms) provide meaningful information for a variety of societal applications, including urban planning, census, sustainable development, security surveillance, agricultural management, etc. The detection of these objects are challenging because their directions are often heavily mixed and not parallel to the orthogonal directions of an image frame due to topography, planning, etc. In addition, there is very limited training data with angle information for most types of objects. In related work, state-of-the-art deep learning frameworks detect objects using orthogonal bounding rectangles (i.e., sides are parallel to the sides of an input image), so they cannot identify the directions of objects and generate loose rectangular bounds on objects. We propose an Unsupervised Augmentation (UA) framework to detect geospatial objects with general minimum bounding rectangles (i.e., with angles). The UA framework contains two schemes, namely a ROtation-Vector (ROV) based scheme and a context-based scheme. The schemes completely avoid the need for: (1) additional ground-truth data with annotated angles; (2) restructuring of existing network architectures; and (3) re-training. Experimental results show that the UA framework can well approximate the angles of objects and generate much tighter bounding boxes on objects. © 2018 Association for Computing Machinery.","Deep learning; Geospatial objects; Rectangles; Remote sensing; Rotations","Geographic information systems; Geometry; Information systems; Information use; Network architecture; Object detection; Remote sensing; Remotely operated vehicles; Rotation; Agricultural management; Geo-spatial objects; Learning frameworks; Limited training data; Minimum bounding rectangle; Orthogonal directions; Rectangles; Security surveillance; Deep learning"
"Nearshore vessel detection based on Scene-mask R-CNN in remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058273687&doi=10.1109%2fICNIDC.2018.8525755&partnerID=40&md5=fe274d61df67edf4c5834db41e2022b2","Object detection method of vessels based on deep learning technology can extract the vessel position and category information in remote sensing image. However, the buildings and artificial facilities in the port area will cause false alarms in the vessel recognition results, which decrease the accuracy. Therefore, containing the object detection and semantic segmentation tasks of deep convolutional neural networks, this paper proposes a ship identification method based on scene-mask R-CNN. Based on the deep convolutional neural network, location information extraction, target classification and target scene discrimination are organically combined to a unified object detection framework. It works in the scene that contains the object, and the context information is used to suppress false alarms that appear in the non-target scene area. Finally, the effectiveness of the proposed method is verified on vessel datasets. © 2018 IEEE.","Deep learning; Nearshore vessels; Object detection; Remote sensing image; Scene mask","Classification (of information); Convolution; Deep learning; Deep neural networks; Digital integrated circuits; Errors; Neural networks; Object detection; Object recognition; Semantics; Deep convolutional neural networks; Detection framework; Nearshores; Object detection method; Remote sensing images; Semantic segmentation; Ship identifications; Target Classification; Remote sensing"
"Ship detection in foggy remote sensing image via scene classification R-CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058343407&doi=10.1109%2fICNIDC.2018.8525532&partnerID=40&md5=e549d2b189d098391d62aeb1c4e6d582","The object detection networks via Faster R-CNN for ship detection have demonstrated impressive performance. However, the complexity of weather conditions in high resolution satellite images exposes the limited capacity of these networks. Images interfered by fog are common in optical remote sensing images. In this paper, we embrace this observation and introduce our research. Unlike SAR images, optical sensor images are very susceptible to the effects of the weather, especially clouds and fog. So, accurate target information cannot be obtained from these image, which reduces the accuracy of ship detection. To solve this problem, we attempts to introduce the image defogging methods into object detection networks to suppress the interference of clouds. Secondly, the SC-R-CNN structure is proposed, which uses the scene classification network (SCN) to realize the classification of fog-containing images and cascaded with the object detection network to form a dual-stream object detection framework. In addition, the combination of defogging methods and the SC-R-CNN network also produces more optimized results. We use the remote sensing image data set containing various types of weather conditions to confirm the validity and accuracy of the proposed method. © 2018 IEEE.","Convolutional neural network; Deep learning; Defogging; Image processing; Object detection; Remote sensing","Deep learning; Digital integrated circuits; Fog; Image processing; Meteorology; Neural networks; Object detection; Object recognition; Optical data processing; Radar imaging; Remote sensing; Ships; Synthetic aperture radar; Convolutional neural network; Defogging; Detection framework; High resolution satellite images; Optical remote sensing; Remote sensing images; Scene classification; Target information; Image classification"
"A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035113175&doi=10.1016%2fj.isprsjprs.2017.11.004&partnerID=40&md5=650ee95c5d4948ee620e0e58fef95ce8","High resolution remote sensing (HRRS) image scene classification plays a crucial role in a wide range of applications and has been receiving significant attention. Recently, remarkable efforts have been made to develop a variety of approaches for HRRS scene classification, wherein deep-learning-based methods have achieved considerable performance in comparison with state-of-the-art methods. However, the deep-learning-based methods have faced a severe limitation that a great number of manually-annotated HRRS samples are needed to obtain a reliable model. However, there are still not sufficient annotation datasets in the field of remote sensing. In addition, it is a challenge to get a large scale HRRS image dataset due to the abundant diversities and variations in HRRS images. In order to address the problem, we propose a semi-supervised generative framework (SSGF), which combines the deep learning features, a self-label technique, and a discriminative evaluation method to complete the task of scene classification and annotating datasets. On this basis, we further develop an extended algorithm (SSGA-E) and evaluate it by exclusive experiments. The experimental results show that the SSGA-E outperforms most of the fully-supervised methods and semi-supervised methods. It has achieved the third best accuracy on the UCM dataset, the second best accuracy on the WHU-RS, the NWPU-RESISC45, and the AID datasets. The impressive results demonstrate that the proposed SSGF and the extended method is effective to solve the problem of lacking an annotated HRRS dataset, which can learn valuable information from unlabeled samples to improve classification ability and obtain a reliable annotation dataset for supervised learning. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; High resolution remote sensing images; Scene classification; Self-label","Classification (of information); Image classification; Remote sensing; Supervised learning; Classification ability; High resolution remote sensing; High resolution remote sensing images; Image scene classification; Learning-based methods; Scene classification; Semi-supervised method; State-of-the-art methods; Deep learning; algorithm; data set; detection method; experimental design; image classification; image resolution; numerical method; numerical model; remote sensing; satellite imagery; supervised learning"
"Deep learning for remote sensing image classification: A survey","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047519048&doi=10.1002%2fwidm.1264&partnerID=40&md5=dc51280b7401be9a7bce93407e88bdf1","Remote sensing (RS) image classification plays an important role in the earth observation technology using RS data, having been widely exploited in both military and civil fields. However, due to the characteristics of RS data such as high dimensionality and relatively small amounts of labeled samples available, performing RS image classification faces great scientific and practical challenges. In recent years, as new deep learning (DL) techniques emerge, approaches to RS image classification with DL have achieved significant breakthroughs, offering novel opportunities for the research and development of RS image classification. In this paper, a brief overview of typical DL models is presented first. This is followed by a systematic review of pixel-wise and scene-wise RS image classification approaches that are based on the use of DL. A comparative analysis regarding the performances of typical DL-based RS methods is also provided. Finally, the challenges and potential directions for further research are discussed. This article is categorized under: Application Areas > Science and Technology Technologies > Classification. © 2018 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals, Inc.","convolutional neural network; deep belief network; deep learning; pixel-wise classification; remote sensing image; scene classification; stacked auto-encoder","Image classification; Military photography; Neural networks; Pixels; Remote sensing; Auto encoders; Convolutional neural network; Deep belief networks; Remote sensing images; Scene classification; Deep learning"
"A deep learning framework for remote sensing image registration","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039951284&doi=10.1016%2fj.isprsjprs.2017.12.012&partnerID=40&md5=fcdc03fde058d9cdf33690d0c8bce17a","We propose an effective deep neural network aiming at remote sensing image registration problem. Unlike conventional methods doing feature extraction and feature matching separately, we pair patches from sensed and reference images, and then learn the mapping directly between these patch-pairs and their matching labels for later registration. This end-to-end architecture allows us to optimize the whole processing (learning mapping function) through information feedback when training the network, which is lacking in conventional methods. In addition, to alleviate the small data issue of remote sensing images for training, our proposal introduces a self-learning by learning the mapping function using images and their transformed copies. Moreover, we apply a transfer learning to reduce the huge computation cost in the training stage. It does not only speed up our framework, but also get extra performance gains. The comprehensive experiments conducted on seven sets of remote sensing images, acquired by Radarsat, SPOT and Landsat, show that our proposal improves the registration accuracy up to 2.4–53.7%. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep neural network; Image registration; Remote sensing image; Self-learning; Transfer learning","Deep neural networks; Image enhancement; Image registration; Mapping; Conventional methods; Information feedback; Learning mapping functions; Registration accuracy; Registration problems; Remote sensing images; Self-learning; Transfer learning; Remote sensing; algorithm; artificial neural network; experimental design; food processing; image analysis; image processing; Landsat; mapping method; RADARSAT; remote sensing; satellite imagery; SPOT"
"Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046352904&doi=10.1016%2fj.isprsjprs.2018.04.014&partnerID=40&md5=1360df6e16ce37f70e4819c7dea8d23b","Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural network; Deep learning; Multispectral; Semantic segmentation; Synthetic imagery; Unmanned aerial system","Computer vision; Convolution; Deep learning; Deep neural networks; Image segmentation; Neural networks; Object detection; Object recognition; Semantics; Unmanned aerial vehicles (UAV); Convolutional neural network; Multi-spectral; Semantic segmentation; Synthetic imagery; Unmanned aerial systems; Remote sensing; algorithm; artificial neural network; computer vision; multispectral image; remote sensing; segmentation; sensor; unmanned vehicle"
"PatternNet: A benchmark dataset for performance evaluation of remote sensing image retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040575209&doi=10.1016%2fj.isprsjprs.2018.01.004&partnerID=40&md5=7babb6a83d29640ed599097635dd22b7","Benchmark datasets are critical for developing, evaluating, and comparing remote sensing image retrieval (RSIR) approaches. However, current benchmark datasets are deficient in that (1) they were originally collected for land use/land cover classification instead of RSIR; (2) they are relatively small in terms of the number of classes as well as the number of images per class which makes them unsuitable for developing deep learning based approaches; and (3) they are not appropriate for RSIR due to the large amount of background present in the images. These limitations restrict the development of novel approaches for RSIR, particularly those based on deep learning which require large amounts of training data. We therefore present a new large-scale remote sensing dataset termed “PatternNet” that was collected specifically for RSIR. PatternNet was collected from high-resolution imagery and contains 38 classes with 800 images per class. Significantly, PatternNet's large scale makes it suitable for developing novel, deep learning based approaches for RSIR. We use PatternNet to evaluate the performance of over 35 RSIR methods ranging from traditional handcrafted feature based methods to recent, deep learning based ones. These results serve as a baseline for future research on RSIR. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Benchmark dataset; Content based image retrieval (CBIR); Convolutional neural networks; Deep learning; Handcrafted features; Remote sensing","Benchmarking; Classification (of information); Content based retrieval; Deep learning; Land use; Neural networks; Benchmark datasets; Content-Based Image Retrieval; Convolutional neural network; Handcrafted features; High resolution imagery; Learning-based approach; Performance evaluations; Remote sensing image retrieval; Remote sensing; algorithm; artificial neural network; benchmarking; data set; image processing; performance assessment; remote sensing; satellite imagery"
"Multi-scale object detection in remote sensing imagery with convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046813617&doi=10.1016%2fj.isprsjprs.2018.04.003&partnerID=40&md5=9b8721d957c4f233b821ed1254ae8b53","Automatic detection of multi-class objects in remote sensing images is a fundamental but challenging problem faced for remote sensing image analysis. Traditional methods are based on hand-crafted or shallow-learning-based features with limited representation power. Recently, deep learning algorithms, especially Faster region based convolutional neural networks (FRCN), has shown their much stronger detection power in computer vision field. However, several challenges limit the applications of FRCN in multi-class objects detection from remote sensing images: (1) Objects often appear at very different scales in remote sensing images, and FRCN with a fixed receptive field cannot match the scale variability of different objects; (2) Objects in large-scale remote sensing images are relatively small in size and densely peaked, and FRCN has poor localization performance with small objects; (3) Manual annotation is generally expensive and the available manual annotation of objects for training FRCN are not sufficient in number. To address these problems, this paper proposes a unified and effective method for simultaneously detecting multi-class objects in remote sensing images with large scales variability. Firstly, we redesign the feature extractor by adopting Concatenated ReLU and Inception module, which can increases the variety of receptive field size. Then, the detection is preformed by two sub-networks: a multi-scale object proposal network (MS-OPN) for object-like region generation from several intermediate layers, whose receptive fields match different object scales, and an accurate object detection network (AODN) for object detection based on fused feature maps, which combines several feature maps that enables small and densely packed objects to produce stronger response. For large-scale remote sensing images with limited manual annotations, we use cropped image blocks for training and augment them with re-scalings and rotations. The quantitative comparison results on the challenging NWPU VHR-10 data set, aircraft data set, Aerial-Vehicle data set and SAR-Ship data set show that our method is more accurate than existing algorithms and is effective for multi-modal remote sensing images. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural networks; Deep learning; Multi-modal remote sensing images; Object detection","Antennas; Convolution; Deep learning; Fighter aircraft; Learning algorithms; Neural networks; Object detection; Object recognition; Training aircraft; Automatic Detection; Convolutional neural network; Localization performance; Quantitative comparison; Receptive field sizes; Remote sensing imagery; Remote sensing images; Representation power; Remote sensing; algorithm; artificial neural network; comparative study; computer vision; data set; detection method; image analysis; machine learning; remote sensing; satellite imagery"
"MugNet: Deep learning for hyperspectral image classification using limited samples","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034069971&doi=10.1016%2fj.isprsjprs.2017.11.003&partnerID=40&md5=bad0c50f26fbe0b2e701033e90134671","In recent years, deep learning based methods have attracted broad attention in the field of hyperspectral image classification. However, due to the massive parameters and the complex network structure, deep learning methods may not perform well when only few training samples are available. In this paper, we propose a small-scale data based method, multi-grained network (MugNet), to explore the application of deep learning approaches in hyperspectral image classification. MugNet could be considered as a simplified deep learning model which mainly targets at limited samples based hyperspectral image classification. Three novel strategies are proposed to construct MugNet. First, the spectral relationship among different bands, as well as the spatial correlation within neighboring pixels, are both utilized via a multi-grained scanning approach. The proposed multi-grained scanning strategy could not only extract the joint spectral-spatial information, but also combine different grains’ spectral and spatial relationship. Second, because there are abundant unlabeled pixels available in hyperspectral images, we take full advantage of these samples, and adopt a semi-supervised manner in the process of generating convolution kernels. At last, the MugNet is built upon the basis of a very simple network which does not include many hyperparameters for tuning. The performance of MugNet is evaluated on a popular and two challenging data sets, and comparison experiments with several state-of-the-art hyperspectral image classification methods are revealed. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Hyperspectral image classification; MugNet; Multi-grained scanning","Classification (of information); Complex networks; Hyperspectral imaging; Image classification; Independent component analysis; Pixels; Scanning; Spectroscopy; Convolution kernel; Learning-based methods; MugNet; Network structures; Scanning strategies; Spatial correlations; Spatial informations; Spatial relationships; Deep learning; algorithm; correlation; data set; experimental study; image classification; model; numerical method; parameter estimation; pixel; spatial analysis"
"One-two-one networks for compression artifacts reduction in remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042096911&doi=10.1016%2fj.isprsjprs.2018.01.003&partnerID=40&md5=2c79d233f619ba404e078f1d6df56ee5","Compression artifacts reduction (CAR) is a challenging problem in the field of remote sensing. Most recent deep learning based methods have demonstrated superior performance over the previous hand-crafted methods. In this paper, we propose an end-to-end one-two-one (OTO) network, to combine different deep models, i.e., summation and difference models, to solve the CAR problem. Particularly, the difference model motivated by the Laplacian pyramid is designed to obtain the high frequency information, while the summation model aggregates the low frequency information. We provide an in-depth investigation into our OTO architecture based on the Taylor expansion, which shows that these two kinds of information can be fused in a nonlinear scheme to gain more capacity of handling complicated image compression artifacts, especially the blocking effect in compression. Extensive experiments are conducted to demonstrate the superior performance of the OTO networks, as compared to the state-of-the-arts on remote sensing datasets and other benchmark datasets. The source code will be available here: https://github.com/bczhangbczhang/. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Compression artifacts reduction; Deep learning; One-two-one network; Remote sensing","Benchmarking; Deep learning; Image compression; 00-01; 99-00; Architecture-based; Benchmark datasets; Compression artifacts; High-frequency informations; Image compression artifacts; Learning-based methods; Remote sensing; data set; Laplace transform; learning; model; network analysis; remote sensing"
"UAV real-time monitoring for forest pest based on deep learning [基于深度学习的森林虫害无人机实时监测方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061202284&doi=10.11975%2fj.issn.1002-6819.2018.21.009&partnerID=40&md5=57f1c52b1e8d17fcd18e7d0bbd39913a","The unmanned aerial vehicle (UAV) remote sensing featured by low cost and flexibility offers a promising solution for pests monitoring by acquiring high resolution forest imagery. So the forest pest monitoring system based on UAV is essential to the early warning of red turpentine beetle (RTB) outbreaks. However, the UAV monitoring method based on image analysis technology suffers from inefficiency and depending on pre-processing, which prohibits the practical application of UAV remote sensing. Due to the long process flow, traditional methods can not locate the outbreak center and track the development of epidemic in time. The RTB is a major forestry invasive pest which damages the coniferous species of pine trees in northern China. This paper focuses on the detection of pines infected by RTBs. A real-time forest pest monitoring method based on deep learning is proposed for UAV forest imagery. The proposed method was consisted of three steps: 1) The UAV equipped with prime lens camera scans the infected forest and collects images at fixes points. 2) The Android client on UAV remote controller receives images and then requests the mobile graphics workstation for infected trees detection through TensorFlow Serving in real time. 3) The mobile graphics workstation runs a tailored SSD300 (single shot multibox detector) model with graphics processing unit (GPU) parallel acceleration to detect infected trees without orthorectification and image mosaic. Compared with Faster R-CNN and other two-stage object detection frameworks, SSD, as a lightweight object detection framework, shows the advantages of real-time and high accuracy. The original SSD300 object detection framework uses truncated VGG16 as basic feature extractor and the 6 layers (named P1-P6) prediction module to detect objects with different sizes. The proposed tailored SSD300 object detection framework includes two parts. First, a 13-layer depthwise separable convolution is used as basic feature extractor, which reduces several times computation overhead compared with the standard convolutions in VGG16. Second, most loss is derived from positive default boxes and these boxes mainly concentrated in P2 and P3 due to the constraints of crown size, UAV flying height and lens' focal length. Therefore, the tailored SSD300 retains only P2 and P3 as prediction module and the other prediction layers are deleted to further reduce computation overhead. Besides, aspect ratio of default boxes is set to {1, 2, 1/2}, since the aspect ratio of crown is approximate 1. The UAV imagery is collected on 6 experimental plots at 50-75 m height. The photos of No. 2 experimental plot are considered as test set and the rest are train set. A total of 82 aerial photos are used in the experiment, including 70 photos in the train set and 12 photos in the test set. The AP and run time of five models are evaluated. The average precision (AP) of the tailored SSD300 model reaches up to 97.22%, which is lower than the AP of original SSD300. While the proposed model has only 18.8 MB parameters, reducing above 530 MB compared with the original model. And the run time is 0. 46 s on a mobile workstation equipped with NVIDIA GTX 1050Ti GPU, while the original model needs 4.56 s. Experimental results demonstrate that the downsize of basic feature extractor and prediction module speed up detection with a little impact on AP. The maximum coverage of aerial photo captured at 75 m height is 38.18 m×50.95 m. When the UAV has a horizontal speed of 15 m/s, it takes 3.4 s to move to the next shooting point without overlap, longer than the detection time. Therefore, the proposed method can simplify the detection process of UAV monitoring and realizes the real-time detection of RTB damaged pines, which introduces a practical and applicable solution for early warning of RTB outbreaks. © 2018, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Deep learning; Diseases; Monitoring; Object detection; Unmanned aerial vehicle","Aerial photography; Antennas; Aspect ratio; Computer graphics; Computer graphics equipment; Convolution; Cost reduction; Deep learning; Diseases; Feature extraction; Forecasting; Forestry; Graphics processing unit; Monitoring; Object detection; Object recognition; Program processors; Remote control; Remote sensing; Unmanned aerial vehicles (UAV); Computation overheads; Coniferous species; Detection framework; Mobile workstations; Ortho-rectification; Real time monitoring; Real-time detection; UAV remote sensing; Aircraft detection"
"Exploration in mapping kernel-based home range models from remote sensing imagery with conditional adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057082754&doi=10.3390%2frs10111722&partnerID=40&md5=a7ffe935913ebd44fcf5adee37acf200","Kernel-based home range models are widely-used to estimate animal habitats and develop conservation strategies. They provide a probabilistic measure of animal space use instead of assuming the uniform utilization within an outside boundary. However, this type of models estimates the home ranges from animal relocations, and the inadequate locational data often prevents scientists from applying them in long-term and large-scale research. In this paper, we propose an end-to-end deep learning framework to simulate kernel home range models. We use the conditional adversarial network as a supervised model to learn the home range mapping from time-series remote sensing imagery. Our approach enables scientists to eliminate the persistent dependence on locational data in home range analysis. In experiments, we illustrate our approach by mapping the home ranges of Bar-headed Geese in Qinghai Lake area. The proposed framework outperforms all baselines in both qualitative and quantitative evaluations, achieving visually recognizable results and high mapping accuracy. The experiment also shows that learning the mapping between images is a more effective way to map such complex targets than traditional pixel-based schemes. © 2018 by the authors.","Bar-headed Geese; Deep learning; Generative adversarial networks; Habitat mapping; Home range; Remote sensing","Animals; Deep learning; Ecosystems; Mapping; Space optics; Adversarial networks; Conservation strategies; Habitat mapping; Home range; Large-scale research; Probabilistic measures; Quantitative evaluation; Remote sensing imagery; Remote sensing"
"Deep Learning with Synthetic Hyperspectral Images for Improved Soil Detection in Multispectral Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058872752&doi=10.1109%2fUEMCON.2018.8796838&partnerID=40&md5=9d07503d3e1f5097aa0082fec45adf05","Remote sensing images play a critical role in natural disaster prevention and damage estimation. With the fast advances in hyperspectral sensors, the hyperspectral imaging also known as imaging spectrometry is becoming a relatively recent trend. Hyperspectral images are expected to be more effective in object detection, environmental monitoring, urban growth monitoring, and damage assessment. In this paper, we present a 4 layers deep convolutional neural network (CNN) model for soil detection by using the combination of 80 synthetic hyperspectral bands and its original 8 multispectral bands which are collected by the WorldView-2 satellite. Our experiment result shows that by using the combined 80 synthetic hyperspectral bands and the original 8 multispectral bands, the area under the curve (AUC) scores of our CNN model for soil detection on the three testing images has been improved by 7.42% in average from 76.26% to 83.48%, as compared to the result by using the 8 multispectral bands alone. We also applied the CNN model onto a set of high-resolution data which is created by pan-sharpening the original multispectral bands and its synthetic hyperspectral bands, which quadrupled the spatial resolution of the combined synthetic hyperspectral bands. With the increased spatial resolution of the combined synthetic hyperspectral bands, the average AUC scores of our CNN model was furtherly improved by 10.02% from 81.44% to 91.47%. This significant improvement indicates that by using the pan-sharpened synthetic hyperspectral bands, the performance of CNN model for soil detection has been greatly improved, the synthetic hyperspectral bands with increased spatial resolution is a great alternative in improving the performance of object detection and classification in remote sensing applications. © 2018 IEEE.","Convolutional Neural Network; Deep Learning; EMAP; Imbalanced Data; Multispectral Satellite Images; Post-Processing; Remote Sensing; Soil Detection; Synthetic Hyperspectral Images","Convolution; Damage detection; Deep learning; Deep neural networks; Disaster prevention; Disasters; Hyperspectral imaging; Image resolution; Mobile telecommunication systems; Multilayer neural networks; Object detection; Object recognition; Remote sensing; Soil testing; Soils; Spectroscopy; Ubiquitous computing; Urban growth; Convolutional neural network; EMAP; Imbalanced data; Multispectral satellite image; Post processing; Image enhancement"
"Remote sensing identification of black cotton soil based on deep belief network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061139646&doi=10.23940%2fijpe.18.11.p28.28202830&partnerID=40&md5=4ddf94518ef8b0f1753e5ef7301b5835","As a type of expansive soil, black cotton soil swells when absorbing water and shrinks when dehydrated, and the cycle of swelling-shrinking movements can readily occur repeatedly. These characteristics result in serious consequences both to land surfaces and to surface buildings such as ground fracturing, building settling, and road buckling and cracking, having extreme adverse effects on the quality and safety of road transportation. With Kitui, Kenya as the research area and a GF-1 remote sensing image as the vector, this study focuses on in-depth exploration of the application of a deep belief network to identify and classify black cotton soil based on the characteristics of the local black cotton soil in the remote sensing image. The results indicate that given the sample database available to this study, when the network depth was 3, the number of nodes in each hidden layer was 60, the learning rate was 0.01, the number of iterations was 20, and the number of samples was 2,000,000. The best classification result could be achieved with a precision of about 90% per the evaluation criteria proposed in this study, indicating a significant advantage of the deep belief network in remote sensing identification of black cotton soil. © 2018 Totem Publisher, Inc. All rights reserved.","Black cotton soil; Classification; Deep learning","Classification (of information); Cotton; Deep learning; Remote sensing; Roads and streets; Black cotton soil; Classification results; Deep belief networks; Evaluation criteria; Number of iterations; Quality and safeties; Remote sensing images; Road transportation; Soils"
"Deep Multi-Feature Learning for Water Body Extraction from Landsat Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060783544&doi=10.3103%2fS0146411618060123&partnerID=40&md5=3a66f8a296cee99942fbf7f431f8d3ba","Abstract: Water body extraction from remote sensing image data has been becoming a really hot topic. Recently, researchers put forward numerous methods for water body extraction, while most of them rely on elaborative feature selection and enough number of training samples. Convolution Neural Network (CNN), one of the implementation models of deep learning, has strong capability for two-dimension images’ classification. A new water body extraction model based on CNNs is established for deep multi-feature learning. Before experiment, image enhancement will be done by Dark Channel Prior. Then we concatenate three kinds of features: spectral information, spatial information that is extracted by Extended Multi-attribute Profile (EMAP) and various water indexes firstly. Next, feature matrixes are acted as the input of CNN-based model for training and classifying. The experimental results showed that the proposed model has better classification performance than Support Vector Machine (SVM) and artificial neural network (ANN). On very limited training set, our model could learn unique and representative features for better water body extraction. © 2018, Allerton Press, Inc.","convolution neural network; deep learning; deep multi-feature learning; EMAPs; feature extraction; water information extraction","Convolution; Deep learning; Extraction; Feature extraction; Image enhancement; Machine learning; Neural networks; Remote sensing; Support vector machines; Classification performance; Convolution neural network; EMAPs; Implementation models; Multi features; Remote sensing images; Two dimension images; Water informations; Data mining"
"Land cover mapping with higher order graph-based co-occurrence model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057073453&doi=10.3390%2frs10111713&partnerID=40&md5=3dfa465eba12b0523668137f1e27c799","Deep learning has become a standard processing procedure in land cover mapping for remote sensing images. Instead of relying on hand-crafted features, deep learning algorithms, such as Convolutional Neural Networks (CNN) can automatically generate effective feature representations, in order to recognize objects with complex image patterns. However, the rich spatial information still remains unexploited, since most of the deep learning algorithms only focus on small image patches that overlook the contextual information at larger scales. To utilize these contextual information and improve the classification performance for high-resolution imagery, we propose a graph-based model in order to capture the contextual information over semantic segments of the image. First, we explore semantic segments which build on the top of deep features and obtain the initial classification result. Then, we further improve the initial classification results with a higher-order co-occurrence model by extending the existing conditional random field (HCO-CRF) algorithm. Compared to the pixel- and object-based CNN methods, the proposed model achieved better performance in terms of classification accuracy. © 2018 by the authors.","Co-occurrencemodel; Deep learning; Graph-based image interpretation; High-resolution image","Deep learning; Image enhancement; Learning algorithms; Mapping; Neural networks; Random processes; Remote sensing; Semantics; Classification accuracy; Classification performance; Conditional random field; Convolutional Neural Networks (CNN); High resolution image; High resolution imagery; Higher order co occurrences; Image interpretation; Classification (of information)"
"Building extraction in very high resolution imagery by dense-attention networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057072892&doi=10.3390%2frs10111768&partnerID=40&md5=03a84f1cee01189f2a4c11525e9e2e03","Building extraction from very high resolution (VHR) imagery plays an important role in urban planning, disaster management, navigation, updating geographic databases, and several other geospatial applications. Compared with the traditional building extraction approaches, deep learning networks have recently shown outstanding performance in this task by using both high-level and low-level feature maps. However, it is difficult to utilize different level features rationally with the present deep learning networks. To tackle this problem, a novel network based on DenseNets and the attention mechanism was proposed, called the dense-attention network (DAN). The DAN contains an encoder part and a decoder part which are separately composed of lightweight DenseNets and a spatial attention fusion module. The proposed encoder-decoder architecture can strengthen feature propagation and effectively bring higher-level feature information to suppress the low-level feature and noises. Experimental results based on public international society for photogrammetry and remote sensing (ISPRS) datasets with only red-green-blue (RGB) images demonstrated that the proposed DAN achieved a higher score (96.16% overall accuracy (OA), 92.56% F1 score, 90.56% mean intersection over union (MIOU), less training and response time and higher-quality value) when compared with other deep learning methods. © 2018 by the authors.","Attention mechanism; Building extraction; Deep learning; Imagery; Very high resolution","Decoding; Disaster prevention; Disasters; Extraction; Remote sensing; Signal encoding; Attention mechanisms; Building extraction; Encoder-decoder architecture; Geospatial applications; Imagery; International society; Traditional buildings; Very high resolution; Deep learning"
"A two-branch CNN architecture for land cover classification of PAN and MS imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057115262&doi=10.3390%2frs10111746&partnerID=40&md5=8ce217f6764aaff689e9d1eb0bc4e869","The use of Very High Spatial Resolution (VHSR) imagery in remote sensing applications is nowadays a current practice whenever fine-scale monitoring of the earth's surface is concerned. VHSR Land Cover classification, in particular, is currently a well-established tool to support decisions in several domains, including urban monitoring, agriculture, biodiversity, and environmental assessment. Additionally, land cover classification can be employed to annotate VHSR imagery with the aim of retrieving spatial statistics or areas with similar land cover. Modern VHSR sensors provide data at multiple spatial and spectral resolutions, most commonly as a couple of a higher-resolution single-band panchromatic (PAN) and a coarser multispectral (MS) imagery. In the typical land cover classification workflow, the multi-resolution input is preprocessed to generate a single multispectral image at the highest resolution available by means of a pan-sharpening process. Recently, deep learning approaches have shown the advantages of avoiding data preprocessing by letting machine learning algorithms automatically transform input data to best fit the classification task. Following this rationale, we here propose a new deep learning architecture to jointly use PAN and MS imagery for a direct classification without any prior image sharpening or resampling process. Our method, namely MultiResoLCC, consists of a two-branch end-to-end network which extracts features from each source at their native resolution and lately combine them to perform land cover classification at the PAN resolution. Experiments are carried out on two real-world scenarios over large areas with contrasted land cover characteristics. The experimental results underline the quality of our method while the characteristics of the proposed scenarios underline the applicability and the generality of our strategy in operational settings. © 2018 by the authors.","Deep learning; Image classification; Image retrieval by land cover; Land cover mapping; Single-sensor multi-resolution data fusion","Biodiversity; Deep learning; Image retrieval; Learning algorithms; Mapping; Network architecture; Remote sensing; Sensor data fusion; Environmental assessment; Land cover; Land cover classification; Land cover mapping; Multi-spectral imagery; Multiresolution data; Remote sensing applications; Very high spatial resolutions; Image classification"
"Optimized Wishart Network for an Efficient Classification of Multifrequency PolSAR Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052673190&doi=10.1109%2fLGRS.2018.2861081&partnerID=40&md5=5ffad1affd72826c172624bd7813e7bb","High-resolution wide-area images are required in the diverse field of research ranging from urban planning and disaster prediction to agriculture and geology. Sometimes the image is taken under harsh weather conditions or at night time. Current optical remote sensing technology does not have the capability to acquire images in such conditions. Synthetic aperture radar (SAR) uses microwave signal which has a long-range propagation characteristic that allows us to capture images in difficult weather conditions. In addition to this, some polarimetric SAR (PolSAR) systems are also capable of capturing images using multifrequency bands simultaneously resulting into a multitude of information in comparison to the optical images. In this letter, we propose a single-hidden layer optimized Wishart network (OWN) and extended OWN for classification of the single-frequency and multifrequancy PolSAR data, respectively. Performance evaluation is conducted on a single-frequency as well as multifrequency SAR data obtained by Airborne Synthetic Aperture Radar. We observed that for combining multiple band information, proposed single-hidden layer network outperforms deep learning-based architecture involving multiple hidden layers. © 2018 IEEE.","k-means algorithm; multifrequency polarimetric synthetic aperture radar (PolSAR) image classification; neural network; optimized Wishart network (OWN); PolSAR; Wishart distance; WN","Deep learning; Geometrical optics; Meteorology; Network layers; Neural networks; Petroleum reservoir evaluation; Polarimeters; Remote sensing; Airborne synthetic aperture radars; k-Means algorithm; Long-range propagation; Optical remote sensing; Performance evaluations; Polarimetric synthetic aperture radars; PolSAR; Wishart; Synthetic aperture radar; artificial neural network; frequency analysis; image classification; optimization; performance assessment; prediction; satellite data; satellite imagery; synthetic aperture radar; urban planning"
"Mapping paddy rice using a Convolutional Neural Network (CNN) with Landsat 8 datasets in the Dongting Lake Area, China","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057103504&doi=10.3390%2frs10111840&partnerID=40&md5=8a95b92c9515819d635abd2a8b37b06b","Rice is one of the world's major staple foods, especially in China. Highly accurate monitoring on rice-producing land is, therefore, crucial for assessing food supplies and productivity. Recently, the deep-learning convolutional neural network (CNN) has achieved considerable success in remote-sensing data analysis. A CNN-based paddy-rice mapping method using the multitemporal Landsat 8, phenology data, and land-surface temperature (LST) was developed during this study. First, the spatial-temporal adaptive reflectance fusion model (STARFM) was used to blend the moderate-resolution imaging spectroradiometer (MODIS) and Landsat data for obtainingmultitemporal Landsat-like data. Subsequently, the threshold method is applied to derive the phenological variables from the Landsat-like (Normalized difference vegetation index) NDVI time series. Then, a generalized single-channel algorithm was employed to derive LST from the Landsat 8. Finally, multitemporal Landsat 8 spectral images, combinedwith phenology and LST data,were employed to extract paddy-rice information using a patch-based deep-learning CNN algorithm. The results show that the proposed method achieved an overall accuracy of 97.06% and a Kappa coefficient of 0.91, which are 6.43% and 0.07 higher than that of the support vector machine method, and 7.68% and 0.09 higher than that of the random forest method, respectively. Moreover, the Landsat-derived rice area is strongly correlated (R2 = 0.9945) with government statistical data, demonstrating that the proposed method has potential in large-scale paddy-rice mapping using moderate spatial resolution images. © 2018 by the authors.","Convolutional neural network; Fusion model; Land-surface temperature; Phenology; Rice","Atmospheric temperature; Biology; Convolution; Decision trees; Deep learning; Food supply; Neural networks; Radiometers; Remote sensing; Spectroscopy; Surface measurement; Surface properties; Convolutional neural network; Fusion model; Land surface temperature; Phenology; Rice; Mapping"
"Vehicle Instance Segmentation from Aerial Image and Video Using a Multitask Learning Residual Fully Convolutional Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049697902&doi=10.1109%2fTGRS.2018.2841808&partnerID=40&md5=c4b5e0ea7d40caf6080366741c8d6124","Object detection and semantic segmentation are two main themes in object retrieval from high-resolution remote sensing images, which have recently achieved remarkable performance by surfing the wave of deep learning and, more notably, convolutional neural networks. In this paper, we are interested in a novel, more challenging problem of vehicle instance segmentation, which entails identifying, at a pixel level, where the vehicles appear as well as associating each pixel with a physical instance of a vehicle. In contrast, vehicle detection and semantic segmentation each only concern one of the two. We propose to tackle this problem with a semantic boundary-aware multitask learning network. More specifically, we utilize the philosophy of residual learning to construct a fully convolutional network that is capable of harnessing multilevel contextual feature representations learned from different residual blocks. We theoretically analyze and discuss why residual networks can produce better probability maps for pixelwise segmentation tasks. Then, based on this network architecture, we propose a unified multitask learning network that can simultaneously learn two complementary tasks, namely, segmenting vehicle regions and detecting semantic boundaries. The latter subproblem is helpful for differentiating 'touching' vehicles that are usually not correctly separated into instances. Currently, data sets with a pixelwise annotation for vehicle extraction are the ISPRS data set and the IEEE GRSS DFC2015 data set over Zeebrugge, which specializes in a semantic segmentation. Therefore, we built a new, more challenging data set for vehicle instance segmentation, called the Busy Parking Lot Unmanned Aerial Vehicle Video data set, and we make our data set available at http://www.sipeo.bgu.tum.de/downloads so that it can be used to benchmark future vehicle instance segmentation algorithms. © 2018 IEEE.","Boundary-aware multitask learning network; Fully convolutional network (FCN); High-resolution remote sensing image/video; Instance semantic segmentation; Residual neural network (ResNet); Vehicle detection","Antennas; Convolution; Deep learning; Extraction; Feature extraction; Job analysis; Network architecture; Neural networks; Object detection; Object recognition; Pixels; Remote sensing; Semantics; Vehicles; Convolutional networks; High resolution remote sensing images; Multitask learning; residual neural network (ResNet); Semantic segmentation; Task analysis; Vehicle detection; Image segmentation; aerial survey; artificial neural network; complementarity; data set; detection method; imagery; pixel; remote sensing; segmentation; videography"
"Change detection in hyperspectral images using recurrent 3D fully convolutional networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057105898&doi=10.3390%2frs10111827&partnerID=40&md5=8b56167ef33b8a883c55f2d2d3c55d2d","Hyperspectral change detection (CD) can be effectively performed using deep-learning networks. Although these approaches require qualified training samples, it is difficult to obtain ground-truth data in the real world. Preserving spatial information during training is difficult due to structural limitations. To solve such problems, our study proposed a novel CD method for hyperspectral images (HSIs), including sample generation and a deep-learning network, called the recurrent three-dimensional (3D) fully convolutional network (Re3FCN), which merged the advantages of a 3D fully convolutional network (FCN) and a convolutional long short-term memory (ConvLSTM). Principal component analysis (PCA) and the spectral correlation angle (SCA) were used to generate training samples with high probabilities of being changed or unchanged. The strategy assisted in training fewer samples of representative feature expression. The Re3FCN was mainly comprised of spectral-spatial and temporal modules. Particularly, a spectral-spatial module with a 3D convolutional layer extracts the spectral-spatial features from the HSIs simultaneously, whilst a temporal module with ConvLSTM records and analyzes the multi-temporal HSI change information. The study first proposed a simple and effective method to generate samples for network training. This method can be applied effectively to cases with no training samples. Re3FCN can perform end-to-end detection for binary and multiple changes. Moreover, Re3FCN can receive multi-temporal HSIs directly as input without learning the characteristics of multiple changes. Finally, the network could extract joint spectral-spatial-temporal features and it preserved the spatial structure during the learning process through the fully convolutional structure. This study was the first to use a 3D FCN and a ConvLSTM for the remote-sensing CD. To demonstrate the effectiveness of the proposed CD method, we performed binary and multi-class CD experiments. Results revealed that the Re3FCN outperformed the other conventional methods, such as change vector analysis, iteratively reweighted multivariate alteration detection, PCA-SCA, FCN, and the combination of 2D convolutional layers-fully connected LSTM. © 2018 by the authors.","3D convolution; Change detection; Convolutional LSTM; Fully convolutional network; Hyperspectral image","Convolution; Deep learning; Hyperspectral imaging; Iterative methods; Principal component analysis; Remote sensing; Rhenium compounds; Sampling; Spectroscopy; Alteration detections; Change detection; Change vector analysis; Convolutional LSTM; Convolutional networks; Spatial-temporal features; Spectral correlation angles; Threedimensional (3-d); Long short-term memory"
"Toward model-generated household listing in low- and middle-income countries using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058009472&doi=10.3390%2fijgi7110448&partnerID=40&md5=887e20d6de46364fd92bf55e02f91461","While governments, researchers, and NGOs are exploring ways to leverage big data sources for sustainable development, household surveys are still a critical source of information for dozens of the 232 indicators for the Sustainable Development Goals (SDGs) in low- and middle-income countries (LMICs). Though some countries’ statistical agencies maintain databases of persons or households for sampling, conducting household surveys in LMICs is complicated due to incomplete, outdated, or inaccurate sampling frames. As a means to develop or update household listings in LMICs, this paper explores the use of machine learning models to detect and enumerate building structures directly from satellite imagery in the Kaduna state of Nigeria. Specifically, an object detection model was used to identify and locate buildings in satellite images. in the test set, the model attained a mean average precision (mAP) of 0.48 for detecting structures, with relatively higher values in areas with lower building density (mAP = 0.65). Furthermore, when model predictions were compared against recent household listings from fieldwork in Nigeria, the predictions showed high correlation with household coverage (Pearson = 0.70; Spearman = 0.81). With the need to produce comparable, scalable SDG indicators, this case study explores the feasibility and challenges of using object detection models to help develop timely enumerated household lists in LMICs. © 2018 by the authors.","Household enumeration; Machine learning; Object detection; Remote sensing; Survey statistics; Sustainable development goals (SDGs)",
"Detection of Vehicles in Multisensor Data via Multibranch Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047649293&doi=10.1109%2fJSTARS.2018.2825099&partnerID=40&md5=de5e5ea6e634e008019cf18cd393d329","Convolutional neural networks, or CNNs, raised the bar for most computer vision problems and have an increasing impact in remote sensing. However, since they usually contain multiple pooling layers, detection of exact borders of small objects at their original resolution remains yet a challenging topic. Additionally, efforts are being made to reduce the amount of training data. In this paper, we investigate the potential of fully convolutional neural networks (FCNs) for individual vehicle detection in combined elevation and optical data using relatively few training samples. By the proposed multibranch CNN, we combine object recognition within a deep learning framework with the object segmentation at a high resolution, for which two CNN branches are employed. Data fusion is accomplished with a pseudo-Siamese approach. The pixelwise classification likelihood, also referred to as heatmap, is harmoniously postprocessed by a vectorization module, which is based on the minimum bounding rectangle (MBR) extraction and allows for delineation of groups of vehicles. Two methods were developed in which MBRs are supported either by pairs of parallel lines or by region growing. Our approach allows efficient training with few training samples, while delivering high-quality detection results and good computational performance. In our detailed evaluation, we investigate the benefits of data fusion and compare our approach to other state-of-the-art networks. Different datasets were used, containing optical images and elevation data, derived either from airborne laser scanning or from photogrammetric reconstruction. The obtained results are very promising with F1 scores up to 97%. © 2018 IEEE.","Convolution; image classification; neural networks; object detection; remote sensing; vehicles","Convolution; Data fusion; Deep learning; Extraction; Feature extraction; Geometrical optics; Image classification; Image segmentation; Job analysis; Neural networks; Object detection; Object recognition; Optical sensors; Personnel training; Sampling; Three dimensional displays; Vehicles; Airborne Laser scanning; Computational performance; Computer vision problems; Convolutional neural network; Minimum bounding rectangle; Optical imaging; Pixelwise classification; Task analysis; Remote sensing; artificial neural network; computer vision; detection method; image classification; photogrammetry; remote sensing; sensor; transport vehicle"
"Spatial resolution and landscape structure along an urban-rural gradient: Do they relate to remote sensing classification accuracy? – A case study in the megacity of Bengaluru, India","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055890837&doi=10.1016%2fj.rsase.2018.10.003&partnerID=40&md5=4876f27bc4e002cfb8426cbdc379ac5e","Along the urban-rural gradient in megacities, the extent and material composition of impervious surfaces are different. This leads to variations in the frequently mentioned heat-island property, but possibly also to different spectral signatures and, consequently, different accuracies in remote sensing image classification. This, in turn, creates a challenge when it comes to selecting suitable image processing techniques. In this study, we examine how the accuracy of land-cover classification changes along an urban-rural gradient as a function of spatial resolution and the gradient in landscape structure. RapidEye, Sentinel-2A and Landsat 8 images were used. Land-cover classification was performed using a deep learning model and landscape metrics were used to assess landscape structure. A high degree of landscape heterogeneity and lowest classification accuracy was observed in the transition zone between urban and rural domains, within a stretch of 15–20 km from the urban center. As expected, spatial resolution was found to be influential in classification accuracy. A comparison of classifications indicates that within rural landscapes finer resolution images retain more spatial and thematic details in land-cover, e.g., RapidEye and Sentinel-2A imagery better distinguish built-up areas within the agricultural landscape and discriminate more of the mapped land-cover/use classes than Landsat 8. Overall accuracy increased with increasing spatial resolution (30 m, 10 m, 5 m) within the urban and rural areas, however, the 10 m resolution image (Sentinel-2A) produced better results in the transition zone. The findings from this study provide a basis for more focused, consistent and possibly more accurate time-series analyses of land-use dynamics at the urban-rural interface. © 2018 Elsevier B.V.","Land-cover; Landscape structure; Map accuracy; Megacities; Spatial metrics; Urban-rural gradient",
"Ground and multi-class classification of Airborne Laser Scanner point clouds using Fully Convolutional Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057137810&doi=10.3390%2frs10111723&partnerID=40&md5=14ac73392dd6f4539e1a9474ab4c7979","Various classification methods have been developed to extract meaningful information from Airborne Laser Scanner (ALS) point clouds. However, the accuracy and the computational efficiency of the existing methods need to be improved, especially for the analysis of large datasets (e.g., at regional or national levels). In this paper, we present a novel deep learning approach to ground classification for Digital Terrain Model (DTM) extraction as well as for multi-class land-cover classification, delivering highly accurate classification results in a computationally efficient manner. Considering the top-down acquisition angle of ALS data, the point cloud is initially projected on the horizontal plane and converted into a multi-dimensional image. Then, classification techniques based on Fully Convolutional Networks (FCN) with dilated kernels are designed to perform pixel-wise image classification. Finally, labels are transferred from pixels to the original ALS points. We also designed a Multi-Scale FCN (MS-FCN) architecture to minimize the loss of information during the point-to-image conversion. In the ground classification experiment, we compared our method to a Convolutional Neural Network (CNN)-based method and LAStools software. We obtained a lower total error on both the International Society for Photogrammetry and Remote Sensing (ISPRS) filter test benchmark dataset and AHN-3 dataset in the Netherlands. In the multi-class classification experiment, our method resulted in higher precision and recall values compared to the traditional machine learning technique using Random Forest (RF); it accurately detected small buildings. The FCN achieved precision and recall values of 0.93 and 0.94 when RF obtained 0.91 and 0.92, respectively. Moreover, our strategy significantly improved the computational efficiency of state-of-the-art CNN-based methods, reducing the point-to-image conversion time from 47 h to 36 min in our experiments on the ISPRS filter test dataset. Misclassification errors remained in situations that were not included in the training dataset, such as large buildings and bridges, or contained noisy measurements. © 2018 by the authors.","Classification; Convolutional Neural Network; Deep learning; DTM extraction; Filtering; LIDAR","Classifiers; Computational efficiency; Convolution; Decision trees; Deep learning; Efficiency; Extraction; Filtration; Image classification; Image enhancement; Laser applications; Neural networks; Optical radar; Pixels; Remote sensing; Scanning; Statistical tests; Classification technique; Computationally efficient; Convolutional neural network; Convolutional Neural Networks (CNN); Land cover classification; Machine learning techniques; Multi-class classification; Multi-dimensional images; Classification (of information)"
"Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042151311&doi=10.1016%2fj.isprsjprs.2018.01.021&partnerID=40&md5=80bb974ab6609c9ef9ffda91c10fb7f6","In remote sensing images, the absolute orientation of objects is arbitrary. Depending on an object's orientation and on a sensor's flight path, objects of the same semantic class can be observed in different orientations in the same image. Equivariance to rotation, in this context understood as responding with a rotated semantic label map when subject to a rotation of the input image, is therefore a very desirable feature, in particular for high capacity models, such as Convolutional Neural Networks (CNNs). If rotation equivariance is encoded in the network, the model is confronted with a simpler task and does not need to learn specific (and redundant) weights to address rotated versions of the same object class. In this work we propose a CNN architecture called Rotation Equivariant Vector Field Network (RotEqNet) to encode rotation equivariance in the network itself. By using rotating convolutions as building blocks and passing only the values corresponding to the maximally activating orientation throughout the network in the form of orientation encoding vector fields, RotEqNet treats rotated versions of the same object with the same filter bank and therefore achieves state-of-the-art performances even when using very small architectures trained from scratch. We test RotEqNet in two challenging sub-decimeter resolution semantic labeling problems, and show that we can perform better than a standard CNN while requiring one order of magnitude less parameters. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Rotation invariance; Semantic labeling; Sub-decimeter resolution","Convolution; Deep learning; Encoding (symbols); Image segmentation; Network architecture; Neural networks; Remote sensing; Semantics; Absolute orientation; Convolutional neural network; Orientation encoding; Remote sensing images; Rotation invariance; Semantic labeling; State-of-the-art performance; Very high resolution; Rotation; accuracy assessment; artificial neural network; land cover; mapping method; numerical model; orientation; remote sensing; resolution; satellite imagery; vector"
"Hyperspectral Unmixing via Deep Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052607337&doi=10.1109%2fLGRS.2018.2857804&partnerID=40&md5=e8185570775022f51e9e19206fa22cf3","Hyperspectral unmixing (HU) is a method used to estimate the fractional abundances corresponding to endmembers in each of the mixed pixels in the hyperspectral remote sensing image. In recent times, deep learning has been recognized as an effective technique for hyperspectral image classification. In this letter, an end-to-end HU method is proposed based on the convolutional neural network (CNN). The proposed method uses a CNN architecture that consists of two stages: the first stage extracts features and the second stage performs the mapping from the extracted features to obtain the abundance percentages. Furthermore, a pixel-based CNN and cube-based CNN, which can improve the accuracy of HU, are presented in this letter. More importantly, we also use dropout to avoid overfitting. The evaluation of the complete performance is carried out on two hyperspectral data sets: Jasper Ridge and Urban. Compared with that of the existing method, our results show significantly higher accuracy. © 2018 IEEE.","Convolutional neural networks (CNNs); end-to-end model; spectral unmixing; spectral-spatial information","Convolution; Feature extraction; Hyperspectral imaging; Neural networks; Pixels; Remote sensing; Spectroscopy; Convolutional neural network; End-to-end models; Indexes; Kernel; Spatial informations; Spectral unmixing; Deep neural networks; artificial neural network; data set; image analysis; image classification; model; pixel; remote sensing; spatial analysis; spectral analysis; California; Jasper Ridge; United States"
"Recurrent Multiresolution Convolutional Networks for VHR Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048883766&doi=10.1109%2fTGRS.2018.2837357&partnerID=40&md5=7c073fd1731112cf2ce27874f0f5987d","Classification of very high-resolution (VHR) satellite images has three major challenges: 1) inherent low intraclass and high interclass spectral similarities; 2) mismatching resolution of available bands; and 3) the need to regularize noisy classification maps. Conventional methods have addressed these challenges by adopting separate stages of image fusion, feature extraction, and postclassification map regularization. These processing stages, however, are not jointly optimizing the classification task at hand. In this paper, we propose a single-stage framework embedding the processing stages in a recurrent multiresolution convolutional network trained in an end-to-end manner. The feedforward version of the network, called FuseNet, aims to match the resolution of the panchromatic and multispectral bands in a VHR image using convolutional layers with corresponding downsampling and upsampling operations. Contextual label information is incorporated into FuseNet by means of a recurrent version called ReuseNet. We compared FuseNet and ReuseNet against the use of separate processing steps for both image fusions, e.g., pansharpening and resampling through interpolation and map regularization such as conditional random fields. We carried out our experiments on a land-cover classification task using a Worldview-03 image of Quezon City, Philippines, and the International Society for Photogrammetry and Remote Sensing 2-D semantic labeling benchmark data set of Vaihingen, Germany. FuseNet and ReuseNet surpass the baseline approaches in both the quantitative and qualitative results. © 2018 IEEE.","Convolutional networks; deep learning; land cover classification; recurrent networks; very high-resolution (VHR) image","Classification (of information); Convolution; Deep learning; Extraction; Feature extraction; Image classification; Image segmentation; Job analysis; Labeling; Remote sensing; Semantics; Signal sampling; Convolutional networks; Kernel; Land cover classification; Recurrent networks; Spatial resolution; Task analysis; Very high resolution (VHR) image; Image fusion; benchmarking; image classification; image resolution; interpolation; land cover; photogrammetry; remote sensing; satellite imagery; WorldView; Baden-Wurttemberg; Germany; National Capital Region; Philippines; Quezon City; Vaihingen an der Enz"
"Utilizing multilevel features for cloud detection on satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057078751&doi=10.3390%2frs10111853&partnerID=40&md5=02f0133cd7095408e6e67c2f3eb05db0","Cloud detection, which is defined as the pixel-wise binary classification, is significant in satellite imagery processing. In current remote sensing literature, cloud detection methods are linked to the relationships of imagery bands or based on simple image feature analysis. These methods, which only focus on low-level features, are not robust enough on the images with difficult land covers, for clouds share similar image features such as color and texture with the land covers. To solve the problem, in this paper, we propose a novel deep learning method for cloud detection on satellite imagery by utilizing multilevel image features with two major processes. The first process is to obtain the cloud probability map from the designed deep convolutional neural network, which concatenates deep neural network features from low-level to high-level. The second part of the method is to get refined cloud masks through a composite image filter technique, where the specific filter captures multilevel features of cloud structures and the surroundings of the input imagery. In the experiments, the proposed method achieves 85.38% intersection over union of cloud in the testing set which contains 100 Gaofen-1 wide field of view images and obtains satisfactory visual cloud masks, especially for those hard images. The experimental results show that utilizing multilevel features by the combination of the network with feature concatenation and the particular filter tackles the cloud detection problem with improved cloud masks. © 2018 by the authors.","Cloud detection; Fully convolutional network; Multilevel features","Convolution; Deep neural networks; Image processing; Neural networks; Remote sensing; Satellite imagery; Binary classification; Cloud detection; Cloud detection method; Convolutional networks; Deep convolutional neural networks; Image feature analysis; Multilevel features; Neural network features; Feature extraction"
"Method for mapping rice fields in complex landscape areas based on pre-trained convolutional neural network from HJ-1 A/b data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058046454&doi=10.3390%2fijgi7110418&partnerID=40&md5=9313188529735e905463598b1f991781","Accurate and timely information about rice planting areas is essential for crop yield estimation, global climate change and agricultural resource management. In this study, we present a novel pixel-level classification approach that uses convolutional neural network (CNN) model to extract the features of enhanced vegetation index (EVI) time series curve for classification. The goal is to explore the practicability of deep learning techniques for rice recognition in complex landscape regions, where rice is easily confused with the surroundings, by using mid-resolution remote sensing images. A transfer learning strategy is utilized to fine tune a pre-trained CNN model and obta in the temporal features of the EVI curve. Support vector machine (SVM), a traditional machine learning approach, is also implemented in the experiment. Finally, we evaluate the accuracy of the two models. Results show that our model performs better than SVM, with the overall accuracies being 93.60% and 91.05%, respectively. Therefore, this technique is appropriate for estimating rice planting areas in southern China on the basis of a pre-trained CNN model by using time series data. And more opportunity and potential can be found for crop classification by remote sensing and deep learning technique in the future study. © 2018 by the authors.","Complex landscape; Convolutional neural network; Mapping rice fields; Time series of vegetation index; Transfer learning",
"A new deep convolutional neural network for fast hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036494607&doi=10.1016%2fj.isprsjprs.2017.11.021&partnerID=40&md5=67decf76482e98532d343130ed5f016f","Artificial neural networks (ANNs) have been widely used for the analysis of remotely sensed imagery. In particular, convolutional neural networks (CNNs) are gaining more and more attention in this field. CNNs have proved to be very effective in areas such as image recognition and classification, especially for the classification of large sets composed by two-dimensional images. However, their application to multispectral and hyperspectral images faces some challenges, especially related to the processing of the high-dimensional information contained in multidimensional data cubes. This results in a significant increase in computation time. In this paper, we present a new CNN architecture for the classification of hyperspectral images. The proposed CNN is a 3-D network that uses both spectral and spatial information. It also implements a border mirroring strategy to effectively process border areas in the image, and has been efficiently implemented using graphics processing units (GPUs). Our experimental results indicate that the proposed network performs accurately and efficiently, achieving a reduction of the computation time and increasing the accuracy in the classification of hyperspectral images when compared to other traditional ANN techniques. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Classification; Convolutional neural networks (CNNs); Deep learning; Graphics processing units (GPUs); Hyperspectral imaging","Classification (of information); Computer graphics; Convolution; Deep learning; Deep neural networks; Graphics processing unit; Hyperspectral imaging; Image recognition; Independent component analysis; Neural networks; Program processors; Spectroscopy; Computation time; Convolutional neural network; Deep convolutional neural networks; High-dimensional; Multidimensional data; Remotely sensed imagery; Spatial informations; Two dimensional images; Image classification; accuracy assessment; artificial neural network; data set; detection method; experimental study; image analysis; image classification; imaging method; multispectral image; satellite imagery; spatial analysis; two-dimensional modeling"
"Deep learning for ground-level PM2.5 prediction from satellite remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064176198&doi=10.1109%2fIGARSS.2018.8519036&partnerID=40&md5=1e4a5d9ed0d10d5359062b0ca72aeaa1","Satellite remote sensing is a promising approach for the estimation of ground-level PM2.5. In this paper, a deep learning framework for satellite-based PM2.5 estimation is presented. Taking advantage of multi-layer learning and layer-by-layer pre-training, deep learning has the great potential to mine nonlinear relationship between PM2.5 and satellite observations. Firstly, the presented deep learning framework can be employed to estimate ground PM2.5 using satellite-derived aerosol optical depth (AOD). Secondly, the AOD products are retrieved from satellite top-of-atmosphere (TOA) reflectance. The deep learning framework can further be adopted to estimate ground PM2.5 directly from satellite TOA reflectance. The presented framework was tested for AOD-based PM2.5 estimation in China and reflectance-based PM2.5 estimation across Wuhan Metropolitan Area, respectively. The results show that the deep learning framework achieves an outstanding performance for both AOD-based and reflectance-based PM2.5 estimation. This study provides an effective way for the satellite-based estimation of PM2.5. © 2018 IEEE.","Deep learning; PM2.5; Satellite remote sensing","Geology; Reflection; Remote sensing; Satellites; Aerosol optical depths; Learning frameworks; Non-linear relationships; PM2.5; Satellite observations; Satellite remote sensing; Satellite remote sensing data; Top of atmospheres; Deep learning"
"State-of-the-art and gaps for deep learning on limited training data in remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064162234&doi=10.1109%2fIGARSS.2018.8518681&partnerID=40&md5=4b3b6d1112fa73ec97a9405faa45dfa3","Deep learning usually requires big data, with respect to both volume and variety. However, most remote sensing applications only have limited training data, of which a small subset is labeled. Herein, we review three state-of-the-art approaches in deep learning to combat this challenge. The first topic is transfer learning, in which some aspects of one domain, e.g., features, are transferred to another domain. The next is unsupervised learning, e.g., autoencoders, which operate on unlabeled data. The last is generative adversarial networks, which can generate realistic looking data that can fool the likes of both a deep learning network and human. The aim of this article is to raise awareness of this dilemma, to direct the reader to existing work and to highlight current gaps that need solving. © 2018 IEEE","Deep learning; Generative adversarial networks; Limited training data; Remote sensing; Transfer learning","Geology; Remote sensing; Adversarial networks; Learning network; Limited training data; Remote sensing applications; State of the art; State-of-the-art approach; Transfer learning; Unlabeled data; Deep learning"
"A remote sensing spatiotemporal fusion model of landsat and modis data via deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064165318&doi=10.1109%2fIGARSS.2018.8518758&partnerID=40&md5=d73595800c2291515bbad39f42e2df86","In this paper, a novel spatiotemporal fusion model based on deep learning is proposed, which handles the huge spatial resolution gap and the nonlinear mapping between the high spatial resolution (HSR) image and the corresponding high temporal resolution (HTR) image at the same imaging time. Considering the huge spatial resolution gap, a two-layer fusion strategy is adopted. In each layer, the convolutional neural network (CNN) model is employed to exploit the non-linear mapping between the HSR and HTR image and reconstruct the high-spatial and high-temporal (HSHT) resolution images. In the experiment, Landsat data is the representation of the high spatial resolution images, MODIS data is used as the corresponding low spatial resolution images. The experimental results on two different datasets clearly illustrate the superiority of the proposed model. © 2018 IEEE","Convolutional neural network; Non-linear mapping; Spatiotemporal fusion","Convolution; Geology; Image resolution; Mapping; Neural networks; Remote sensing; Convolutional neural network; High spatial resolution; High spatial resolution images; High temporal resolution; Nonlinear mappings; Spatial resolution; Spatial resolution images; Spatio-temporal fusions; Deep learning"
"CSRS-SIAT: A benchmark remote sensing dataset to semantic-enabled and cross-scales scene recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058617061&doi=10.1109%2fIGARSS.2018.8518209&partnerID=40&md5=8960bc7036e9da7aa2082b6d046af8a4","The deep learning has been widely used in scene recognition of remote sensing images. However, the accuracy of deep learning relays on the size of training dataset to the utmost. The remote sensing images have various spatial scales and semantics, which are not fully considered in the existing datasets. In this paper, a benchmark remote sensing dataset named as Cross-Scale Remote Sensing dataset of Shenzhen Institutes of Advanced Technology (CSRS-SIAT) is proposed, which has about 100 classes according to the land cover and land use field, and due to the cross-scale characteristics of remote sensing images, the experiments using traditional and state-of-the-art deep learning algorithms shows that there still need more efforts to achieve better results. © 2018 IEEE","Cross-scale; CSRS-SIAT; Deep learning; Remote sensing; Scene recognition",
"Crops classification from Sentinel-2A multi-spectral remote sensing images based on convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064163171&doi=10.1109%2fIGARSS.2018.8518860&partnerID=40&md5=04f84595d38c7f40d7c0569ea6b8bf38","Deep learning technology such as convolutional neural networks (CNN) can extract the distinguishable and representative features of different land cover from remote sensing images in a hierarchical way to classify. However, in the field of agriculture, there are few application of crops classification from multi-spectral remote sensing images based on deep learning. In this context, we compared the classification methods of CNN and support vector machines (SVM) in extracting the spatial distribution of crops planting area from Sentineal-2A multi-spectral remote sensing images in Yuanyang county, China. For the region of study, both methods obtained reasonable spatial distribution of different crops, the verification results show that the overall accuracy of CNN is 95.6% which is superior to SVM. © 2018 IEEE.","CNN; Crops classification; Multi-spectral; Remote sensing; Sentinel-2A","Convolution; Crops; Deep learning; Geology; Image classification; Neural networks; Spatial distribution; Support vector machines; Classification methods; Convolutional neural network; Learning technology; Multi-spectral; Overall accuracies; Remote sensing images; Sentinel-2A; Verification results; Remote sensing"
"A comparison of deep learning architectures for semantic mapping of very high resolution images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064183280&doi=10.1109%2fIGARSS.2018.8518533&partnerID=40&md5=08f5a2c31b8b8079f1ccf5f5ff904621","Semantic mapping of land cover is a key, but challenging, problem in remote sensing. Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have shown outstanding performance in this task. In order to develop refined deep learning pipeline for meeting the rising need for accurate semantic mapping in remote sensing images, this paper study and compare a number of advanced deep learning segmentation architectures, which have obtained state-of-the-art results on computer vision contests like the Pascal VOC. To further analyze and compare the effectiveness of some elaborate layers and underlying structures introduced by these architectures, we evaluate them by re-implementing, train and test them on ISPRS Potsdam dataset. Our results show that a promising performance with overall F1 score above 87% and mIoU of 79% can be obtained by only using the RGB images, without any post-processing such as conditional random field (CRF) smoothing. At last, we propose several possible approaches to further enhance the deep learning architectures to better deal with high-resolution aerial images. We therefore consider this work to be helpful for the remote sensing research community. © 2018 IEEE.",,"Antennas; Deep neural networks; Geology; Image enhancement; Image segmentation; Mapping; Network architecture; Neural networks; Random processes; Semantics; Statistical tests; Conditional random field; Convolutional neural network; High-resolution aerial images; Learning architectures; Remote sensing images; Research communities; Semantic mapping; Very high resolution (VHR) image; Remote sensing"
"A novel deep learning framework by combination of subspace-based feature extraction and convolutional neural networks for hyperspectral images classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064175308&doi=10.1109%2fIGARSS.2018.8518956&partnerID=40&md5=460f94a1319ba18877b2725c09003e8a","Approaches based on deep learning have gained an increased attention in the recent years in particular Remote Sensing. Convolutional Neural Networks (CNNs) as one of these deep learning techniques has demonstrated remarkable performance in visual recognition applications. However, using well-known pre-train models such as GoogleNet and VGGNet in the area of hyperspectral image classification due to the high dimensionality and the insufficient training samples is intractable. The current study proposed a new and fixes CNN architecture for two real hyperspectral data sets. To overcome curse of dimensionality we perform a subspace-based feature extraction method by calculating the orthonormal basis of correlation matrix for each class to reduce the dimensionality of hyperspectral images and increasing signal to noise ratio. This framework combines the proposed CNN architecture and subspace reduction method to prepare informative features (from subspace method) and designing optimized CNN by considering limitation of training samples. Also, feature generated by subspace reduction method is compatible by the nature of class based CNNs and a logistic regression as a classifier in the last layer of proposed architecture. Experimental results from two real and well-known hyperspectral images, the Indiana Pines and the Pavia University scenes show that the proposed strategy leads to a performance improvement, as opposed to using the original data and conventional feature extraction strategies which have been employed during the recent approaches. The classification overall accuracy of ca. 98.1% and 98.3% were obtained in Indian Pine and Pavia University respectively. © 2018 IEEE.","Convolutional Neural Network; Deep Learning; Dimension Reduction; Feature Extraction; Hyperspectral Image Classification; Subspace-based Feature Extraction","Classification (of information); Convolution; Extraction; Feature extraction; Geology; Hyperspectral imaging; Image classification; Image enhancement; Network architecture; Neural networks; Remote sensing; Sampling; Signal to noise ratio; Spectroscopy; Convolutional neural network; Curse of dimensionality; Dimension reduction; Feature extraction methods; Images classification; Logistic regressions; Proposed architectures; Subspace based; Deep learning"
"Oil-palm tree detection in aerial images combining deep learning classifiers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064250169&doi=10.1109%2fIGARSS.2018.8519239&partnerID=40&md5=5311ae3308bb340dba4a22f6d07ecc3b","Palm oil is the largest vegetable oil in the world in terms of produced volume, and 75% of global production is used for food and cooking purposes. Sustainable management of the producing areas calls for the frequent assessment of field conditions. In this paper, we investigate an automatic algorithm based on deep learning that is capable to build an inventory of individual oil-palm trees using aereal color images collected by unmanned aerial vehicles. The idea consists of combining the outputs of two independent convolutional neural networks, trained on partially distinct subsets of samples and different spatial scales to capture coarse and fine details of image patches. The estimated posterior probabilities are combined by simple averaging as to improve detection accuracy and estimate the confidence for each individual detection. Non-maxima suppression removes weak detections. Experiments at three commercial oil-palm tree plantations sites aged two, four, and 16 years in Northern Brazil revealed overall detection accuracies in the range 91.2–98.8% using orthomosaics of decimeter spatial resolution. The proposed approach can be a useful component of a forest monitoring system based on remote sensing. © 2018 IEEE.","Classification; Convolutional neural networks; Forest inventory; Remote sensing; Tree counting","Antennas; Classification (of information); Convolution; Forestry; Geology; Image classification; Neural networks; Palm oil; Palmprint recognition; Remote sensing; Thermal processing (foods); Automatic algorithms; Convolutional neural network; Forest inventory; Forest monitoring systems; Non-maxima suppressions; Posterior probability; Sustainable management; Tree counting; Deep learning"
"Color: Cycling offline learning and online representing for remote sensing dataflow","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063138083&doi=10.1109%2fIGARSS.2018.8517826&partnerID=40&md5=cfa818e83844a9132cffeb2f31538f7a","In recent years, many model driven frameworks have achieved great success in remote sensing. For large scale remote sensing dataflow, however, its ability of processing is far from enough. In this paper, we propose a novel cycling data driven framework based on deep learning, which consists of three components: offline learning, online representing and sample refining. That online representing makes predictions on unlabeled remote sensing images and uses them to retrain the model by offline learning while sample refining as a router refines coarse predictions and forwards refined predictions to offline learning. Because the process is like circling between offline learning and online representing, we refer to our framework as circling offline learning and online representing (COLOR). We show the excellent performance and potentiality on airplane object detection by applying a fairly straightforward implement of COLOR using our GF2 airplane object detection dataset. Code and dataset will be made available. © 2018 IEEE","Data driven; Deep learning; Offline learning; Online representing; Remote sensing dataflow",
"Multitask classification of remote sensing scenes using deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064172277&doi=10.1109%2fIGARSS.2018.8518874&partnerID=40&md5=5d1c56f2951be24f63362fa1ea503f20","The problem of scene classification in remote sensing (RS) images has attracted a lot of attention recently. Many datasets have been presented in the literature for this purpose with each claiming to be the benchmark dataset. In this paper, we propose a different approach to the RS community. Instead of putting our effort in building larger and large scene datasets, we argue that it is better to build a machine learning framework that can learn from all available datasets. We formulate this as a multitask learning problem where each dataset represents a task. Then, we present a deep learning solution that can perform multitask learning. We test the proposed multitask network on three popular scene datasets, namely UC Merced, KSA, and AID datasets. Preliminary results show the promising capabilities of this solution at sharing information between tasks and improving the classification accuracy. © 2018 IEEE.","Convolutional Neural Network; Deep learning; Multitask classification; Scene classification","Classification (of information); Deep learning; Geology; Large dataset; Learning algorithms; Neural networks; Remote sensing; Benchmark datasets; Classification accuracy; Convolutional neural network; Multitask classifications; Multitask learning; Remote sensing images; Scene classification; Sharing information; Deep neural networks"
"CNN based renormalization method for ship detection in VHR remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064180763&doi=10.1109%2fIGARSS.2018.8518680&partnerID=40&md5=3c2b6b6aee28f2e2649c0fff049c26f1","Ship detection with very high resolution (VHR) remote sensing image has recently been an attractive topic due to rapid development of deep learning. Current researches on ship detection are generally confronted with a big challenge that existing methods failed to get high quality of object proposal with good intersection-over-union (IOU) before detection. In this paper, a Convolutional Neural Network (CNN) based renormalization method is proposed to improve the quality of object proposal. First, CNN is used to predict shape information of candidate ships' which are involved with rotation, location and scale in patches. Then, a renormalization net is designed to adjust the candidate ships in patches by correcting the shape information and renormalizing it to uniform patch. In this way, good candidate objects in patches could be generated and will be helpful with improving following ship detection. The proposed renormalization net was tested on a Google-Earth handcraft dataset. The experimental result demonstrates the proposed renormalization net greatly improve the ship detection with both of good detection accuracy and high IOU. © 2018 IEEE.","CNN; Remote sensing; Renormalization; Ship detection","Deep learning; Geology; Neural networks; Ships; Convolutional neural network; Detection accuracy; Remote sensing images; Renormalization; Renormalization methods; Shape information; Ship detection; Very high resolution; Remote sensing"
"Deep learning crop classification approach based on sparse coding of time series of satellite data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063132043&doi=10.1109%2fIGARSS.2018.8518263&partnerID=40&md5=82e9167d4c4c28c8b69c0fbb5240c365","Crop classification maps based on high resolution remote sensing data are essential for supporting sustainable land management. The most challenging problems for their producing are collecting of ground based training and validation datasets, non-regular satellite data acquisition and cloudiness. To increase the efficiency of ground data utilization it is important to develop classifiers able to be trained on the data collected in the previous year. In this study, we propose new deep learning method for providing crop classification maps using in-situ data that has been collected in the previous year. Main idea of the study is to utilize deep learning approach based on sparse autoencoder. At the first stage it is trained on satellite data only and then neural network fine-tuning is conducted based on in-situ data form the previous year. Taking into account that collecting ground truth data is very time consuming and challenging task, the proposed approach allows us to avoid necessity for annual collecting in-situ data for the same territory. Experimental results for the territory of Ukraine show that this technique is rather efficient and provides reliable crop classification maps with overall accuracy higher than 85.9%. © 2018 IEEE","Autoencoder; Crop mapping; Deep learning; Neural network; Sentinel-1; Sparse coding",
"Deep learning-based methodological approach for vineyard early disease detection using hyperspectral data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064256603&doi=10.1109%2fIGARSS.2018.8519136&partnerID=40&md5=b6de01e1a0472cb193cf9c6b40dfae26","Machine Learning (ML) progressed significantly in the last decade, evolving the computer-based learning/prediction paradigm to a much more effective class of models known as Deep learning (DL). Since then, hyperspectral data processing relying on DL approaches is getting more popular, competing with the traditional classification techniques. In this paper, a valid ML/DL-based works applied to hyperspectral data processing is reviewed in order to get an insight regarding the approaches available for the effective meaning extraction from this type of data. Next, a general DL-based methodology focusing on hyperspectral data processing to provide farmers and winemakers effective tools for earlier threat detection is proposed. © 2018 IEEE.","Agriculture; Deep learning; Forestry; Hyperspectral data; Machine learning; Remote sensing","Agriculture; Data handling; Forestry; Geology; Learning systems; Machine learning; Remote sensing; Classification technique; Computer-based learning; Early disease detection; Effective tool; Hyperspectral Data; Methodological approach; Threat detection; Deep learning"
"A two-stream unified interpretation network for heterogeneous remote sensing images classification.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063141871&doi=10.1109%2fIGARSS.2018.8517746&partnerID=40&md5=9a85aeb5e5802f7c4156ae69516ec6b8","The conventional studies on different types of remote sensing (RS) images classifications are conducted separately. Thanks to the powerful potential of deep learning to automatically learn features from data, exploring a unified method is possible. Moreover, recent research shows that sparse and low-rank representations can convey valuable information for patterns classification. Therefore, this paper presents a two-stream heterogeneous RS images unified interpretation network (HRSIUI-Net). One stream is to transfer the pre-trained fully convolutional network to learn deep multi-scale spatial features of RS data. The other stream is to employ a subspace learning based on graph embedding to learn the sparse and low-rank subspace representations of high-dimensional features. And then, two streams of learned subspace features are integrated for classification combined with an SVM. The experimental results on two typical RS data indicate that HRSIUI-Net can achieve competitive performance. © 2018 IEEE","Deep learning; Fully convolutional network; Graph embedding; Remote sensing image classification; Sparse and low-rank representations; Subspace learning",
"Toward the use of deep learning for topographic feature extraction from high resolution optical satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064279819&doi=10.1109%2fIGARSS.2018.8519171&partnerID=40&md5=ca10b2221c78df42c010a543486940c1","This paper introduces the exploitation of a convolutional neural network for the extraction of topographic features from high-resolution optical satellite imagery. A UNET based model was trained for seven feature classes of roads, buildings, waterbodies using two 3-band (RGB) images for a study site in Kingston (Canada). The trained model’s accuracy was evaluated on eight tiles of 8000x8000 pixels using a confusion matrix, the overall accuracy and kappa. The results show overall accuracy varying between 90 % and 99 % and kappa varying between 0.48 and 0.98, with five of the eight tiles being over 0.85. The model generally produced accurate predictions, except for commercial and industrial buildings and for unpaved roads, which were under represented in the training data. The project provided perspective for the development of a training database for topographic feature extraction using deep learning and for expansion to the national scale. © 2018 Crown.","Deep learning; High-resolution optical imagery; Topographic feature extraction","Building materials; Extraction; Feature extraction; Geology; Neural networks; Office buildings; Optical resolving power; Remote sensing; Satellite imagery; Accurate prediction; Confusion matrices; Convolutional neural network; High-resolution optical imagery; Industrial buildings; Optical satellite imagery; Overall accuracies; Topographic features; Deep learning"
"Deep learning - A new approach for multi-label scene classification in PlanetScope and sentinel-2 imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063165204&doi=10.1109%2fIGARSS.2018.8517499&partnerID=40&md5=de9b9d2264f3c412399814356f0d9dc6","Motivated by the increasing availability of high-resolution satellite imagery, we developed deep learning models able to efficiently and accurately classify the atmospheric conditions and dominant classes of land cover/land use in commercial PlanetScope imagery acquired over the Amazon rainforest. In specific, we trained deep convolutional neural network (CNN) to perform multi-label scene classification of high-resolution (&lt;10 m) satellite imagery. We also discuss the challenges and opportunities in training deep CNN models for multi-label scene classification. Finally, we investigate the transferability of our PlanetScope-trained models to freely available Sentinel-2 imagery acquired over the wet tropics of Australia. Our best performing model achieved an F ß of 0.91, which was only 2% short of the top performing model in the Understanding the Amazon from Space Kaggle competition [1]. We also find that our models are suitable for classifying similar resolution satellite imagery, such as Sentinel-2. © 2018 Crown","Classification; CNN; Deep learning; PlanetScope; Remote sensing; Sentinel-2",
"Deep learning neural networks for land use land cover mapping","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060333475&doi=10.1109%2fIGARSS.2018.8518619&partnerID=40&md5=67819cbb296dfefa6d6ab171777da35d","The importance of accurate and timely information describing the nature and extent of land resources and changes over time is increasing. This research examines the application of deep learning neural networks (DLNN) to the analysis of satellite imagery with specific focus on the production of land use/land cover maps. DLNN have made considerable strides in pattern recognition and machine learning over the last several years. However, their application to remote sensing is less well developed as the technology was originally designed for simple photographs and not satellite imagery. This research presents the results of an experimental study conducted that developed a DLNN to generate land use/land cover maps of the southern agricultural region of Manitoba, Canada. The results of this approach demonstrate a clear advantage in processing time once the DLNN is properly trained when compared to human based semi-automated process. © 2018 IEEE.","Big Data; Land Cover Mapping; Machine Learning; Neural Networks","Automation; Big data; Geology; Land use; Learning systems; Machine learning; Mapping; Neural networks; Pattern recognition; Remote sensing; Satellite imagery; Automated process; Land cover mapping; Land resources; Land use/ land covers; Land use/land cover; Learning neural networks; Manitoba , Canada; Processing time; Deep neural networks"
"Three applications of deep learning algorithms for object detection in satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063147895&doi=10.1109%2fIGARSS.2018.8518102&partnerID=40&md5=2df071654b11fb1c4c0e278ecfe7b424","Detection of objects in images has been long used in computer vision applications (image and video analysis) in fields such as surveillance or robotics. The last decade saw a break-through in this area when deep convolutional neural networks were introduced, in addition of the GPU computing capacity. In remote sensing, satellite images are also used for feature extraction and often classic machine learning techniques are used for the classification of the pixels in the image. This paper shows how one of the networks developed for the ImageNet challenge can be applied to satellite imagery for object detection using three examples: roads, palm trees and cars. © 2018 IEEE","Classification; Convolutional networks; Deep learning; Machine learning; Object detection; Remote sensing; VGG",
"Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057296226&doi=10.1109%2fIGARSS.2018.8519248&partnerID=40&md5=2a6dfd8f259c83da6317aec786d61c29","In this paper, we address the challenge of land use and land cover classification using Sentinel-2 satellite images. The key contributions are as follows. We present a novel dataset based on Sentinel-2 satellite images covering 13 different spectral bands and consisting of 10 classes with in total 27,000 labeled images. We evaluate state-of-the-art deep Convolutional Neural Networks (CNNs) on this novel dataset with its different spectral bands. We also evaluate deep CNNs on existing remote sensing datasets and compare the obtained results. With the proposed novel dataset, we achieved an overall classification accuracy of 98.57%. The classification system resulting from the proposed research opens a gate towards various Earth observation applications. We demonstrate how the classification system can assist in improving geographical maps. © 2018 IEEE.","Convolutional neural network; Dataset; Deep learning; Earth observation; Land cover classification; Land use classification; Machine learning","Convolution; Deep learning; Deep neural networks; Geology; Land use; Learning systems; Neural networks; Observatories; Remote sensing; Convolutional neural network; Dataset; Earth observations; Land cover classification; Landuse classifications; Classification (of information)"
"Super-resolution of remote sensing images based on transferred generative adversarial network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063138028&doi=10.1109%2fIGARSS.2018.8517442&partnerID=40&md5=39673883be5d097f6a3bd4bf2fe8f3fc","Single image super-resolution (SR) has been widely studied in recent years as a crucial technique for remote sensing applications. This paper proposes a SR method for remote sensing images based on a transferred generative adversarial network (TGAN). Different from the previous GAN-based SR approaches, the novelty of our method mainly reflects from two aspects. First, the batch normalization layers are removed to reduce the memory consumption and the computational burden, as well as raising the accuracy. Second, our model is trained in a transfer-learning fashion to cope with the insufficiency of training data, which is the crux of applying deep learning methods to remote sensing applications. The model is firstly trained on an external dataset DIV2K and further fine-tuned with the remote sensing dataset. Our experimental results demonstrate that the proposed method is superior to SRCNN and SRGAN in terms of both the objective evaluation and the subjective perspective. © 2018 IEEE","Generative adversarial network; Remote sensing images; Super-resolution; Transfer learning",
"Deep semantic hashing retrieval of remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064171151&doi=10.1109%2fIGARSS.2018.8519276&partnerID=40&md5=2820a5ade2238d7303939b94d2317958","Due to the rapid evolution of satellite systems, traditional nearest neighbor image retrieval methods used in large-scale image retrieval usually cause ""curse of dimensionality"" that leads to boosting feature storage and slow retrieval speed. The hashing method, which aims at mapping the high-dimensional data to compact binary hash codes in Hamming space and quickly calculates the Hamming distance by bit operation and XOR operation, can effectively achieve search and retrieval with remaining similarity for big data. In this paper, we propose a novel image retrieval method based on deep hashing learning, called deep semantic hashing(DSH), attempting to mining the semantic information of remote sensing(RS) images. Experiments carried out on an archive of RS images point out that DSH outperforms other methods to achieve the state-of-the-art performance in image retrieval applications. © 2018 IEEE.","Deep learning; Hashing methods; Image retrieval; Remote sensing; Semantic mining","Clustering algorithms; Data mining; Deep learning; Digital storage; Geology; Hamming distance; Hash functions; Image retrieval; Remote sensing; Semantics; Space optics; Curse of dimensionality; Hashing method; High dimensional data; Remote sensing images; Retrieval applications; Search and retrieval; Semantic minings; State-of-the-art performance; Search engines"
"Optical remote sensing change detection through deep Siamese network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063152084&doi=10.1109%2fIGARSS.2018.8518178&partnerID=40&md5=34c7169fca58c832ad29ae1c432548ff","This paper presents a change detection approach for optical remote sensing images based on deep learning. Due to the excellent performance of Convolutional Neural Network (CNN) in feature learning, two models are explored in this work, where the proposed algorithms show how to learn, directly from images, a similarity function to compare bi-temporal images. Two-stream network named as Siamese network is presented. First, bi-temporal images are fed directly into the proposed network. Second, a combination of the aforementioned model with a perceptual loss is presented, this combination focus on high representational features that are extracted from a pre-trained network on large dataset of natural images (ImageNet) rather than opting directly on remote sensing images. Experimental results on real dataset show the effectiveness and the superiority of the proposed framework. © 2018 IEEE","Change detection; Convolutional network; Remote sensing; Siamese network",
"End-to-end learning of polygons for remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060881091&doi=10.1109%2fIGARSS.2018.8518116&partnerID=40&md5=4c8d3e3cb1b0b971ff292620d6b63728","While geographic information systems typically use polygonal representations to map Earth's objects, most state-of-the-art methods produce maps by performing pixelwise classification of remote sensing images, then vectorizing the outputs. This paper studies if one can learn to directly output a vectorial semantic labeling of the image. We here cast a mapping problem as a polygon prediction task, and propose a deep learning approach which predicts vertices of the polygons outlining objects of interest. Experimental results on the Solar photovoltaic array location dataset show that the proposed network succeeds in learning to regress polygon coordinates, yielding directly vectorial map outputs. © 2018 IEEE","Convolutional neural networks; Deep learning; High-resolution aerial images; Polygon; Regression; Vectorial",
"Deep convolutional segmentation of remote sensing imagery: A simple and efficient alternative to stitching output labels","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064154866&doi=10.1109%2fIGARSS.2018.8518701&partnerID=40&md5=6a1e9d246cf41f427e160cb0285619a5","In this work we consider the application of convolutional neural networks (CNNs) for the semantic segmentation of remote sensing imagery (e.g., aerial color or hyperspectral imagery). In segmentation the goal is to provide a dense pixel-wise labeling of the input imagery. However, remote sensing imagery is usually stored in the form of very large images, called “tiles”, which are too large to be segmented directly using most CNNs and their associated hardware. During label inference (i.e., obtaining labels for a new large tile) smaller sub-images, called “patches”, are extracted uniformly over a tile and the resulting label maps are “stitched” (or concatenated) to create a tile-sized label map. This approach suffers from computational inefficiency and risks of discontinuities at the boundaries between the output of individual patches. In this work we propose a simple alternative approach in which the input size of the CNN is dramatically increased only during label inference. We evaluate the performance of the proposed approach against a standard stitching approach using two popular segmentation CNN models on the INRIA building labeling dataset. The results suggest that the proposed approach substantially reduces label inference time, while also yielding modest overall label accuracy increases. This approach also contributed to our winning entry (overall performance) in the INRIA building labeling competition. © 2018 IEEE","Aerial imagery; Building detection; Convolutional neural networks; Deep learning; Semantic segmentation","Aerial photography; Antennas; Building materials; Convolution; Deep learning; Geology; Neural networks; Semantics; Spectroscopy; Aerial imagery; Building detection; Convolutional neural network; Hyper-spectral imageries; Large images; Pixel-wise labeling; Remote sensing imagery; Semantic segmentation; Remote sensing"
"Attention based network for remote sensing scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064157076&doi=10.1109%2fIGARSS.2018.8519232&partnerID=40&md5=7025271a2c406dc6d11f03db0ade2be6","Scene classification of very high resolution remote sensing images is becoming more and more important because of its wide range of applications. However, previous works are mainly based on handcrafted features which do not have enough adaptability and expression ability. In this paper, inspired by the attention mechanism of human visual system, we propose a novel attention based network (AttNet) for scene classification. It can focus selectively on some key areas of images so that it can abandon redundant information. Essentially, AttNet gives a way to readjust the signal of supervision, and it is one of the first successful attempts on visual attention for remote sensing scene classification. Our method is evaluated on the UC Merced Land-Use Dataset, in comparison with some state-of-the-art methods. The experimental result shows that the proposed method makes a great improvement on both convergence speed and classification accuracy, and it also shows the effectiveness of visual attention for this task. © 2018 IEEE.","Convolutional neural networks; Deep learning; Long short-term memory; Remote sensing; Scene classification; Visual attention","Behavioral research; Deep learning; Geology; Land use; Long short-term memory; Neural networks; Attention mechanisms; Classification accuracy; Convolutional neural network; Human Visual System; Scene classification; State-of-the-art methods; Very high resolution; Visual Attention; Remote sensing"
"Stacked encoder-decoders for accurate semantic segmentation of very high resolution satellite datasets","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064215162&doi=10.1109%2fIGARSS.2018.8519113&partnerID=40&md5=3799f7e34162a61e3a2882003643143e","Semantic segmentation is currently a mainstream method for addressing several remote sensing applications, achieving recently remarkable performance by employing deep learning techniques. In particular, this is the case for pixel-wise dense classification models in very high resolution remote sensing datasets. In this paper, we exploit the use of a relatively deep architecture based on repetitive downscale-upscale processes that had been previously employed for human pose estimation tasks. By integrating such a model, we are aiming to capture and extract low-level details, such as small objects, object boundaries and edges. Experimental results and quantitative evaluation has been performed on the publicly available ISPRS (WGIII4) benchmark dataset indicating the potential of the proposed approach. © 2018 IEEE.","Building detection; Car detection; Fully convolutional networks; Stacked hourglass networks","Classification (of information); Deep learning; Geology; Semantics; Signal encoding; Building detection; Car detection; Classification models; Convolutional networks; Human pose estimations; Quantitative evaluation; Remote sensing applications; Semantic segmentation; Remote sensing"
"Adaptive spatial-scale-aware deep convolutional neural network for high-resolution remote sensing imagery scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063128652&doi=10.1109%2fIGARSS.2018.8518290&partnerID=40&md5=ad3f360012e1da2adf6281ec99c5ef81","High-resolution remote sensing (HRRS) scene classification plays an important role in numerous applications. During the past few decades, a lot of remarkable efforts have been made to develop various methods for HRRS scene classification. In this paper, focusing on the problems of complex context relationship and large differences of object scale in HRRS scene images, we propose a deep CNN-based scene classification method, which not only enables to enhance the ability of spatial representation, but adaptively recalibrates channel-wise feature responses to suppress useless feature channels. We evaluated the proposed method on a publicly large-scale dataset with several state-of-the-art convolutional neural network (CNN) models. The experimental results demonstrate that the proposed method is effective to extract high-level category features for HRRS scene classification. © 2018 IEEE.","Convolutional Neural Networks (CNNs); Deep learning; High-resolution remote sensing scene classification; Spatial coding; Weight-Adptive",
"Hierarchical region based convolution neural network for multiscale object detection in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063127286&doi=10.1109%2fIGARSS.2018.8518345&partnerID=40&md5=52b55df9b4c233744ed3666ea088ed2f","In this paper, we propose a novel Faster R-CNN based method to detect multiscale objects in very high resolution optical remote sensing images. Firstly, a pre-trained CNN is used to extract features from an input image; and then a set of object candidates are generated. To efficiently detect objects with various scales, we design a hierarchical selective filtering (HSF) layer to map features in different scales to the same scale space. The HSF layer can be applied on both region proposal and the subsequent detection network. More importantly, it can be plugged into Faster R-CNN network without modifying its architecture, meanwhile boosting the performance on detecting objects with varying scales. The proposed model can be trained in an end-to-end manner. We test our network on three datasets containing different multiscale objects, including airplanes, ships and buildings, which are collected from Google Earth images and GaoFen-2 images. Experiments demonstrate high precision and robustness of our method. © 2018 IEEE.","Deep learning; Multiscale analysis; Object detection; Optical image; Remote sensing",
"Deep learning hyperspectral image classification using multiple class-based denoising autoencoders, mixed pixel training augmentation, and morphological operations","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064228798&doi=10.1109%2fIGARSS.2018.8519368&partnerID=40&md5=e3bf52c271a925776d62f860cb611d3b","Herein, we present a system for hyperspectral image segmentation that utilizes multiple class-based denoising autoencoders which are efficiently trained. Moreover, we present a novel hyperspectral data augmentation method for labelled HSI data using linear mixtures of pixels from each class, which helps the system with edge pixels which are almost always mixed pixels. Finally, we utilize a deep neural network and morphological hole-filling to provide robust image classification. Results run on the Salinas dataset verify the high performance of the proposed algorithm. © 2018 IEEE.","Deep learning; Denoising autoencoder; Remote sensing","Deep learning; Deep neural networks; Geology; Image segmentation; Mathematical morphology; Pixels; Remote sensing; Spectroscopy; Auto encoders; Edge pixels; Hole filling; Hyperspectral Data; Linear mixtures; Mixed pixel; Morphological operations; Multiple class; Image classification"
"Estimating the NDVI from SAR by convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064232084&doi=10.1109%2fIGARSS.2018.8519459&partnerID=40&md5=8e864cfe6e0f95945251bf084fd1dc82","Since optical remote sensing images are useless in cloudy conditions, a possible alternative is to resort to synthetic aperture radar (SAR) images. However, many conventional techniques for Earth monitoring applications require specific spectral features which are defined only for multispectral data. For this reason, in this work we propose to estimate missing spectral features through data fusion and deep learning, exploiting both temporal and cross-sensor dependencies on Sentinel-1 and Sentinel-2 time-series. The proposed approach, validated focusing on the estimation of the normalized difference vegetation index (NDVI), shows very interesting results with a large performance gain over the linear regression approach according to several accuracy indicators. © 2018 IEEE.","Data fusion; Deep learning; Multitemporal; Synthetic aperture radar (SAR); Vegetation monitoring","Data fusion; Deep learning; Geology; Neural networks; Radar imaging; Remote sensing; Sensor data fusion; Vegetation; Conventional techniques; Convolutional neural network; Monitoring applications; Multi-temporal; Normalized difference vegetation index; Optical remote sensing; Synthetic aperture radar (SAR) images; Vegetation monitoring; Synthetic aperture radar"
"Cloud cover assessment in satellite images via deep ordinal classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064149209&doi=10.1109%2fIGARSS.2018.8519035&partnerID=40&md5=0308a42999fa33c9ab34fd9ef2d4ab1b","The percentage of cloud cover is one of the key indices for satellite data products. To date, cloud cover assessment is performed manually in most groundstations. To facilitate the process, this paper proposes a deep learning approach for cloud cover assessment in quicklook satellite images. The quicklook images from Centre for Remote Imaging, Sensing and Processing (CRISP) are used for demonstration. Same as the manual operation, given a quicklook image, the algorithm returns 8 labels ranging from A to E and *, indicating the cloud percentages in different areas of the image. This is achieved by constructing 8 improved VGG-16 models, where parameters such as the loss function, learning rate and dropout are tailored for better performance. Results indicate that approach is promising, as around 85% of sub-scenes are correctly labelled, and the accuracy is even higher if one ordinal error is accepted. This paper demonstrates a new application in remote sensing using state-of-the-art deep learning techniques. © 2018 IEEE.","Classification; Cloud cover assessment; Deep learning; Ordinal classification; Satellite images","Classification (of information); Deep learning; Geology; Learning algorithms; Remote sensing; Satellites; Cloud cover; Learning approach; Learning techniques; Manual operations; New applications; Ordinal classification; Satellite images; State of the art; Image classification"
"Can SAR images and optical images transfer with each other?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064147595&doi=10.1109%2fIGARSS.2018.8518921&partnerID=40&md5=445382ff8906c7640cf51634c3f753c5","Synthetic aperture radar (SAR) and optical imaging are different remote sensing methods. Given a SAR image, is it possible to predict what the observed scene looks like in an optical image? Transfer between SAR data and optical data seems to be impossible. However, this article shows examples that by applying deep learning techniques on high resolution airborne SAR images and GoogleEarth optical images, the SAR images and optical images can transfer with each other. The transferring help us to better understand the relationship between SAR and optical image, and can be potentially used to transfer detection or classification algorithms for optical image straightforwardly to be applied on SAR image. © 2018 IEEE.","CycleGAN; Deep learning; Generative adversarial networks; Image transfer between optical; SAR images; Transfer learning","Deep learning; Geology; Geometrical optics; Remote sensing; Synthetic aperture radar; Adversarial networks; CycleGAN; Image transfer; SAR Images; Transfer learning; Radar imaging"
"Time-scale transferring deep convolutional neural network for mapping early rice","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064152058&doi=10.1109%2fIGARSS.2018.8518952&partnerID=40&md5=f45b463cb7343ef846f896e31056d62e","In recent years, the use of deep learning in remote sensing domain has made it possible to automate mapping in large-scale. In this paper, we propose a transfer learning method which pre-train a convolutional neural network (CNN) with middle-resolution remote sensing data in 2016, and fine-tune it in following years with a spot of high-resolution remote sensing data in 2017. We used the fine-tuned model to mapping the early-rice in 25 countries which cost only 21 minutes, and yielded an overall accuracy of 81.68%. The result demonstrate that the convolutional neural network model can transfer in different time period with little adjustment in a very high accuracy. © 2018 IEEE.","Convolutional neural network; Middle-resolution data; Time-scale; Transfer learning","Convolution; Data communication systems; Deep neural networks; Geology; Mapping; Neural networks; Convolutional neural network; High resolution remote sensing; Middle-resolution data; Overall accuracies; Remote sensing data; Time-scales; Transfer learning; Transfer learning methods; Remote sensing"
"Palm trees counting in remote sensing imagery using regression convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064277516&doi=10.1109%2fIGARSS.2018.8519188&partnerID=40&md5=8483807847a182206b410e877f1d62fe","Date palm trees are important economic crops in many countries and counting their numbers in a plantation area is crucial information for predicting the yield of date fruits, determination of insurance and financial aids, etc. In this abstract, a supervised tree counting framework is proposed using Convolutional Neural Network (CNN). The proposed approach casts the counting process into a regression problem, instead of following the classification or detection framework. To further decrease the prediction error of counting, we fine-tuned a pretrained CNN architecture into regression model. As the final output, not only the tree count is estimated for an image, but also its spatial density map is provided. Trained with small image patches cropped from airborne dataset, the proposed method is compared to manual counting and obtains good performance. © 2018 IEEE.","CNN based regression; Convolutional neural network; Deep learning; Tree counting","Convolution; Deep learning; Forestry; Geology; Neural networks; Regression analysis; CNN based regression; Convolutional neural network; Detection framework; Prediction errors; Regression problem; Remote sensing imagery; Spatial densities; Tree counting; Remote sensing"
"Enhancing the classification of remote sensing data using multiband compact texture unit descriptor and deep convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064158609&doi=10.1109%2fIGARSS.2018.8518501&partnerID=40&md5=e988323244e5d604679a4c677220bcc1","This abstract proposes a method to enhance the classification of high spatial remotely sensed imagery by using Multiband Compact Texture Unit (MBCTU) descriptor and pre-trained convolutional neural networks (CNN) feature extractor. MBCTU was used in order to take into account intra- and interband spatial interactions by characterizing texture using relative pixel values in a multispectral neighborhood instead of a monoband neighborhood. The derived new encoded images are fed to a deep feature extractor (fine-tuned CNN). Finally, the obtained deep features are classified using k nearest neighbor's (KNN) algorithm to produce the classification map. The proposed approach is applied to the classification of hyperspectral and multispectral datasets. Results indicate that the proposed approach can achieve accurate classification results compared to other approaches using the full spectral dataset. © 2018 IEEE","Convolutional neural network; Deep learning; Hyperspectral; K nearest neighbor's classifier; Multiband compact texture unit","Classification (of information); Convolution; Deep learning; Geology; Image enhancement; Learning algorithms; Motion compensation; Nearest neighbor search; Neural networks; Remote sensing; Textures; Classification results; Convolutional neural network; HyperSpectral; K-nearest neighbors; Multiband; Multispectral datasets; Remote sensing data; Remotely sensed imagery; Deep neural networks"
"Can we generate good samples for hyperspectral classification? - A generative adversarial network based method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064149453&doi=10.1109%2fIGARSS.2018.8519295&partnerID=40&md5=c0bb8e59db9fd7d6ddaad1df9060bb2d","The insufficiency of training samples is really a great challenge for hyperspectral image (HSI) classification. Samples generation is a commonly used technique in deep learning based remote sensing field which can extend the training set. However, previous methods ignore the real distribution of the training samples in the feature space and thus can hardly ensure that the generated samples possess the same patterns with the real ones. In this paper, we propose a generative adversarial network based method (SpecGAN) to handle this problem. Different from traditional GAN framework where the generated samples have no categories, for the first time we take the label information into consideration for hyperspectral images. Feeding a random noise z and a class label vector y into the generator, we can get a spectral sample of the corresponding category. The experiments on the Pavia University data set demonstrate the potential of the proposed SpecGAN in spectral samples generation. © 2018 IEEE.","Deep learning; Generative adversarial network; Hyperspectral image classification; Sample generation","Deep learning; Geology; Hyperspectral imaging; Image classification; Sampling; Space optics; Spectroscopy; Adversarial networks; Feature space; Hyper-spectral classification; Label information; Real distribution; Sample generations; Training sample; Training sets; Remote sensing"
"Multi-source remote sensing data classification via fully convolutional networks and post-classification processing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063162766&doi=10.1109%2fIGARSS.2018.8518295&partnerID=40&md5=9d7be971e401e6c279901950ea40a579","This paper presents a new data fusion methodology named Fusion-FCN for the classification of multi-source remote sensing data using fully convolutional networks (FCNs). Three different types of data including LiDAR data, hyperspectral images and very high resolution images are utilized in the proposed framework. Considering the confusions between similar categories (e.g., road and highway), we further implement post-classification processing with the topological relationship among different objects based on the result yielded by the proposed Fusion-FCN. The proposed method achieved an overall accuracy of 80.78% and a kappa coefficient of 0.80, which ranked first in the 2018 IEEE GRSS Data Fusion Contest. © 2018 IEEE.","Data fusion; Deep learning; Fully convolutional network; Image segmentation",
"Object detection in remote sensing images with center only","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063142230&doi=10.1109%2fIGARSS.2018.8517860&partnerID=40&md5=e09839614667f914de50e6dc696f8b3a","There are a lot of works aiming to reduce the need of human annotations for object detection: self supervised training, interactive verification instead of annotation or weakly supervised training. For example, only pointing object centres is a faster to annotate but weaker ground truth than providing bounding boxes or detailed segmentation mask. Although not usable for large areas such as roads, vegetation and buildings, centers can be used to learn adequate detectors and segmentors. We perform a comparative analysis on four public remote sensing datasets on the task of vehicle detection and show that centre annotations is a competitive baseline compared to other more sophisticated annotations. © 2018 IEEE","Deep learning; Object centres; Semantic segmentation; Weak labeling",
"Learning deep relationship for image change detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064157559&doi=10.1109%2fIGARSS.2018.8518693&partnerID=40&md5=ac597ee1799b74aca2a87d5b473d64b7","Very high resolution image change detection is difficult due to the low interclass variability and the resulting high overlap between the changed class and the unchanged class. To address the above difficulties, the concept”relationship” is proposed to represent the intraclass similarity and interclass difference, which is established by interclass couples and intraclass couples. By relationship representation and relationship learning, intraclass couples can be compressed into a compact cluster, and the distances between interclass couples are enlarged. To better discover the complex relationship hidden in change features, relationship learning is integrated into a deep learning framework, where the relationship is learned progressively. In consequence, the final change detection performance is improved with the reduced overlap between the changed class and the unchanged class. Experiments demonstrate the effectiveness of the proposed approach. © 2018 IEEE","Deep Learning; Interclass Couple; Intraclass Couple; Relationship Learning; Relationship Representation","Geology; Remote sensing; Complex relationships; Image change detection; Inter class; Intra class; Learning frameworks; Relationship Learning; Relationship Representation; Very high resolution (VHR) image; Deep learning"
"Exploring convolutional LSTM for PolSAR image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064178587&doi=10.1109%2fIGARSS.2018.8518517&partnerID=40&md5=55f3ee1bbd9ad7c66d2062b86cc850be","Polarimetric synthetic aperture radar (PolSAR) image classification is one of the most important applications in PolSAR image processing. More and more deep learning methods are applied to PolSAR image classification. As we know, the polarimetric response of a target is related to the orientation of the target, but the features in rotation domain are not fully used in deep learning. We use a convolutional LSTM (ConvLSTM) along with a sequence of polarization coherent matrices in rotation domain for PolSAR image classification. First, nine different polarization orientation angles (POA) are used to generate nine polarization coherent matrices in rotation domain. Second, a deep learning model that stacked with multiple ConvLSTM layers and fully connected layers is proposed for classification. Finally, the sequence of polarization coherent matrices is fed into the ConvLSTM to classify PolSAR images. Experiments show that the classification results of ConvLSTM are better than the LeNet-5. © 2018 IEEE.","Classification; Convolutional LSTM; PolSAR; Rotation domain","Classification (of information); Convolution; Deep learning; Geology; Long short-term memory; Polarimeters; Polarization; Remote sensing; Rotation; Synthetic aperture radar; Classification results; Convolutional LSTM; Learning methods; Learning models; Polarimetric synthetic aperture radars; Polarization orientation angles; PolSAR; Rotation domain; Image classification"
"Convolutional highway unit network for large-scale classification with GF-3 dual-pol SAR data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064147767&doi=10.1109%2fIGARSS.2018.8518737&partnerID=40&md5=9cee7d26642cf086376ab5e24caa6461","Synthetic Aperture Radar (SAR) is able to image the Earth's surface in all weather conditions, regardless of whether it is day or night. It can provide high quality data for large-scale mapping. In recent years, the deep learning method, which has robust performance, can automatically extract hierarchic features of images with reduced manual participation, and is beneficial to the rapid implementation of large-scale mapping. Based on the theory of deep learning, we propose an end-to-end framework for the dense, pixel-wise classification of GF-3 dual-pol SAR imagery with convolutional highway unit network (U-net). U-net consists of a stack of learned convolutional highway unit that extract hierarchical contextual image features, and is a popular form of deep learning networks. A series of experiments show that the networks consider a large amount of context to provide fine-grained classification maps. The overall classification accuracy is 85.0%, Kappa is 0.803. The results are good enough to meet the needs of the mapping application. © 2018 IEEE.","Convolutional highway unit network; Deep learning; GF-3; Large-scale classification","Convolution; Deep learning; Geology; Image processing; Mapping; Remote sensing; Synthetic aperture radar; Classification accuracy; Classification maps; GF-3; High quality data; Large scale classifications; Learning network; Mapping applications; Robust performance; Radar imaging"
"Predicting landscapes as seen from space from environmental conditions","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061616822&doi=10.1109%2fIGARSS.2018.8519427&partnerID=40&md5=451e8f72ed938b70ac10d3cf76deb8ee","Satellite images are information rich snapshots of ecosystems and landscapes. In consequence, the features in the images strongly depend on the environmental conditions. Such dependency between climate and landscapes has been regarded since the beginning of earth sciences; however, it has never been taken as literally as in the present study. We adapted a deep learning generative model as a first demonstration of the potential behind deep learning for spatial pattern generation in geoscience. The purpose is to build a conditional Generative Adversarial Network (cGAN) useful to establish the relationship between two loosely linked set of variables that show multitude of complex spatial features such as climate conditions to aerial image. We trained a custom cGAN to generate Sentinel-2 multispectral imagery given a set of climatic and terrain predictors. Results show that the generated imagery shares many characteristics with the real one. In some cases, the quality of the generated imagery is high enough to deceive humans. We envision that such use of deep learning for geoscience could become an important tool to test the effects of climate on landscapes and ecosystems. © 2018 IEEE.","Climate; Deep learning; GAN; Landscape ecology; Satellite imagery; Sentinel 2","Antennas; Earth (planet); Ecosystems; Geology; Remote sensing; Satellite imagery; Space optics; Adversarial networks; Climate; Climate condition; Environmental conditions; Generative model; Landscape ecology; Multi-spectral imagery; Sentinel 2; Deep learning"
"The influence of SAR image quantization method on detection precision","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064173156&doi=10.1109%2fIGARSS.2018.8518783&partnerID=40&md5=2cdd08790bf16622c46820d79ab1e4a5","In this paper, we combine deep learning with radar image processing to explore the influence of different quantization methods on the final detection performance of the radar image subjected to strong points after different quantification methods. Considering problems caused by the characteristics of SAR image data, the LeNet network model in deep learning was used to train and verify the quantified radar images respectively. The impact of different quantization methods on SAR image classification and detection was analyzed. The most friendly way to quantify the actual radar images was explored. Radar image target detection based on depth learning provides the basis for exploration. © 2018 IEEE.","Deep learning; Detection; Quantitative methods; SAR Image","Deep learning; Error detection; Geology; Image compression; Remote sensing; Synthetic aperture radar; Tracking radar; Detection performance; Detection precision; Network modeling; Quantification methods; Quantitative method; SAR image classifications; SAR Images; Radar imaging"
"Object detection in satellite imagery using 2-step convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064189947&doi=10.1109%2fIGARSS.2018.8518587&partnerID=40&md5=fa9d6d0140b1be88662432a64826495f","This paper presents an efficient object detection method from satellite imagery. Among a number of machine learning al-gorithms, we proposed a combination of two convolutional neural networks (CNN) aimed at high precision and high re-call, respectively. We validated our models using golf courses as target objects. The proposed deep learning method demon-strated higher accuracy than previous object identification methods. © 2018 IEEE.","Convo-lutional neural networks; Golf course; Negative mining; Object detection; Remote sensing","Convolution; Deep learning; Geology; Neural networks; Object recognition; Recreational facilities; Remote sensing; Satellite imagery; Sports; Convolutional neural network; Efficient object detections; Golf course; High-precision; Learning methods; Negative minings; Object identification; Target object; Object detection"
"Classification of rare building change using CNN with multi-class focal loss","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063125186&doi=10.1109%2fIGARSS.2018.8517563&partnerID=40&md5=88247bf4247c6faec7b29dc56198366a","In the remote sensing, supervised deep learning has recently achieved great success of information extraction. However, it requires a large training data in order to effectively learn. In building change classifications, collecting such training data is an extremely expensive and time-consuming process, because of the rarity of positive classes. Learning of a data set including rare classes has two major problems, (1) class imbalance and (2) overfitting. In this study, we verify the effectiveness of focal loss in the building change classification. From our experimental results, not only the class imbalance but also the overfitting is affected the down-weighting effect of the focal loss. The focal loss automatically adjusts learning speed for each class. © 2018 IEEE","Aerial image; Building change; Convolutional neural network; Deep learning; Focal loss",
"The effect of focal loss in semantic segmentation of high resolution aerial image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064255998&doi=10.1109%2fIGARSS.2018.8519409&partnerID=40&md5=7b73335c3999643e88646aecc6465423","The semantic segmentation of High Resolution Remote Sensing (HRRS) images is the fundamental research area of the earth observation. Convolutional Neural Network (CNN), which has achieved superior performance in computer vision task, is also useful for semantic segmentation of HRRS images. In this work, focal loss is used instead of cross-entropy loss in training of CNN to handle the imbalance in training data. To evaluate the effect of focal loss, we train SegNet and FCN with focal loss and confirm improvement in accuracy in ISPRS 2D Semantic Labeling Contest dataset, especially when γ is 0.5 in SegNet. © 2018 IEEE.","CNN; Deep learning; Focal loss; Semantic segmentation","Antennas; Deep learning; Geology; Neural networks; Remote sensing; Semantics; Convolutional neural network; Cross entropy; Earth observations; Fundamental research; High resolution remote sensing; High-resolution aerial images; Semantic labeling; Semantic segmentation; Image segmentation"
"Ship detection without sea-land segmentation for large-scale highresolution optical satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064270646&doi=10.1109%2fIGARSS.2018.8519391&partnerID=40&md5=f4de477c1f9ae22292ce3c038ada8b64","Ship detection is an important and challenging topic in remote sensing applications. In current literatures, sea-land segmentation is generally requested before ship detection. This makes the implementation of the methods highly complicated. Therefore, based on Faster R-CNN, this paper proposes a ship detection method for large-scale images, which does not need sea-land segmentation as preprocessing step and can detect ships directly from complicated background including sea and land. We use large-scale images consisting of GF-1 and GF-2 satellite images to test our network. Experimental results prove that the proposed method plays a role in removing the interference of objects on land. © 2018 IEEE.","Deep learning; High-resolution satellite images; Sea-land segmentation; Ship detection","Deep learning; Geology; Remote sensing; Satellites; Ships; High resolution satellite images; High-resolution optical satellite images; Pre-processing step; Remote sensing applications; Satellite images; Sea-land segmentations; Ship detection; Image segmentation"
"Large-scale land cover classification in Gaofen-2 satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063158992&doi=10.1109%2fIGARSS.2018.8518389&partnerID=40&md5=aa305f4972d88f3021c61d5bde486a19","Many significant applications need land cover information of remote sensing images that are acquired from different areas and times, such as change detection and disaster monitoring. However, it is difficult to find a generic land cover classification scheme for different remote sensing images due to the spectral shift caused by diverse acquisition condition. In this paper, we develop a novel land cover classification method that can deal with large-scale data captured from widely distributed areas and different times. Additionally, we establish a large-scale land cover classification dataset consisting of 150 Gaofen-2 imageries as data support for model training and performance evaluation. Our experiments achieve outstanding classification accuracy compared with traditional methods. © 2018 IEEE.","Deep learning; Gaofen-2; High-resolution remote sensing image; Land cover classification",
"Building detection and segmentation using a CNN with automatically generated training data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064182092&doi=10.1109%2fIGARSS.2018.8518521&partnerID=40&md5=2c9d5f0fd969632ed18d92fe4d4e4298","Significantly outperforming traditional machine learning methods, deep convolutional neural networks have gained increasing popularity in the application of image classification and segmentation. Nevertheless, deep learning-based methods usually require a large amount of training data, which is quite labor-intensive and time-demanding. To deal with the problem in generating training data, we propose in this paper a novel approach to generate image annotations by transferring labels from aerial images to UAV images and refine the annotations using a densely connected CRF model with an embedded naive Bayes classifier. The generated annotations not only present correct semantic labels, but also preserve accurate class boundaries. To validate the utility of these automatic annotations, we deploy them as training data for pixel-wise image segmentation and compare the results with the segmentation using manual annotations. Experiment results demonstrate that the automatic annotations can achieve comparable segmentation accuracy as the manual annotations. © 2018 IEEE.","Automatic image annotation; Deep learning; Image segmentation; Label propagation","Antennas; Classification (of information); Classifiers; Deep learning; Deep neural networks; Geology; Image analysis; Image annotation; Neural networks; Remote sensing; Semantics; Automatic image annotation; Automatically generated; Convolutional neural network; Label propagation; Learning-based methods; Machine learning methods; Naive Bayes classifiers; Segmentation accuracy; Image segmentation"
"Semi-supervised classification of hyperspectral data for geologic body based on generative adversarial networks at tianshan area","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064161136&doi=10.1109%2fIGARSS.2018.8518946&partnerID=40&md5=e745a827c1e2f9c909ee474a63c7923b","Hyperspectral remote sensing data contains near continuous spectral information of the object, which is very suitable for mineral classification and geologic body mapping. However, the collecting of a lot of labeled hyperspectral data is expensive, time-consuming and labor-intensive. We choose a semi-supervised method to classify hyperspectral data based on a generative adversarial nertwork (GAN), just use a small amount of labeled data, named HSGAN. The GAN is made up of a generator and a discriminator, and the generator generates data similar to the real data so that the discriminator cannot tell if it is real data or generated data. We designed a one-dimensional GAN to extract spectral features from hyperspectral data. Using this method, we test the Tianshan hyperspectral data and use the actual geological map as the ground-truth produced by us. We find that HSGAN still achieves better results than the traditional CNN and SVM. © 2018 IEEE.","Deep learning; Generative adversarial networks (GAN); Geological mapping; Hymap data; Semi-supervised learning (SSL)","Deep learning; Geology; Machine learning; Photomapping; Remote sensing; Supervised learning; Adversarial networks; Geological mapping; Hymap data; Hyperspectral remote sensing data; Semi-supervised classification; Semi-supervised learning (SSL); Semi-supervised method; Spectral information; Classification (of information)"
"Learning speckle suppression in SAR images without ground truth: Application to sentinel-1 time-series","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062524942&doi=10.1109%2fIGARSS.2018.8519370&partnerID=40&md5=e2a32ded81bb82516031040a1c0cc2bf","This paper proposes a method of denoising SAR images, using a deep learning method, which takes advantage of the abundance of data to learn on large stacks of images of the same scene. The approach is based on the use of convolutional networks, used as auto-encoders. Learning is led on a large pile of images acquired on the same area, and assumes that the images of this stack differ only by the speckle noise. Several pairs of images are chosen randomly in the stack, and the network tries to predict the slave image from the master image. In this prediction, the network can not predict the noise because of its random nature. Also the application of this network to a new image fulfills the speckle filtering function. Results are given on Sentinel 1 images. They show that this approach is qualitatively competitive with literature. © 2018 IEEE.","Deep learning; SAR; Speckle filtering","Deep learning; Forecasting; Geology; Piles; Remote sensing; Speckle; Synthetic aperture radar; Auto encoders; Convolutional networks; De-noising; Ground truth; Learning methods; Speckle filtering; Speckle noise; Speckle suppression; Radar imaging"
"Cloud-GAN: Cloud removal for sentinel-2 imagery using a cyclic consistent generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064168458&doi=10.1109%2fIGARSS.2018.8519033&partnerID=40&md5=bd33a91f221d44b923417ea9badca61e","Cloud cover is a serious impediment in land surface analysis from Remote Sensing images either causing complete obstruction (thick clouds) with loss of information or blurry effects when being semi-transparent (thin clouds). While thick clouds require complete pixel replacement, thin cloud removal is fairly challenging as the atmospheric and land-cover information is inter-twined. In this paper, we address this problem and propose a Cloud-GAN to learn the mapping between cloudy images and cloud-free images. The adversarial loss in the proposed method constrains the distribution of generated images to be close enough to the underlying distribution of the non-cloudy images. An additional cycle consistency loss is used to further restrain the generator to predict cloud-free images only of the same scene as reflected in the cloudy images. Our method not only rejects the necessity of any paired (cloud/cloud-free) training dataset but also avoids the need of any additional (expensive) spectral source of information such as Synthetic Aperture Radar imagery which is cloud penetrable. Lastly, we demonstrate the efficacy of our technique by training on an openly available and fairly new Sentinel-2 Imagery dataset consisting of real clouds. We also show significant improvement in PSNR values after removing clouds on synthetic images thus validating the competency of our methodology. © 2018 IEEE.","Cloud removal; Deep learning; Generative adversarial networks; Sentinel-2 imagery","Deep learning; Geology; Radar imaging; Remote sensing; Surface analysis; Synthetic aperture radar; Tracking radar; Adversarial networks; Cloud removal; Land cover informations; Land surface analysis; Remote sensing images; Sentinel-2 imagery; Synthetic Aperture Radar Imagery; Underlying distribution; Image enhancement"
"Ocean eddy identification and tracking using neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064172923&doi=10.1109%2fIGARSS.2018.8519261&partnerID=40&md5=22c3dcff4d44a54d406052bdc1409f45","Global climate change plays an essential role in our daily life. Mesoscale ocean eddies have a significant impact on global warming, since they affect the ocean dynamics, the energy as well as the mass transports of ocean circulation. From satellite altimetry we can derive high-resolution, global maps containing ocean signals with dominating coherent eddy structures. The aim of this study is the development and evaluation of a deep-learning based approach for the analysis of eddies. In detail, we develop an eddy identification and tracking framework with two different approaches that are mainly based on feature learning with convolutional neural networks. Furthermore, state-of-the-art image processing tools and object tracking methods are used to support the eddy tracking. In contrast to previous methods, our framework is able to learn a representation of the data in which eddies can be detected and tracked in more objective and robust way. We show the detection and tracking results on sea level anomalies (SLA) data from the area of Australia and the East Australia current, and compare our two eddy detection and tracking approaches to identify the most robust and objective method. © 2018 IEEE.","Convolutional neural networks; Mesoscale eddies; Optical flow; Semantic segmentation","Convolution; Deep learning; Geology; Global warming; Image processing; Neural networks; Optical data processing; Optical flows; Sea level; Semantics; Convolutional neural network; Detection and tracking; Global climate changes; Learning-based approach; Mesoscale eddy; Satellite altimetry; Sea level anomaly; Semantic segmentation; Remote sensing"
"High resolution SAR image classification with deeper convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064165617&doi=10.1109%2fIGARSS.2018.8518829&partnerID=40&md5=3d709237e2eddb62b83d512ffc8624a3","Deeper architectures are proven to be beneficial for the classification performance obviously in computer vision field. Inspired by this, deep CNNs are expected to make progress in the SAR target classification problem as well. However, it is hard to train deeper CNNs for SAR images. Such CNNs have millions of parameters to be determined in the network (for example the VGGNet has more than 130 million parameters), hence large-scale dataset is indispensable when training a deep CNN. But there is no large-scale annotated SAR target dataset, and data acquisition and annotation is much more costly for SAR images. With inadequate data, the network is easy to be overfitting. Several methods based on deep learning have been proposed for SAR image classifications, but they cannot get rid of the aforementioned data limitation of labelled SAR images. To solve this problem, this paper proposes a microarchitecture called CompressUnit (CU). With CU, we design a deeper CNN. Compared with the network with the fewest parameters for SAR image classification in literature so far, our network is 2X deeper with only about 10% of parameters. In this way, we get a deeper network with much fewer parameters. This network is easier to be trained with limited SAR data and is more likely to get rid of overfitting. © 2018 IEEE.","CompressUnit; Deeper CNN; SAR images","Computer architecture; Data acquisition; Deep learning; Geology; Image classification; Large dataset; Neural networks; Radar target recognition; Remote sensing; Synthetic aperture radar; Classification performance; CompressUnit; Convolutional neural network; Deeper CNN; Micro architectures; SAR image classifications; SAR Images; Target Classification; Radar imaging"
"Large-scale semantic classification: Outcome of the first year of inria aerial image labeling benchmark","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060884859&doi=10.1109%2fIGARSS.2018.8518525&partnerID=40&md5=68148fe56aeba9330b3fed8b902bf602","Over the recent years, there has been an increasing interest in large-scale classification of remote sensing images. In this context, the Inria Aerial Image Labeling Benchmark has been released online in December 2016. In this paper, we discuss the outcomes of the first year of the benchmark contest, which consisted in dense labeling of aerial images into building/not building classes, covering areas of five cities not present in the training set. We present four methods with the highest numerical accuracies, all four being convolutional neural network approaches. It is remarkable that three of these methods use the U-net architecture, which has thus proven to become a new standard in image dense labeling. © 2018 IEEE.","Aerial images; Classification benchmark; Convolutional neural networks; Deep learning; U-net","Antennas; Convolution; Deep learning; Geology; Neural networks; Numerical methods; Remote sensing; Semantics; Aerial images; Convolutional neural network; First year; Large scale classifications; NET architecture; Numerical accuracy; Semantic classification; Training sets; Image classification"
"Towards automated vessel detection and type recognition from VHR optical satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064280469&doi=10.1109%2fIGARSS.2018.8519121&partnerID=40&md5=7f54a121d1f235ef772405efa57dcc75","Vessel detection and type recognition is crucial in any maritime surveillance application. This component aims at preventing or investigating unlawful actions present at sea. Modern very high resolution (VHR) optical satellite sensors are able to capture images with spatial resolution up to 0.3m per pixel, which is sufficient to distinguish ship features such as bridge position, cranes, landing pads and many others and thus possible to differentiate ship types. This paper presents a new method for automatic vessel detection and type recognition based on fusion of deep convolutional neural network architectures (CNN), which has potential for near-real time (NRT) applications. © 2018 IEEE.","CNN; Convolutional neural networks; Deep learning; Object classification; Object detection; Optical remote sensing; Vessel detection; Vessel type recognition","Convolution; Deep learning; Deep neural networks; Geology; Network architecture; Neural networks; Object detection; Ships; Convolutional neural network; Object classification; Optical remote sensing; Vessel detection; Vessel type recognition; Remote sensing"
"A conditional generative adversarial network to fuse SAR and multispectral optical data for cloud removal from Sentinel-2 images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056354296&doi=10.1109%2fIGARSS.2018.8519215&partnerID=40&md5=86a96bdfdcfcff984120e3d9c682d5d1","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and hazefree MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-1 SAR data confirm that our extended SAR-OptcGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input. © 2018 IEEE.","Cloudremoval; Data fusion; Deep learning; Generative adversarial network (GAN); Optical remote sensing; SAR","Data fusion; Deep learning; Geology; Network architecture; Remote sensing; Synthetic aperture radar; Adversarial networks; Cloudremoval; Equivalent model; Multi-spectral; Optical data; Optical remote sensing; Sentinel-1; Single sensor; Radar imaging"
"Deep hybrid wavelet network for ice boundary detection in radra imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064155196&doi=10.1109%2fIGARSS.2018.8518617&partnerID=40&md5=21f23cc09d1aae2e0e803a02f1a159e9","This paper proposes a deep convolutional neural network approach to detect Ice surface and bottom layers from radar imagery. Radar images are capable to penetrate the Ice surface and provide us with valuable information from the underlying layers of ice surface. In recent years, deep hierarchical learning techniques for object detection and segmentation greatly improved the performance of traditional techniques based on hand-crafted feature engineering. We designed a deep convolutional network to produce the images of surface and bottom ice boundary. Our network take advantage of undecimated wavelet transform to provide the higest level of information from radar images, as well as multilayer and multi-scale optimized architecture. In this work, radar images from 2009-2016 NASA Operation IceBridge Mission are used to train and test the network. Our network outperformed the state-of-the art accuracy. © 2018 IEEE.","Deep learning; Holistically nested edge detection; Ice Boundary detection; Radar; Wavelet transform","Convolution; Deep learning; Deep neural networks; Edge detection; Geology; Ice; NASA; Neural networks; Object detection; Radar; Remote sensing; Tracking radar; Wavelet transforms; Boundary detection; Convolutional networks; Convolutional neural network; Feature engineerings; Hierarchical learning; Optimized architectures; Traditional techniques; Undecimated wavelet transform; Radar imaging"
"An investigation on self-normalized deep neural networks for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063164249&doi=10.1109%2fIGARSS.2018.8517449&partnerID=40&md5=89c12d39a028b21caf9d393ecfa913a9","Computational advances have allowed for the development of deep learning (DL) applied to remote sensing data and, particularly, to hyperspectral image (HSI) classification. Deeper architectures are able to establish a better separation of the characteristics of the data, allowing for a better and accurate performance. However, it is known that employing very deep architectures with many abstraction levels can result in a loss of information due to the fact that deep networks often normalize each data individually, without considering the set of adjacent data. To address this issue, this paper implements a self-normalizing neural network (SNN) in order to extract high-level abstract representations without losing information due to the data initialization. The selected activation function (scaled exponential linear units or SELU) normalizes the data considering their neighborhood's information and a special dropout technique (a-dropout), obtaining good classification performance while maintaining the data characteristics across the successive layers. Obtained results show that the proposal improves the performance with few training samples. © 2018 IEEE",,
"A two-branch network with semi-supervised learning for hyperspectral classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063129658&doi=10.1109%2fIGARSS.2018.8517816&partnerID=40&md5=ba47ad7c5ad2dcdea03eec713b06b979","In order to promote progress on fusion and analysis methodologies for multi-source remote sensing data, The Image Analysis and Data Fusion Technical Committee organized the 2018 IEEE GRSS Data Fusion contest. In this contest, we proposed a two-branch convolution network for hyperspectral image classification with a data re-sampling strategy and semi-supervised learning to address three existing problems, i.e. multi-scale feature learning, data imbalance, and small size of the dataset. The contest showed that our proposal achieved the best performance on two metrics: the overall accuracy of 77.39% and a kappa coefficient of 0.76 on the hyperspectral images provided by 2018 IEEE GRSS Data Fusion Contest. © 2018 IEEE","Deep learning; Hyperspectral image; Image classification; Semi-supervised learning",
"GaN-based domain adaptation for object classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064183505&doi=10.1109%2fIGARSS.2018.8518649&partnerID=40&md5=75cb981dde6b10d5f044e416bdb451f3","Recent trends in image classification focus on training deep neural networks that require having a large amount of training images related to the considered task. However, obtaining enough labeled image samples is often time-consuming and expensive. An alternative solution proposed is to transfer the knowledge learned while solving one problem to another but related problem, also called transfer learning. Domain adaptation is a type of transfer learning that deals with learning a model that performs well on two datasets that have different (but somehow correlated) data distributions. In this work, we present a new domain adaptation method based on generative adversarial networks (GANs) in the context of aerial image classification. Experimental results obtained on two datasets for a single object scenario show that the proposed method is particularly promising. © 2018 IEEE","Deep learning; Domain adaptation; GAN; Transfer learning","Antennas; Deep learning; Deep neural networks; Gallium nitride; Geology; III-V semiconductors; Remote sensing; Adversarial networks; Alternative solutions; Data distribution; Domain adaptation; Labeled images; Object classification; Training image; Transfer learning; Image classification"
"FuseNet: End-to-end multispectral VHR image fusion and classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064275120&doi=10.1109%2fIGARSS.2018.8519214&partnerID=40&md5=097fbbb92e1838340643ce3d3e60c81d","Classification of very high resolution (VHR) satellite images faces two major challenges: 1) inherent low intra-class and high inter-class spectral similarities and 2) mismatching resolution of available bands. Conventional methods have addressed these challenges by adopting separate stages of image fusion and spatial feature extraction steps. These steps, however, are not jointly optimizing the classification task at hand. We propose a single-stage framework embedding these processing stages in a multiresolution convolutional network. The network, called FuseNet, aims to match the resolution of the panchromatic and multispectral bands in a VHR image using convolutional layers with corresponding downsampling and upsampling operations. We compared FuseNet against the use of separate processing steps for image fusion, such as pansharpening and resampling through interpolation. We also analyzed the sensitivity of the classification performance of FuseNet to a selected number of its hyperparameters. Results show that FuseNet surpasses conventional methods. © 2018 IEEE.","Convolutional networks; Deep learning; Image fusion; Land cover classification; VHR image","Convolution; Deep learning; Geology; Image fusion; Remote sensing; Signal sampling; Classification performance; Classification tasks; Conventional methods; Convolutional networks; Land cover classification; Spectral similarity; Very high resolution; VHR images; Image classification"
"Deep auto-encoder network for hyperspectral image unmixing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064212975&doi=10.1109%2fIGARSS.2018.8519571&partnerID=40&md5=7f9737334a66e1e5b4f2c4d9795b4012","In this paper, we propose a deep auto-encoder network for the unmixing for hyperspectral data with outliers and low signal to noise ratio. The proposed deep auto-encoder net-work composes of two parts. The first part of the network adopts stacked non-negative sparse auto-encoder to learn the spectral signatures such that to generate a good initialization for the network. In the second part of the network, a varia-tional auto-encoder is employed to perform unmixing, aiming at the endmember signatures and abundance fractions. The effectiveness of the proposed method is verified by using a synthetic data set. In our comparison with other state-of-the- A rt unmixing methods, the proposed approach demonstrates highly competitive performance. © 2018 IEEE.","Deep learning; Hyperspectral unmixing; Non-negative s-parse auto-encoder; Variational auto-encoder","Deep learning; Geology; Remote sensing; Signal to noise ratio; Spectroscopy; Auto encoders; Competitive performance; Hyperspectral Data; Hyperspectral unmixing; Low signal-to-noise ratio; Non negatives; Spectral signature; Synthetic datasets; Network coding"
"Inshore ship detection in sar images based on deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064208998&doi=10.1109%2fIGARSS.2018.8519555&partnerID=40&md5=8d550cadd5c83fdcf3918db233d73aa4","Inshore ship detection in SAR image faces difficulties on correctly identifying near-shore ships and onshore objects. This article proposes a multi-scale full convolutional network (MS-FCN) based sea-land segmentation method and applies a rotatable bounding box based object detection method (DRBox) to solve the inshore ship detection problem. The sea region and land region are separated by MS-FCN then DRBox is applied on sea region. The proposed method combines global information and local information of SAR image to achieve high accuracy. The networks are trained with Chinese Gaofen-3 satellite images. Experiments on the testing image show most inshore ships are successfully located by the proposed method. © 2018 IEEE.","Deep learning; Full convolutional networks; Inshore ship detection; Object detection networks; Synthetic aperture radar","Convolution; Deep learning; Deep neural networks; Geology; Object detection; Object recognition; Remote sensing; Ship testing; Ships; Synthetic aperture radar; Convolutional networks; Detection networks; Global informations; Local information; Object detection method; Satellite images; Sea-land segmentations; Ship detection; Radar imaging"
"CNN-based target detection in hyperspectral imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064200301&doi=10.1109%2fIGARSS.2018.8519104&partnerID=40&md5=f3a452e129c71d154aa44116fb34773c","This paper proposes a hyperspectral target detection framework with convolutional neural network (CNN). The number of training samples is first sufficiently enlarged by subtraction method to maximize the advantages of the multilayer CNN. Next, the CNN is given a target detection function by labelling the new pixels subtracted between target and background classes as 1, and the pixels subtracted between pixels within both the same and different background classes as 0. Finally, for each testing pixel, the difference between the central pixel and its adjacent pixels is input into the framework. If the testing pixel belongs to the target, the output score is close to the target label. Aircrafts and vehicles are selected as targets of interest in the experiment conducted to validate the proposed method. The experiment results show that the proposed method has an advantage over classic hyperspectral target detection algorithms in terms of precision and robustness. © 2018 IEEE.","Convolutional neural network; Deep learning; Hyperspectral; Remote sensing; Target detection","Convolution; Deep learning; Geology; Neural networks; Pixels; Remote sensing; Signal detection; Spectroscopy; Target tracking; Convolutional neural network; Detection functions; Hyper-spectral imageries; HyperSpectral; Hyperspectral target detection; Subtraction method; Target and background; Targets of interest; Radar target recognition"
"Deep semantic segmentation of aerial imagery based on multi-modal data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064179575&doi=10.1109%2fIGARSS.2018.8519225&partnerID=40&md5=db5215dad8db3fc4337e642d133ffa10","In this paper, we focus on the use of multi-modal data to achieve a semantic segmentation of aerial imagery. Thereby, the multi-modal data is composed of a true orthophoto, the Digital Surface Model (DSM) and further representations derived from these. Taking data of different modalities separately and in combination as input to a Residual Shuffling Convolutional Neural Network (RSCNN), we analyze their value for the classification task given with a benchmark dataset. The derived results reveal an improvement if different types of geometric features extracted from the DSM are used in addition to the true orthophoto. © 2018 IEEE.","Aerial imagery; Deep learning; Multi-modal data; Semantic segmentation; Shuffling-CNN","Aerial photography; Antennas; Classification (of information); Deep learning; Geology; Modal analysis; Neural networks; Semantics; Aerial imagery; Benchmark datasets; Classification tasks; Convolutional neural network; Digital surface models; Multi-modal data; Semantic segmentation; Shuffling-CNN; Remote sensing"
"Fully convolutional networks for multi-temporal SAR image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064172068&doi=10.1109%2fIGARSS.2018.8518780&partnerID=40&md5=b6593b8e90f7dd7fed70c7daa9afb3ef","Classification of crop types from multi-temporal SAR data is a complex task because of the need to extract spatial and temporal features from images affected by speckle. Previous methods applied speckle filtering and then classification in two separate processing steps. This paper introduces fully convolutional networks (FCN) for pixel-wise classification of crops from multi-temporal SAR data. It applies speckle filtering and classification in a single framework. Furthermore, it also uses dilated kernels to increase the capability to learn long distance spatial dependencies. The proposed FCN was compared with patch-based convolutional neural network (CNN) and support vector machine (SVM) classifiers. The proposed method performed better when compared with the patch-based CNN and SVM. © 2018 IEEE","Deep learning; Fully convolutional networks; Remote Sensing; SAR; Sentinel-1","Convolution; Crops; Deep learning; Geology; Image classification; Neural networks; Remote sensing; Speckle; Support vector machines; Synthetic aperture radar; Convolutional networks; Convolutional neural network; Multi-temporal SAR; Multi-temporal SAR images; Sentinel-1; Spatial dependencies; Speckle filtering; Temporal features; Radar imaging"
"Semi-supervised classification of hyperspectral data based on generative adversarial networks and neighborhood majority voting","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064156520&doi=10.1109%2fIGARSS.2018.8518846&partnerID=40&md5=6c1fc214faf8ae409ab95b27173bfa78","How to classify hyperspectral images using few training samples is an important and challenging problem because the collection of the samples is difficult and expensive. Because semi-supervised approaches can utilize information contained in the unlabeled samples and labeled samples, it is a suitable choice. A novel semi-supervised spectral-spatial classification method for hyperspectral data based on generative adversarial network (GAN) is proposed in this paper. First, we use a custom one-dimensional GAN to train the hyperspectral data to obtain spectral features. After using a new small convolutional neural network (CNN) to classify the spectral features, we use a new classification method based on a majority voting strategy further to improve the classification result. The performance of our method is evaluated on ROSIS image data, and the results show that the proposed method can acquire satisfactory results when compared with traditional methods using a few of labeled samples. © 2018 IEEE.","Deep learning; Generative adversarial networks (GAN); Hyperspectral images classification; Semi-supervised learning (SSL); Spectral-spatial classification","Deep learning; Geology; Image classification; Machine learning; Neural networks; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Classification methods; Classification results; Convolutional neural network; HyperSpectral; Semi-supervised classification; Semi-supervised learning (SSL); Spectral-spatial classification; Classification (of information)"
"Image translation between SAR and optical imagery with generative adversarial nets","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064152008&doi=10.1109%2fIGARSS.2018.8518719&partnerID=40&md5=fa80fae182f802ab5ddf804463de552f","In this paper, we propose a method for the translation from Synthetic Aperture Radar (SAR) to optical images using conditional Generative Adversarial Networks (cGANs). Satellite images have been widely utilized for various purposes, such as natural environment monitoring (pollution, forest or rivers), transportation improvement and prompt emergency response to disasters. However, the obscurity caused by clouds leads to unstable monitoring of the ground situation while using the optical camera. Images captured by a longer wavelength are introduced to reduce the effects of clouds. In particular, SAR images are known to be nearly unaffected by clouds and are often used for stably observing the ground situation. On the other hand, SAR images have lower spatial resolution and visibility than optical images. Therefore, we propose a deep neural network that generates optical images from SAR images. Finally, we confirm the feasibility of the proposed network on a dataset consisting of optical images and the corresponding SAR images. © 2018 IEEE.","Deep learning; GANs; SAR; Satellite imagery","Deep learning; Deep neural networks; Geology; Geometrical optics; Image enhancement; Pollution control; Remote sensing; River pollution; Satellite imagery; Space-based radar; Synthetic aperture radar; Adversarial networks; Emergency response; GANs; Image translation; Natural environments; Optical imagery; Satellite images; Spatial resolution; Radar imaging"
"The influence of sampling methods on pixel-wise hyperspectral image classification with 3D convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064149338&doi=10.1109%2fIGARSS.2018.8518671&partnerID=40&md5=86acfb0849518f9b209f9b2f64f19f70","Supervised image classification is one of the essential techniques for generating semantic maps from remotely sensed images. The lack of labeled ground truth datasets, due to the inherent time effort and cost involved in collecting training samples, has led to the practice of training and validating new classifiers within a single image. In line with that, the dominant approach for the division of the available ground truth into disjoint training and test sets is random sampling. This paper discusses the problems that arise when this strategy is adopted in conjunction with spectral-spatial and pixel-wise classifiers such as 3D Convolutional Neural Networks (3D CNN). It is shown that a random sampling scheme leads to a violation of the independence assumption and to the illusion that global knowledge is extracted from the training set. To tackle this issue, two improved sampling strategies based on the Density-Based Clustering Algorithm (DBSCAN) are proposed. They minimize the violation of the train and test samples independence assumption and thus ensure an honest estimation of the generalization capabilities of the classifier. © 2018 IEEE","Clustering; Convolutional Neural Networks (CNNs); DBSCAN; Deep learning; Hyperspectral image classification; Sampling strategies","Classification (of information); Clustering algorithms; Convolution; Deep learning; Geology; Hyperspectral imaging; Neural networks; Pixels; Remote sensing; Semantics; Spectroscopy; Clustering; Convolutional neural network; DBSCAN; Density-based clustering algorithms; Generalization capability; Independence assumption; Sampling strategies; Supervised image classifications; Image classification"
"Multilevel semantic labeling of mobile homes from overhead imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063146845&doi=10.1109%2fIGARSS.2018.8517660&partnerID=40&md5=633e9bd3a4135ed780b976bdecb6b873","Finding where people live and the vulnerabilities of man-made facilities during natural disasters is not only critical for rescue efforts but also essential for damage assessment in the aftermath. New advances from machine learning and high performance computing are leveraging on the availability of high resolution satellite imagery to generate geographical maps for man-made facilities at scale. Mapping from satellite imagery can be a daunting task due to the enormous amount of data to be processed over large areas. In this short paper we take advantage of annotated satellite imagery and automate the semantic labeling of mobile home parks using an efficient framework rooted in patch-based and pixel-level classification. This multilevel labeling effort is a precursor for deploying very large scale deep convolutional neural networks toward broad and finer characterization of man-made structures from one-meter resolution NAIP images. © 2018 IEEE","Aerial imagery; Convolutional neural networks; Deep learning; Mobile home park; Multilevel; Remote sensing; Semantic segmentation",
"Comparative study of feature extraction approaches for ship classification in moderate-resolution sar imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059003786&doi=10.1109%2fIGARSS.2018.8518966&partnerID=40&md5=119fe1059dd9b089e221ba3b99a8fd5a","This paper presents a comparative study of existing feature extraction approaches for ship classification in moderateresolution synthetic aperture radar (SAR) images. Ship classification is a key functionality in many maritime surveillance applications. For efficient ship classification, appropriate feature extraction is crucial. Most of existing studies have used high-resolution images. For maritime surveillance, however, wide-area coverage is essential whereas it inevitably reduces the spatial resolution. In this paper, we evaluate the applicability of representative methods to moderate-resolution images. The evaluated methods are hand-crafted feature extraction (HCF), principal component analysis (PCA) and autoencoder (AE) based on neuralnetwork. The evaluation is done on the basis of accuracy for two-class ship classification into tanker and cargo. The experiments demonstrate that AE outperforms HCF and PCA in classification accuracy by 7.4% and 2.6%, respectively. Furthermore, AE performs best even in classification of challenging cases such as small ships. © 2018 IEEE.","Deep learning; Feature extraction; Maritime; Moderate resolution; SAR; Ship classification","Deep learning; Extraction; Feature extraction; Geology; Image classification; Principal component analysis; Radar imaging; Remote sensing; Ships; Synthetic aperture radar; Classification accuracy; Comparative studies; High resolution image; Maritime; Maritime surveillance; Moderate resolution; Ship classification; Synthetic aperture radar (SAR) images; Classification (of information)"
"Polsar target classification using polarimetric-feature-driven deep convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058874116&doi=10.1109%2fIGARSS.2018.8518579&partnerID=40&md5=f0c0730c409d2d03efef7d9b33c8e1c7","Deep convolutional neural network (CNN) techniques have been utilized to enhance polarimetric synthetic aperture radar (PolSAR) image classification performance. This work contributes to a current challenge that is how to adapt deep CNN classifier for PolSAR classification with limited training samples while keeping good generalization performance. A polarimetric-feature-driven deep CNN classification scheme is established with both classical rollinvariant polarimetric features and hidden polarimetric features in the rotation domain to drive the proposed deep CNN model. Comparison studies validate the efficiency and superiority of the proposal. For the benchmark AIRSAR data, the proposed method achieves the state-of-the-art classification accuracies. Meanwhile, the convergence speed from the proposed CNN approach is about 2.3 times faster than the normal CNN method. For multi-temporal UAVSAR datasets, the proposed scheme achieves comparably high classification accuracies as the normal CNN method for train-used temporal data, while for train-not-used data it obtains average 4.86% higher overall accuracy than the normal CNN method. Furthermore, the proposed strategy can also produce very promising classification accuracy with very limited training samples. © 2018 IEEE.","Classification; CNN classifier; Deep learning; Multitemporal; Polarimetric feature; PolSAR; Rotation domain","Convolution; Deep learning; Deep neural networks; Digital storage; Geology; Image enhancement; Neural networks; Polarimeters; Remote sensing; Sampling; Synthetic aperture radar; Classification performance; Convolutional neural network; Generalization performance; Multi-temporal; Polarimetric features; Polarimetric synthetic aperture radars; PolSAR; Rotation domain; Classification (of information)"
"Effective building extraction by learning to detect and correct erroneous labels in segmentation mask","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063130092&doi=10.1109%2fIGARSS.2018.8517854&partnerID=40&md5=a3258b76b92d1caee4a9e22eec7a62e3","Semantic segmentation is pivotal for remote sensing image analysis. Although existing segmentation techniques perform well on similar landscape images, their generalization capability on an entirely different landscape is extremely poor. One of the primary reasons is that they partially or wholly, neglect the underlying relationship that exist in the joint space of input and output variables. Thus, effectively they lack to impose structure in their output predictions which is necessary for successful segmentation. In this paper, we address this problem and propose a novel solution by modeling the joint distribution of input-output variable which in turn enforces some structure in the initial segmentation mask. To this end, we first detect erroneous labels, in the form of Error maps, in the initial building masks. These Error maps are then used to correct the corresponding erroneous labels through a replacement technique. We evaluate our methodology on the benchmark Inria Aerial Image Labeling dataset, which is a large scale high resolution dataset for building footprint segmentation. In contrast to previous methods, our predicted segmentation masks are much closer to ground truth, owning to the fact that they are able to effectively correct both the large errors as well as the blobby effects. We lastly perform on par with other state-of-the-arts, validating the efficacy of our technique. © 2018 IEEE","Building footprint extraction; Deep learning; High-resolution imagery; Semantic segmentation; Structured prediction",
"Feature extraction and classification of hyperspectral image based on 3d-convolution neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057023224&doi=10.1109%2fDDCLS.2018.8515930&partnerID=40&md5=c2f7eb24189e099efc50facf99cfac4e","Deep learning has huge potential for hyperspectral image (HSI) classification. In order to fully exploit the information in HSI and improve the classification accuracy, a new classification method based on 3D-convolutional neural network (3D-CNN) is proposed. In the meantime, virtual samples are introduced to solve the problem of insufficient samples of HSI. The experimental results show that the proposed method has a good application prospect in HSI classification. © 2018 IEEE.","Deep learning; feature extraction; image classification; remote sensing image; virtual samples","Convolution; Deep learning; Extraction; Feature extraction; Image classification; Neural networks; Remote sensing; Spectroscopy; Application prospect; Classification accuracy; Classification methods; Convolution neural network; Convolutional neural network; Feature extraction and classification; Remote sensing images; Virtual sample; Classification (of information)"
"Cloud segmentation of remote sensing images on landsat-8 by deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060958631&doi=10.1145%2f3291801.3291839&partnerID=40&md5=70fcd163785e76e81bafc1dd4c7e1cd1","Cloud is always the weak and even uninformative area inevitably existing in the remote sensing images, and greatly limits the development of remote sensing application. Accurate and automatic detection and segmentation of clouds in satellite scenes is a key problem for the application of remote sensing images. In this paper, we propose a fully convolutional network (FCN) based on deep learning framework for cloud segmentation from remote sensing images. First, the obtained remote sensing image is preprocessed by using band merging and size cropping. Second the image is input into the FCN to achieve the cloud segmentation. To demonstrate the effectiveness of the method, we conduct experiments on Landsat-8 satellite dataset. The overall accuracy of our proposed method for cloud segmentation is higher than 88%. Experimental results demonstrate that both thin and thick cloud can be well segmented with higher accuracy and robustness by using our method. © 2018 Association for Computing Machinery.","Cloud segmentation; Deep learning; Fully convolutional network (FCN); Landsat-8","Big data; Convolution; Deep learning; Image segmentation; Automatic Detection; Cloud segmentation; Convolutional networks; LANDSAT; Learning frameworks; Overall accuracies; Remote sensing applications; Remote sensing images; Remote sensing"
"Multi-Scale Convolutional Neural Network for Remote Sensing Scene Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057107188&doi=10.1109%2fEIT.2018.8500107&partnerID=40&md5=a4b9e3802dd404ea84225937aff84aad","In recent years the problem of scene classification in remote sensing has attracted a considerable amount of attention. Solution for this important problem based on deep convolutional neural networks (CNN) are currently state-of-the-art. So far all CNNs used images of fixed size (typically 224× 224 which commonly used in other fields of computer vision). In this paper, we propose a multi-scale deep CNN architecture that can accept variable image sizes. We achieve this by using multiple CNN, that share some or all parameters, followed by a merge layer, fully connected layers, and finally a softmax layer for classification. In each epoch we train the network with a batch of images of all scales. We have implemented this architecture using three SqueezeNet CNNs trained on three different scales of scene images. Then we carried out experiments on three well know datasets, namely UC Merced, KSA, and AID datasets. Preliminary results show that this multi-scale CNN do converge just as the traditional single-scale training, and leads to better testing accuracy. © 2018 IEEE.","Convolutional neural networks (CNN); Deep learning; Deep neural networks; Remote sensing; Scene classification","Convolution; Deep learning; Deep neural networks; Network architecture; Neural networks; Convolutional neural network; Convolutional Neural Networks (CNN); Deep convolutional neural networks; Fully-connected layers; Problem-based; Scene classification; State of the art; Testing accuracy; Remote sensing"
"A tutorial on modelling and inference in undirected graphical models for hyperspectral image analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046017131&doi=10.1080%2f01431161.2018.1465614&partnerID=40&md5=55980172ef58fe20be5293fcc223ac51","Undirected graphical models have been successfully used to jointly model the spatial and the spectral dependencies in earth observing hyperspectral images. They produce less noisy, smooth, and spatially coherent land-cover maps and give top accuracies on many datasets. Moreover, they can easily be combined with other state-of-the-art approaches, such as deep learning. This has made them an essential tool for remote-sensing researchers and practitioners. However, graphical models have not been easily accessible to the larger remote-sensing community as they are not discussed in standard remote-sensing textbooks and not included in the popular remote-sensing software and toolboxes. In this tutorial, we provide a theoretical introduction to Markov random fields and conditional random fields-based spatial–spectral classification for land-cover mapping along with a detailed step-by-step practical guide on applying these methods using freely available software. Furthermore, the discussed methods are benchmarked on four public hyperspectral datasets for a fair comparison among themselves and easy comparison with the vast number of methods in literature which use the same datasets. The source code necessary to reproduce all the results in the paper is published on-line to make it easier for the readers to apply these techniques to different remote-sensing problems. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Deep learning; Image segmentation; Independent component analysis; Markov processes; Photomapping; Spectroscopy; Conditional random field; Freely available software; Land cover mapping; Markov Random Fields; Number of methods; Spectral classification; Spectral dependency; State-of-the-art approach; Remote sensing; image analysis; land cover; mapping; Markov chain; numerical model; remote sensing"
"Change Detection Based on the Combination of Improved SegNet Neural Network and Morphology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056527750&doi=10.1109%2fICIVC.2018.8492747&partnerID=40&md5=378f5aa9d9d9e54d26139c585150ec19","Through the analysis of satellite remote sensing image data, the identification of newly added buildings in the same area can be realized to judge the use of land. The identification of newly added buildings based on remote sensing images, involving image object extraction, semantic segmentation and change detection. The difficulty is not only to identify the changes of remote sensing images in different periods, but also to identify the newly added buildings with the original buildings. Both of the recognition effect and the detection precision of the traditional method based on mathematical modeling need to be improved. SegNet neural network is a kind of deep convolution neural network. It shows good performance in dealing with the task of semantic segmentation of single image, but it is directly applied to building change detection with low accuracy. The simulation results show that the improved SegNet neural network method improves the accuracy of the quantitative evaluation index F1 score by 8.6% compared with the conventional SegNet network in the newly added building detection effect in the same area in 2015 and 2017. In addition, the situation that the change detection result will produce a large number of noise, a combination of improved SegNet network and image morphological method is adopted to eliminate the noise and reduce the misjudgment. The simulation results show that the F1 index increased further by 1.4% on the basis of 8.6%. © 2018 IEEE.","Building change detection; Convolutional neural network; Deep learning; Morphology; Remote sensing images","Buildings; Convolution; Deep learning; Feature extraction; Image segmentation; Morphology; Neural networks; Remote sensing; Semantics; Building change detection; Convolution neural network; Convolutional neural network; Neural network method; Quantitative evaluation; Remote sensing images; Satellite remote sensing; Semantic segmentation; Image enhancement"
"Multi-modal Remote Sensing Image Classification for Low Sample Size Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056502553&doi=10.1109%2fIJCNN.2018.8489351&partnerID=40&md5=6229620fd42861e2cb8eb531c2e8d87a","Recently, multiple and heterogeneous remote sensing images have provided a new development opportunity for Earth observation research. Utilizing deep learning to gain the shared representative information between different modalities is important to resolve the problem of geographical region classification. In this paper, a CNN-based multi-modal framework for low-sample-size data classification of remote sensing images is introduced. This method has three main stages. Firstly, features are extracted from high- and low-resolution remote sensing images separately using multiple convolution layers. Then, the two types of features are fused at the fusion algorithm layer. Finally, the fused features are used to train a classifier. The novelty of this method is that not only it considers the complementary relationship between the two modalities, but enhances the value of a small number of samples. Based on our experiments, the proposed model can obtain a state-of-the-art performance, being more accurate than the comparable architectures, such as single-modal LeNet, NanoNets and multi-modal HL-LeNet that are trained with a double size of samples. © 2018 IEEE.","convolution neural network; deep learning; high level feature fusion; multi-modal; remote sensing classification","Convolution; Deep learning; Image classification; Remote sensing; Complementary relationship; Convolution neural network; High-level features; Multi-modal; Region classifications; Remote sensing classification; Remote sensing image classification; State-of-the-art performance; Classification (of information)"
"Deep learning integrated with multiscale pixel and object features for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056498337&doi=10.1109%2fPRRS.2018.8486304&partnerID=40&md5=aa167af0d5f7ef0ce86db89f2603f5c1","The spectral resolution and spatial resolution of hyperspectral images are continuously improving, providing rich information for interpreting remote sensing image. How to improve the image classification accuracy has become the focus of many studies. Recently, Deep learning is capable to extract discriminating high-level abstract features for image classification task, and some interesting results have been acquired in image processing. However, when deep learning is applied to the classification of hyperspectral remote sensing images, the spectral-based classification method is short of spatial and scale information; the image patch-based classification method ignores the rich spectral information provided by hyperspectral images. In this study, a multi-scale feature fusion hyperspectral image classification method based on deep learning was proposed. Firstly, multiscale features were obtained by multi-scale segmentation. Then multiscale features were input into the convolution neural network to extract high-level features. Finally, the high-level features were used for classification. Experimental results show that the classification results of the fusion multi-scale features are better than the single-scale features and regional feature classification results. © 2018 IEEE.","CNN; Deep learning; Multiscale feature fusion; Object based image analysis","Deep learning; Image classification; Image enhancement; Image fusion; Independent component analysis; Remote sensing; Spectroscopy; Classification accuracy; Classification methods; Classification results; Convolution neural network; Hyperspectral Remote Sensing Image; Multi-scale features; Multiscale segmentation; Object based image analysis; Classification (of information)"
"Multi-branch regression network for building classification using remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056520400&doi=10.1109%2fPRRS.2018.8486177&partnerID=40&md5=b9ef4b6babf84cc63694b891676b5c06","Convolutional neural networks (CNN) are widely used for processing high-resolution remote sensing images like segmentation or classification, and have been demonstrated excellent performance in recent years. In this paper, a novel classification framework based on segmentation method, called Multi-branch regression network (named as MBR-Net) is proposed. The proposed method can generate multiple losses rely on training images in different size of information. In addition, a complete training strategy for classifying remote sensing images, which can reduce the influence of uneven samples is also developed. Experimental results with Inrial aerial dataset demonstrate that the proposed framework can provide much better results compared to state-of-the-art U-Net and generate fine-grained prediction maps. © 2018 IEEE.","Building classification; Deep learning; Multi-branch regression network; Remote sensing images","Antennas; Deep learning; Image classification; Image segmentation; Neural networks; Regression analysis; Classification framework; Convolutional Neural Networks (CNN); Fine-grained predictions; High resolution remote sensing images; Remote sensing images; Segmentation methods; State of the art; Training strategy; Remote sensing"
"Deep learning-based enhancement of hyperspectral images using simulated ground truth","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056493043&doi=10.1109%2fPRRS.2018.8486408&partnerID=40&md5=1afef7afa6d2e341cb9545ae0e545dbc","The paper addresses the problem of imaging quality enhancement for the Offner hyperspectrometer using a convolutional neural network. We use a deep convolutional neural network with residual training and PReLU activation, inspired by the super-resolution task for RGB images. In the case of hyperspectral imaging, it is often a problem to find a large enough ground truth dataset for training a neural network from scratch. Transfer learning using the network pretrained for RGB images with some pre- and postprocessing is one of the possible workarounds. In this paper, we propose to simulate the necessary ground truth data using non-imaging spectrometer. The obtained dataset with partially simulated ground truth is then used to train the convolutional neural network directly for hyperspectral image quality enhancement. The proposed training approach also allows to incorporate distortions specific for hyperspectral images into the enhancement procedure. It allows to successfully remove the striping distortions inherent to the Offner scheme of image acquisition. The experimental results of the proposed approach show a significant quality gain. © 2018 IEEE.","Convolutional neural networks; Hyperspectral image enhancement; Imaging hyperspectrometer; Offner scheme; Training from scratch","Convolution; Deep neural networks; Hyperspectral imaging; Image quality; Independent component analysis; Neural networks; Pattern recognition; Quality control; Remote sensing; Spectroscopy; Convolutional neural network; Deep convolutional neural networks; Ground truth data; Ground-truth dataset; Imaging quality; Offner scheme; Super resolution; Transfer learning; Image enhancement"
"Collaborative classification of hyperspectral and LIDAR data using unsupervised image-to-image CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056496704&doi=10.1109%2fPRRS.2018.8486164&partnerID=40&md5=df9c256f612a794aee0eddc3d4150a08","Currently, how to efficiently exploit useful information from multi-source remote sensing data for better Earth observation becomes an interesting but challenging problem. In this paper, we propose an collaborative classification framework for hyperspectral image (HSI) and Light Detection and Ranging (LIDAR) data via image-to-image convolutional neural network (CNN). There is an image-to-image mapping, learning a representation from input source (i.e., HSI) to output source (i.e., LIDAR). Then, the extracted features are expected to own characteristics of both HSI and LIDAR data, and the collaborative classification is implemented by integrating hidden layers of the deep CNN. Experimental results on two real remote sensing data sets demonstrate the effectiveness of the proposed framework. © 2018 IEEE.","Convolutional Neural Network; Data Fusion; Deep Learning; Hyperspectral Image","Convolution; Data fusion; Deep learning; Hyperspectral imaging; Image classification; Neural networks; Optical radar; Remote sensing; Spectroscopy; Collaborative classifications; Convolutional neural network; Convolutional Neural Networks (CNN); Earth observations; Image mapping; Input sources; Light detection and ranging; Remote sensing data; Classification (of information)"
"An elegant end-to-end fully convolutional network (E3FCN) for green tide detection using MODIS data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056562190&doi=10.1109%2fPRRS.2018.8486160&partnerID=40&md5=bd820b7da9fb2582179111fa9a5a4901","Using remote sensing (RS) data to monitor the onset, proliferation and decline of green tide (GT) has great significance for disaster warning, trend prediction and decision-making support. However, remote sensing images vary under different observing conditions, which bring big challenges to detection missions. This paper proposes an accurate green tide detection method based on an Elegant End-to-End Fully Convolutional Network (E3FCN) using Moderate Resolution Imaging Spectroradiometer (MODIS) data. In preprocessing, RS images are firstly separated into subimages by a sliding window. To detect GT pixels more efficiently, the original Fully Convolutional Neural Network (FCN) architecture is modified into E3FCN, which can be trained end-to-end. The E3FCN model can be divided into two parts, contracting path and expanding path. The contracting path aims to extract high-level features and the expanding path aims to provide a pixel-level prediction by using a skip technique. The prediction result of whole image is generated by merging the prediction results of subimages, which can also improve the final performance. Experiment results show that the average precision of E3FCN on the whole data sets is 98.06%, compared to 73.27% of Support Vector Regression (SVR), 71.75% of Normalized Difference Vegetation Index (NDVI), and 64.41% of Enhanced Vegetation Index (EVI). © 2018 IEEE.","Deep learning; Elegant End-to-End Fully Convolutional Network (E3FCN); Green tide; Moderate Resolution Imaging Spectroradiometer (MODIS); Remote sensing","Convolution; Decision making; Deep learning; Forecasting; Image enhancement; Neural networks; Pattern recognition; Pixels; Radiometers; Satellite imagery; Spectrometers; Vegetation; Convolutional networks; Convolutional neural network; Enhanced vegetation index; Green tides; Moderate resolution imaging spectroradiometer; Moderate resolution imaging spectroradiometer datum; Normalized difference vegetation index; Support vector regression (SVR); Remote sensing"
"Using a VGG-16 network for individual tree species detection with an object-based approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056493030&doi=10.1109%2fPRRS.2018.8486395&partnerID=40&md5=aab392902372c849f70d3707d147844d","Acquiring information about forest stands such as individual tree species is crucial for monitoring forests. To date, such information is assessed by human interpreters using airborne or an Unmanned Aerial Vehicle (UAV), which is time/cost consuming. The recent advancement in remote sensing image acquisition, such as WorldView-3, has increased the spatial resolution up to 30 cm and spectral resolution up to 16 bands. This advancement has significantly increased the potential for Individual Tree Species Detection (ITSD). In order to use the single source Worldview-3 images, our proposed method first segments the image to delineate trees, and then detects trees using a VGG-16 network. We developed a pipeline for feeding the deep CNN network using the information from all the 8 visible-near infrareds' bands and trained it. The result is compared with two state-of-the-art ensemble classifiers namely Random Forest (RF) and Gradient Boosting (GB). Results demonstrate that the VGG-16 outperforms all the other methods reaching an accuracy of about 92.13%. © 2018 IEEE.","Convolutional Neural Network; Deep Learning; Gradient Boosting; Individual Tree Species Detection; Random Forest; VGG-16","Antennas; Decision trees; Deep learning; Forestry; Neural networks; Object detection; Unmanned aerial vehicles (UAV); Convolutional neural network; Gradient boosting; Individual tree; Random forests; VGG-16; Remote sensing"
"Hyperspectral Remote Sensing Image Classification Based on Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056100155&doi=10.23919%2fChiCC.2018.8484034&partnerID=40&md5=a9a9c9d27cd6c9102b7d5d5257695706","Remote sensing hyperspectral imaging can obtain rich spectral information of terrestrial objects, which allows the indistinguishable matter in the traditional wideband remote sensing to be distinguished in hyperspectral remote sensing. Hyperspectral image has the characteristics of 'combining image with spectrum'. Making full use of spectral information and spatial information in hyperspectral image is the premise of obtaining accurate classification results. At present, most of hyperspectral data feature extraction algorithms mainly utilize local spatial information in the same channel and spectral information in the same spatial location of different channels. However, these methods require a large amount of prior knowledge, it is difficult to fully grasp the hyperspectral data of all spatial and spectral information, and the model generalization ability is poor. With the development of deep learning, convolutional neural network shows superior performance in all kinds of visual tasks, especially in the two-dimensional image classification, and could get a high classification accuracy. In this paper, an image classification method based on three-dimensional convolution neural network is proposed based on the structural properties of hyperspectral data. In the proposed method, first the stereo image blocks of hyperspectral data are intercepted, then multi-layer convolution and pooling operation of extracted blocks by convolutional neural network are implemented to obtain the essential information of hyperspectral data, finally the classification of hyperspectral data is completed. The experimental results show the proposed method could provide better feature expression and classification accuracy for hyperspectral image. © 2018 Technical Committee on Control Theory, Chinese Association of Automation.","Convolutional Neural Network; Deep Learning Classification; Hyperspectral Image","Computerized tomography; Convolution; Data mining; Deep learning; Hyperspectral imaging; Image classification; Neural networks; Remote sensing; Spectroscopy; Stereo image processing; Vision; Classification accuracy; Classification methods; Classification results; Convolution neural network; Convolutional neural network; Hyperspectral remote sensing; Hyperspectral Remote Sensing Image; Two dimensional images; Classification (of information)"
"Target Detection of Hyperspectral Image Based on Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056098749&doi=10.23919%2fChiCC.2018.8483788&partnerID=40&md5=f5e4b20f064eb24a0770410a7971e438","Convolutional neural networks (CNN) has been applied in image classification and target detection successfully, however, it is rarely introduced to the field of hyperspectral image (HSI) target detection. Therefore, in this paper, a hyperspectral image (HSI) target detection method based on CNN is proposed. Firstly, the raw HSI data is preprocessed and the spectral information could be obtained. Secondly, to extract the feature information, a CNN is trained and the parameters of the network are adjusted according to a HSI. Finally, the targets will be calibrated according to the extracted features. To estimate the target detection performance of the proposed method, deep belief network (DBN) and SVM methods are compared in the experiment of the real world AVIRIS HSI experiment. Numerical results show that the proposed method has promising prospect in the field of HSI target detection. © 2018 Technical Committee on Control Theory, Chinese Association of Automation.","CNN; DBN; Deep learning; Remote sensing image; Target recognition","Convolution; Deep learning; Neural networks; Numerical methods; Remote sensing; Spectroscopy; Convolutional neural network; Convolutional Neural Networks (CNN); Deep belief network (DBN); Detection performance; Feature information; Remote sensing images; Spectral information; Target recognition; Radar target recognition"
"Change detection based on Faster R-CNN for high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052092690&doi=10.1080%2f2150704X.2018.1492172&partnerID=40&md5=6c9557f8ca471ae6e80df740e3219955","Change detection is of great significance in remote sensing. The advent of high-resolution remote sensing images has greatly increased our ability to monitor land use and land cover changes from space. At the same time, high-resolution remote sensing images present a new challenge over other satellite systems, in which time-consuming and tiresome manual procedures must be needed to identify the land use and land cover changes. In recent years, deep learning (DL) has been widely used in the fields of natural image target detection, speech recognition, face recognition, etc., and has achieved great success. Some scholars have applied DL to remote sensing image classification and change detection, but seldomly to high-resolution remote sensing images change detection. In this letter, faster region-based convolutional neural networks (Faster R-CNN) is applied to the detection of high-resolution remote sensing image change. Compared with several traditional and other DL-based change detection methods, our proposed methods based on Faster R-CNN achieve higher overall accuracy and Kappa coefficient in our experiments. In particular, our methods can reduce a large number of false changes. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Deep learning; Face recognition; Land use; Neural networks; Object recognition; Space optics; Speech recognition; Change detection; Convolutional neural network; High resolution remote sensing images; Kappa coefficient; Land use and land cover change; Overall accuracies; Remote sensing image classification; Satellite system; Remote sensing; accuracy assessment; detection method; image classification; image resolution; land cover; land use; numerical method; remote sensing; satellite imagery"
"Recognition and location of typical scenes in large hyperspectral remote sensing image based on deep transfer learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054923663&doi=10.1088%2f1742-6596%2f1087%2f6%2f062035&partnerID=40&md5=a7552942687af8612c2aa99fed065f38","The recognition and location of military scenes in hostile battlefield are of great strategic significance. Such scenes are the main targets of our long-range reconnaissance and directional strike. Deep transfer learning algorithm is always adopted to improve the accuracy of image recognition based on DCNN model. And on this basis, this paper mainly studied the application of deep transfer learning algorithm to recognize and locate typical scenes in large hyperspectral remote sensing image. Nichetargeting and impeccable DCNN model was accomplished after the training by typical scenes dataset. In the face of a large hyperspectral remote sensing image, the method of grid cutting, recognizing one by one and marking distinctively could pinpoint the location of typical scenes within. Experimental results showed that deep transfer learning algorithm could get a good application in the fast recognition and accurate location of typical scenes in large hyperspectral remote sensing image. © Published under licence by IOP Publishing Ltd.",,"Artificial intelligence; Deep learning; Image enhancement; Image recognition; Location; Optical character recognition; Remote sensing; Accurate location; Fast recognition; Hyperspectral Remote Sensing Image; Long-range reconnaissance; Recognition and locations; Transfer learning; Learning algorithms"
"Fusion of deep learning with adaptive bilateral filter for building outline extraction from remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056477620&doi=10.1117%2f1.JRS.12.046018&partnerID=40&md5=426fe7fabc44a1e7c9ee8213ea7c612e","Solving the problem of building extraction from remote sensing images, which have high spatial resolution, is considered to be one of the most challenging issues in the field of photogrammetry science and remote sensing. The purpose of this study is to present an innovative algorithm, named adaptive bilateral filter (ABF) + segment-based neural network, which is based on the fusion of deep convolutional neural networks (DCNNs) and adaptive ABF and has resulted in improvements in the accuracy of building extraction from remote sensing images with high spatial resolution. The building extraction process in this study includes the following steps: applying the ABF to the research data set and optimizing its parameters in order to improve the building outlines, designing, and training the DCNN, SegNet, based on the improved data set and optimizing it using an adaptive moment estimation algorithm and assessing the impact of applying the ABF + SegNet algorithm to automatic building outline extraction. The proposed algorithm in this study is tested on three sets of remote sensing data from the cities of Potsdam, Indianapolis, and Tehran. The results indicate that the ABF + SegNet algorithm is able to extract the buildings from remote sensing color images with suitable accuracy. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","adaptive bilateral filter; building outline extraction; convolutional neural networks; deep learning; remote sensing imagery; semantic segmentation","Adaptive filtering; Adaptive filters; Buildings; Convolution; Deep learning; Deep neural networks; Extraction; Image enhancement; Image resolution; Image segmentation; Neural networks; Nonlinear filtering; Semantics; Bilateral filters; Convolutional neural network; Outline extractions; Remote sensing imagery; Semantic segmentation; Remote sensing"
"Low-shot learning for the semantic segmentation of remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047608453&doi=10.1109%2fTGRS.2018.2833808&partnerID=40&md5=782277bdfee004d60ecdff8b3e292083","Recent advances in computer vision using deep learning with RGB imagery (e.g., object recognition and detection) have been made possible thanks to the development of large annotated RGB image data sets. In contrast, multispectral image (MSI) and hyperspectral image (HSI) data sets contain far fewer labeled images, in part due to the wide variety of sensors used. These annotations are especially limited for semantic segmentation, or pixelwise classification, of remote sensing imagery because it is labor intensive to generate image annotations. Low-shot learning algorithms can make effective inferences despite smaller amounts of annotated data. In this paper, we study low-shot learning using self-taught feature learning for semantic segmentation. We introduce: 1) an improved self-taught feature learning framework for HSI and MSI data and 2) a semisupervised classification algorithm. When these are combined, they achieve the state-of-the-art performance on remote sensing data sets that have little annotated training data available. These low-shot learning frameworks will reduce the manual image annotation burden and improve semantic segmentation performance for remote sensing imagery. © 2018 IEEE.","Deep learning; feature learning; hyperspectral imaging; self-taught learning; semantic segmentation; semisupervised","Deep learning; Hyperspectral imaging; Image annotation; Image enhancement; Image segmentation; Object detection; Object recognition; Remote sensing; Semantics; Spectroscopy; Feature learning; Manual image annotations; Object recognition and detections; Self-taught learning; Semantic segmentation; Semi-supervised; Semi-supervised classification; State-of-the-art performance; Learning algorithms; algorithm; computer vision; data set; multispectral image; remote sensing; segmentation; supervised classification; supervised learning"
"Recurrent neural networks for remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053455457&doi=10.1049%2fiet-cvi.2017.0420&partnerID=40&md5=3f8c67559e0d9edd54cade7c0047e171","Automatically classifying an image has been a central problem in computer vision for decades. A plethora of models has been proposed, from handcrafted feature solutions to more sophisticated approaches such as deep learning. The authors address the problem of remote sensing image classification, which is an important problem to many real world applications. They introduce a novel deep recurrent architecture that incorporates high-level feature descriptors to tackle this challenging problem. Their solution is based on the general encoder-decoder framework. To the best of the authors' knowledge, this is the first study to use a recurrent network structure on this task. The experimental results show that the proposed framework outperforms the previous works in the three datasets widely used in the literature. They have achieved a state-of-the-art accuracy rate of 97.29% on the UC Merced dataset. © The Institution of Engineering and Technology 2018.",,"Deep learning; Recurrent neural networks; Remote sensing; Accuracy rate; Central problems; General encoder decoders; High-level features; Real-world; Recurrent networks; Remote sensing image classification; State of the art; Image classification"
"Multilevel Building Detection Framework in Remote Sensing Images Based on Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053169150&doi=10.1109%2fJSTARS.2018.2866284&partnerID=40&md5=0a3d77680a8d5a39fc1b329c53dbfd7b","In this paper, we propose a hierarchical building detection framework based on deep learning model, which focuses on accurately detecting buildings from remote sensing images. To this end, we first construct the generation model of the multilevel training samples using the Gaussian pyramid technique to learn the features of building objects at different scales and spatial resolutions. Then, the building region proposal networks are put forward to quickly extract candidate building regions, thereby increasing the efficiency of the building object detection. Based on the candidate building regions, we establish the multilevel building detection model using the convolutional neural networks (CNNs), from which the generic image features of each building region proposal are calculated. Finally, the obtained features are provided as inputs for training CNNs model, and the learned model is further applied to test images for the detection of unknown buildings. Various experiments using the Datasets I and II (in Section V-A) show that the proposed framework increases the mean average precision values of building detection by 3.63%, 3.85%, and 3.77%, compared with the state-of-the-art methods, i.e., Method IV. Besides, the proposed method is robust to the buildings having different spatial textures and types. © 2008-2012 IEEE.","Building detection; candidate building regions; convolutional neural networks (CNNs); multilevel framework; remote sensing images","Convolution; Deep learning; Feature extraction; Learning systems; Neural networks; Object detection; Object recognition; Personnel training; Remote sensing; Building detection; Building regions; Convolutional neural network; multilevel framework; Proposals; Remote sensing images; Buildings; artificial neural network; building; conceptual framework; data set; detection method; image analysis; remote sensing; spatiotemporal analysis"
"Scene classification of remote sensing image based on deep network and multi-scale features fusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048709218&doi=10.1016%2fj.ijleo.2018.06.024&partnerID=40&md5=d96f5093d072a967f840f74d084febdc","Owing to the complexity of spatial and structural patterns of remote sensing images, scene classification of them is still an open problem and remains active in the community. The inadequacy of labeled samples leads to low accuracy of remote sensing image scene classification. To solve this problem, a classification method DCNN_MSFF is proposed based on deep convolutional neural networks (DCNN) and multi-scale features fusion (MSFF). Firstly, the remote sensing images are transformed to obtain a number of different scale ones for augmentation. Then, they are input into the DCNN for features extraction, and the different scale features of the convolutional and the fully-connected layers are encoded or pooled averagely. Finally, the processed features are fused, and the multi-kernel support vector machine (MKSVM) is used to classify the scenes. The test results in the commonly used remote sensing datasets show that, this proposed method outperforms the state-of-the-art ones in the scene classification of remote sensing images. In this paper, the multi-scale images and features of the convolutional and the fully-connected layers in the deep learning process are utilized to enhance the representation abilities of the classification features. At the same time, the MKSVM is used to improve the generalization ability of the fusion features, so the classification result is better. © 2018 Elsevier GmbH","Deep convolutional neural networks; Features fusion; Multi-kernel support vector machine; Remote sensing image; Scene classification","Convolution; Deep neural networks; Image classification; Image enhancement; Image fusion; Neural networks; Remote sensing; Support vector machines; Deep convolutional neural networks; Features fusions; Multi-kernel; Remote sensing images; Scene classification; Classification (of information)"
"Towards operational satellite-based damage-mapping using U-net convolutional network: A case study of 2011 Tohoku Earthquake-Tsunami","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055410848&doi=10.3390%2frs10101626&partnerID=40&md5=92e1436260c3468c2cb66ff37ad28dc4","The satellite remote-sensing-based damage-mapping technique has played an indispensable role in rapid disaster response practice, whereas the current disaster response practice remains subject to the low damage assessment accuracy and lag in timeliness, which dramatically reduces the significance and feasibility of extending the present method to practical operational applications. Therefore, a highly efficient and intelligent remote-sensing image-processing framework is urgently required to mitigate these challenges. In this article, a deep learning algorithm for the semantic segmentation of high-resolution remote-sensing images using the U-net convolutional network was proposed to map the damage rapidly. The algorithm was implemented within a Microsoft Cognitive Toolkit framework in the GeoAI platform provided by Microsoft. The study takes the 2011 Tohoku Earthquake-Tsunami as a case study, for which the pre- and post-disaster high-resolution WorldView-2 image is used. The performance of the proposed U-net model is compared with that of deep residual U-net. The comparison highlights the superiority U-net for tsunami damage mapping in this work. Our proposed method achieves the overall accuracy of 70.9% in classifying the damage into ""washed away,"" ""collapsed,"" and ""survived"" at the pixel level. In future disaster scenarios, our proposed model can generate the damage map in approximately 2-15 min when the preprocessed remote-sensing datasets are available. Our proposed damage-mapping framework has significantly improved the application value in operational disaster response practice by substantially reducing the manual operation steps required in the actual disaster response. Besides, the proposed framework is highly flexible to extend to other scenarios and various disaster types, which can accelerate operational disaster response practice. © 2018 by the authors.","2011 Tohoku earthquake and tsunami; Microsoft Cognitive Toolkit; Operational damage-mapping; Semantic segmentation; U-net convolutional neural network","Convolution; Damage detection; Deep learning; Earthquakes; Image segmentation; Mapping; Neural networks; Remote sensing; Semantics; Tsunamis; 2011 tohoku earthquakes; Convolutional neural network; Damage mapping; MicroSoft; Semantic segmentation; Emergency services"
"Evaluating late blight severity in potato crops using unmanned aerial vehicles and machine learning algorithms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055457969&doi=10.3390%2frs10101513&partnerID=40&md5=b9053d8f01ef46a017e5ef5353265cda","This work presents quantitative prediction of severity of the disease caused by Phytophthora infestans in potato crops using machine learning algorithms such as multilayer perceptron, deep learning convolutional neural networks, support vector regression, and random forests. The machine learning algorithms are trained using datasets extracted from multispectral data captured at the canopy level with an unmanned aerial vehicle, carrying an inexpensive digital camera. The results indicate that deep learning convolutional neural networks, random forests and multilayer perceptron using band differences can predict the level of Phytophthora infestans affectation on potato crops with acceptable accuracy. © 2018 by the authors.","Deep learning; Multispectral; Neural networks; Phytophthora infestans; Remote sensing; UAV","Antennas; Convolution; Crops; Decision trees; Deep learning; Deep neural networks; Forecasting; Multilayer neural networks; Multilayers; Neural networks; Plants (botany); Remote sensing; Unmanned aerial vehicles (UAV); Canopy level; Convolutional neural network; Multi-spectral; Multi-spectral data; Phytophthora infestans; Quantitative prediction; Random forests; Support vector regression (SVR); Learning algorithms"
"Multi-resolution feature fusion for image classification of building damages with convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055426516&doi=10.3390%2frs10101636&partnerID=40&md5=2c6b90d6f39ec473e336deb169c1a633","Remote sensing images have long been preferred to perform building damage assessments. The recently proposed methods to extract damaged regions from remote sensing imagery rely on convolutional neural networks (CNN). The common approach is to train a CNN independently considering each of the different resolution levels (satellite, aerial, and terrestrial) in a binary classification approach. In this regard, an ever-growing amount of multi-resolution imagery are being collected, but the current approaches use one single resolution as their input. The use of up/down-sampled images for training has been reported as beneficial for the image classification accuracy both in the computer vision and remote sensing domains. However, it is still unclear if such multi-resolution information can also be captured from images with different spatial resolutions such as imagery of the satellite and airborne (from both manned and unmanned platforms) resolutions. In this paper, three multi-resolution CNN feature fusion approaches are proposed and tested against two baseline (mono-resolution) methods to perform the image classification of building damages. Overall, the results show better accuracy and localization capabilities when fusing multi-resolution feature maps, specifically when these feature maps are merged and consider feature information from the intermediate layers of each of the resolution level networks. Nonetheless, these multi-resolution feature fusion approaches behaved differently considering each level of resolution. In the satellite and aerial (unmanned) cases, the improvements in the accuracy reached 2% while the accuracy improvements for the airborne (manned) case was marginal. The results were further confirmed by testing the approach for geographical transferability, in which the improvements between the baseline and multi-resolution experiments were overall maintained. © 2018 by the authors.","Aerial; Deep learning; Dilated convolutions; Earthquake; Residual connections; Satellite; UAV","Antennas; Convolution; Damage detection; Deep learning; Earthquakes; Image fusion; Neural networks; Remote sensing; Satellite imagery; Satellites; Unmanned aerial vehicles (UAV); Binary Classification Approach; Classification accuracy; Convolutional neural network; Convolutional Neural Networks (CNN); Manned and unmanned platforms; Multi-resolution feature; Multi-resolution imagery; Remote sensing imagery; Image classification"
"Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049446921&doi=10.1016%2fj.isprsjprs.2018.06.007&partnerID=40&md5=3847a486732b09a64005c2bc4aa83ee2","When approaching the semantic segmentation of overhead imagery in the decimeter spatial resolution range, successful strategies usually combine powerful methods to learn the visual appearance of the semantic classes (e.g. convolutional neural networks) with strategies for spatial regularization (e.g. graphical models such as conditional random fields). In this paper, we propose a method to learn evidence in the form of semantic class likelihoods, semantic boundaries across classes and shallow-to-deep visual features, each one modeled by a multi-task convolutional neural network architecture. We combine this bottom-up information with top-down spatial regularization encoded by a conditional random field model optimizing the label space across a hierarchy of segments with constraints related to structural, spatial and data-dependent pairwise relationships between regions. Our results show that such strategy provide better regularization than a series of strong baselines reflecting state-of-the-art technologies. The proposed strategy offers a flexible and principled framework to include several sources of visual and structural information, while allowing for different degrees of spatial regularization accounting for priors about the expected output structures. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Aerial imagery; Conditional random fields; Convolutional neural networks; Decimeter resolution; Multi-task learning; Semantic boundary detection; Semantic segmentation","Aerial photography; Antennas; Convolution; Deep learning; Network architecture; Neural networks; Random processes; Semantics; Aerial imagery; Conditional random field; Convolutional neural network; Multitask learning; Semantic boundary; Semantic segmentation; Image segmentation; algorithm; detection method; numerical model; satellite data; satellite imagery; segmentation; spatial resolution"
"Dual-dense convolution network for change detection of high-resolution panchromatic imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054536250&doi=10.3390%2fapp8101785&partnerID=40&md5=51e2444d9e2ff3e856c12dd519658030","This paper presents a robust change detection algorithm for high-resolution panchromatic imagery using a proposed dual-dense convolutional network (DCN). In this work, a joint structure of two deep convolutional networks with dense connectivity in convolution layers is designed in order to accomplish change detection for satellite images acquired at different times. The proposed network model detects pixel-wise temporal change based on local characteristics by incorporating information from neighboring pixels. Dense connection in convolution layers is designed to reuse preceding feature maps by connecting them to all subsequent layers. Dual networks are incorporated by measuring the dissimilarity of two temporal images. In the proposed algorithm for change detection, a contrastive loss function is used in a learning stage by running over multiple pairs of samples. According to our evaluation, we found that the proposed framework achieves better detection performance than conventional algorithms, in area under the curve (AUC) of 0.97, percentage correct classification (PCC) of 99%, and Kappa of 69, on average. © 2018 by the authors.","Change detection; Convolutional network; Deep learning; Panchromatic; Remote sensing",
"Deep extraction of cropland parcels from very high-resolution remotely sensed imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055862836&doi=10.1109%2fAgro-Geoinformatics.2018.8476002&partnerID=40&md5=37ff1fb0ac6bb7eb15484e796fbb99c9","Extracting cropland parcels from high resolution remote sensing images is a basic task for precision agriculture and other fields. Object based image analysis rely heavily on segmentation methods and can't satisfy the parcels' requisition in most situation. Inspired by the recent remarkable improvement on image understanding with deep learning, we propose a deep-edge guided method for cropland parcels extraction. Focus on the boundaries of these parcels, hard edge and soft edge are extracted respectively with U-Net and RCF model. Then all edges with the land type of cropland are constructed into parcels. At last accurate cropland-parcels are achieved. © 2018 IEEE.","cropland parcels; deep learning; high-resolution remote sensing; semantic segmentation","Extraction; Image enhancement; Image segmentation; Remote sensing; Semantics; cropland parcels; High resolution remote sensing; High resolution remote sensing images; Land types; Object based image analysis; Segmentation methods; Semantic segmentation; Very high resolution; Deep learning"
"Digital mapping of soil available phosphorus supported by AI technology for precision agriculture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055876987&doi=10.1109%2fAgro-Geoinformatics.2018.8476007&partnerID=40&md5=ce9dd9aa17385681cc7ca2bc60540101","Precision agriculture has been proposed to improve the sustainability of agriculture and solve the environmental pollution of soil. In precision agriculture process, the management of water and fertilizer is carried out on agricultural operation units. Therefore, acquisition of accurate soil nutrient distribution information is a key step for precision agriculture application and digital soil mapping is an effective technology. Significant progress has been made in digital soil mapping over the past 20 years. However, the current digital soil mapping framework was implemented based on grids, which was not consistent with the operation units of precision agriculture. This paper proposed a geo-parcel based digital soil mapping framework on the support of artificial intelligence technology for precision agriculture application. Two key technologies were studied for the implementation of this framework. Geo-parcels automatic extraction was the basis of this method, and a modified VGG 16 network was used for geo-parcels' accurate boundary extraction from high resolution images. Different machine learning methods were attempted to construct the relationship between soil available phosphorus and environment on geo-parcels. We chose an agricultural region in Zhongning County, Ningxia Province as the study area, and the new digital soil mapping framework was applied for soil available phosphorus mapping. This research showed that geo-parcel based digital mapping method could reduce the number of prediction units more than 50% for fine soil mapping, and effectively improve the prediction and application efficiency. This study was an attempt to realize soil mapping based on agricultural operation units for precision agriculture application. The high resolution remote sensing images provide basic data for the realization of this idea and the development of AI technology provides technical support for it. In the future, we will carry out experiments in larger areas to further optimize this method and key technologies for the applications in more complex environments. © 2018 IEEE.","deep learning; geo-parcels; precision agriculture; soil available phosphorus; soil mapping","Artificial intelligence; Computer graphics; Deep learning; Extraction; Mapping; Phosphorus; Precision agriculture; Remote sensing; Soil surveys; Sustainable development; Water management; Artificial intelligence technologies; Available phosphorus; Environmental pollutions; geo-parcels; High resolution remote sensing images; Machine learning methods; Soil mapping; Soil nutrient distributions; Soils"
"THE SEN1-2 DATASET for DEEP LEARNING in SAR-OPTICAL DATA FUSION","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056307335&doi=10.5194%2fisprs-annals-IV-1-141-2018&partnerID=40&md5=e930093d753d503c4ce1e8f276132f0b","While deep learning techniques have an increasing impact on many technical fields, gathering sufficient amounts of training data is a challenging problem in remote sensing. In particular, this holds for applications involving data from multiple sensors with heterogeneous characteristics. One example for that is the fusion of synthetic aperture radar (SAR) data and optical imagery. With this paper, we publish the SEN1-2 dataset to foster deep learning research in SAR-optical data fusion. SEN1-2 comprises 282;384 pairs of corresponding image patches, collected from across the globe and throughout all meteorological seasons. Besides a detailed description of the dataset, we show exemplary results for several possible applications, such as SAR image colorization, SAR-optical image matching, and creation of artificial optical images from SAR input data. Since SEN1-2 is the first large open dataset of this kind, we believe it will support further developments in the field of deep learning for remote sensing as well as multi-sensor data fusion. © Authors 2018.","data fusion; deep learning; optical remote sensing; Sentinel-1; Sentinel-2; Synthetic aperture radar (SAR)",
"ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060420156&partnerID=40&md5=25dca186baaafd905b9bee3755c13d49","The proceedings contain 18 papers. The topics discussed include: short term urban traffic forecasting using deep learning; a novel model and tool for energy renovation planning in French residential buildings and districts; a decision support tool on derelict buildings for urban regeneration; defining semantic levels of detail for indoor maps; computing feedback for citizens' proposals in participative urban planning; modeling below- and above-ground utility network features with the CITYGML utility network ADE: experiences from Rotterdam; mobility atlas booklet: an urban dashboard design and implementation; interactive spatial web-applications as new means of support for urban decision-making processes; city profile: using smart data to create digital urban spaces; adaptive traffic light cycle time controller using microcontrollers and crowdsource data of Google APIS for developing countries; smart city WEBGIS applications: proof of work concept for high-level quality-of-service assurance; and identifying pedestrian movement behavior using object detection methods and land-use agglomeration analysis.",,
"Dwelling extraction in refugee camps using CNN - First experiences and lessons learnt","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056199231&doi=10.5194%2fisprs-archives-XLII-1-161-2018&partnerID=40&md5=563dd2e30b3d4c076b5a45123c9e354a","There is a growing use of Earth observation (EO) data for support planning in humanitarian crisis response. Information about number and dynamics of displaced population in camps is essential to humanitarian organizations for decision-making and action planning. Dwelling extraction and categorisation is a challenging task, due to the problems in separating different dwellings under different conditions, with wide range of sizes, colour and complex spatial patterns. Nowadays, so-called deep learning techniques such as deep convolutional neural network (CNN) are used for understanding image content and object recognition. Although recent developments in the field of computer vision have introduced CNN networks as a practical tool also in the field of remote sensing, the training step of these techniques is rather time-consuming and samples for the training process are rarely transferable to other application fields. These techniques also have not been fully explored for mapping camps. Our study analyses the potential of a CNN network for dwelling extraction to be embedded as initial step in a comprehensive object-based image analysis (OBIA) workflow. The results were compared to a semi-automated, i.e. combined knowledge-/sample-based, OBIA classification. The Minawao refugee camp in Cameroon served as a case study due to its well-organised, clearly distinguishable dwelling structure. We use manually delineated objects as initial input for the training samples, while the CNN network is structured with two convolution layers and one max pooling. © Authors 2018. CC BY 4.0 License.","(Semi)-automated object-based image analysis (OBIA); Camp dwellings extraction; Convolutional neural network (CNN)","Automation; Convolution; Decision making; Deep neural networks; Extraction; Housing; Neural networks; Object recognition; Remote sensing; Application fields; Convolutional Neural Networks (CNN); Deep convolutional neural networks; Earth observation data; Learning techniques; Object based image analysis (OBIA); Spatial patterns; Training process; Image analysis"
"A deep learning study of extracting navigation area from CAD blueprints","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056274641&doi=10.5194%2fisprs-annals-IV-4-155-2018&partnerID=40&md5=ba524e173126e6e4dc605b99bbfdcc93","Deep learning technology is a cutting edge topic of AI region, and draws more attention from photogrammetry and remote sensing society. In this study, we strive to combine deep learning with CAD designs to extract navigation area (room). To this, we mark more than 200 2D building blueprint in CAD forms to construct the learning set to train object detection model based on TensorFlow. This model is the faster R-CNN inception v2 model from COCO dataset. The test and result section is composed of three parts: First part demonstrates the model performance on learning dataset; second part applies the generated model to extract rooms from untrained raw CAD blueprints; Third part covers the comparison between deep learning extracted result and geometric based algorithm extracted result. Test result shows that the deep learning approach could achieve higher accuracy than geometric approach under regular shape situations. In conclusion, we have proposed a well-trained deep learning model that could be utilized to construct a schema of the navigation area for 2D CAD blueprints. © 2018 Auhtors.","CAD blueprints; Deep learning; Extraction; Navigation area",
"A super-resolution method-based pipeline for fundus fluorescein angiography imaging 08 Information and Computing Sciences 0801 Artificial Intelligence and Image Processing Robert Koprowski","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053714092&doi=10.1186%2fs12938-018-0556-7&partnerID=40&md5=412dedfabfc91817b6e57bf5af308ebb","Background: Fundus fluorescein angiography (FFA) imaging is a standard diagnostic tool for many retinal diseases such as age-related macular degeneration and diabetic retinopathy. High-resolution FFA images facilitate the detection of small lesions such as microaneurysms, and other landmark changes, in the early stages; this can help an ophthalmologist improve a patient's cure rate. However, only low-resolution images are available in most clinical cases. Super-resolution (SR), which is a method to improve the resolution of an image, has been successfully employed for natural and remote sensing images. To the best of our knowledge, no one has applied SR techniques to FFA imaging so far. Methods: In this work, we propose a SR method-based pipeline for FFA imaging. The aim of this pipeline is to enhance the image quality of FFA by using SR techniques. Several SR frameworks including neighborhood embedding, sparsity-based, locally-linear regression and deep learning-based approaches are investigated. Based on a clinical FFA dataset collected from Second Affiliated Hospital to Xuzhou Medical University, each SR method is implemented and evaluated for the pipeline to improve the resolution of FFA images. Results and conclusion: As shown in our results, most SR algorithms have a positive impact on the enhancement of FFA images. Super-resolution forests (SRF), a random forest-based SR method has displayed remarkable high effectiveness and outperformed other methods. Hence, SRF should be one potential way to benefit ophthalmologists by obtaining high-resolution FFA images in a clinical setting. © 2018 The Author(s).","Convolutional network; Fundus fluorescein angiography imaging; Machine learning; Random forest; Super-resolution","Angiography; Artificial intelligence; Decision trees; Deep learning; Eye protection; Learning systems; Medical imaging; Ophthalmology; Optical resolving power; Pipelines; Remote sensing; Age-related macular degeneration; Convolutional networks; Fluorescein angiography; Learning-based approach; Low resolution images; Random forests; Super resolution; Superresolution methods; Image enhancement; algorithm; Article; China; deep learning based approach; feasibility study; fluorescence angiography; fundus fluorescein angiography; image quality; image resolution; imaging and display; locally linear regression approach; machine learning; neighborhood embedding approach; priority journal; quantitative analysis; random forest; sparsity based approach; super resolution method; diagnostic imaging; eye; eye fundus; fluorescence angiography; human; image processing; procedures; statistical model; Deep Learning; Eye; Fluorescein Angiography; Fundus Oculi; Humans; Image Processing, Computer-Assisted; Linear Models"
"Improved classification of satellite imagery using spatial feature maps extracted from social media","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056156143&doi=10.5194%2fisprs-archives-XLII-4-335-2018&partnerID=40&md5=560691f38ae7cd5c850d35453055a8dc","In this work, we consider the exploitation of social media data in the context of Remote Sensing and Spatial Information Sciences. To this end, we explore a way of augmenting and integrating information represented by geo-located feature vectors into a system for the classification of satellite images. For that purpose, we present a quite general data fusion framework based on Convolutional Neural Network (CNN) and an initial examination of our approach on features from geo-located social media postings on the Twitter and Sentinel images. For this examination, we selected six simple Twitter features derived from the metadata, which we believe could contain information for the spatial context. We present initial experiments using geotagged Twitter data from Washington DC and Sentinel images showing this area. The goal of classification is to determine local climate zones (LCZ). First, we test whether our selected feature maps are not correlated with the LCZ classification at the geo-tag position. We apply a simple boost tree classifier on this data. The result turns out not to be a mere random classifier. Therefore, this data can be correlated with LCZ. To show the improvement by our method, we compare classification with and without the Twitter feature maps. In our experiments, we apply a standard pixel-based CNN classification of the Sentinel data and use it as a baseline model. After that, we expand the input augmenting additional Twitter feature maps within the CNN and assess the contribution of these additional features to the overall F1-score of the classification, which we determine from spatial cross-validation. © Authors 2018.","Classification; Data fusion; Deep learning; Satellite images; Social media mining","Data fusion; Deep learning; Image classification; Image enhancement; Image fusion; Neural networks; Remote sensing; Satellite imagery; Social networking (online); Trees (mathematics); Convolutional Neural Networks (CNN); Integrating information; Satellite images; Social media datum; Social media minings; Spatial cross validations; Spatial features; Spatial information science; Classification (of information)"
"Transfer learning for soil spectroscopy based on convolutional neural networks and its application in soil clay content mapping using hyperspectral imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053929070&doi=10.3390%2fs18093169&partnerID=40&md5=66d8d8850205170dcb80c47a3486e325","Soil spectra are often measured in the laboratory, and there is an increasing number of large-scale soil spectral libraries establishing across the world. However, calibration models developed from soil libraries are difficult to apply to spectral data acquired from the field or space. Transfer learning has the potential to bridge the gap and make the calibration model transferrable from one sensor to another. The objective of this study is to explore the potential of transfer learning for soil spectroscopy and its performance on soil clay content estimation using hyperspectral data. First, a one-dimensional convolutional neural network (1D-CNN) is used on Land Use/Land Cover Area Frame Survey (LUCAS) mineral soils. To evaluate whether the pre-trained 1D-CNN model was transferrable, LUCAS organic soils were used to fine-tune and validate the model. The finetuned model achieved a good accuracy (coefficient of determination (R2) = 0.756, root-mean-square error (RMSE) = 7.07 and ratio of percent deviation (RPD) = 2.26) for the estimation of clay content. Spectral index, as suggested as a simple transferrable feature, was also explored on LUCAS data, but did not performed well on the estimation of clay content. Then, the pre-trained 1D-CNN model was further fine-tuned by field samples collect in the study area with spectra extracted from HyMap imagery, achieved an accuracy of R2 = 0.601, RMSE = 8.62 and RPD = 1.54. Finally, the soil clay map was generated with the fine-tuned 1D-CNN model and hyperspectral data. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","CNNs; Deep learning; Hyperspectral imagery; Soil spectroscopy; Transfer learning","Convolution; Deep learning; Land use; Libraries; Mean square error; Neural networks; Remote sensing; Spectroscopy; CNNs; Coefficient of determination; Convolutional neural network; Hyper-spectral imageries; Land use/land cover; Root mean square errors; Soil spectroscopies; Transfer learning; Soils"
"Hough Transform Guided Deep Feature Extraction for Dense Building Detection in Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054220029&doi=10.1109%2fICASSP.2018.8461407&partnerID=40&md5=16a8b3f9c75134642186c3979429ba40","Detecting dense buildings without elevation information is an important and challenging task in remote sensing applications. In this paper, we present a novel cascaded deep neural network architecture, incorporating multi-stage region proposal detection and Hough transform to obtain better mid-level semantic information for man-made objects. This proposed network can be trained end-to-end by multi-loss jointly. We train and test it on a large building dataset collected from Google Earth, including buildings from urban, suburban and rural areas. Experiments demonstrate great robustness and superiority of our method to various buildings over other convolutional neural network (CNN) based detection methods. © 2018 IEEE.","Building detection; CNN; Deep learning; Hough transform; Remote sensing",
"Road Extraction from Multi-Source High-Resolution Remote Sensing Image Using Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063500524&doi=10.1109%2fICALIP.2018.8455367&partnerID=40&md5=8822979ca8044247f2b65a341230046e","The traditional network information update depends on the field measurement or artificial interpretation of surveying and mapping in remote sensing image. The shortcomings are obvious: high cost, long cycle, and consume a large amount of manpower. In order to solve this problem, we use the convolution neural network in the deep learning to complete road information extraction from high resolution image. The road training database combines two kinds of high resolution remote sensing data which are the GaoFen-2 and World View. Two different training models are used to compare the results. Furthermore, the results of the two models are combined to obtain more accurate and improved road extraction results. © 2018 IEEE.","convolutional networks; multi-source; remote sensing; road extracion","Convolution; Deep learning; Feature extraction; Neural networks; Remote sensing; Roads and streets; Convolution neural network; Convolutional networks; Convolutional neural network; High resolution remote sensing; High resolution remote sensing images; Multi-Sources; road extracion; Road information extractions; Image processing"
"Photon-limited face image super-resolution based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052695171&doi=10.1364%2fOE.26.022773&partnerID=40&md5=d471410df61adb6d655578b7205d1523","With one single photon camera (SPC), imaging under ultra weak-lighting conditions may have wide-ranging applications ranging from remote sensing to night vision, but it may seriously suffer from the problem of under-sampled inherent in SPC detection. Some approaches have been proposed to solve the under-sampled problem by detecting the objects many times to generate high-resolution images and performing noise reduction to suppress the Poission noise inherent in low-flux operation. To address the under-sampled problem more effectively, a new approach is developed in this paper to reconstruct high-resolution images with lower-noise by seamlessly integrating low-light-level imaging with deep learning. In our new approach, all the objects are detected only once by SPC, where a deep network is learned to reduce noise and reconstruct high-resolution images from the detected noisy under-sampled images. In order to demonstrate our proposal is feasible, we first select a special category to verify by experiment, which are human faces. Such deep network is able to recover high-resolution and lower-noise face images from new noisy under-sampled face images and the resolution can achieve 4x up-scaling factor. Our experimental results have demonstrated that our proposed method can generate high-quality images from only ~0.2 detected signal photon per pixel. © 2018 Optical Society of America.",,"Image denoising; Image reconstruction; Object detection; Particle beams; Photons; Remote sensing; High quality images; High resolution; High resolution image; Low light level imaging; New approaches; Photon limiteds; Single photons; Wide-ranging applications; Deep learning; article; deep learning; face; human; noise; photon"
"A Survey of GPU Implementations for Hyperspectral Image Classification in Remote Sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063256586&doi=10.1080%2f07038992.2018.1559725&partnerID=40&md5=d4f4bcabe23b73787f8c174cb50b125b","Effective classification algorithm is a key to extracting interesting and useful information from hyperspectral images (HSI). Many researchers have worked on developing effective algorithms for HSI classifications and research is still ongoing to improve on the existing algorithms. HSI classification is a complex task due to the nature of the data involved and external factors that affect the accuracy of the classification results. Due to the complexity of this problem and the enormous computing time involved, researchers have focused their work on developing parallel algorithms. GPU-accelerated computing is the use of a graphics processing unit (GPU) together with a CPU to accelerate deep learning and other computing intensive applications. The general purpose graphic processing units have been considered as one of the most common co-processors that can help accelerate parallel applications. Implementation of parallel algorithms on GPU has significantly improved the classification of hyperspectral images. This paper is focused on the study of the available GPU implementations. It examines the performance, summarizes the major developments and concerns in the research work. It also describes the major challenges in GPU implementations for HIS. © 2019, Copyright © CASI.",,"Classification (of information); Computer graphics; Computer graphics equipment; Data mining; Deep learning; Image classification; Image coding; Image enhancement; Parallel algorithms; Program processors; Remote sensing; Spectroscopy; Classification algorithm; Classification results; Effective algorithms; External factors; General purpose graphic processing units; GPU implementation; GPU-accelerated; Parallel application; Graphics processing unit"
"Implications of remote sensing data analysis using deep learning techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059991854&doi=10.1166%2fjctn.2018.7553&partnerID=40&md5=8f854400a20055f87797b8dc4df7ddcc","The fastest growing trend in data analysis is Deep Learning Techniques (DL). Machine learning techniques are becoming increasingly important and outperforms in many situations, e.g., image or speech processing. One of the most important deep learning architecture is represented by Convolution Neural Network (CNN). The purpose of this paper is to give a overview of remote sensing image processing using deep learning and discuss the challenges in recent development of deep learning for remote sensing data, and acknowledge the information to make deep learning an implicit model to handle the large scale data processing as a advantageous challenge like analyzing urbanization, climate change, vegetation and many more. Copyright © 2018 American Scientific Publishers.","Convolution neural networks; Deep Learning; Earth observation; Remote sensing",
"Road extraction from high-resolution remote sensing imagery using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053594697&doi=10.3390%2frs10091461&partnerID=40&md5=c1e97776a7116d5b9230553319534e13","The road network plays an important role in the modern traffic system; as development occurs, the road structure changes frequently. Owing to the advancements in the field of high-resolution remote sensing, and the success of semantic segmentation success using deep learning in computer version, extracting the road network from high-resolution remote sensing imagery is becoming increasingly popular, and has become a new tool to update the geospatial database. Considering that the training dataset of the deep convolutional neural network will be clipped to a fixed size, which lead to the roads run through each sample, and that different kinds of road types have different widths, this work provides a segmentation model that was designed based on densely connected convolutional networks (DenseNet) and introduces the local and global attention units. The aim of this work is to propose a novel road extraction method that can efficiently extract the road network from remote sensing imagery with local and global information. A dataset from Google Earth was used to validate the method, and experiments showed that the proposed deep convolutional neural network can extract the road network accurately and effectively. This method also achieves a harmonic mean of precision and recall higher than other machine learning and deep learning methods. © 2018 by the authors.","Deep learning; Global attention; High resolution; Pyramid attention; Road network extraction","Convolution; Deep learning; Deep neural networks; Extraction; Feature extraction; Neural networks; Roads and streets; Semantics; Traffic control; Deep convolutional neural networks; Global attention; High resolution; High resolution remote sensing; High resolution remote sensing imagery; Pyramid attention; Road extraction method; Road network extraction; Remote sensing"
"Remote sensing image fusion based on deep learning non-subsampled shearlet [结合深度学习的非下采样剪切波遥感图像融合]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058460507&doi=10.5768%2fJAO201839.0502001&partnerID=40&md5=fb15cee57f61cb1455ef60dc4ed61fae","Remote-sensing image fusion refers to the method of selectively and strategically combining image information with different observation characteristics obtained by different sensors to obtain a new image with better observation characteristics. A deep-sensing image fusion algorithm combined with non-subsampled shearlet transform (NSST) was proposed. In this algorithm, the spatial resolution of multi-spectral (MS) image is enhanced by an improved super-resolution reconstruction network. The panchromatic (PAN) image histogram-matched refers to each component of the reconstructed MS image. And the corresponding channel image is subjected to NSST transformation to obtain low-frequency sub-bands and several high-frequency direction sub-bands, respectively. To obtain low-frequency fusion coefficient, the low-frequency region uses an adaptive weighted average rule based on the gradient region, while the high-frequency sub-bands adopt the local spatial frequency maximum rule to obtain the high-frequency fusion coefficient, and finally the fused image can be obtained by inverse NSST transform reconstruction. The MS images City and Inland in different datasets were upsampled by the bicubic interpolation method. With the proposed algorithm, the general image quality index (UIQI) was 0.988 6 and 0.932 1, respectively, and the spectral angle mapping (SAM) was 1.872 1 and 2.143 2, respectively. Experimental results show that the image structure of the fusion algorithm in this paper is more clear, the saved spectral information is more complete, the fusion quality is better than the contrast algorithm, and the fusion image is more conducive to human visual observation. © 2018, Editorial Board, Journal of Applied Optics. All right reserved.","Deep learning; NSST; Remote sensing image fusion; Super-resolution reconstruction",
"Study on the Combined Application of CFAR and Deep Learning in Ship Detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048560790&doi=10.1007%2fs12524-018-0787-x&partnerID=40&md5=5f050cd0f6779358a1eeed5f78a10420","To maintain national socio-economic development and maritime rights and interests, it is necessary to obtain the space location information of various ships. Therefore, it is important to detect the locations of ships accurately and rapidly. At present, ship detection is mainly carried out by combining satellite remote sensing imaging with constant false alarm rate (CFAR) detection. However, with the rapid development of satellite remote sensing technology, remote sensing data have gradually begun to show the characteristics of “big data”; additionally, the accuracy and speed of ship detection can be improved by analysing big data, such as by deep learning. Thus, a ship detection algorithm that combines CFAR and CNN is proposed based on the CFAR global detection algorithm and image recognition with the CNN model. Compared with the multi-level CFAR algorithm that is based on multithreading, the algorithm in this paper is more suitable for application to ship detection systems. © 2018, The Author(s).","CFAR; CNN; Deep learning; Ship detection","accuracy assessment; algorithm; artificial neural network; detection method; imaging method; remote sensing; satellite altimetry; socioeconomic conditions"
"Toward Ultralightweight Remote Sensing with Harmonic Lenses and Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050976167&doi=10.1109%2fJSTARS.2018.2856538&partnerID=40&md5=8126e5246323285800b0d6627d81bb4b","In this paper, we describe our advances in manufacturing a 256-layer 7-μm thick harmonic lens with 150 and 300 mm focal distances combined with color correction, deconvolution, and a feedforwarding deep learning neural network capable of producing images approaching photographic visual quality. While reconstruction of images taken with diffractive optics was presented in previous works, this paper is the first to use deep neural networks during the restoration step. The level of imaging quality we achieved with our imaging system can facilitate the emergence of ultralightweight remote sensing cameras for nano- and pico-satellites, and for aerial remote sensing systems onboard small UAVs and solar-powered airplanes. © 2008-2012 IEEE.","Color correction; deconvolution; deep learning; harmonic lens; point spread function (PSF) estimation; remote sensing","Antennas; Deconvolution; Deep learning; Deep neural networks; Harmonic analysis; Lenses; Neural networks; Optical transfer function; Solar energy; Aerial remote sensing; Color correction; Convolutional neural network; Focal distances; Learning neural networks; Remote sensing cameras; Ultra lightweights; Visual qualities; Remote sensing; artificial neural network; correction; estimation method; imaging method; photography; remote sensing; unmanned vehicle"
"Remote Sensing Image Super-Resolution using Multi-Scale Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082131511&doi=10.1109%2fUCMMT45316.2018.9015801&partnerID=40&md5=776b512048330c47615174be34b49eda","Remote sensing images have advantages in large-area imaging and macroscopic integrity. However, in most commercial applications, further recognition and processing becomes difficult due to the low spatial resolution of the acquired images. Therefore, improving the resolution of remote sensing images has important practical significance. To solve this problem, we propose a remote sensing image super-resolution method based on deep learning technology. In order to obtain more detailed image information, we introduce multi-scale convolution to implement feature extraction and deconvolution be used to achieve the final 3× image reconstruction without bicubic interpolation. Experimental results show that our network achieves better performance than prior art methods and visual improvement of our results is easily noticeable. © 2018 IEEE.",,"Arts computing; Convolution; Convolutional neural networks; Deep learning; Image enhancement; Image reconstruction; Millimeter waves; Optical resolving power; Terahertz waves; Bicubic interpolation; Commercial applications; Detailed images; Learning technology; Prior arts; Remote sensing images; Spatial resolution; Visual improvements; Remote sensing"
"Evaluation of the effect of feature extraction strategy on the performance of high-resolution remote sensing image scene classification [特征提取策略对高分辨率遥感图像场景分类性能影响的评估]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054837402&doi=10.11834%2fjrs.20188015&partnerID=40&md5=2c311a1ca35c45fd86d52158784b9a88","Remote sensing image scene classification aims to tag remote sensing images with semantic categories according to the content of the image and is important in disaster monitoring, environmental detection, and urban planning. Scene classification results can provide valuable information about object recognition and image retrieval and can effectively improve the performance of image interpretation. The general process of remote sensing image scene classification mainly consists of feature extraction and scene classification based on image features. Given that the design of classifiers is relatively mature, this work focuses on feature extraction strategy. The influence of various strategies on the performance of scene classification is short of unified evaluation, which limits its development. The effect of various feature extraction strategies on the performance of high-resolution remote sensing image scene classification is evaluated in this study. In the second section of this paper, existing feature extraction strategies are divided into two categories: (1) hand-designed and (2) data-driven feature extraction. Hand-designed features, such as Color Histograms (CH) and Scale Invariant Feature Transform (SIFT), provide the primary description of images and are presented in the early period. Further abstract description of the images is introduced by coding of hand-designed features, such as Bag of Visual Words (BoVW) and has higher classification accuracy than hand-designed features. However, these feature extraction strategies generally suffer from poor generalization capability due to specific requirements for designing. Furthermore, hand-designed features require significant domain knowledge. By contrast, data-driven features can directly learn powerful features from a large number of sample images and are generally divided into shallow and deep learning features. Shallow learning feature extraction mainly involves Principal Component Analysis (PCA), Independent Component Analysis (ICA), and sparse coding algorithms. Typical deep learning feature extraction strategies include stacked autoencoder (SAE), Deep Belief Network (DBN), and Convolutional Neural Network (CNN). Compared with deep learning models, shallow learning models can be regarded as a neural network with a single hidden layer and thus cannot capture high-level semantic features. The superiority of deep learning features is obvious when dealing with complex scene classification. Furthermore, CNN-based features exhibit improved performance compared with SAE- and DBN-based features because the one-dimensional structure of SAE and DBN destroys the spatial information of images. In the third section of this paper, 29 feature descriptors are quantitatively compared in UC Merced, AID, and NWPU RESISC-45 datasets and eight combinations of feature descriptors are quantitatively compared in the NWPU RESISC-45 dataset. The effect of different feature extraction strategies on the performance of scene classification and the complexity of each dataset are evaluated through quantitative comparison. The experimental results are as follows. (1) The classification accuracy and stability of hand-designed features is poor, however the efficiency of most features is satisfactory and can attain better performance by combining with other types of features. (2) Among all feature extraction strategies, the coding of hand-designed features possesses moderate levels of classification accuracy, efficiency, and stability. (3) The classification accuracy and stability of data-driven features are best, but most of them have low efficiency. (4) AlexNet, a deep learning model with few layers, exhibits the best comprehensive performance and is suitable for occasions that require high classification accuracy, efficiency, and stability. (5) Some scene classes belonging to land use type are easy to be confused because of similar landmark buildings or sites. Moreover, some scene classes belonging to land cover type are easy to be confused because of their similar geomorphologic features. (6) The recently proposed NWPU RESISC-45 dataset is more complex than the other datasets and is more challenging for scene classification algorithms. Finally, the summary and conclusion of this paper are presented, and the discussion of future development is provided. On the one hand, combining prior knowledge introduced by hand-designed features with the CNN model may be one of the future development directions. On the other hand, introducing Generative Adversarial Networks (GAN) into CNN training may be a research hotspot in the future. In addition, remote sensing parameters, such as NDVI and NDWI, and multi-spectral information can be integrated with current feature extraction strategies for practical applications. © 2018, Science Press. All right reserved.","Data driven features; Deep learning; Feature extraction strategy; Hand-designed features; High-resolution; Scene classification","Abstracting; Codes (symbols); Complex networks; Data mining; Deep learning; Efficiency; Extraction; Feature extraction; Image classification; Image coding; Image enhancement; Image retrieval; Independent component analysis; Land use; Neural networks; Object recognition; Principal component analysis; Remote sensing; Semantics; Stability; Convolutional Neural Networks (CNN); Data driven; Hand-designed features; High resolution; High resolution remote sensing images; Independent component analyses (ICA); Scale invariant feature transforms; Scene classification; Classification (of information)"
"Detecting building edges from high spatial resolution remote sensing imagery using richer convolution features network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053608743&doi=10.3390%2frs10091496&partnerID=40&md5=9335825952dc800ab8368c618e9a4269","As the basic feature of building, building edges play an important role in many fields such as urbanization monitoring, city planning, surveying and mapping. Building edges detection from high spatial resolution remote sensing (HSRRS) imagery has always been a long-standing problem. Inspired by the recent success of deep-learning-based edge detection, a building edge detection model using a richer convolutional features (RCF) network is employed in this paper to detect building edges. Firstly, a dataset for building edges detection is constructed by the proposed most peripheral constraint conversion algorithm. Then, based on this dataset the RCF network is retrained. Finally, the edge probability map is obtained by RCF-building model, and this paper involves a geomorphological concept to refine edge probability map according to geometric morphological analysis of topographic surface. The experimental results suggest that RCF-building model can detect building edges accurately and completely, and that this model has an edge detection F-measure that is at least 5% higher than that of other three typical building extraction methods. In addition, the ablation experiment result proves that using the most peripheral constraint conversion algorithm can generate more superior dataset, and the involved refinement algorithm shows a higher F-measure and better visual effect contrasted with the non-maximal suppression algorithm. © 2018 by the authors.","Building edges detection; High spatial resolution remote sensing imagery; Richer convolution features","Buildings; Convolution; Deep learning; Edge detection; Image resolution; Remote sensing; Ablation experiments; Conversion algorithm; High spatial resolution; Morphological analysis; Non-maximal suppressions; Refinement algorithms; Remote sensing imagery; Richer convolution features; Feature extraction"
"ERN: Edge loss reinforced semantic segmentation network for remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053633005&doi=10.3390%2frs10091339&partnerID=40&md5=72e67c21c70a40e64be08ee6b2b36e32","The semantic segmentation of remote sensing images faces two major challenges: high inter-class similarity and interference from ubiquitous shadows. In order to address these issues, we develop a novel edge loss reinforced semantic segmentation network (ERN) that leverages the spatial boundary context to reduce the semantic ambiguity. The main contributions of this paper are as follows: (1) we propose a novel end-to-end semantic segmentation network for remote sensing, which involves multiple weighted edge supervisions to retain spatial boundary information; (2) the main representations of the network are shared between the edge loss reinforced structures and semantic segmentation, which means that the ERN simultaneously achieves semantic segmentation and edge detection without significantly increasing the model complexity; and (3) we explore and discuss different ERN schemes to guide the design of future networks. Extensive experimental results on two remote sensing datasets demonstrate the effectiveness of our approach both in quantitative and qualitative evaluation. Specifically, the semantic segmentation performance in shadow-affected regions is significantly improved. © 2018 by the authors.","CNN; Deep learning; Edge loss reinforced network; Remote sensing; Semantic segmentation","Deep learning; Edge detection; Image segmentation; Reinforcement; Semantic Web; Semantics; Boundary information; Future networks; Model complexity; Qualitative evaluations; Reinforced structures; Remote sensing images; Semantic ambiguities; Semantic segmentation; Remote sensing"
"Transfering Super Resolution Convolutional Neural Network for Remote Sensing Data Sharpening","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073894401&doi=10.1109%2fWHISPERS.2018.8747223&partnerID=40&md5=8f873bf4f7e3d14140cffc88170992b2","Pansharpening process aims at fusing low-spatial/high-spectral resolutions multispectral/hyperspectral (MS/HS) remote sensing data with high-spatial resolution and without spectral diversity panchromatic (PAN) ones.This paper explores different data preparation possibilities, learning strategies and architectures, used in the convolutional neural network (CNN) approaches, for improving the performance of the pansharpening process of remote sensing MS/HS data.Also, in this paper, the super resolution CNN (SRCNN) architecture is adapted by adding a normalization step in the training phase of the CNN-based pansharpening process. Then, training datasets are prepared for fitting the generalization need.Experiments based on multi-source datasets are performed to evaluate the performance of the proposed SRCNN-based pansharpening architecture. The preliminary results are promising since they show that the proposed approach is competitive with some literature methods. © 2018 IEEE.","convolutional neural networks; deep learning; fusion; Multispectral/Hyperspectral imaging; pansharpening; super-resolution","Convolution; Deep learning; Deep neural networks; Fusion reactions; Image processing; Network architecture; Neural networks; Optical resolving power; Spectroscopy; Convolutional neural network; High spatial resolution; Multi-spectral; Pan-sharpening; Remote sensing data; Spectral diversity; Super resolution; Training data sets; Remote sensing"
"A multiple-feature reuse network to extract buildings from remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053638861&doi=10.3390%2frs10091350&partnerID=40&md5=16ca7dc865589b9ad51b835c4f088547","Automatic building extraction from remote sensing imagery is important in many applications. The success of convolutional neural networks (CNNs) has also led to advances in using CNNs to extract man-made objects from high-resolution imagery. However, the large appearance and size variations of buildings make it difficult to extract both crowded small buildings and large buildings. High-resolution imagery must be segmented into patches for CNN models due to GPU memory limitations, and buildings are typically only partially contained in a single patch with little context information. To overcome the problems involved when using different levels of image features with common CNN models, this paper proposes a novel CNN architecture called a multiple-feature reuse network (MFRN) in which each layer is connected to all the subsequent layers of the same size, enabling the direct use of the hierarchical features in each layer. In addition, the model includes a smart decoder that enables precise localization with less GPU load. We tested our model on a large real-world remote sensing dataset and obtained an overall accuracy of 94.5% and an 85% F1 score, which outperformed the compared CNN models, including a 56-layer fully convolutional DenseNet with 93.8% overall accuracy and an F1 score of 83.5%. The experimental results indicate that the MFRN approach to connecting convolutional layers improves the performance of common CNN models for extracting buildings of different sizes and can achieve high accuracy with a consumer-level GPU. © 2018 by the authors.","Building extraction; CNN; Deep learning; FCN","Buildings; Convolution; Deep learning; Extraction; Neural networks; Automatic building extraction; Building extraction; Context information; Convolutional neural network; Extracting buildings; Hierarchical features; High resolution imagery; Remote sensing imagery; Remote sensing"
"Object Detection in Remote Sensing Imagery with Multi-scale Deformable Convolutional Networks [基于多尺度形变特征卷积网络的高分辨率遥感影像目标检测]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055114992&doi=10.11947%2fj.AGCS.2018.20170595&partnerID=40&md5=0bfe7dc77ce1fadb75c60d91e3c4b23d","Traditional target detection methods based on sliding window search paradigm and hand-craft based features are difficult to be applied to the multi-class target detection of very-high-resolution remote sensing images. In this paper, we proposed a deformable convolutional networks based multi-class target detection method by introducing deformable convolution layer and deformable RoI (Region-of-Interest) pooling layer. Specially, our method consists of two sub networks:a region proposal network aims to predict candidate regions from several layers with different filter size, and a region classification network for discrimination and regression. The quantitative comparison results on the challenging NWPU VHR-10 data set, large-scale Google Earth images, GF-2 and JL-1 images show that our method is more accurate and robust than existing algorithms. © 2018, Surveying and Mapping Press. All right reserved.","Deep learning; Deformable convolutional layer; Deformable pooling layer; Object detection; Remote sensing","Convolution; Deep learning; Deformation; Image segmentation; Object detection; Object recognition; Web browsers; Convolutional networks; Deformable convolutional layer; Deformable pooling layer; Quantitative comparison; Region classifications; Remote sensing imagery; Sliding window searches; Very high resolution; Remote sensing; algorithm; data set; deconvolution; deformation; detection method; machine learning; remote sensing; satellite imagery"
"Deep Convolutional Neural Network for Complex Wetland Classification Using Optical Remote Sensing Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049340721&doi=10.1109%2fJSTARS.2018.2846178&partnerID=40&md5=68594c45af0959682a4f83d138b4f0ac","The synergistic use of spatial features with spectral properties of satellite images enhances thematic land cover information, which is of great significance for complex land cover mapping. Incorporating spatial features within the classification scheme have been mainly carried out by applying just low-level features, which have shown improvement in the classification result. By contrast, the application of high-level spatial features for classification of satellite imagery has been underrepresented. This study aims to address the lack of high-level features by proposing a classification framework based on convolutional neural network (CNN) to learn deep spatial features for wetland mapping using optical remote sensing data. Designing a fully trained new convolutional network is infeasible due to the limited amount of training data in most remote sensing studies. Thus, we applied fine tuning of a pre-existing CNN. Specifically, AlexNet was used for this purpose. The classification results obtained by the deep CNN were compared with those based on well-known ensemble classifiers, namely random forest (RF), to evaluate the efficiency of CNN. Experimental results demonstrated that CNN was superior to RF for complex wetland mapping even by incorporating the small number of input features (i.e., three features) for CNN compared to RF (i.e., eight features). The proposed classification scheme is the first attempt, investigating the potential of fine-tuning pre-existing CNN, for land cover mapping. It also serves as a baseline framework to facilitate further scientific research using the latest state-of-art machine learning tools for processing remote sensing data. © 2008-2012 IEEE.","AlexNet; convolutional neural network (CNN); deep learning; random forest (RF); spatial feature; wetland mapping","Complex networks; Convolution; Data handling; Decision trees; Deep learning; Deep neural networks; Feature extraction; Image classification; Image enhancement; Neural networks; Optical sensors; Photomapping; Remote sensing; Satellite imagery; Satellites; Wetlands; AlexNet; Convolutional neural network; Convolutional Neural Networks (CNN); Optical imaging; Random forests; Spatial features; Classification (of information); artificial neural network; land cover; machine learning; mapping; remote sensing; satellite imagery; wetland"
"Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047250998&doi=10.1016%2fj.rse.2018.04.050&partnerID=40&md5=1d98f42baf597ddb960ff738c241961d","Urban land-use mapping is a significant yet challenging task in the field of remote sensing. Although numerous classification methods have been developed for obtaining land-use information in urban areas, the accuracy and efficiency of these methods are insufficient to meet the requirements of real-world applications such as urban planning and land management. In recent years, deep learning techniques, especially deep convolutional neural networks (DCNN), have achieved an astonishing level of performance in image classification. However, the traditional DCNN methods do not focus on multispectral remote sensing images with more than three channels, and they are limited by their training samples. In addition, these methods uniformly decompose large images into small processing units, which chop up the land-use patterns and produce land-use maps with obvious “blocks”. In this study, a semi-transfer deep convolutional neural network (STDCNN) approach is proposed to overcome these weaknesses. The proposed STDCNN has three parts: one part involves a transferred DCNN with deep architecture; another part is designed to analyze multispectral images; and the final part fuses the first two parts into a classification layer. Moreover, a skeleton-based decomposing method using street block data is devised to maintain the integrity of the land-use patterns. In two case studies, the proposed method is used to generate urban land-use maps from a WorldView-3 image of a 143 km2 area of Hong Kong and a WorldView-2 image of a 25 km2 area of Shenzhen. The results show that the proposed STDCNN obtains an overall accuracy (OA) of 91.25% and a Kappa coefficient (Kappa) of 0.903 for Hong Kong land-use classification, and an OA of 80% and a Kappa of 0.780 for Shenzhen land-use classification. In addition, due to the proposed skeleton-based decomposition method, the proposed method can produce better land-use maps for real-world urban applications. © 2018 Elsevier Inc.","Classification; Deep convolutional neural networks; High spatial resolution; Land-use mapping; Multispectral remote sensing image; Skeleton extraction; Street block; Transfer learning","Classification (of information); Convolution; Deep neural networks; Image processing; Image resolution; Land use; Musculoskeletal system; Neural networks; Remote sensing; Urban planning; Deep convolutional neural networks; High spatial resolution; Land-use mappings; Multispectral remote sensing image; Skeleton extraction; Transfer learning; Mapping; artificial neural network; decomposition analysis; extraction method; geological mapping; image classification; image processing; land management; land use planning; multispectral image; remote sensing; spatial resolution; urban area; urban planning; WorldView; China; Guangdong; Hong Kong; Shenzhen"
"Exploiting ConvNet diversity for flooding identification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049144059&doi=10.1109%2fLGRS.2018.2845549&partnerID=40&md5=e41700cb16ceaad8b5cd895a8fef906a","Flooding is the world's most costly type of natural disaster in terms of both economic losses and human causalities. A first and essential procedure toward flood monitoring is based on identifying the area most vulnerable to flooding, which gives authorities relevant regions to focus. In this letter, we propose several methods to perform flooding identification in high-resolution remote sensing images using deep learning. Specifically, some proposed techniques are based upon unique networks, such as dilated and deconvolutional ones, whereas others were conceived to exploit diversity of distinct networks in order to extract the maximum performance of each classifier. The evaluation of the proposed methods was conducted in a high-resolution remote sensing data set. Results show that the proposed algorithms outperformed the state-of-the-art baselines, providing improvements ranging from 1% to 4% in terms of the Jaccard Index. © 2018 IEEE.","Flooding identification; inundation; MediaEval; natural disaster; remote sensing; satellites","Deep learning; Disasters; Floods; Image processing; Image resolution; Job analysis; Losses; Monitoring; Personnel training; Satellites; Support vector machines; Flood monitoring; High resolution remote sensing; High resolution remote sensing images; inundation; MediaEval; Natural disasters; State of the art; Task analysis; Remote sensing; flooding; identification method; monitoring; natural disaster; remote sensing; satellite; vulnerability"
"Supervised classification of multisensor remotely sensed images using a deep learning framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053635170&doi=10.3390%2frs10091429&partnerID=40&md5=143d9fa7730af34afd7cffdb29b19ec6","In this paper, we present a convolutional neural network (CNN)-based method to efficiently combine information from multisensor remotely sensed images for pixel-wise semantic classification. The CNN features obtained from multiple spectral bands are fused at the initial layers of deep neural networks as opposed to final layers. The early fusion architecture has fewer parameters and thereby reduces the computational time and GPU memory during training and inference. We also propose a composite fusion architecture that fuses features throughout the network. The methods were validated on four different datasets: ISPRS Potsdam, Vaihingen, IEEE Zeebruges and Sentinel-1, Sentinel-2 dataset. For the Sentinel-1,-2 datasets, we obtain the ground truth labels for three classes from OpenStreetMap. Results on all the images show early fusion, specifically after layer three of the network, achieves results similar to or better than a decision level fusion mechanism. The performance of the proposed architecture is also on par with the state-of-the-art results. © 2018 by the authors.","Deep learning; Image classification; Multisensor data; Sentinel data","Deep learning; Deep neural networks; Image classification; Network architecture; Neural networks; Remote sensing; Semantics; Convolutional Neural Networks (CNN); Decision level fusion; Multi-sensor data; Proposed architectures; Remotely sensed images; Semantic classification; Sentinel data; Supervised classification; Classification (of information)"
"Autonomous Structural Visual Inspection Using Region-Based Deep Learning for Detecting Multiple Damage Types","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035234880&doi=10.1111%2fmice.12334&partnerID=40&md5=cf9b656fd922a5ecac9a14eb602b3f0f","Computer vision-based techniques were developed to overcome the limitations of visual inspection by trained human resources and to detect structural damage in images remotely, but most methods detect only specific types of damage, such as concrete or steel cracks. To provide quasi real-time simultaneous detection of multiple types of damages, a Faster Region-based Convolutional Neural Network (Faster R-CNN)-based structural visual inspection method is proposed. To realize this, a database including 2,366 images (with 500 × 375 pixels) labeled for five types of damages—concrete crack, steel corrosion with two levels (medium and high), bolt corrosion, and steel delamination—is developed. Then, the architecture of the Faster R-CNN is modified, trained, validated, and tested using this database. Results show 90.6%, 83.4%, 82.1%, 98.1%, and 84.7% average precision (AP) ratings for the five damage types, respectively, with a mean AP of 87.8%. The robustness of the trained Faster R-CNN is evaluated and demonstrated using 11 new 6,000 × 4,000-pixel images taken of different structures. Its performance is also compared to that of the traditional CNN-based method. Considering that the proposed method provides a remarkably fast test speed (0.03 seconds per image with 500 × 375 resolution), a framework for quasi real-time damage detection on video using the trained networks is developed. © 2017 Computer-Aided Civil and Infrastructure Engineering",,"Concretes; Cracks; Deep learning; Failure (mechanical); Neural networks; Pixels; Steel corrosion; Structural analysis; Concrete cracks; Convolutional neural network; Different structure; Multiple damages; Simultaneous detection; Structural damages; Visual inspection; Visual inspection method; Damage detection; artificial neural network; computer system; crack; damage mechanics; database; detection method; learning; real time; remote sensing; steel structure; structural analysis; visualization"
"Analysis of capsulenets towards hyperspectral classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073900233&doi=10.1109%2fWHISPERS.2018.8747122&partnerID=40&md5=c2e8ad6bc91aad141c92468c4438aab9","The deep network features are being widely explored for improving the classification of remote sensing images. However, for hyperspectral datasets, the spectral features are found to be more significant as compared to their spatial counterparts. In this study, a deep learning framework is proposed for modelling the spectral features. Unlike the conventional strategies, the approach simultaneously optimizes both the feature extraction and the classification stages. In this approach, the spectral features derived from different levels of hierarchies, re-modelled as capsules, are used to label the given spectrum based on an iterative dynamic routing process. Consequently, unlike the regular convolutional architectures, here the relative locations of the spectral artefacts are also taken into consideration. Along with the margin loss, a spectral-angle-based reconstruction loss is also employed to facilitate proper regularization. Experiments over different standard datasets indicate that the proposed approach performs better when compared to the prominent approaches. Furthermore, in comparison with the former deep learning models, our approach is found to be less sensitive to the network parameters and achieves better accuracy even with lesser network depth. © 2018 IEEE.","Capsulenet; Classification; CNN; Hyperspectral","Classification (of information); Deep learning; Image enhancement; Spectroscopy; Capsulenet; Classification of remote sensing image; Hyper-spectral classification; HyperSpectral; Learning frameworks; Network parameters; Relative location; Spectral feature; Remote sensing"
"Land cover classification from multi-temporal, multi-spectral remotely sensed imagery using patch-based recurrent neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048748802&doi=10.1016%2fj.neunet.2018.05.019&partnerID=40&md5=9784d1302db3fc72d748e4d0b4667162","Environmental sustainability research is dependent on accurate land cover information. Even with the increased number of satellite systems and sensors acquiring data with improved spectral, spatial, radiometric and temporal characteristics and the new data distribution policy, most existing land cover datasets are derived from a pixel-based, single-date multi-spectral remotely sensed image with an unacceptable accuracy. One major bottleneck for accuracy improvement is how to develop an accurate and effective image classification protocol. By incorporating and utilizing multi-spectral, multi-temporal and spatial information in remote sensing images and considering the inherit spatial and sequential interdependence among neighboring pixels, we propose a new patch-based recurrent neural network (PB-RNN) system tailored for classifying multi-temporal remote sensing data. The system is designed by incorporating distinctive characteristics of multi-temporal remote sensing data. In particular, it uses multi-temporal–spectral–spatial samples and deals with pixels contaminated by clouds/shadow present in multi-temporal data series. Using a Florida Everglades ecosystem study site covering an area of 771 square kilometers, the proposed PB-RNN system has achieved a significant improvement in the classification accuracy over a pixel-based recurrent neural network (RNN) system, a pixel-based single-image neural network (NN) system, a pixel-based multi-image NN system, a patch-based single-image NN system, and a patch-based multi-image NN system. For example, the proposed system achieves 97.21% classification accuracy while the pixel-based single-image NN system achieves 64.74%. By utilizing methods like the proposed PB-RNN one, we believe that much more accurate land cover datasets can be produced over large areas. © 2018 Elsevier Ltd","Deep learning; Land cover classification; LSTMs; Multi-temporal remote sensing imagery; Patch-based RNNs; Spatial context","Classification (of information); Deep learning; Image classification; Image enhancement; Pixels; Recurrent neural networks; Sustainable development; Land cover classification; LSTMs; Multi-temporal remote sensing; Patch based; Spatial context; Remote sensing; algorithm; artificial neural network; back propagation; calibration; controlled study; cropland; environmental sustainability; land use; Letter; measurement accuracy; multi spectral remote sensing imagery; multi temporal remote sensing imagery; patch based recurrent neural network; priority journal; process optimization; quantitative analysis; radiometry; remote sensing; urban area; automated pattern recognition; machine learning; procedures; satellite imagery; standards; Machine Learning; Neural Networks (Computer); Pattern Recognition, Automated; Satellite Imagery"
"Fusion of images and point clouds for the semantic segmentation of large-scale 3D scenes based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046796612&doi=10.1016%2fj.isprsjprs.2018.04.022&partnerID=40&md5=f2a2003dfc76d0c5074141d90c967f33","We address the issue of the semantic segmentation of large-scale 3D scenes by fusing 2D images and 3D point clouds. First, a Deeplab-Vgg16 based Large-Scale and High-Resolution model (DVLSHR) based on deep Visual Geometry Group (VGG16) is successfully created and fine-tuned by training seven deep convolutional neural networks with four benchmark datasets. On the val set in CityScapes, DVLSHR achieves a 74.98% mean Pixel Accuracy (mPA) and a 64.17% mean Intersection over Union (mIoU), and can be adapted to segment the captured images (image resolution 2832 ∗ 4256 pixels). Second, the preliminary segmentation results with 2D images are mapped to 3D point clouds according to the coordinate relationships between the images and the point clouds. Third, based on the mapping results, fine features of buildings are further extracted directly from the 3D point clouds. Our experiments show that the proposed fusion method can segment local and global features efficiently and effectively. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","2D image; 3D point cloud; 3D scene segmentation; High-resolution; Large-scale","Deep neural networks; Image resolution; Neural networks; Pixels; Semantics; Three dimensional computer graphics; 2D images; 3D point cloud; 3D scenes; High resolution; Large-scale; Image segmentation; accuracy assessment; algorithm; data set; image analysis; image resolution; mapping method; numerical method; pixel; segmentation; three-dimensional modeling; two-dimensional modeling"
"Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053596445&doi=10.3390%2frs10091487&partnerID=40&md5=044081e414dc380d935d2ab56c8d8042","The microtopography associated with ice-wedge polygons governs many aspects of Arctic ecosystem, permafrost, and hydrologic dynamics from local to regional scales owing to the linkages between microtopography and the flow and storage of water, vegetation succession, and permafrost dynamics. Wide-spread ice-wedge degradation is transforming low-centered polygons into high-centered polygons at an alarming rate. Accurate data on spatial distribution of ice-wedge polygons at a pan-Arctic scale are not yet available, despite the availability of sub-meter-scale remote sensing imagery. This is because the necessary spatial detail quickly produces data volumes that hamper both manual and semi-automated mapping approaches across large geographical extents. Accordingly, transforming big imagery into 'science-ready' insightful analytics demands novel image-to-assessment pipelines that are fueled by advanced machine learning techniques and high-performance computational resources. In this exploratory study, we tasked a deep-learning driven object instance segmentation method (i.e., the Mask R-CNN) with delineating and classifying ice-wedge polygons in very high spatial resolution aerial orthoimagery. We conducted a systematic experiment to gauge the performances and interoperability of the Mask R-CNN across spatial resolutions (0.15 m to 1 m) and image scene contents (a total of 134 km2) near Nuiqsut, Northern Alaska. The trained Mask R-CNN reported mean average precisions of 0.70 and 0.60 at thresholds of 0.50 and 0.75, respectively. Manual validations showed that approximately 95% of individual ice-wedge polygons were correctly delineated and classified, with an overall classification accuracy of 79%. Our findings show that the Mask R-CNN is a robust method to automatically identify ice-wedge polygons from fine-resolution optical imagery. Overall, this automated imagery-enabled intense mapping approach can provide a foundational framework that may propel future pan-Arctic studies of permafrost thaw, tundra landscape evolution, and the role of high latitudes in the global climate system. © 2018 by the authors.","Arctic; Deep learning; Ice-wedge polygons; Mask-RCNN; VHSR imagery","Aerial photography; Antennas; Automation; Deep learning; Deep neural networks; Digital storage; Geometry; Image resolution; Interoperability; Mapping; Neural networks; Permafrost; Remote sensing; Arctic; Classification accuracy; Computational resources; Deep convolutional neural networks; Ice wedges; Machine learning techniques; Very high spatial resolutions; VHSR imagery; Ice"
"Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and LiDAR data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048334038&doi=10.1016%2fj.isprsjprs.2018.06.005&partnerID=40&md5=2df0e2d5d6ae885d3d85633a8c605668","Semantic segmentation of LiDAR and high-resolution aerial imagery is one of the most challenging topics in the remote sensing domain. Deep convolutional neural network (CNN) and its derivatives have recently shown the abilities in pixel-wise prediction of remote sensing data. Many existing deep learning methods fuse LiDAR and high-resolution aerial imagery towards an inter-modal mode and thus overlook the intra-modal statistical characteristics. Additionally, the patch-based CNNs could generate the salt-and-pepper artifacts as characterized by isolated and spurious pixels on the object boundaries and patch edges leading to unsatisfied labelling results. This paper presents a semantic segmentation scheme that combines multi-filter CNN and multi-resolution segmentation (MRS). The multi-filter CNN aggregates LiDAR data and high-resolution optical imagery by multi-modal data fusion for semantic labelling, and the MRS is further used to delineate object boundaries for reducing the salt-and-pepper artifacts. The proposed method is validated against two datasets: the ISPRS 2D semantic labelling contest of Potsdam and an area of Guangzhou in China labelled based on existing geodatabases. Various designs of data fusion strategy, CNN architecture and MRS scale are analyzed and discussed. Compared with other classification methods, our method improves the overall accuracies. Experiment results show that our combined method is an efficient solution for the semantic segmentation of LiDAR and high-resolution imagery. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","High-resolution imagery; LiDAR; Multi-modal fusion; Multi-resolution segmentation; Semantic segmentation","Aerial photography; Antennas; Bandpass filters; Convolution; Data fusion; Deep neural networks; Lithium compounds; Modal analysis; Neural networks; Pixels; Remote sensing; Semantic Web; Semantics; Convolutional neural network; Deep convolutional neural networks; High resolution aerial imagery; High resolution imagery; High-resolution optical imagery; Multi resolutions; Multi-modal fusion; Semantic segmentation; Optical radar; artifact; artificial neural network; filter; image resolution; lidar; pixel; remote sensing; satellite data; segmentation; China; Guangdong; Guangzhou"
"Deep fusion of multi-view and multimodal representation of ALS point cloud for 3D terrain scene recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044289466&doi=10.1016%2fj.isprsjprs.2018.03.011&partnerID=40&md5=1a27109c0bc039739b8dc661cae716e3","Terrain scene category is useful not only for some geographical or environmental researches, but also for choosing suitable algorithms or proper parameters of the algorithms for several point cloud processing tasks to achieve better performance. However, there are few studies in point cloud processing focusing on terrain scene classification at present. In this paper, a novel deep learning framework for 3D terrain scene recognition using 2D representation of sparse point cloud is proposed. The framework has two key components. (1) Initially, several suitable discriminative low-level local features are extracted from airborne laser scanning point cloud, and 3D terrain scene is encoded into multi-view and multimodal 2D representation. (2) A two-level fusion network embedded with feature- and decision-level fusion strategy is designed to fully exploit the 2D representation of 3D terrain scene, which can be trained end-to-end. Experiment results show that our method achieves an overall accuracy of 96.70% and a kappa coefficient of 0.96 in recognizing nine categories of terrain scene point clouds. Extensive design choices of the underlying framework are tested, and other typical methods from literature for related research are compared. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","3D scene recognition; ALS; Deep learning; Fusion network; Multi-view representation","Aluminum; Landforms; 3D scenes; Airborne Laser scanning; Decision level fusion; Environmental researches; Learning frameworks; Multi-views; Overall accuracies; Scene classification; Deep learning; accuracy assessment; airborne survey; algorithm; cloud; data processing; experimental study; learning; performance assessment; terrain"
"Semantic line framework-based indoor building modeling using backpacked laser scanning point cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046133599&doi=10.1016%2fj.isprsjprs.2018.03.025&partnerID=40&md5=32fef0620bb54395c0dd88a693a852a5","Indoor building models are essential in many indoor applications. These models are composed of the primitives of the buildings, such as the ceilings, floors, walls, windows, and doors, but not the movable objects in the indoor spaces, such as furniture. This paper presents, for indoor environments, a novel semantic line framework-based modeling building method using backpacked laser scanning point cloud data. The proposed method first semantically labels the raw point clouds into the walls, ceiling, floor, and other objects. Then line structures are extracted from the labeled points to achieve an initial description of the building line framework. To optimize the detected line structures caused by furniture occlusion, a conditional Generative Adversarial Nets (cGAN) deep learning model is constructed. The line framework optimization model includes structure completion, extrusion removal, and regularization. The result of optimization is also derived from a quality evaluation of the point cloud. Thus, the data collection and building model representation become a united task-driven loop. The proposed method eventually outputs a semantic line framework model and provides a layout for the interior of the building. Experiments show that the proposed method effectively extracts the line framework from different indoor scenes. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Indoor modeling; Line framework extraction; Mobile laser scanning; Point clouds; Semantic labeling","Deep learning; Floors; Laser applications; Scanning; Semantics; Structural optimization; Framework optimization; Indoor applications; Laser scanning; Laser scanning point clouds; Model-building methods; Point cloud; Quality evaluation; Semantic labeling; Walls (structural partitions); analytical framework; building; experimental study; indoor air; laser method; numerical model; optimization"
"3D façade labeling over complex scenarios: A case study using convolutional neural network and structure-from-motion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053636024&doi=10.3390%2frs10091435&partnerID=40&md5=386b2bcff33fe6b7855d5a39eb743374","Urban environments are regions in which spectral variability and spatial variability are extremely high, with a huge range of shapes and sizes, and they also demand high resolution images for applications involving their study. Due to the fact that these environments can grow even more over time, applications related to their monitoring tend to turn to autonomous intelligent systems, which together with remote sensing data could help or even predict daily life situations. The task of mapping cities by autonomous operators was usually carried out by aerial optical images due to its scale and resolution; however new scientific questions have arisen, and this has led research into a new era of highly-detailed data extraction. For many years, using artificial neural models to solve complex problems such as automatic image classification was commonplace, owing much of their popularity to their ability to adapt to complex situations without needing human intervention. In spite of that, their popularity declined in the mid-2000s, mostly due to the complex and time-consuming nature of their methods and workflows. However, newer neural network architectures have brought back the interest in their application for autonomous classifiers, especially for image classification purposes. Convolutional Neural Networks (CNN) have been a trend for pixel-wise image segmentation, showing flexibility when detecting and classifying any kind of object, even in situations where humans failed to perceive differences, such as in city scenarios. In this paper, we aim to explore and experiment with state-of-the-art technologies to semantically label 3D urban models over complex scenarios. To achieve these goals, we split the problem into two main processing lines: first, how to correctly label the façade features in the 2D domain, where a supervised CNN is used to segment ground-based façade images into six feature classes, roof, window, wall, door, balcony and shop; second, a Structure-from-Motion (SfM) and Multi-View-Stereo (MVS) workflow is used to extract the geometry of the façade, wherein the segmented images in the previous stage are then used to label the generated mesh by a ""reverse"" ray-tracing technique. This paper demonstrates that the proposed methodology is robust in complex scenarios. The façade feature inferences have reached up to 93% accuracy over most of the datasets used. Although it still presents some deficiencies in unknown architectural styles and needs some improvements to be made regarding 3D-labeling, we present a consistent and simple methodology to handle the problem. © 2018 by the authors.","3D reconstruction; Deep-learning; Façade feature detection; Structure-from-motion","Antennas; Complex networks; Convolution; Deep learning; Image classification; Image segmentation; Intelligent systems; Network architecture; Neural networks; Problem solving; Ray tracing; Remote sensing; Roofs; 3D reconstruction; Automatic image classification; Autonomous intelligent systems; Convolutional neural network; Convolutional Neural Networks (CNN); Feature detection; State-of-the-art technology; Structure from motion; Stereo image processing"
"Generative Adversarial Networks for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043383663&doi=10.1109%2fTGRS.2018.2805286&partnerID=40&md5=e832d8017551b5a4c9265496d935eae2","A generative adversarial network (GAN) usually contains a generative network and a discriminative network in competition with each other. The GAN has shown its capability in a variety of applications. In this paper, the usefulness and effectiveness of GAN for classification of hyperspectral images (HSIs) are explored for the first time. In the proposed GAN, a convolutional neural network (CNN) is designed to discriminate the inputs and another CNN is used to generate so-called fake inputs. The aforementioned CNNs are trained together: the generative CNN tries to generate fake inputs that are as real as possible, and the discriminative CNN tries to classify the real and fake inputs. This kind of adversarial training improves the generalization capability of the discriminative CNN, which is really important when the training samples are limited. Specifically, we propose two schemes: 1) a well-designed 1D-GAN as a spectral classifier and 2) a robust 3D-GAN as a spectral-spatial classifier. Furthermore, the generated adversarial samples are used with real training samples to fine-tune the discriminative CNN, which improves the final classification performance. The proposed classifiers are carried out on three widely used hyperspectral data sets: Salinas, Indiana Pines, and Kennedy Space Center. The obtained results reveal that the proposed models provide competitive results compared to the state-of-the-art methods. In addition, the proposed GANs open new opportunities in the remote sensing community for the challenging task of HSI classification and also reveal the huge potential of GAN-based methods for the analysis of such complex and inherently nonlinear data. © 2018 IEEE.","Convolutional neural network (CNN); deep learning; generative adversarial network (GAN); hyperspectral image (HSI) classification","Classification (of information); Convolution; Deep learning; Feature extraction; Gallium nitride; Gas generators; Hyperspectral imaging; III-V semiconductors; Independent component analysis; Neural networks; Personnel training; Remote sensing; Sampling; Space platforms; Spectroscopy; Adversarial networks; Classification performance; Convolutional Neural Networks (CNN); Discriminative networks; Generalization capability; Kennedy space centers; Spectral classifier; State-of-the-art methods; Image classification; artificial neural network; data set; image classification; methodology; network analysis; remote sensing; spatial analysis; spectral analysis; California; Florida [United States]; Indiana; Kennedy Space Center; Salinas [California]; United States"
"Stacked nonnegative sparse autoencoders for robust hyperspectral unmixing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048624420&doi=10.1109%2fLGRS.2018.2841400&partnerID=40&md5=b689fb9fbce809f8420d3bffa14f95c2","As an unsupervised learning tool, autoencoder has been widely applied in many fields. In this letter, we propose a new robust unmixing algorithm that is based on stacked nonnegative sparse autoencoders (NNSAEs) for hyperspectral data with outliers and low signal-to-noise ratio. The proposed stacked autoencoders network contains two main steps. In the first step, a series of NNSAE is used to detect the outliers in the data. In the second step, a final autoencoder is performed for unmixing to achieve the endmember signatures and abundance fractions. By taking advantage from nonnegative sparse autoencoding, the proposed approach can well tackle problems with outliers and low noise-signal ratio. The effectiveness of the proposed method is evaluated on both synthetic and real hyperspectral data. In comparison with other unmixing methods, the proposed approach demonstrates competitive performance. © 2018 IEEE.","Deep learning; Hyperspectral remote sensing; Nonnegative sparse autoencoder (NNSAE); Unmixing","Deep learning; Hyperspectral imaging; Neural networks; Neurons; Personnel training; Remote sensing; Spectroscopy; Statistics; Anomaly detection; Auto encoders; Competitive performance; Hyperspectral remote sensing; Hyperspectral unmixing; Low signal-to-noise ratio; Unmixing; Unmixing algorithms; Signal to noise ratio; algorithm; image classification; outlier; remote sensing; signal-to-noise ratio; spectral analysis; unsupervised classification"
"Deep Learning Neural Networks for sUAS-Assisted Structural Inspections: Feasibility and Application","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053861869&doi=10.1109%2fICUAS.2018.8453409&partnerID=40&md5=4d7dc210a60aa129eefaa1e78b9296fa","This paper investigates the feasibility of using a Deep Learning Convolutional Neural Network (DLCNN) in inspection of concrete decks and buildings using small Unmanned Aerial Systems (sUAS). The training dataset consists of images of lab-made bridge decks taken with a point-and-shoot high resolution camera. The network is trained on this dataset in two modes: fully trained (94.7% validation accuracy) and transfer learning (97.1% validation accuracy). The testing datasets consist of 1620 sub-images from bridge decks with the same cracks, 2340 sub-images from bridge decks with similar cracks, and 3600 sub-images from a building with different cracks, all taken by sUAS. The sUAS used in the first dataset has a low-resolution camera whereas the sUAS used in the second and third datasets has a camera comparable to the point-and-shoot camera. In this study it has been shown that it is feasible to apply DLCNNs in autonomous civil structural inspections with comparable results to human inspectors when using off-the-shelf sUAS and training datasets collected with point-and-shoot handheld cameras. © 2018 IEEE.","Concrete Crack Detection; Convolutional Neural Networks; Deep Learning; Infrastructure Inspection; Remote Sensing; small Unmanned Aerial Systems","Antennas; Bridge decks; Cameras; Concretes; Convolution; Crack detection; Deep learning; Fighter aircraft; Neural networks; Remote sensing; Training aircraft; Unmanned aerial vehicles (UAV); Concrete cracks; Convolutional neural network; Hand-held cameras; High resolution camera; Learning neural networks; Structural inspections; Training data sets; Unmanned aerial systems; Deep neural networks"
"Psgan: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062900060&doi=10.1109%2fICIP.2018.8451049&partnerID=40&md5=1849950b74eeded4a0073d53039a6838","Remote sensing image fusion (also known as pan-sharpening) aims to generate a high resolution multi -spectral image from inputs of a high spatial resolution single band panchromatic (PAN) image and a low spatial resolution multi-spectral (MS) image. In this paper, we propose PSGAN, a generative adversarial network (GAN) for remote sensing image pansharpening. To the best of our knowledge, this is the first attempt at producing high quality pan-sharpened images with GANs. The PSGAN consists of two parts. Firstly, a two-stream fusion architecture is designed to generate the desired high resolution multi -spectral images, then a fully convolutional network serving as a discriminator is applied to distinct 'real' or 'pan-sharpened' MS images. Experiments on images acquired by Quickbird and GaoFen-1 satellites demonstrate that the proposed PSGAN can fuse PAN and MS images effectively and significantly improve the results over the state of the art traditional and CNN based pan-sharpening methods. © 2018 IEEE.","Deep learning; GAN; Image fusion; Pan-sharpening; Remote sensing","Deep learning; Image fusion; Image resolution; Remote sensing; Spectroscopy; Adversarial networks; Convolutional networks; Fusion architecture; High spatial resolution; Multispectral images; Pan-sharpening; Panchromatic (Pan) image; Remote sensing images; Image enhancement"
"A method of aircraft detection using fully convolutional network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053868301&doi=10.1109%2fICCSEC.2017.8446851&partnerID=40&md5=e88b1a4af370abb1ee010850120be148","Traditional methods on aircraft detection in remote sensing images rely on handcrafted design, which is difficult to detect and recognize the target in complex scenes and multi-scale conditions. In this paper, we tackle these two problems by proposing a method for aircraft detection based on the fully convolutional neural network(FCNN). FCNN can obtain the location of the aircraft quickly and directly by minimizing a multi-task loss. Through data augmentation and transfer learning, the classification accuracy of FCN is much improved. In order to recognize small targets, we combine the resolution information with priori knowledge of aircraft to construct image pyramid structure on the test images. The experimental results show that the method with less parameters has higher accuracy and the model is simple to train. © 2017 IEEE.","Aircraft detection; Deep learning; Fully convolutional neural network","Aircraft; Computer control systems; Convolution; Deep learning; Neural networks; Remote sensing; Classification accuracy; Convolutional networks; Convolutional neural network; Data augmentation; Image pyramids; Priori knowledge; Remote sensing images; Transfer learning; Aircraft detection"
"A Survey on Spatial Prediction Methods","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052713683&doi=10.1109%2fTKDE.2018.2866809&partnerID=40&md5=16db2808f8c0ce54eed518bd4a008987","With the advancement of GPS and remote sensing technologies, large amounts of geospatial data are being collected from various domains, driving the need for effective and efficient prediction methods. Given spatial data samples with explanatory features and targeted responses at a set of locations, the spatial prediction problem aims to learn a model that can predict the response variable based on explanatory features. The problem is important with broad applications in earth science, urban informatics, geosocial media analytics and public health, but is challenging due to the unique characteristics of spatial data, including spatial autocorrelation, heterogeneity, limited ground truth, and multiple scales and resolutions. This paper provides a systematic review on principles and methods in spatial prediction. We provide a taxonomy of methods categorized by the key challenge they address. For each method, we introduce its underlying assumption, theoretical foundation, and discuss its advantages and disadvantages. We also discuss spatiotemporal extensions of methods. Our goal is to help interdisciplinary domain scientists choose techniques to solve their problems, and more importantly, to help data mining researchers to understand the main principles and methods in spatial prediction and identify future research opportunities. IEEE","Data mining; deep learning; Diseases; Earth; Media; Predictive models; Public healthcare; spatial big data; spatial classification and regression; Spatial databases; Spatial prediction; spatiotemporal prediction; survey","Data mining; Deep learning; Forecasting; Problem solving; Remote sensing; Surveying; Surveys; Efficient predictions; Remote sensing technology; Research opportunities; Spatial autocorrelations; Spatial classification; Spatial prediction; Spatio-temporal prediction; Theoretical foundations; Big data"
"A method for extracting the leaf litter distribution area in forest using chip feature","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049962774&doi=10.1080%2f01431161.2018.1484965&partnerID=40&md5=741d9e761a234fb40c1cea03b37eeba9","The leaf litter layer is an important surface coverage in the forest ecosystem, which imposes an important influence on soil and water conservation and affects the soil physical and chemical characteristics as well as the ecological environment. It is also a significant factor in estimating vegetation coverage. Currently, most remote-sensing methods to obtain leaf litter target are based on the analysis of the spectral features. However, the leaf litter and the soil background are easily confused. The traditional image processing method based on the pixel spectral information alone fails to make full use of the texture and shape information of the target and diminishes the extraction effect. With the research object focused on the continuous distribution area of leaf litter, whose features are intra-class complicated and inter-class boundary blurred, this article proposes a method to extract the leaf litter distribution area in forest by using the Convolutional Neural Network (CNN) to automatically retrieve the image block (which is also called image chip) features. The ResNet50, VGG16, and VGG19 models based on CNN are, respectively, used to extract and to learn the features of the image chips so that they can automatically obtain the deeper, more abstractive and representative features. The accuracy of this method in leaf litter target extraction has reached 95.1% which demonstrated that this new method has higher classification accuracy and better generalization ability than the traditional method of manually selecting image features. Moreover, the use of pretraining models can solve the problem in deep learning with small sample size. Through the study of scale effects, we also found the appropriate segmentation scale for the chips. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Deep learning; Ecosystems; Extraction; Forestry; Neural networks; Remote sensing; Soil conservation; Soils; Water conservation; Classification accuracy; Continuous distribution; Convolutional Neural Networks (CNN); Ecological environments; Generalization ability; Image processing - methods; Physical and chemical characteristics; Soil and water conservation; Image processing; artificial neural network; forest ecosystem; image analysis; leaf litter; remote sensing; spectral analysis; vegetation cover"
"Multiscale rotated bounding box-based deep learning method for detecting ship targets in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052123649&doi=10.3390%2fs18082702&partnerID=40&md5=c4d5c49e80e766165ab5b6b934297416","Since remote sensing images are captured from the top of the target, such as from a satellite or plane platform, ship targets can be presented at any orientation. When detecting ship targets using horizontal bounding boxes, there will be background clutter in the box. This clutter makes it harder to detect the ship and find its precise location, especially when the targets are in close proximity or staying close to the shore. To solve these problems, this paper proposes a deep learning algorithm using a multiscale rotated bounding box to detect the ship target in a complex background and obtain the location and orientation information of the ship. When labeling the oriented targets, we use the five-parameter method to ensure that the box shape is maintained rectangular. The algorithm uses a pretrained deep network to extract features and produces two divided flow paths to output the result. One flow path predicts the target class, while the other predicts the location and angle information. In the training stage, we match the prior multiscale rotated bounding boxes to the ground-truth bounding boxes to obtain the positive sample information and use it to train the deep learning model. When matching the rotated bounding boxes, we narrow down the selection scope to reduce the amount of calculation. In the testing stage, we use the trained model to predict and obtain the final result after comparing with the score threshold and nonmaximum suppression post-processing. Experiments conducted on a remote sensing dataset show that the algorithm is robust in detecting ship targets under complex conditions, such as wave clutter background, target in close proximity, ship close to the shore, and multiscale varieties. Compared to other algorithms, our algorithm not only exhibits better performance in ship detection but also obtains the precise location and orientation information of the ship. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Multiscale rotated bounding box; Remote sensing image; Ship detection","Clutter (information theory); Complex networks; Learning algorithms; Location; Radar clutter; Remote sensing; Rotation; Ships; Background clutter; Bounding box; Clutter background; Complex background; Non-maximum suppression; Orientation information; Remote sensing images; Ship detection; Deep learning; algorithm; article; calculation; deep learning; remote sensing"
"CNN based technique for automatic tree counting using very high resolution data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053147637&doi=10.1109%2fICDI3C.2018.00036&partnerID=40&md5=5d32da43ad5ab3170b7769b7b7c61287","Coconut is one of the economically grown crops in India. In this paper, we develop an automatic method for counting the number of coconut trees in UAV images. The availability of high resolution remote sensing images helps people in having large amounts of detailed digital imaging of vegetation areas. Today, the estimated coconut tree count can be determined in a short duration of time through high resolution drone images with low cost and labor. The goal is to find new methods to determine coconut trees using remote sensing. Deep learning techniques with convolutional neural network (CNN) algorithms is used to detect the coconut trees. © 2018 IEEE.","Automatic; CNN; Deep learning; Remote sensing; UAV","Deep learning; Forestry; Neural networks; Trees (mathematics); Unmanned aerial vehicles (UAV); Automatic; Automatic method; Automatic tree counting; Convolutional Neural Networks (CNN); High resolution remote sensing images; Learning techniques; Short durations; Very high resolution datum; Remote sensing"
"Hyperspectral Remote Sensing Image Classification Based on Three-Dimensional Convolution Neural Network Combined with Conditional Random Field Optimization [三维卷积神经网络模型联合条件随机场优化的高光谱遥感影像分类]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054512563&doi=10.3788%2fAOS201838.0828001&partnerID=40&md5=b7741b028fc624537ef88c0b8f50d12c","Hyperspectral remote sensing image classification is usually based on the spectral features of objects, but there are plenty of spatial informations in the images. The effective use of spatial information can significantly improve the image classification effect. Because of the special structure of convolution neural network (CNN), CNN has been successfully applied in the field of image classification, and has a good effect on the classification of two-dimensional images. How to improve classification performance through deep learning combined with spatial-spectral information is a key point. Combining the spatial features and spectral information of hyperspectral images, we have developed a three-dimensional convolution neural network model (3D-CNN) for hyperspectral pixel classification, and the multi labels conditional random field is optimized on the basis of the initial classification. Three general open hyperspectral datasets (Indian Pines dataset, Pavia University dataset, Pavia Center dataset) are selected for testing. Experiments show that the accuracy is greatly improved after the classification optimization, the overall accuracy can reach 98%, and the Kappa coefficient reaches 97.2%. © 2018, Chinese Lasers Press. All right reserved.","Condition random field; Hyperspectral image; Remote sensing; Spatial-spectral information; Three-dimensional convolution neural network","Convolution; Deep learning; Hyperspectral imaging; Image classification; Image enhancement; Independent component analysis; Random processes; Remote sensing; Spectroscopy; Statistical tests; Classification performance; Conditional random field; Convolution neural network; Hyperspectral Remote Sensing Image; Random fields; Spatial informations; Spectral information; Two dimensional images; Classification (of information)"
"3-D deep learning approach for remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045766689&doi=10.1109%2fTGRS.2018.2818945&partnerID=40&md5=b280c277afea286b912f4375f1afcbef","Recently, a variety of approaches have been enriching the field of remote sensing (RS) image processing and analysis. Unfortunately, existing methods remain limited to the rich spatiospectral content of today's large data sets. It would seem intriguing to resort to deep learning (DL)-based approaches at this stage with regard to their ability to offer accurate semantic interpretation of the data. However, the specificity introduced by the coexistence of spectral and spatial content in the RS data sets widens the scope of the challenges presented to adapt DL methods to these contexts. Therefore, the aim of this paper is first to explore the performance of DL architectures for the RS hyperspectral data set classification and second to introduce a new 3-D DL approach that enables a joint spectral and spatial information process. A set of 3-D schemes is proposed and evaluated. Experimental results based on well-known hyperspectral data sets demonstrate that the proposed method is able to achieve a better classification rate than state-of-the-art methods with lower computational costs. © 1980-2012 IEEE.","Classification; deep learning (DL); hyperspectral; pixel-based; remote sensing (RS)","Classification (of information); Computer architecture; Image classification; Job analysis; Learning systems; Network architecture; Neural networks; Remote sensing; Semantics; Tools; Classification rates; HyperSpectral; Remote sensing image classification; Remote sensing images; Semantic interpretation; Spatial informations; State-of-the-art methods; Task analysis; Deep learning; data set; image classification; image processing; numerical method; pixel; remote sensing; satellite imagery; three-dimensional flow; three-dimensional modeling"
"Unsupervised deep feature learning for remote sensing image retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051622720&doi=10.3390%2frs10081243&partnerID=40&md5=94c88539921b2edc6b429bd4ffa21bcb","Due to the specific characteristics and complicated contents of remote sensing (RS) images, remote sensing image retrieval (RSIR) is always an open and tough research topic in the RS community. There are two basic blocks in RSIR, including feature learning and similarity matching. In this paper, we focus on developing an effective feature learning method for RSIR. With the help of the deep learning technique, the proposed feature learning method is designed under the bag-of-words (BOW) paradigm. Thus, we name the obtained feature deep BOW (DBOW). The learning process consists of two parts, including image descriptor learning and feature construction. First, to explore the complex contents within the RS image, we extract the image descriptor in the image patch level rather than the whole image. In addition, instead of using the handcrafted feature to describe the patches, we propose the deep convolutional auto-encoder (DCAE) model to deeply learn the discriminative descriptor for the RS image. Second, the k-means algorithm is selected to generate the codebook using the obtained deep descriptors. Then, the final histogrammic DBOW features are acquired by counting the frequency of the single code word. When we get the DBOW features from the RS images, the similarities between RS images are measured using L1-norm distance. Then, the retrieval results can be acquired according to the similarity order. The encouraging experimental results counted on four public RS image archives demonstrate that our DBOW feature is effective for the RSIR task. Compared with the existing RS image features, our DBOW can achieve improved behavior on RSIR. © 2018 by the authors.","Feature learning; Remote sensing image retrieval","Deep learning; Image enhancement; Remote sensing; Deep feature learning; Feature construction; Feature learning; k-Means algorithm; Learning techniques; Remote sensing image retrieval; Remote sensing images; Similarity-matching; Image retrieval"
"A Deep-Learning-Based Sea Search and Rescue Algorithm by UAV Remote Sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082471782&doi=10.1109%2fGNCC42960.2018.9019134&partnerID=40&md5=827f8b901c639c5b9bd79cc08ebf32f9","With the promotion of economic globalization, the demand for air travel and maritime travel in all countries has increased significantly. However, the accompanying rate of accidents at sea is increasing rapidly year by year. After the accident, the survival rate of people is closely related to the speed of rescue. Artificial eyesight and radar electronic search and rescue (SAR) are limited due to inaccuracy and limited resources. The rapid development of unmanned aerial vehicle (UAV) technology in recent years has made it possible to obtain stable, high-resolution images from airborne cameras. In this paper, a method of SAR using images captured by UAVs is proposed. The detection of victim is divided into two steps. First, in order to improve the system's real-time, the possible position of a victim may be located from the image through a simple feature to avoid exhaustive search. This step is called hypothesis generation (HG). Then, the pre-trained convolutional neural network (CNN) is used to test the HG region to ensure the accuracy of the system. This step is called hypothesis verification (HV). The result shows that taking advantage of pre-processing + CNN has better comprehensive performance in SAR at sea. © 2018 IEEE.","convolutional neural network (CNN); hypothesis generation (HG); hypothesis verification (HV); search and rescue (SAR); unmanned aerial vehicle (UAV)","Accidents; Antennas; Convolution; Convolutional neural networks; Image enhancement; Remote sensing; Search engines; Synthetic aperture radar; Unmanned aerial vehicles (UAV); Airborne cameras; Comprehensive performance; Economic globalization; High resolution image; Hypothesis generation; Hypothesis verifications; Search and rescue; UAV remote sensing; Deep learning"
"A deep learning approach to DTM extraction from imagery using rule-based training labels","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048560269&doi=10.1016%2fj.isprsjprs.2018.06.001&partnerID=40&md5=eb478ae24b0d768cdfcdb92906b3c721","Existing algorithms for Digital Terrain Model (DTM) extraction still face difficulties due to data outliers and geometric ambiguities in the scene such as contiguous off-ground areas or sloped environments. We postulate that in such challenging cases, the radiometric information contained in aerial imagery may be leveraged to distinguish between ground and off-ground objects. We propose a method for DTM extraction from imagery which first applies morphological filters to the Digital Surface Model to obtain candidate ground and off-ground training samples. These samples are used to train a Fully Convolutional Network (FCN) in the second step, which can then be used to identify ground samples for the entire dataset. The proposed method harnesses the power of state-of-the-art deep learning methods, while showing how they can be adapted to the application of DTM extraction by (i) automatically selecting and labelling dataset-specific samples which can be used to train the network, and (ii) adapting the network architecture to consider a larger surface area without unnecessarily increasing the computational burden. The method is successfully tested on four datasets, indicating that the automatic labelling strategy can achieve an accuracy which is comparable to the use of manually labelled training samples. Furthermore, we demonstrate that the proposed method outperforms two reference DTM extraction algorithms in challenging areas. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Aerial photogrammetry; Deep learning; Digital Terrain Models (DTM); Fully Convolutional Networks (FCN); Unmanned Aerial Vehicles (UAV)","Aerial photography; Antennas; Convolution; Extraction; Network architecture; Photogrammetry; Sampling; Unmanned aerial vehicles (UAV); Aerial photogrammetry; Automatic labelling; Computational burden; Convolutional networks; Digital surface models; Digital terrain model; Extraction algorithms; Morphological filters; Deep learning; algorithm; data set; digital terrain model; numerical method; photogrammetry; satellite imagery"
"Satellite image spoofing: Creating remote sensing dataset with generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051359490&doi=10.4230%2fLIPIcs.GIScience.2018.67&partnerID=40&md5=73d08a83cff8caabec976d31d4f8f08c","The rise of Artificial Intelligence (AI) has brought up both opportunities and challenges for today's evolving GIScience. Its ability in image classification, object detection and feature extraction has been frequently praised. However, it may also apply for falsifying geospatial data. To demonstrate the thrilling power of AI, this research explored the potentials of deep learning algorithms in capturing geographic features and creating fake satellite images according to the learned 'sense'. Specifically, Generative Adversarial Networks (GANs) is used to capture geographic features of a certain place from a group of web maps and satellite images, and transfer the features to another place. Corvallis is selected as the study area, and fake datasets with 'learned' style from three big cities (i.e. New York City, Seattle and Beijing) are generated through CycleGAN. The empirical results show that GANs can 'remember' a certain 'sense of place' and further apply that 'sense' to another place. With this paper, we would like to raise both public and GIScientists' awareness in the potential occurrence of fake satellite images, and its impacts on various geospatial applications, such as environmental monitoring, urban planning, and land use development. © Chun X. Xu and Bo Zhao.","Deep learning and AI; Fake satellite image; GANs; Geographic feature","Deep learning; Geographic information systems; Land use; Learning algorithms; Object detection; Remote sensing; Satellites; Adversarial networks; Environmental Monitoring; GANs; Geo-spatial data; Geographic feature; Geospatial applications; Satellite images; Sense of place; Feature extraction"
"Building Footprint Extraction From VHR Remote Sensing Images Combined With Normalized DSMs Using Fused Fully Convolutional Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052910086&doi=10.1109%2fJSTARS.2018.2849363&partnerID=40&md5=25ce3b7f9718670f5ac0a3de593da70b","Automatic building extraction and delineation from high-resolution satellite imagery is an important but very challenging task, due to the extremely large diversity of building appearances. Nowadays, it is possible to use multiple high-resolution remote sensing data sources, which allow the integration of different information in order to improve the extraction accuracy of building outlines. Many algorithms are built on spectral-based or appearance-based criteria, from single or fused data sources, to perform the building footprint extraction. But the features for these algorithms are usually manually extracted, which limits their accuracy. Recently developed fully convolutional networks (FCNs), which are similar to normal convolutional neural networks (CNN), but the last fully connected layer is replaced by another convolution layer with a large ""receptive field,"" quickly became the state-of-the-art method for image recognition tasks, as they bring the possibility to perform dense pixelwise classification of input images. Based on these advantages, i.e., the automatic extraction of relevant features, and dense classification of images, we propose an end-to-end FCN, which effectively combines the spectral and height information from different data sources and automatically generates a full resolution binary building mask. Our architecture (Fused-FCN4s) consists of three parallel networks merged at a late stage, which helps propagating fine detailed information from earlier layers to higher levels, in order to produce an output with more accurate building outlines. The inputs to the proposed Fused-FCN4s are three-band (RGB), panchromatic (PAN), and normalized digital surface model (nDSM) images. Experimental results demonstrate that the fusion of several networks is able to achieve excellent results on complex data. Moreover, the developed model was successfully applied to different cities to show its generalization capacity. © 2018 IEEE.","Binary classification; building footprint; data fusion; deep learning; fully convolutional networks (FCNs); satellite images","Buildings; Convolution; Data fusion; Deep learning; Extraction; Image classification; Image recognition; Neural networks; Remote sensing; Satellite imagery; Automatic building extraction; Binary classification; Building footprint; Convolutional networks; Convolutional Neural Networks (CNN); High resolution remote sensing; High resolution satellite imagery; Satellite images; Classification (of information); artificial neural network; building; data set; image analysis; remote sensing; satellite imagery"
"Hyperspectral image classification via a random patches network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047272285&doi=10.1016%2fj.isprsjprs.2018.05.014&partnerID=40&md5=cc38c2c111ced14bc2cf486e2a247042","Due to the remarkable achievements obtained by deep learning methods in the fields of computer vision, an increasing number of researches have been made to apply these powerful tools into hyperspectral image (HSI) classification. So far, most of these methods utilize a pre-training stage followed by a fine-tuning stage to extract deep features, which is not only tremendously time-consuming but also depends largely on a great deal of training data. In this study, we propose an efficient deep learning based method, namely, Random Patches Network (RPNet) for HSI classification, which directly regards the random patches taken from the image as the convolution kernels without any training. By combining both shallow and deep convolutional features, RPNet has the advantage of multi-scale, which possesses a better adaption for HSI classification, where different objects tend to have different scales. In the experiments, the proposed method and its two variants RandomNet and RPNet–single are tested on three benchmark hyperspectral data sets. The experimental results demonstrate the RPNet can yield a competitive performance compared with existing methods. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Feature extraction; Hyperspectral image classification; Random Patches Network (RPNet); RandomNet","Convolution; Deep learning; Feature extraction; Hyperspectral imaging; Spectroscopy; Competitive performance; Convolution kernel; Hyperspectral Data; Learning methods; Learning-based methods; Random Patches Network (RPNet); RandomNet; Training data; Image classification; algorithm; computer vision; data set; experimental design; image classification; network analysis; satellite data"
"Targeted grassland monitoring at parcel level using sentinels, street-level images and field observations","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058096998&doi=10.3390%2fRS10081300&partnerID=40&md5=39b03a9172fff0c94aa5ce2a173b2c43","The introduction of high-resolution Sentinels combined with the use of high-quality digital agricultural parcel registration systems is driving the move towards at-parcel agricultural monitoring. The European Union's Common Agricultural Policy (CAP) has introduced the concept of CAP monitoring to help simplify the management and control of farmers' parcel declarations for area support measures. This study proposes a proof of concept of this monitoring approach introducing and applying the concept of 'markers'. Using Sentinel-1- and -2-derived (S1 and S2) markers, we evaluate parcels declared as grassland in the Gelderse Vallei in the Netherlands covering more than 15,000 parcels. The satellite markers-respectively based on crop-type deep learning classification using S1 backscattering and coherence data and on detecting bare soil with S2 during the growing season-aim to identify grassland-declared parcels for which (1) the marker suggests another crop type or (2) which appear to have been ploughed during the year. Subsequently, a field-survey was carried out in October 2017 to target the parcels identified and to build a relevant ground-truth sample of the area. For the latter purpose, we used a high-definition camera mounted on the roof of a car to continuously sample geo-tagged digital imagery, as well as an app-based approach to identify the targeted fields. Depending on which satellite-based marker or combination of markers is used, the number of parcels identified ranged from 2.57% (marked by both the S1 and S2 markers) to 17.12% of the total of 11,773 parcels declared as grassland. After confirming with the ground-truth, parcels flagged by the combined S1 and S2 marker were robustly detected as non-grassland parcels (F-score = 0.9). In addition, the study demonstrated that street-level imagery collection could improve collection efficiency by a factor seven compared to field visits (1411 parcels/day vs. 217 parcels/day) while keeping an overall accuracy of about 90% compared to the ground-truth. This proposed way of collecting in situ data is suitable for the training and validating of high resolution remote sensing approaches for agricultural monitoring. Timely country-wide wall-to-wall parcel-level monitoring and targeted in-season parcel surveying will increase the efficiency and effectiveness of monitoring and implementing agricultural policies. © 2019 by the authors.","Agriculture; CAP; Crowdsourcing; Deep learning; Geo-tagged street-level pictures; Grassland; Meadow; Sentinel-1; Sentinel-2; TensorFlow","Agriculture; Crops; Crowdsourcing; Deep learning; Efficiency; Image enhancement; Remote sensing; Surveys; Geo-tagged street-level pictures; Grassland; Meadow; Sentinel-1; Sentinel-2; TensorFlow; Monitoring"
"Semantic labeling using a low-power neuromorphic platform","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048174460&doi=10.1109%2fLGRS.2018.2834522&partnerID=40&md5=a27aa4b7e7d60b2447b75007b68c6e9a","Deep learning is a powerful technique for the analysis of remote sensing imagery. For applications that require real-time processing on mobile platforms, a low power consumption processing unit is advantageous. The human brain is remarkably powerful at image recognition tasks while operating at very low power consumption levels. Neuromorphic computing designs aim to achieve energy efficiency through the use of spiking neurons and low-precision synapses to perform data processing. We demonstrate here the classification of red, green, blue and depth and hyperspectral data sets using a neuromorphic processing unit (IBM TrueNorth Neurosynaptic System). The convolutional neural-network architecture of the classifier network has been adapted to fit the neuromorphic architecture. The results on overhead imagery and hyperspectral imagery data show that neuromorphic platforms can achieve the state-of-the-art performance in semantic labeling with significantly ( 1000 ×) lower power consumption than traditional GPU-based solutions. © 2018 IEEE.","Brain-inspired computing; deep neural networks (DNNs); hyperspectral imaging; ISPRS 2-D semantic labeling; semantic labeling","Brain; Classification (of information); Data handling; Deep neural networks; Electric power utilization; Energy efficiency; Green computing; Hyperspectral imaging; Image recognition; Neural networks; Remote sensing; Semantics; Spectroscopy; Brain-inspired computing; Convolutional neural network; Hyper-spectral imageries; Lower-power consumption; Neuromorphic Architectures; Neuromorphic computing; Semantic labeling; State-of-the-art performance; Network architecture; artificial neural network; brain; classification; computer; energy efficiency; information processing; remote sensing; satellite imagery"
"Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue, France","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051656995&doi=10.3390%2frs10081217&partnerID=40&md5=7b03131fd32dec2d03d387eaf4f88b06","The development and improvement of methods to map agricultural land cover are currently major challenges, especially for radar images. This is due to the speckle noise nature of radar, leading to a less intensive use of radar rather than optical images. The European Space Agency Sentinel-1 constellation, which recently became operational, is a satellite system providing global coverage of Synthetic Aperture Radar (SAR) with a 6-days revisit period at a high spatial resolution of about 20 m. These data are valuable, as they provide spatial information on agricultural crops. The aim of this paper is to provide a better understanding of the capabilities of Sentinel-1 radar images for agricultural land cover mapping through the use of deep learning techniques. The analysis is carried out on multitemporal Sentinel-1 data over an area in Camargue, France. The data set was processed in order to produce an intensity radar data stack from May 2017 to September 2017. We improved this radar time series dataset by exploiting temporal filtering to reduce noise, while retaining as much as possible the fine structures present in the images. We revealed that even with classical machine learning approaches (K nearest neighbors, random forest, and support vector machines), good performance classification could be achieved with F-measure/Accuracy greater than 86% and Kappa coefficient better than 0.82. We found that the results of the two deep recurrent neural network (RNN)-based classifiers clearly outperformed the classical approaches. Finally, our analyses of the Camargue area results show that the same performance was obtained with two different RNN-based classifiers on the Rice class, which is the most dominant crop of this region, with a F-measure metric of 96%. These results thus highlight that in the near future these RNN-based techniques will play an important role in the analysis of remote sensing time series. © 2018 by the authors.","Agricultural land cover map; Camargue; France; Gated recurrent unit; K nearest neighbors; Long-short term memory; Random forest; Recurrent neural network; SAR; Sentinel-1; Vector support machines","Crops; Decision trees; Deep neural networks; Geometrical optics; Image enhancement; Long short-term memory; Mapping; Motion compensation; Nearest neighbor search; Recurrent neural networks; Remote sensing; Space flight; Space-based radar; Synthetic aperture radar; Time series; Time series analysis; Agricultural land; Camargue; France; Gated recurrent unit; K-nearest neighbors; Random forests; Sentinel-1; Radar imaging"
"A Deep Network Architecture for Super-Resolution-Aided Hyperspectral Image Classification with Classwise Loss","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048893491&doi=10.1109%2fTGRS.2018.2832228&partnerID=40&md5=709d7bbe627d6819aa0c875357440ac3","The supervised deep networks have shown great potential in improving the classification performance. However, training these supervised deep networks is very challenging for hyperspectral image given the fact that usually only a small amount of labeled samples are available. In order to overcome this problem and enhance the discriminative ability of the network, in this paper, we propose a deep network architecture for a super-resolution (SR)-aided hyperspectral image classification with classwise loss (SRCL). First, a three-layer SR convolutional neural network (SRCNN) is employed to reconstruct a high-resolution image from a low-resolution image. Second, an unsupervised triplet-pipeline CNN (TCNN) with an improved classwise loss is built to encourage intraclass similarity and interclass dissimilarity. Finally, SRCNN, TCNN, and a classification module are integrated to define the SRCL, which can be fine-tuned in an end-to-end manner with a small amount of training data. Experimental results on real hyperspectral images demonstrate that the proposed SRCL approach outperforms other state-of-the-art classification methods, especially for the task in which only a small amount of training data are available. © 1980-2012 IEEE.","Classwise loss; convolutional neural networks (CNNs); deep learning; hyperspectral image classification; remote sensing; super-resolution (SR)","Classification (of information); Convolution; Deep learning; Hyperspectral imaging; Image classification; Image enhancement; Independent component analysis; Neural networks; Optical resolving power; Remote sensing; Spectroscopy; Classification methods; Classification performance; Convolutional neural network; Discriminative ability; High resolution image; Low resolution images; State of the art; Super resolution; Network architecture; artificial neural network; experimental study; image analysis; image classification; remote sensing; spectral analysis; spectral resolution; supervised learning"
"Infrastructure quality assessment in Africa using satellite imagery and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051526695&doi=10.1145%2f3219819.3219924&partnerID=40&md5=4673dbc7b4e60127bca10dee25aaec58","The UN Sustainable Development Goals allude to the importance of infrastructure quality in three of its seventeen goals. However, monitoring infrastructure quality in developing regions remains prohibitively expensive and impedes efforts to measure progress toward these goals. To this end, we investigate the use of widely available remote sensing data for the prediction of infrastructure quality in Africa. We train a convolutional neural network to predict ground truth labels from the Afrobarometer Round 6 survey using Landsat 8 and Sentinel 1 satellite imagery. Our best models predict infrastructure quality with AUROC scores of 0.881 on Electricity, 0.862 on Sewerage, 0.739 on Piped Water, and 0.786 on Roads using Landsat 8. These performances are significantly better than models that leverage OpenStreetMap or nighttime light intensity on the same tasks. We also demonstrate that our trained model can accurately make predictions in an unseen country after fine-tuning on a small sample of images. Furthermore, the model can be deployed in regions with limited samples to predict infrastructure outcomes with higher performance than nearest neighbor spatial interpolation. © 2018 Association for Computing Machinery.","Computational sustainability; Deep learning; Remote sensing","Data mining; Forecasting; Neural networks; Remote sensing; Satellite imagery; Sustainable development; Computational sustainability; Convolutional neural network; Developing regions; Nearest neighbors; Night-time lights; Quality assessment; Remote sensing data; Spatial interpolation; Deep learning"
"New source of geospatial data: Crowdsensing by assisted and autonomous vehicle technologies","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051458451&doi=10.5194%2fisprs-archives-XLII-4-W8-211-2018&partnerID=40&md5=197a57fa7d8282ebd1ae854df67182fd","The ongoing proliferation of remote sensing technologies in the consumer market has been rapidly reshaping the geospatial data acquisition world, and subsequently, the data processing as well as information dissemination processes. Smartphones have clearly established themselves as the primary crowdsourced data generators recently, and provide an incredible volume of remote sensed data with fairly good georeferencing. Besides the potential to map the environment of the smartphone users, they provide information to monitor the dynamic content of the object space. For example, real-time traffic monitoring is one of the most known and widely used real-time crowdsensed application, where the smartphones in vehicles jointly contribute to an unprecedentedly accurate traffic flow estimation. Now we are witnessing another milestone to happen, as driverless vehicle technologies will become another major source of crowdsensed data. Due to safety concerns, the requirements for sensing are higher, as the vehicles should sense other vehicles and the road infrastructure under any condition, not just daylight in favorable weather conditions, and at very fast speed. Furthermore, the sensing is based on using redundant and complementary sensor streams to achieve a robust object space reconstruction, needed to avoid collisions and maintain normal travel patterns. At this point, the remote sensed data in assisted and autonomous vehicles are discarded, or partially recorded for R&D purposes. However, in the long run, as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication technologies mature, recording data will become a common place, and will provide an excellent source of geospatial information for road mapping, traffic monitoring, etc. This paper reviews the key characteristics of crowdsourced vehicle data based on experimental data, and then the processing aspects, including the Data Science and Deep Learning components. © Authors 2018.","Autonomous/Driverless Vehicles; Crowdsensing/Crowdsourcing; Deep Learning; Mobile Mapping","Data acquisition; Data handling; Deep learning; Information dissemination; Mapping; Open source software; Open systems; Remote sensing; Smartphones; Space optics; Traffic control; Vehicles; Autonomous vehicle technologies; Communication technologies; Complementary-sensors; Crowd sensing; Geo-spatial informations; Mobile mapping; Remote sensing technology; Vehicle to infrastructure (V2I); Vehicle to vehicle communications"
"Coastal mapping and kitesurfing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051561218&doi=10.5194%2fisprs-archives-XLII-4-W8-3-2018&partnerID=40&md5=76373542d95d6b75cda6d26b2b4a15e1","Collecting data on aquatic biodiversity is very challenging because of the difficulty to access underwater ecosystems. Over the years, field surveys have become easier and cheaper with the development of low cost electronics. Commercial and recreational vessels, including sailboats, can now substantially complement expensive scientific surveys and arrays of observation buoys deployed across the world oceans (Pesant et al., 2015, Karsenti et al., 2011). Meanwhile, a large variety of marine animals such as birds, mammals, and fish have become data collection platforms for both biological and environmental parameters through the advent of archival tags. It becomes obvious that data collection in coastal and high seas will become more popular and that citizen will play a growing role in acquiring information on ocean dynamics (physical, chemical and biological parameters). However, currently, very few attempts have been made to use Human beings as observation platforms. In this paper we describe large datasets (more than 200,000 pictures) that have been recently collected along the coast of Mauritius by using popular and cheap platforms such as kite surf and Stand Up Paddle. We describe the characteristics of the data collected and showcase how they can be geolocated and used to complement remote sensing and mapping in order to drastically extend the current scope of ""old school"" fieldwork. We point out some of the main limitations encountered which need to be addressed to foster this citizen science approach such as data storage and transmission, deep learning to automate image recognition. The methods are all based on open source softwares. © Authors 2018.","Action cameras; Citizen science; Coral reef mapping; Deep learning; Ocean and coastal observing systems; Photogrammetry; R; Surfing","Animals; Aquatic ecosystems; Biodiversity; Data acquisition; Deep learning; Digital storage; Fish; Image recognition; Mapping; Open systems; Photogrammetry; Remote sensing; Sailing vessels; Surveys; Aquatic biodiversity; Chemical and biologicals; Citizen science; Coral reef; Environmental parameter; Observing systems; Recreational vessels; Surfing; Open source software"
"Deep Learning Method for Hyperspectral Remote Sensing Images with Small Samples [小样本高光谱遥感图像深度学习方法]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061973235&doi=10.16182%2fj.issn1004731x.joss.201807039&partnerID=40&md5=a63339f824e6022ed6774a8c0bbaf73a","In order to solve the problem of large information dimension and fewer labeled training samples of hyperspectral remote sensing images, this paper proposes a hyperspectral remote sensing image classification framework HSI-CNN, which reduces the number of model parameters while maintaining the depth of neural network. Image pattern invariance and spectral channel contribution rate are analyzed, and the spectral redundancy information is reduced by principal component analysis. A full convolution neural network structure suitable for small sample hyperspectral remote sensing images is designed and the amount of network parameters is effectively reduced. Three kinds of HSI-CNN structures are proposed and compared. The experimental results on Pavia University and Salinas hyperspectral remote sensing data sets show that HSI-CNN can extract the spectral feature information only by using a small amount of training samples effectively. © 2018, The Editorial Board of Journal of System Simulation. All right reserved.","Convolution neural network; Deep learning; Hyperspectral remote sensing image classification; Principal component analysis; Spectral redundancy",
"Aircraft recognition in remote sensing images based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050636758&doi=10.1109%2fYAC.2018.8406498&partnerID=40&md5=faf54d60ee49390f71d36127bfc52600","Object recognition is one of the fundamental issues in the field of computer vision. In traditional methods, invariant features are extracted from segmented targets for recognition. However, there is no common method for segmentation of aircraft targets so far due to the complex backgrounds, illuminations, noise and other practical factors. Therefore, in this paper, we propose a method for aircraft identification in remote sensing images based on HOG and deep learning features. We train two classifiers, one is the SVM classifier based on HOG feature, and the other is a classifier based on deep convolutional neural network VGGNet. First, we use the SVM classifier to identify the aircraft in the picture roughly, then we use the deep learning classifier to exclude misidentified targets. In this way, this coarse to fine framework can significantly improve the speed and accuracy of aircraft recognition in remote sensing images. At the same time, our method has a better generalization capability than the traditional methods. Experimental results demonstrate the robustness of our method. © 2018 IEEE.","Aircraft recognition; Deep learning; HOG feature","Aircraft; Classification (of information); Deep learning; Deep neural networks; Image enhancement; Neural networks; Object recognition; Aircraft targets; Complex background; Deep convolutional neural networks; Generalization capability; Hog features; Invariant features; Learning classifiers; Remote sensing images; Remote sensing"
"Content based image retrieval of remote sensing images based on deep features [Uzaktan algilanmiş görüntülerin derin özniteliklere dayali içerik tabanli erişimi]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050795905&doi=10.1109%2fSIU.2018.8404707&partnerID=40&md5=fb549e5202e8cf682b7475472b0a5aac","This paper presents the results of applying deep features to the problem of content based image retieval of remote sensing images. Extraction of deep features from the last layers of a trained convolutional neural network from deep learning approaches demonstrates a higher performance than feature extraction using shallow methods. In this paper we used deep features obtained from a fine tuned convolutional neural network and we also focused on experiments of dimension reduction methods of these deep features. We test these methods using UCM Merced and RSSCN7 datasets. Despite their shorter length deep features obtained as a result of dimension reduction methods, are shown to achieve higher performance of content-based retrieval. © 2018 IEEE.","Content-based image retrieval (CBIR); Convolutional Neural Networks; Dimension reduction; Remote sensing","Convolution; Deep learning; Extraction; Neural networks; Remote sensing; Signal processing; Content based images; Content-Based Image Retrieval; Convolutional neural network; Dimension reduction; Dimension reduction method; Learning approach; Remote sensing images; Content based retrieval"
"Benchmark Meta-Dataset of High-Resolution Remote Sensing Imagery for Training Robust Deep Learning Models in Machine-Assisted Visual Analytics","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065994506&doi=10.1109%2fAIPR.2018.8707433&partnerID=40&md5=92eef0e1d7a644c58d9df4ec1b54937c","Recent years have seen the publication of various high-resolution remote sensing imagery benchmark datasets. These datasets, while diverse in design, have many co-occurring object classes that are of interest for various application domains of Earth observation. In this research, we present our evaluation of a new meta-benchmark dataset combining object classes from the UC Merced, WHU-RS19, PatternNet, and RESISC-45 benchmark datasets. We provide open-source resources to acquire the individual benchmark datasets and then agglomerate them into a new meta-dataset (MDS). Prior research has shown that contemporary deep convolutional neural networks are able to achieve cross-validation accuracies in the range of 95-100% for the 33 identified object classes. Our analysis shows that the overall accuracy for all object classes from these benchmarks is approximately 98.6%. In this work, we investigate the utility of agglomerating the benchmarks into an MDS to train more generalizable, and therefore translatable from lab to real-world, deep machine learning (DML) models. We evaluate numerous state-of-the-art architectures, as well as our data-driven DML model fusion techniques. Finally, we compare MDS performance with that of the benchmark datasets to evaluate the performance versus cost trade-off of using multiple DML in an ensemble system. © 2018 IEEE.",,"Benchmarking; Deep neural networks; Economic and social effects; Neural networks; Pattern recognition; Benchmark datasets; Convolutional neural network; Cross validation; Earth observations; Ensemble systems; High resolution remote sensing imagery; Overall accuracies; State of the art; Remote sensing"
"SAR Target Recognition with Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065970234&doi=10.1109%2fAIPR.2018.8707419&partnerID=40&md5=eb67b3b07d0a30bb3b59a722ad5abc45","The automated detection and classification of objects in imagery is an important topic for many applications in remote sensing. These can include the counting of cars and ships and the tracking of military vehicles for the defense and intelligence industry. Synthetic aperture radar (SAR) provides day/night and all-weather imaging capabilities. SAR is a powerful data source for Deep Learning (DL) algorithms to provide automatic target recognition (ATR) capabilities. DL classification was shown to be extremely effective on multi-spectral satellite imagery during the IARPA Functional Map of the World (fMoW). In our work we look to extend these techniques to SAR. We start by applying ResNet-18 to the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset. The MSTAR program, sponsored by DARPA and AFRL, consists of SAR collections of military style targets using an aerial X-band radar with one-foot resolution. We achieved an overall classification accuracy of 99% on 10 different classes of targets, confirming previously published results. We then extend this classifier to investigate an emerging target and the effects of limited training data on system performance. © 2018 IEEE.","AI; artificial intelligence; ATR; classification; CNN; deep learning; image understanding; machine learning; neural networks; recognition; synthetic aperture radar; target recognition","Antennas; Artificial intelligence; Automatic target recognition; Classification (of information); Deep learning; Image understanding; Learning systems; Military vehicles; Neural networks; Object detection; Radar imaging; Remote sensing; Satellite imagery; Synthetic aperture radar; Automated detection and classification; Classification accuracy; Defense and intelligences; Imaging capabilities; Limited training data; recognition; Stationary targets; Target recognition; Radar target recognition"
"Deep Learning Approach for Building Detection in Satellite Multispectral Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065985533&doi=10.1109%2fIS.2018.8710471&partnerID=40&md5=d2955c6419ab5243a2723bc6a84a3636","Building detection from satellite multispectral imagery data is being a fundamental but a challenging problem mainly because it requires correct recovery of building footprints from high-resolution images. In this work, we propose a deep learning approach for building detection by applying numerous enhancements throughout the process. Initial dataset is preprocessed by 2-sigma percentile normalization. Then data preparation includes ensemble modelling where 3 models were created while incorporating OpenStreetMap data. Binary Distance Transformation (BDT) is used for improving data labeling process and the U-Net (Convolutional Networks for Biomedical Image Segmentation) is modified by adding batch normalization wrappers. Afterwards, it is explained how each component of our approach is correlated with the final detection accuracy. Finally, we compare our results with winning solutions of SpaceNet 2 competition for real satellite multispectral images of Vegas, Paris, Shanghai and Khartoum, demonstrating the importance of our solution for achieving higher building detection accuracy. © 2018 IEEE.","Batch normalization; Binary distance transformation (BDT); Building detection; OpenStreetMap data; Satellite multispectral imagery; U- Net","Buildings; Image enhancement; Image segmentation; Intelligent systems; Metadata; Remote sensing; Satellite imagery; Batch normalization; Building detection; Distance transformation; Multi-spectral imagery; OpenStreetMap data; Deep learning"
"Hyperspectral Image Classification Based on Convolutional Neural Networks with Adaptive Network Structure","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065975229&doi=10.1109%2fICOT.2018.8705785&partnerID=40&md5=7b333a3c9dea811b455689dc874f3d75","Hyperspectral image (HSI) contains various spectral and spatial information, which is often used in remote sensing image analysis and widely used in areas of the people's daily life. Due to the advances of powerful feature representations, deep learning based methods are receiving increasing attention and getting acceptable classification results. As a representative of the deep learning methods, convolutional neural networks (CNNs) have shown their great ability in HSI classification tasks. However, the hyper-parameters of CNNs based HSI classification methods are often obtained through experience (e.g., the number of convolutional layers), and how to determine the number of convolutional layers (the model of convolutional layers connection) via data is seldom studied in existing CNNs based HSI classification methods. To deal with this problem, this paper proposes an effective approach to learn a structure of CNNs (e.g., a data-determined layers number of CNNs) in HSI classification tasks, where the CNNs structure can be learned via genetic algorithm (GA). with the learned adaptive CNNs structure can aquire better HSI classification result. Experimental results on two datasets demonstrate the effectiveness of the proposed method. © 2018 IEEE.","adaptive CNNs structure; Convolutional neural networks; hyper-parameter; hyperspectral image classification; the number of convolutional layers","Citrus fruits; Convolution; Deep learning; Genetic algorithms; Hyperspectral imaging; Neural networks; Remote sensing; Spectroscopy; Classification methods; Classification results; Convolutional neural network; Feature representation; Hyper-parameter; Learning-based methods; Remote sensing images; the number of convolutional layers; Image classification"
"Roads Detection of Aerial Image with FCN-CRF Model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065413697&doi=10.1109%2fVCIP.2018.8698718&partnerID=40&md5=d764f7c84210a2a821b00df91e525858","this paper describes a deep learning based model for roads detection in Aerial image. In general, standard CNN networks would have less ability for tiny objects detection in remote sensing image. With this regard, we propose a novel fully convolutional network, which utilizes deconvolution layers and feature map fussing to take as input intensity and pixel-wise labeling. Moreover, the class prediction are used as the input to Condition Random Field (CRF) for the final pixel prediction. The Batch Normalization (BN) algorithm and two stages training strategy were used in our model to reduce the time cost of model training. Several experimental results conducted in Massach-useets. Road dataset demonstrate the superiority of our model with respect to accuracy and time cost. © 2018 IEEE.","Aerial image; CRF; FCN; Road detection","Antennas; Deep learning; Pixels; Remote sensing; Roads and streets; Visual communication; Aerial images; Convolutional networks; Learning Based Models; Objects detection; Pixel-wise labeling; Remote sensing images; Road detection; Training strategy; Object detection"
"Comparison on Generative Adversarial Networks - A Study","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065640687&doi=10.1109%2fICSCCC.2018.8703267&partnerID=40&md5=58155d051aa1abe8791a3d2595bfc668","Various new deep learning models have been invented, among which generative adversarial networks have gained exceptional prominence in last four years due to its property of image synthesis. GANs have been utilized in diverse fields ranging from conventional areas like image processing, biomedical signal processing, remote sensing, video generation to even off beat areas like sound and music generation. In this paper, we provide an overview of GANs along with its comparison with other networks, as well as different versions of Generative Adversarial Networks. © 2018 IEEE.","Generative Adversarial Networks; Machine learning","Deep learning; Learning systems; Remote sensing; Adversarial networks; Diverse fields; Image synthesis; Learning models; Video generation; Image processing"
"Illegal Buildings Detection from Satellite Images using GoogLeNet and Cadastral Map","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065979192&doi=10.1109%2fIS.2018.8710565&partnerID=40&md5=c2166fdd8750362bac35dd5a2ee2b420","Automatic illegal building detection from satellite imagery is a specific and important problem for both research community and government agencies, which has not been sufficiently investigated since it combines the challenge of automatic remote sensing data interpretation and verification with a cadastral map. Recovery of building footprints from satellite images is a very complicated process because building areas and their surroundings are represented with various color intensities and complex features. This paper proposes a methodology that incorporates image processing techniques with deep learning tools for the automatization of State Inspection in the field of illegal buildings detection. Our approach integrates four various computer vision-based building detection techniques for generating region proposals with the following classification by retrained GoogLeNet convolutional neural network (CNN) and the final validation whether detected building areas are legal according to a state cadastral map. Our tests with satellite imagery dataset show that the proposed approach achieves acceptable results in both building detection and legality assessment. © 2018 IEEE.","Cadastral map; GoogLeNet CNN; Illegal buildings detection; Region proposal generation; Satellite imagery; Transfer learning","Buildings; Crime; Deep learning; Image processing; Intelligent systems; Neural networks; Remote sensing; Statistical tests; Cadastral maps; Convolutional neural network; GoogLeNet CNN; Image processing technique; Region proposal generation; Remote sensing data; Research communities; Transfer learning; Satellite imagery"
"Aircraft Detection Based on Multiple Scale Faster-RCNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066314786&doi=10.1109%2fICVRV.2018.00026&partnerID=40&md5=d0f5fd967123b961f4284a60384324ca","Remote sensing image recognition has been widely used in civil and military fields. In view of plenty of interference factors in remote-sensing aircraft, such as shade, noise, the changing of perspective, etc. An improved target recognition algorithm in remote sensing image based on Faster-RCNN is proposed which uses a standard Region Proposal Network (RPN) generation and incorporates feature maps from shallower convolution feature maps. Convolution neural network is adopted to recognize aircraft target in complex environment, enhance the global context and local information to avoid information loss in the process of feature extraction, which improves recognition rate. Simulation results show that the feasibility of aircraft target recognition algorithm in remoting sensing image and the scale and posture changes of target can be overcome. Meanwhile, the proposed algorithm has higher recognition effect and stronger robustness than traditional Faster-RCNN and BP neural network and support vector machine (SVM) methods. © 2018 IEEE.","aircraft recognition; convolution neural network; deep learning; remote sensing image","Aircraft; Aircraft detection; Convolution; Deep learning; Image enhancement; Image recognition; Military photography; Neural networks; Optical character recognition; Support vector machines; Virtual reality; Visualization; BP neural networks; Complex environments; Convolution neural network; Interference factor; Local information; Remote sensing aircrafts; Remote sensing images; Target recognition algorithms; Remote sensing"
"Landscape classification with deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049867479&doi=10.3390%2fgeosciences8070244&partnerID=40&md5=d4e72a07385f62ebd4a89101983cc624","The application of deep learning, specifically deep convolutional neural networks (DCNNs), to the classification of remotely-sensed imagery of natural landscapes has the potential to greatly assist in the analysis and interpretation of geomorphic processes. However, the general usefulness of deep learning applied to conventional photographic imagery at a landscape scale is, at yet, largely unproven. If DCNN-based image classification is to gain wider application and acceptance within the geoscience community, demonstrable successes need to be coupled with accessible tools to retrain deep neural networks to discriminate landforms and land uses in landscape imagery. Here, we present an efficient approach to train/apply DCNNs with/on sets of photographic images, using a powerful graphical method called a conditional random field (CRF), to generate DCNN training and testing data using minimal manual supervision. We apply the method to several sets of images of natural landscapes, acquired from satellites, aircraft, unmanned aerial vehicles, and fixed camera installations. We synthesize our findings to examine the general effectiveness of transfer learning to landscape-scale image classification. Finally, we show how DCNN predictions on small regions of images might be used in conjunction with a CRF for highly accurate pixel-level classification of images. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Aerial imagery; Deep learning; Image classification; Image segmentation; Land cover; Land use; Landforms; Machine learning; Remote sensing; Unmanned aerial systems",
"Using Semantic Relationships among Objects for Geospatial Land Use Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065962270&doi=10.1109%2fAIPR.2018.8707405&partnerID=40&md5=f87bc2770adc49ab73d2f4eacc212ffa","The geospatial land recognition is often cast as a local-region based classification problem. We show in this work, that prior knowledge, in terms of global semantic relationships among detected regions, allows us to leverage semantics and visual features to enhance land use classification in aerial imagery. To this end, we first estimate the top-k labels for each region using an ensemble of CNNs called Hydra. Twelve different models based on two state-of-the-art CNN architectures, ResNet and DenseNet, compose this ensemble. Then, we use Grenander's canonical pattern theory formalism coupled with the common-sense knowledge base, ConceptNet, to impose context constraints on the labels obtained by deep learning algorithms. These constraints are captured in a multi-graph representation involving generators and bonds with a flexible topology, unlike an MRF or Bayesian networks, which have fixed structures. Minimizing the energy of this graph representation results in a graphical representation of the semantics in the given image. We show our results on the recent fMoW challenge dataset. It consists of 1,047,691 images with 62 different classes of land use, plus a false detection category. The biggest improvement in performance with the use of semantics was for false detections. Other categories with significantly improved performance were: zoo, nuclear power plant, park, police station, and space facility. For the subset of fMow images with multiple bounding boxes the accuracy is 72.79% without semantics and 74.06% with semantics. Overall, without semantic context, the classification performance was 77.04%. With semantics, it reached 77.98%. Considering that less than 20% of the dataset contained more than one ROI for context, this is a significant improvement that shows the promise of the proposed approach. © 2018 IEEE.","convolutional neural networks; pattern theory; Remote sensing","Aerial photography; Antennas; Bayesian networks; Deep learning; Facilities; Knowledge based systems; Knowledge management; Land use; Magnetorheological fluids; Neural networks; Nuclear fuels; Nuclear power plants; Pattern recognition; Remote sensing; Semantics; Topology; Classification performance; Commonsense knowledge; Convolutional neural network; Graph representation; Graphical representations; Landuse classifications; Pattern theory; Semantic relationships; Image enhancement"
"Exploring geo-tagged photos for land cover validation with deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046867280&doi=10.1016%2fj.isprsjprs.2018.04.025&partnerID=40&md5=4cd714895dc52d36a2a820aff88c46aa","Land cover validation plays an important role in the process of generating and distributing land cover thematic maps, which is usually implemented by high cost of sample interpretation with remotely sensed images or field survey. With an increasing availability of geo-tagged landscape photos, the automatic photo recognition methodologies, e.g., deep learning, can be effectively utilised for land cover applications. However, they have hardly been utilised in validation processes, as challenges remain in sample selection and classification for highly heterogeneous photos. This study proposed an approach to employ geo-tagged photos for land cover validation by using the deep learning technology. The approach first identified photos automatically based on the VGG-16 network. Then, samples for validation were selected and further classified by considering photos distribution and classification probabilities. The implementations were conducted for the validation of the GlobeLand30 land cover product in a heterogeneous area, western California. Experimental results represented promises in land cover validation, given that GlobeLand30 showed an overall accuracy of 83.80% with classified samples, which was close to the validation result of 80.45% based on visual interpretation. Additionally, the performances of deep learning based on ResNet-50 and AlexNet were also quantified, revealing no substantial differences in final validation results. The proposed approach ensures geo-tagged photo quality, and supports the sample classification strategy by considering photo distribution, with accuracy improvement from 72.07% to 79.33% compared with solely considering the single nearest photo. Consequently, the presented approach proves the feasibility of deep learning technology on land cover information identification of geo-tagged photos, and has a great potential to support and improve the efficiency of land cover validation. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Accuracy assessment; Convolutional neural network; Crowdsourced photos; Land cover; Sample classification","Image enhancement; Maps; Neural networks; Probability distributions; Accuracy assessment; Convolutional neural network; Crowdsourced photos; Land cover; Sample classification; Deep learning; accuracy assessment; artificial neural network; data interpretation; heterogeneity; land cover; learning; model validation; probability; recognition; remote sensing; satellite imagery; tagging; California; United States"
"Object-based deep convolutional autoencoders for high-resolution remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050095109&doi=10.1117%2f1.JRS.12.035002&partnerID=40&md5=6fb40f828b2c0e4088e97945ad1ed98c","Deep learning has been applied to many fields due to its capability for unsupervised feature learning. However, it is unsuitable for remote sensing image classification of deep convolutional networks due to the regular and fixed shape of receptive fields in current methods, which cannot freely adapt for ground objects with various shapes and scales. To tackle this problem, we propose object-based deep convolutional autoencoders (ODCAEs) to encode high-resolution remote sensing image features automatically. The receptive fields of deep convolutional autoencoders were designed based on a fractal net evolution approach to adapt for various ground objects in images, retaining category purity while providing adequate information. We collected 109,333 samples of seven classes from high-resolution satellite images over various locations to train the ODCAE, which encoded high level features automatically, coupled with a support vector machine for classification. We assessed the classification results using two WorldView-II image locations. The proposed ODCAE approach achieved higher accuracy than three manual design feature systems. Thus, the proposed ODCAE approach is useful and efficient for feature learning problems for high-resolution remote sensing images. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","high resolution; image classification; object-based deep convolutional autoencoders; remote sensing","Convolution; Deep learning; Machine learning; Remote sensing; Support vector machines; Autoencoders; Classification results; Convolutional networks; High resolution; High resolution remote sensing images; High resolution satellite images; Remote sensing image classification; Unsupervised feature learning; Image classification"
"Deep learning for pixel-level image fusion: Recent advances and future prospects","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033460261&doi=10.1016%2fj.inffus.2017.10.007&partnerID=40&md5=93e443cfd5eb69f4c88913c9acec15c7","By integrating the information contained in multiple images of the same scene into one composite image, pixel-level image fusion is recognized as having high significance in a variety of fields including medical imaging, digital photography, remote sensing, video surveillance, etc. In recent years, deep learning (DL) has achieved great success in a number of computer vision and image processing problems. The application of DL techniques in the field of pixel-level image fusion has also emerged as an active topic in the last three years. This survey paper presents a systematic review of the DL-based pixel-level image fusion literature. Specifically, we first summarize the main difficulties that exist in conventional image fusion research and discuss the advantages that DL can offer to address each of these problems. Then, the recent achievements in DL-based image fusion are reviewed in detail. More than a dozen recently proposed image fusion methods based on DL techniques including convolutional neural networks (CNNs), convolutional sparse representation (CSR) and stacked autoencoders (SAEs) are introduced. At last, by summarizing the existing DL-based image fusion methods into several generic frameworks and presenting a potential DL-based framework for developing objective evaluation metrics, we put forward some prospects for the future study on this topic. The key issues and challenges that exist in each framework are discussed. © 2017 Elsevier B.V.","Convolutional neural network; Convolutional sparse representation; Deep learning; Image fusion; Stacked autoencoder","Convolution; Deep learning; Image processing; Learning systems; Medical imaging; Medical problems; Neural networks; Pixels; Remote sensing; Security systems; Auto encoders; Convolutional neural network; Digital photography; Image fusion methods; Image processing problems; Objective evaluation; Pixel-level image fusion; Sparse representation; Image fusion"
"Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050484641&doi=10.3390%2frs10071119&partnerID=40&md5=28970c356b9c6ff0382aa29ca9d86353","Despite recent advances of deep Convolutional Neural Networks (CNNs) in various computer vision tasks, their potential for classification of multispectral remote sensing images has not been thoroughly explored. In particular, the applications of deep CNNs using optical remote sensing data have focused on the classification of very high-resolution aerial and satellite data, owing to the similarity of these data to the large datasets in computer vision. Accordingly, this study presents a detailed investigation of state-of-the-art deep learning tools for classification of complex wetland classes using multispectral RapidEye optical imagery. Specifically, we examine the capacity of seven well-known deep convnets, namely DenseNet121, InceptionV3, VGG16, VGG19, Xception, ResNet50, and InceptionResNetV2, for wetland mapping in Canada. In addition, the classification results obtained from deep CNNs are compared with those based on conventional machine learning tools, including Random Forest and Support Vector Machine, to further evaluate the efficiency of the former to classify wetlands. The results illustrate that the full-training of convnets using five spectral bands outperforms the other strategies for all convnets. InceptionResNetV2, ResNet50, and Xception are distinguished as the top three convnets, providing state-of-the-art classification accuracies of 96.17%, 94.81%, and 93.57%, respectively. The classification accuracies obtained using Support Vector Machine (SVM) and Random Forest (RF) are 74.89% and 76.08%, respectively, considerably inferior relative to CNNs. Importantly, InceptionResNetV2 is consistently found to be superior compared to all other convnets, suggesting the integration of Inception and ResNet modules is an efficient architecture for classifying complex remote sensing scenes such as wetlands. © 2018 by the authors.","Convolutional neural network; Deep learning; Fine-tuning; Full-training; Land cover classification; Machine learning; Multispectral images; RapidEye; Wetland","Antennas; Classification (of information); Complex networks; Computer vision; Convolution; Decision trees; Deep learning; Learning systems; Mapping; Neural networks; Remote sensing; Support vector machines; Wetlands; Convolutional neural network; Fine tuning; Land cover classification; Multispectral images; Rapideye; Deep neural networks"
"Feature extraction and recognition of erosion gully based on remote sensing image in the black soil region in Northeast China [东北黑土区侵蚀沟遥感影像特征提取与识别]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051716175&doi=10.11834%2fjrs.20187165&partnerID=40&md5=5cd787b51fce464754eaeb730352760a","Northeast China is a major commodity grain base in China. The protection of cultivated lands in Northeast China is crucial for safeguarding the food security in China. The recognition of erosion gullies is an important means of monitoring soil erosion. Furthermore, remote sensing technology is extensively used in this field given the multiple advantages of this technology. However, the traditional methods based on remote sensing mostly depend on manual interpretations. Therefore, the degree of the automation and the efficiency are relatively low. In this study, multi-level features were extracted, thereby effectively describing the specific objects by using machine and deep learnings, and erosion gullies were identified based on these features to improve the accuracy and efficiency of recognizing erosion gullies. In this study, we first cut the remote sensing images in a fixed size and labeled these images manually to create datasets as training samples that consist of two categories, namely, farmland and erosion gully. Second, we extracted spectral and textural features based on this dataset as low-level features, encoded SIFT features through ScSPM as middle-level features, and extracted high-level features by using CNN. Third, a linear SVM and a softmax classifier were applied to classify the remote sensing images based on the multi-level features to identify the images with erosion gullies. Finally, we completed a set of methods to extract the feature and recognize the erosion gully, thereby providing a robust support for protecting arable land in the black soil area of Northeast China. The multi-level features extracted through the proposed method demonstrate specific capabilities in identifying erosion gully images. In the test phase, results show that the recognition outcome based on low-level features exhibits the lowest accuracy (91.1%), whereas the recognition accuracy based on middle-level features is the highest (98.5%). However, both features require a manual design. Hence, the degree of automation is limited. By contrast, the CNN can extract high-level features and automatically achieve an "" end-to-end"" learning, which highly improves the degree of automation of erosion gully recognition. Furthermore, the recognition accuracy based on high-level features is 95.5%, which satisfies the expectation of this study. The recognition accuracy is slightly lower in the validation phase than in the test phase because the images typically contain several irrelevant objects in the practical application, thereby preventing the improvement of the accuracy. However, the proposed method can generally identify the erosion gullies in the images with a reasonable practicality. Low-level features demonstrate several advantages, such as simple calculation and low time consumed. However, the capability to describe the erosion gully is relatively poor, thus resulting in low recognition accuracy. By contrast, the methods based on middle- and high-level features can identify nearly all the erosion gullies in the images, although these methods are time-consuming during the early training phase. Specifically, the method based on high-level features can automatically recognize the erosion gully. This study shows that deep learning has a great potential in remote sensing image application. If the sample size is continuously increased and the network structure expanded, then the recognition accuracy of erosion gully can be further improved. © 2018, Science Press. All right reserved.","Convolution neural network; Erosion gullies; High-level features; Low-level features; Middle-level features; Training samples","Automation; Deep learning; Efficiency; Erosion; Food supply; Landforms; Remote sensing; Sampling; Soils; Convolution neural network; High-level features; Low-level features; Middle-level features; Training sample; Image enhancement"
"A light and faster regional convolutional neural network for object detection in optical remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047063507&doi=10.1016%2fj.isprsjprs.2018.05.005&partnerID=40&md5=8ebf4e068aabf833eb62aac0cc0ec0ab","Detection of objects from satellite optical remote sensing images is very important for many commercial and governmental applications. With the development of deep convolutional neural networks (deep CNNs), the field of object detection has seen tremendous advances. Currently, objects in satellite remote sensing images can be detected using deep CNNs. In general, optical remote sensing images contain many dense and small objects, and the use of the original Faster Regional CNN framework does not yield a suitably high precision. Therefore, after careful analysis we adopt dense convoluted networks, a multi-scale representation and various combinations of improvement schemes to enhance the structure of the base VGG16-Net for improving the precision. We propose an approach to reduce the test-time (detection time) and memory requirements. To validate the effectiveness of our approach, we perform experiments using satellite remote sensing image datasets of aircraft and automobiles. The results show that the improved network structure can detect objects in satellite optical remote sensing images more accurately and efficiently. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep convolution neural network; Deep learning (DL); Object detection; Remote sensing images","Convolution; Deep neural networks; Image enhancement; Neural networks; Object detection; Object recognition; Satellites; Convolution neural network; Convolutional neural network; Deep convolutional neural networks; Governmental applications; Multiscale representations; Remote sensing images; Satellite optical remote sensing; Satellite remote sensing; Remote sensing; accuracy assessment; airborne survey; algorithm; artificial neural network; automobile; data set; detection method; precision; remote sensing; satellite imagery"
"Aircraft type recognition in remote sensing images based on feature learning with conditional generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050462362&doi=10.3390%2frs10071123&partnerID=40&md5=cb7314439db7c09e4a2c52e34aba21af","Aircraft type recognition plays an important role in remote sensing image interpretation. Traditional methods suffer from bad generalization performance, while deep learning methods require large amounts of data with type labels, which are quite expensive and time-consuming to obtain. To overcome the aforementioned problems, in this paper, we propose an aircraft type recognition framework based on conditional generative adversarial networks (GANs). First, we design a new method to precisely detect aircrafts' keypoints, which are used to generate aircraft masks and locate the positions of the aircrafts. Second, a conditional GAN with a region of interest (ROI)-weighted loss function is trained on unlabeled aircraft images and their corresponding masks. Third, an ROI feature extraction method is carefully designed to extract multi-scale features from the GAN in the regions of aircrafts. After that, a linear support vector machine (SVM) classifier is adopted to classify each sample using their features. Benefiting from the GAN, we can learn features which are strong enough to represent aircrafts based on a large unlabeled dataset. Additionally, the ROI-weighted loss function and the ROI feature extraction method make the features more related to the aircrafts rather than the background, which improves the quality of features and increases the recognition accuracy significantly. Thorough experiments were conducted on a challenging dataset, and the results prove the effectiveness of the proposed aircraft type recognition framework. © 2018 by the authors.","Aircraft type recognition; Convolutional neural networks; Generative adversarial networks","Deep learning; Extraction; Feature extraction; Image segmentation; Neural networks; Remote sensing; Support vector machines; Adversarial networks; Aircraft type recognition; Convolutional neural network; Feature extraction methods; Generalization performance; Linear Support Vector Machines; Remote sensing image interpretations; Weighted loss function; Fighter aircraft"
"Deriving high spatiotemporal remote sensing images using deep convolutional network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050504377&doi=10.3390%2frs10071066&partnerID=40&md5=de512119c8536d73d5704045256f0afd","Due to technical and budget limitations, there are inevitably some trade-offs in the design of remote sensing instruments, making it difficult to acquire high spatiotemporal resolution remote sensing images simultaneously. To address this problem, this paper proposes a new data fusion model named the deep convolutional spatiotemporal fusion network (DCSTFN), which makes full use of a convolutional neural network (CNN) to derive high spatiotemporal resolution images from remotely sensed images with high temporal but low spatial resolution (HTLS) and low temporal but high spatial resolution (LTHS). The DCSTFN model is composed of three major parts: the expansion of the HTLS images, the extraction of high frequency components from LTHS images, and the fusion of extracted features. The inputs of the proposed network include a pair of HTLS and LTHS reference images from a single day and another HTLS image on the prediction date. Convolution is used to extract key features from inputs, and deconvolution is employed to expand the size of HTLS images. The features extracted from HTLS and LTHS images are then fused with the aid of an equation that accounts for temporal ground coverage changes. The output image on the prediction day has the spatial resolution of LTHS and temporal resolution of HTLS. Overall, the DCSTFN model establishes a complex but direct non-linear mapping between the inputs and the output. Experiments with MODerate Resolution Imaging Spectroradiometer (MODIS) and Landsat Operational Land Imager (OLI) images show that the proposed CNN-based approach not only achieves state-of-the-art accuracy, but is also more robust than conventional spatiotemporal fusion algorithms. In addition, DCSTFN is a faster and less time-consuming method to perform the data fusion with the trained network, and can potentially be applied to the bulk processing of archived data. © 2018 by the authors.","Convolutional neural network; Deep learning; Landsat; MODIS; Spatiotemporal data fusion","Budget control; Convolution; Data handling; Deep learning; Economic and social effects; Image fusion; Image resolution; Neural networks; Radiometers; Remote sensing; Satellite imagery; Convolutional neural network; Convolutional Neural Networks (CNN); LANDSAT; Moderate resolution imaging spectroradiometer; MODIS; Remote sensing instruments; Spatio-temporal data; Spatio-temporal resolution; Image processing"
"Fuzzy autoencoder for multiple change detection in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052968467&doi=10.1117%2f1.JRS.12.035014&partnerID=40&md5=bd7165fd69ef2fb7507d8e5a129d140e","This paper establishes the fuzzy autoencoder (FAE) to detect multiple changes between two one-dimensional multitemporal images. Different from the traditional approaches based on the pixel intensity, FAE includes a multilayer structure through self-reconstruction to extract the feature from an image. Due to the existence of noise in the images, the raw data tend to be corrupted and fail to detect the real changes. Therefore, the fuzzy number is introduced to the autoencoder to establish the FAE which is able to suppress the noise and learn robust features. In this way, the information in the fuzzy domain is introduced into the input, and in practice the fuzzy domain is discretized to facilitate the calculation. In addition, the weighted Frobenius norm is used to establish the loss function which can be minimized to achieve the optimal parameters. The framework is highlighted by the newly designed FAE. As the fuzzy number is introduced into the autoencoder, more information concerning the fuzzy domain is taken into consideration and thus the impact brought by the noise is relieved to a large extent. Hence, the FAE can generate robust features, enhancing its performance on deep feature representation learning. Several tests on three datasets show us the proper parameter settings, and the experimental results from the FAE framework and the other compared approaches demonstrate its effectiveness and robustness in terms of accuracy and elapsed time. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","change detection; fuzzy autoencoder; fuzzy neural networks; fuzzy number; remote sensing","Deep learning; Fuzzy logic; Fuzzy rules; Remote sensing; Auto encoders; Change detection; Feature representation; Fuzzy numbers; Information concerning; Multilayer structures; Remote sensing images; Traditional approaches; Fuzzy neural networks"
"Dense connectivity based two-stream deep feature fusion framework for aerial scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050505107&doi=10.3390%2frs10071158&partnerID=40&md5=b95e14ca8645f2cb7ccdb561a2413f3a","Aerial scene classification is an active and challenging problem in high-resolution remote sensing imagery understanding. Deep learning models, especially convolutional neural networks (CNNs), have achieved prominent performance in this field. The extraction of deep features from the layers of a CNN model is widely used in these CNN-based methods. Although the CNN-based approaches have obtained great success, there is still plenty of room to further increase the classification accuracy. As a matter of fact, the fusion with other features has great potential for leading to the better performance of aerial scene classification. Therefore, we propose two effective architectures based on the idea of feature-level fusion. The first architecture, i.e., texture coded two-stream deep architecture, uses the raw RGB network stream and the mapped local binary patterns (LBP) coded network stream to extract two different sets of features and fuses them using a novel deep feature fusion model. In the second architecture, i.e., saliency coded two-stream deep architecture, we employ the saliency coded network stream as the second stream and fuse it with the raw RGB network stream using the same feature fusion model. For sake of validation and comparison, our proposed architectures are evaluated via comprehensive experiments with three publicly available remote sensing scene datasets. The classification accuracies of saliency coded two-stream architecture with our feature fusion model achieve 97.79%, 98.90%, 94.09%, 95.99%, 85.02%, and 87.01% on the UC-Merced dataset (50% and 80% training samples), the Aerial Image Dataset (AID) (20% and 50% training samples), and the NWPU-RESISC45 dataset (10% and 20% training samples), respectively, overwhelming state-of-the-art methods. © 2018 by the authors.","Aerial scene classification; Convolutional neural network (CNN); Mapping local binary patterns (LBP) codes; Remote sensing; Saliency detection; Two-stream deep feature fusion model","Antennas; Classification (of information); Convolution; Deep learning; Network coding; Neural networks; Remote sensing; Sampling; Convolutional Neural Networks (CNN); Feature fusion; Local binary pattern (LBP); Saliency detection; Scene classification; Network architecture"
"Ship classification for unbalanced SAR dataset based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051804139&doi=10.1117%2f1.JRS.12.035010&partnerID=40&md5=1df6c40aea76a9972a09eb88fe1960a9","Ship classification in synthetic aperture radar (SAR) images is essential in remote sensing but still full of challenges in the deep learning era. The unbalanced dataset and lack of models are two limitations. Upsampling with data augmentation and ratio batching are proposed to solve the first problem. Upsampling with data augmentation is upsampling by cropping and flipping. It can improve the diversity of the dataset. Ratio batching is realized by choosing the same amount of ships per class in each minibatch. It can make the model converge faster and better. To solve the second problem, a new loss function and convolutional neural network model are proposed. The new loss function can maximize the intraclass compactness and interclass separation simultaneously. Dense residual network has two submodules. One is the identity mapping through elementwise summation to reuse old features. The other is dense connection through concatenation to exploit new features. The designed architecture is suitable for the task of SAR ship classification. We use the confusion matrix and accuracy averaged on classes to measure the performance. From the experiments, we can find that the proposed ideas have excellent performance especially on the rare classes. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","convolutional neural network; loss function; ship classification; synthetic aperture radar; unbalanced","Classification (of information); Convolution; Deep learning; Neural networks; Radar imaging; Remote sensing; Ships; Signal sampling; Confusion matrices; Convolutional neural network; Data augmentation; Identity mappings; Loss functions; Ship classification; Synthetic aperture radar (SAR) images; unbalanced; Synthetic aperture radar"
"Early drought plant stress detection with bi-directional long-term memory networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050029341&doi=10.14358%2fPERS.84.7.459&partnerID=40&md5=6f34f7946325ea0cd1e883793bec931b","Early drought stress detection is a promising strategy that enables us to move from a reactive to a more proactive approach to manage drought risks and impacts. In this work, we apply for the first time the Bidirectional Long Short-Term Memory (BLSTM) networks to RGB images for accurate drought plant stress detection in the early stage. In addition, an optimal data collection strategy (ODCS) is investigated to use less time and manpower for the purpose of accurate early drought stress condition detection. The proposed method is validated on two independently collected RGB image datasets. In both datasets, the BLSTM method achieves competitive classification performances compared to three other deep learning methods. By using the proposed ODCS, our method can use only ⅔ of the entire dataset to achieve 74.6 percent F-score for the patch sequence classification and 72.0 percent F-score for the image sequence classification. © 2018 American Society for Photogrammetry and Remote Sensing.",,"Deep learning; Drought; Stresses; Bi-directional; Classification performance; Drought stress; Drought stress conditions; Learning methods; Long term memory; Pro-active approach; Sequence classification; Classification (of information); classification; data acquisition; data set; detection method; drought stress; image analysis; learning; memory; model validation"
"Hyperspectral image classification based on deep auto-encoder and hidden Markov random field","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050218440&doi=10.1109%2fFSKD.2017.8393336&partnerID=40&md5=aedb268c33bed82325b0e0ea9702adbe","Hyperspectral Image (HSI) classification is one of the most persistent issue in remote sensing field. Recently, deep learning has attracted attention in HSI Classification field due to its accuracy and stronger generalization. This paper proposes a new spectral-spatial HSI classification approach developed on the deep learning concept of stacked-auto-encoders (SAE) based deep feature extraction and hidden Markov random field based segmentation. Specifically, First the SAE model is implemented as a spectral information-based classifier to extract the deep spectral features. Second, spatial information is obtained by using effective Hidden Markov random field (HMRF) based segmentation technique. Finally, maximum voting based criteria is employed to merge the extracted spectral and spatial information, which results in the precise spectral-spatial HSI classification. The characterization of the HSI with spectral spatial features results into more comprehensive analysis of HSI and to a more accurate classification. In general, use of spectral information resulted from the SAE process and spatial information by means of HMRF based segmentation and merging of spectral and spatial information by means of maximum voting based criteria, has a significant effect on the accuracy of the HSI classification. Experiments on real diverse hyperspectral data sets with different contexts and resolutions acquired by AVIRIS and ROSIS sensors show the accuracy of the proposed method and confirms that results of the proposed classification approach are comparable to several recently proposed HSI classification techniques. © 2017 IEEE.",,"Deep learning; Fuzzy systems; Image classification; Image segmentation; Information use; Markov processes; Remote sensing; Signal encoding; Spectroscopy; Classification approach; Classification technique; Comprehensive analysis; Hidden Markov random fields; Hyperspectral Data; Segmentation techniques; Spatial informations; Spectral information; Classification (of information)"
"Deep transfer learning for crop yield prediction with remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050459639&doi=10.1145%2f3209811.3212707&partnerID=40&md5=6ab3261dea6837cc7f4ef696578dc5d8","Accurate prediction of crop yields in developing countries in advance of harvest time is central to preventing famine, improving food security, and sustainable development of agriculture. Existing techniques are expensive and difficult to scale as they require locally collected survey data. Approaches utilizing remotely sensed data, such as satellite imagery, potentially provide a cheap, equally effective alternative. Our work shows promising results in predicting soybean crop yields in Argentina using deep learning techniques. We also achieve satisfactory results with a transfer learning approach to predict Brazil soybean harvests with a smaller amount of data. The motivation for transfer learning is that the success of deep learning models is largely dependent on abundant ground truth training data. Successful crop yield prediction with deep learning in regions with little training data relies on the ability to fine-tune pre-trained models. © 2018 Copyright held by the owner/author(s).","Agriculture; Deep learning; Sustainability","Agriculture; Crops; Developing countries; Food supply; Forecasting; Remote sensing; Satellite imagery; Sustainable development; Accurate prediction; Food security; Learning models; Learning techniques; Remote sensing data; Remotely sensed data; Training data; Transfer learning; Deep learning"
"Survey on semantic image segmentation techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050032971&doi=10.1109%2fISS1.2017.8389420&partnerID=40&md5=df363c8ff7ab42802545b6a9c553cd7b","Semantic image segmentation is a vast area of interest for computer vision and machine learning researchers. Many vision applications need accurate and efficient image segmentation and segment classification mechanisms for assessing the visual contents and perform the real-time decision making. The application area includes remote sensing, autonomous driving, indoor navigation, video surveillance and virtual or augmented reality systems etc. The segmentation and classification of objects generate the specific performance parameters for various applications which require detailed domain analysis. There are broad range of applications where remote sensing image scene classification play an important role and has been receiving remarkable attention. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This survey paper provides a review of different traditional methods of image segmentation and classification. By comparing these methods with semantic image segmentation using deep learning it is assumed to show the far better result. © 2017 IEEE.","Deep learning; Feature and preprocessing method; Random decision forest; Segmentation pipeline; Semantic segmentation; Unsupervised segmentation","Augmented reality; Computer vision; Decision making; Deep learning; Image classification; Learning algorithms; Remote sensing; Security systems; Semantics; Surveys; Augmented reality systems; Classification mechanism; Decision forest; Pre-processing method; Real time decision-making; Semantic image segmentations; Semantic segmentation; Unsupervised segmentation; Image segmentation"
"A method of multi-scale total convolution network driven remote sensing image repair","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051396889&doi=10.14188%2fj.2095-6045.2017446&partnerID=40&md5=d8983e0921252c9f5b3008fa023c85c8","Combined with deep learning, a multi-scale and full-convolution neural network-driven remote sensing image restoration method is proposed. The Gaussian pyramid was constructed for multi-scale transformation. The image was segmented by FNEA(fractal net evolution approach) algorithm. The optimal matching pixels are found by using the activation function values between the convolution layers, and finally the pixels are filled in the repaired regions by the full convolution neural network back propagation function. Several groups of experiments have verified that the multiscale full-convolution neural network method can well repair the remote sensing images, which has the advantages of high automation, high efficiency and good visual restoration. © 2018 Wuhan University. All rights reserved.","Fnea segmentation; Full convolution neural network; Gaussian pyramid; Multi-scale; Reverse broadcast function","algorithm; artificial neural network; Gaussian method; imaging method; pixel; remote sensing; segmentation"
"Deep Learning for Hyperspectral Imagery Classification: The State of the Art and Prospects [深度学习在高光谱图像分类领域的研究现状与展望]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054412181&doi=10.16383%2fj.aas.2018.c170190&partnerID=40&md5=722f31f72683208b913f20c816e31c20","Hyperspectral imagery (HSI) classification occupies an important place in the earth observation technology of hyperspectral remote sensing, and it is widely used in both military and civil fields. However, due to HSI's characteristics including high dimensionality in data, high correlation between spectrum and mixing in spectrum, HSI classification faces great challenges. In recent years, as new deep learning technology emerges, the HSI classification methods based on deep learning have achieved some breakthroughs in methodology and performance and provided new opportunities for the research of HSI classification. In this paper, we review the research background, actuality of HSI classification technologies and several common datasets. Then, we provide a brief overview of several typical deep learning models. Finally, we introduce some deep learning based HSI classification methods in detail, summarize the main function and existing problems of deep learning in HSI classification, and present some prospects for future work. Copyright © 2018 Acta Automatica Sinica. All rights reserved.","Convolutional neural network (CNN); Deep belief network; Deep learning; Hyperspectral imagery (HSI) classification; Stacked autoencoder","Classification (of information); Image classification; Neural networks; Remote sensing; Spectroscopy; Classification technology; Convolutional Neural Networks (CNN); Deep belief networks; Earth Observation Technology; Hyperspectral imagery; Hyperspectral imagery classifications; Hyperspectral remote sensing; Stacked autoencoder; Deep learning"
"A Variational Approach for Automatic Man-made Object Detection from Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050477655&doi=10.11947%2fj.AGCS.2018.20170642&partnerID=40&md5=76e2e222bb7102a936e1f05328743fd9","Man-made object detection is important for object detection from remote sensing images. In this paper we propose a variational approach for man-made object detection which formulates the man-made object detection problem as a problem of variational energy optimization. In this method, an image is firstly segmented into superpixels, and the saliency map by combining image features such as texture, color and gradient is computed. In second step, we construct an energy function with saliency, area, edge, texture and intensity variance constrains. The energy function is solved via variational method to obtain the foreground, which is the detected man-made objects. The proposed approach on several remote sensing images is evaluated and compared with the C-V model, MRF model and deep learning based semantic segmentation. Experimental results show that the proposed approach can effectively detect man-made objects on remote sensing images with low false alarm and false negatives rates. The comparison and analysis with deep learning based method are also presented. © 2018, Surveying and Mapping Press. All right reserved.","Deep learning; Energy function; Man-made object; Semantic segmentation; Variational method","Deep learning; Image segmentation; Object recognition; Ordinary differential equations; Remote sensing; Semantics; Comparison and analysis; Energy functions; Learning-based methods; Man made objects; Remote sensing images; Semantic segmentation; Variational approaches; Variational methods; Object detection; algorithm; detection method; image analysis; numerical method; numerical model; pixel; remote sensing; satellite imagery; segmentation; texture"
"Salient Object Detection from Multi-spectral Remote Sensing Images with Deep Residual Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050459942&doi=10.11947%2fj.AGCS.2018.20170633&partnerID=40&md5=21adac47e279d2c2a6573913bfa0fa1b","This paper focuses on intelligent photogrammetry deep learning: deep residual method. Salient object detection aims at identifying the visually interesting object regions that are consistent with human perception. Multispectral remote sensing images provide rich radiometric information in revealing the physical properties of the observed objects, therefore promise a great potential in salient object detection tasks. Conventional salient object detection methods often employ handcrafted features to predict saliency by evaluating the pixel-wise or superpixel-wise similarity. With the recent emergence of deep learning based approaches, in particular, fully convolutional neural networks, there has been profound progress in visual saliency detection. However, this success has not been extended to multispectral remote sensing images, and existing multispectral salient object detection methods are still mainly based on handcrafted features, essentially due to the difficulties in image acquisition and labeling. In this paper, we propose a novel deep residual network based on a top-down model, which is trained in an end-to-end manner to tackle the above issues in multispectral salient object detection. Our model effectively exploits the saliency cues at different levels of the deep residual network. To overcome the limited availability of remote sensing images in training of our deep residual network, we also introduce a new spectral image reconstruction model that can generate multispectral images from RGB images. Our extensive experimental evaluations using both multispectral and RGB salient object detection datasets demonstrate a significant performance improvement of more than 10% compared with the state-of-the-art methods. © 2018, Surveying and Mapping Press. All right reserved.","Deep residual network; Remote sensing image processing; Salient object detection; Spectral super-resolution; Top-down model","Deep learning; Feature extraction; Image reconstruction; Neural networks; Object recognition; Pixels; Remote sensing; Spectroscopy; Convolutional neural network; Multispectral remote sensing image; Remote sensing image processing; Salient object detection; Spectral image reconstruction; Super resolution; Top down models; Visual saliency detections; Object detection; algorithm; artificial neural network; detection method; image processing; network analysis; numerical method; numerical model; photogrammetry; pixel; remote sensing; satellite imagery"
"Region-wise deep feature representation for remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048956941&doi=10.3390%2frs10060871&partnerID=40&md5=75e53595bdf503e051d6e37359cb5f9d","Effective feature representations play an important role in remote sensing image analysis tasks. With the rapid progress of deep learning techniques, deep features have been widely applied to remote sensing image understanding in recent years and shown powerful ability in image representation. The existing deep feature extraction approaches are usually carried out on the whole image directly. However, such deep feature representation strategies may not effectively capture the local geometric invariance of target regions in remote sensing images. In this paper, we propose a novel region-wise deep feature extraction framework for remote sensing images. First, regions that may contain the target information are extracted from one whole image. Then, these regions are fed into a pre-trained convolutional neural network (CNN) model to extract regional deep features. Finally, the regional deep features are encoded by an improved Vector of Locally Aggregated Descriptors (VLAD) algorithm to generate the feature representation for the image. We conducted extensive experiments on remote sensing image classification and retrieval tasks based on the proposed region-wise deep feature extraction framework. The comparison results show that the proposed approach is superior to the existing CNN feature extraction methods. © 2018 by the authors.","Convolutional neural networks (CNNs); Feature representation; Image retrieval; Scene classification","Convolution; Deep learning; Extraction; Feature extraction; Image classification; Image enhancement; Image retrieval; Neural networks; Convolutional neural network; Convolutional Neural Networks (CNN); Feature extraction methods; Feature representation; Remote sensing image classification; Remote sensing images; Scene classification; Vector of locally aggregated descriptors; Remote sensing"
"A benchmark dataset for performance evaluation of multi-label remote sensing image retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048990314&doi=10.3390%2frs10060964&partnerID=40&md5=844a2626f86327bc83edfb3a07e15308","Benchmark datasets are essential for developing and evaluating remote sensing image retrieval (RSIR) approaches. However, most of the existing datasets are single-labeled, with each image in these datasets being annotated by a single label representing the most significant semantic content of the image. This is sufficient for simple problems, such as distinguishing between a building and a beach, but multiple labels are required for more complex problems, such as RSIR. This motivated us to present a new benchmark dataset termed ""MLRSIR"" that was labeled from an existing single-labeled remote sensing archive. MLRSIR contained a total of 17 classes, and each image had at least one of 17 pre-defined labels. We evaluated the performance of RSIR methods ranging from traditional handcrafted feature-based methods to deep-learning-based ones on MLRSIR. More specifically, we compared the performances of RSIR methods from both single-label and multi-label perspectives. These results presented the advantages of multiple labels over single labels for interpreting complex remote sensing images, and serve as a baseline for future research on multi-label RSIR. © 2018 by the authors.","Convolutional neural networks; Handcrafted features; Multi-label benchmark dataset; Multi-label image retrieval; Remote sensing image retrieval (RSIR); Single-label image retrieval","Benchmarking; Complex networks; Deep learning; Neural networks; Remote sensing; Semantics; Benchmark datasets; Convolutional neural network; Handcrafted features; Label images; Multi-label; Remote sensing image retrieval; Image retrieval"
"Disaster damage detection through synergistic use of deep learning and 3D point cloud features derived from very high resolution oblique aerial images, and multiple-kernel-learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014668630&doi=10.1016%2fj.isprsjprs.2017.03.001&partnerID=40&md5=c55c6594390a87f3387aa8eaf4592348","Oblique aerial images offer views of both building roofs and façades, and thus have been recognized as a potential source to detect severe building damages caused by destructive disaster events such as earthquakes. Therefore, they represent an important source of information for first responders or other stakeholders involved in the post-disaster response process. Several automated methods based on supervised learning have already been demonstrated for damage detection using oblique airborne images. However, they often do not generalize well when data from new unseen sites need to be processed, hampering their practical use. Reasons for this limitation include image and scene characteristics, though the most prominent one relates to the image features being used for training the classifier. Recently features based on deep learning approaches, such as convolutional neural networks (CNNs), have been shown to be more effective than conventional hand-crafted features, and have become the state-of-the-art in many domains, including remote sensing. Moreover, often oblique images are captured with high block overlap, facilitating the generation of dense 3D point clouds – an ideal source to derive geometric characteristics. We hypothesized that the use of CNN features, either independently or in combination with 3D point cloud features, would yield improved performance in damage detection. To this end we used CNN and 3D features, both independently and in combination, using images from manned and unmanned aerial platforms over several geographic locations that vary significantly in terms of image and scene characteristics. A multiple-kernel-learning framework, an effective way for integrating features from different modalities, was used for combining the two sets of features for classification. The results are encouraging: while CNN features produced an average classification accuracy of about 91%, the integration of 3D point cloud features led to an additional improvement of about 3% (i.e. an average classification accuracy of 94%). The significance of 3D point cloud features becomes more evident in the model transferability scenario (i.e., training and testing samples from different sites that vary slightly in the aforementioned characteristics), where the integration of CNN and 3D point cloud features significantly improved the model transferability accuracy up to a maximum of 7% compared with the accuracy achieved by CNN features alone. Overall, an average accuracy of 85% was achieved for the model transferability scenario across all experiments. Our main conclusion is that such an approach qualifies for practical use. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","3D point cloud features; CNN features; Model transferability; Multiple-kernel-learning; Oblique images; Structural damage detections; Transfer learning; UAV","Classification (of information); Damage detection; Deep learning; Deep neural networks; Disasters; Integration testing; Neural networks; Remote sensing; Structural analysis; Unmanned aerial vehicles (UAV); 3D point cloud; CNN features; Model transferabilities; Multiple Kernel Learning; Oblique images; Structural damage detection; Transfer learning; Feature extraction; accuracy assessment; aerial photography; artificial neural network; disaster; earthquake; image analysis; model validation; stakeholder; supervised learning; three-dimensional modeling; training"
"Human Activity Recognition from Body Sensor Data using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045621921&doi=10.1007%2fs10916-018-0948-z&partnerID=40&md5=d38a1a8667d076a64b1d54e560b866d7","In recent years, human activity recognition from body sensor data or wearable sensor data has become a considerable research attention from academia and health industry. This research can be useful for various e-health applications such as monitoring elderly and physical impaired people at Smart home to improve their rehabilitation processes. However, it is not easy to accurately and automatically recognize physical human activity through wearable sensors due to the complexity and variety of body activities. In this paper, we address the human activity recognition problem as a classification problem using wearable body sensor data. In particular, we propose to utilize a Deep Belief Network (DBN) model for successful human activity recognition. First, we extract the important initial features from the raw body sensor data. Then, a kernel principal component analysis (KPCA) and linear discriminant analysis (LDA) are performed to further process the features and make them more robust to be useful for fast activity recognition. Finally, the DBN is trained by these features. Various experiments were performed on a real-world wearable sensor dataset to verify the effectiveness of the deep learning algorithm. The results show that the proposed DBN outperformed other algorithms and achieves satisfactory activity recognition performance. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Body sensor data; Deep belief network; Deep learning; Human activity recognition","accelerometry; Article; artificial neural network; deep belief network; discriminant analysis; human; human activities; learning algorithm; machine learning; physical activity; principal component analysis; sitting; standing; support vector machine; walking; algorithm; ambulatory monitoring; exercise test; machine learning; movement (physiology); physiology; procedures; remote sensing; reproducibility; Algorithms; Exercise Test; Humans; Machine Learning; Monitoring, Ambulatory; Movement; Neural Networks (Computer); Remote Sensing Technology; Reproducibility of Results"
"High-Resolution Remote Sensing Image Classification Method Based on Convolutional Neural Network and Restricted Conditional Random Field","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048949712&doi=10.3390%2frs10060920&partnerID=40&md5=62fd092f9e5de09d4270873a602813db","Convolutional neural networks (CNNs) can adapt to more complex data, extract deeper characteristics from images, and achieve higher classification accuracy in remote sensing image scene classification and object detection compared to traditional shallow-model methods. However, directly applying common-structure CNNs to pixel-based remote sensing image classification will lead to boundary or outline distortions of the land cover and consumes enormous computation time in the image classification stage. To solve this problem, we propose a high-resolution remote sensing image classification method based on CNN and the restricted conditional random field algorithm (CNN-RCRF). CNN-RCRF adopts CNN superpixel classification instead of pixel-based classification and uses the restricted conditional random field algorithm (RCRF) to refine the superpixel result image into a pixel-based result. The proposed method not only takes advantage of the classification ability of CNNs but can also avoid boundary or outline distortions of the land cover and greatly reduce computation time in classifying images. The effectiveness of the proposed method is tested with two high-resolution remote sensing images, and the experimental results show that the CNN-RCRF outperforms the existing traditional methods in terms of overall accuracy, and CNN-RCRF's computation time is much less than that of traditional pixel-based deep-model methods. © 2018 by the authors.","Conditional random field; Convolutional neural network; Deep learning; Pixel-based classification; Remote sensing images","Convolution; Data mining; Deep learning; Neural networks; Object detection; Object recognition; Pixels; Random processes; Remote sensing; Superpixels; Classification ability; Classification accuracy; Conditional random field; Convolutional neural network; High resolution remote sensing images; Pixel based classifications; Remote sensing image classification; Remote sensing images; Image classification"
"Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034623511&doi=10.1016%2fj.isprsjprs.2017.11.011&partnerID=40&md5=9345d53592e049672c7c4ec6ec74e8bd","In this work, we investigate various methods to deal with semantic labeling of very high resolution multi-modal remote sensing data. Especially, we study how deep fully convolutional networks can be adapted to deal with multi-modal and multi-scale remote sensing data for semantic labeling. Our contributions are threefold: (a) we present an efficient multi-scale approach to leverage both a large spatial context and the high resolution data, (b) we investigate early and late fusion of Lidar and multispectral data, (c) we validate our methods on two public datasets with state-of-the-art results. Our results indicate that late fusion make it possible to recover errors steaming from ambiguous data, while early fusion allows for better joint-feature learning but at the cost of higher sensitivity to missing data. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Data fusion; Deep learning; Remote sensing; Semantic mapping","Data fusion; Deep learning; Semantics; Convolutional networks; High resolution data; Multi-scale approaches; Multi-spectral data; Remote sensing data; Semantic mapping; Urban remote sensing; Very high resolution; Remote sensing; algorithm; data acquisition; data set; lidar; mapping method; numerical method; remote sensing; satellite data"
"Learning a River Network Extractor Using an Adaptive Loss Function","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044097591&doi=10.1109%2fLGRS.2018.2811754&partnerID=40&md5=3e2ff081322cd1eec080d4784991e42a","We have created a deep-learning-based river network extraction model, called DeepRiver, that learns the characteristics of rivers from synthetic data and generalizes them to natural data. To train this model, we created a very large database of exemplary synthetic local channel segments, including channel intersections. Our model uses a special loss function that automatically shifts the focus to the hardest-to-learn parts of an input image. This adaptive loss function makes it possible to learn to detect river centerlines, including the centerlines at junctions and bifurcations. DeepRiver learns to separate between rivers and oceans, and therefore, it is able to reliably extract rivers in coastal regions. The model produces maps of river centerlines, which have the potential to be quite useful for analyzing the properties of river networks. © 2004-2012 IEEE.","Coastal systems; convolutional neural networks; deep learning; river network extraction","Deep learning; Extraction; Neural networks; Personnel training; Remote sensing; Adaptation models; Coastal systems; Computational model; Convolutional neural network; Indexes; Oceans; River network; Rivers; algorithm; artificial neural network; coastal zone; machinery; river channel; river system"
"Urban land cover classification with missing data modalities using deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048608109&doi=10.1109%2fJSTARS.2018.2834961&partnerID=40&md5=e0e6c0608be11e0fbe85999c955c2ee8","Automatic urban land cover classification is a fundamental problem in remote sensing, e.g., for environmental monitoring. The problem is highly challenging, as classes generally have high interclass and low intraclass variances. Techniques to improve urban land cover classification performance in remote sensing include fusion of data from different sensors with different data modalities. However, such techniques require all modalities to be available to the classifier in the decision-making process, i.e., at test time, as well as in training. If a data modality is missing at test time, current state-of-the-art approaches have in general no procedure available for exploiting information from these modalities. This represents a waste of potentially useful information. We propose as a remedy a convolutional neural network (CNN) architecture for urban land cover classification which is able to embed all available training modalities in the so-called hallucination network. The network will in effect replace missing data modalities in the test phase, enabling fusion capabilities even when data modalities are missing in testing. We demonstrate the method using two datasets consisting of optical and digital surface model (DSM) images. We simulate missing modalities by assuming that DSM images are missing during testing. Our method outperforms both standard CNNs trained only on optical images as well as an ensemble of two standard CNNs. We further evaluate the potential of our method to handle situations where only some DSM images are missing during testing. Overall, we show that we can clearly exploit training time information of the missing modality during testing. © 2008-2012 IEEE.","Convolutional neural networks (CNN); deep learning; land cover classification; missing data modalities; remote sensing","Convolution; Decision making; Deep learning; Deep neural networks; Geometrical optics; Learning systems; Neural networks; Optical sensors; Personnel training; Remote sensing; Testing; Convolutional Neural Networks (CNN); Deep convolutional neural networks; Digital surface model (DSM); Land cover classification; Missing data; Optical imaging; State-of-the-art approach; Urban land cover classification; Classification (of information); artificial neural network; data set; land classification; land cover; machine learning; remote sensing; urban area"
"Exploring the potential of conditional adversarial networks for optical and SAR Image Matching","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044761197&doi=10.1109%2fJSTARS.2018.2803212&partnerID=40&md5=49f77a044147131a385ffa132f9f4010","Tasks such as the monitoring of natural disasters or the detection of change highly benefit from complementary information about an area or a specific object of interest. The required information is provided by fusing high accurate coregistered and georeferenced datasets. Aligned high-resolution optical and synthetic aperture radar (SAR) data additionally enable an absolute geolocation accuracy improvement of the optical images by extracting accurate and reliable ground control points (GCPs) from the SAR images. In this paper, we investigate the applicability of a deep learning based matching concept for the generation of precise and accurate GCPs from SAR satellite images by matching optical and SAR images. To this end, conditional generative adversarial networks (cGANs) are trained to generate SAR-like image patches from optical images. For training and testing, optical and SAR image patches are extracted from TerraSAR-X and PRISM image pairs covering greater urban areas spread over Europe. The artificially generated patches are then used to improve the conditions for three known matching approaches based on normalized cross-correlation (NCC), scale-invariant feature transform (SIFT), and binary robust invariant scalable key (BRISK), which are normally not usable for the matching of optical and SAR images. The results validate that a NCC-, SIFT-, and BRISK-based matching greatly benefit, in terms of matching accuracy and precision, from the use of the artificial templates. The comparison with two state-of-the-art optical and SAR matching approaches shows the potential of the proposed method but also revealed some challenges and the necessity for further developments. © 2008-2012 IEEE.","Artificial image generation; conditional generative adversarial networks (cGANs); multisensor image matching; optical satellite images; synthetic aperture radar (SAR)","Adaptive optics; Deep learning; Disasters; Geology; Geometrical optics; Image enhancement; Image matching; Optical correlation; Optical image storage; Optical sensors; Rock mechanics; Space-based radar; Synthetic aperture radar; Adversarial networks; Artificial image; Biomedical optical imaging; Multi sensor images; Optical distortion; Optical imaging; Optical satellite images; Radar imaging; accuracy assessment; correlation; data set; image resolution; optical method; remote sensing; satellite imagery; synthetic aperture radar; TerraSAR-X; Europe"
"Detection of roadside vegetation using Fully Convolutional Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046338134&doi=10.1016%2fj.imavis.2018.03.008&partnerID=40&md5=ceb5c7b7770b25db99089c21c434bebb","Vegetation detection is a common procedure in remote sensing, but recently it has also been applied in robotics as an aid in navigation of autonomous vehicles. In this paper, we present a method for roadside vegetation detection intended for traffic infrastructure maintenance. While many published methods use Near Infrared images for vegetation detection, our method uses images from the visible spectrum which allows the use of a common color camera on-board a vehicle. Deep neural networks have proven to be a very promising machine learning technique and have shown excellent results in different computer vision problems. In this paper, we show that Fully Convolutional Neural Networks can be effectively used in a real-world application for detecting roadside vegetation. For training and testing purposes, we have created our own image database which contains roadside vegetation in various conditions. We present promising experimental results and a discussion of encountered problems in a real-world application as well as a comparison with several alternative approaches. © 2018 Elsevier B.V.","Convolutional neural networks; Deep learning; Image analysis; Roadside maintenance; Vegetation detection","Convolution; Deep learning; Deep neural networks; Image analysis; Infrared devices; Infrared imaging; Neural networks; Remote sensing; Robots; Vegetation; Autonomous Vehicles; Computer vision problems; Convolutional networks; Convolutional neural network; Machine learning techniques; Near- infrared images; Traffic infrastructure; Training and testing; Roadsides"
"A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026643598&doi=10.1016%2fj.isprsjprs.2017.07.014&partnerID=40&md5=552742582ae5c4e6ccea4d98849f8375","The contextual-based convolutional neural network (CNN) with deep architecture and pixel-based multilayer perceptron (MLP) with shallow structure are well-recognized neural network algorithms, representing the state-of-the-art deep learning method and the classical non-parametric machine learning approach, respectively. The two algorithms, which have very different behaviours, were integrated in a concise and effective way using a rule-based decision fusion approach for the classification of very fine spatial resolution (VFSR) remotely sensed imagery. The decision fusion rules, designed primarily based on the classification confidence of the CNN, reflect the generally complementary patterns of the individual classifiers. In consequence, the proposed ensemble classifier MLP-CNN harvests the complementary results acquired from the CNN based on deep spatial feature representation and from the MLP based on spectral discrimination. Meanwhile, limitations of the CNN due to the adoption of convolutional filters such as the uncertainty in object boundary partition and loss of useful fine spatial resolution detail were compensated. The effectiveness of the ensemble MLP-CNN classifier was tested in both urban and rural areas using aerial photography together with an additional satellite sensor dataset. The MLP-CNN classifier achieved promising performance, consistently outperforming the pixel-based MLP, spectral and textural-based MLP, and the contextual-based CNN in terms of classification accuracy. This research paves the way to effectively address the complicated problem of VFSR image classification. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural network; Feature representation; Fusion decision; Multilayer perceptron; VFSR remotely sensed imagery","Aerial photography; Classification (of information); Convolution; Image resolution; Learning systems; Multilayer neural networks; Multilayers; Neural networks; Pixels; Remote sensing; Classification accuracy; Classification confidence; Convolutional neural network; Feature representation; Machine learning approaches; Multi layer perceptron; Neural network algorithm; Remotely sensed imagery; Image classification; aerial photography; algorithm; artificial neural network; data set; image classification; machine learning; pixel; remote sensing; satellite imagery; satellite sensor; spatial resolution"
"A Hybrid CNN + Random Forest Approach to Delineate Debris Covered Glaciers Using Deep Features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048596968&doi=10.1007%2fs12524-018-0750-x&partnerID=40&md5=ce89e8af0a53d1d372afc90def96e3a9","The main aim of this study is to propose a novel hybrid deep learning framework approach for accurate mapping of debris covered glaciers. The framework comprises of integration of several CNNs architecture, in which different combinations of Landsat 8 multispectral bands (including thermal band), topographic and texture parameters are passed as input for feature extraction. The output of an ensemble of these CNNs is hybrid with random forest model for classification. The major pillars of the framework include: (1) technique for implementing topographic and atmospheric corrections (preprocessing), (2) the proposed hybrid of ensemble of CNNs and random forest classifier, and (3) procedures to determine whether a pixel predicted as snow is a cloud edge/shadow (post-processing). The proposed approach was implemented on the multispectral Landsat 8 OLI (operational land imager)/TIRS (thermal infrared sensor) data and Shuttle Radar Topography Mission Digital Elevation Model for the part of the region situated in Alaknanda basin, Uttarakhand, Himalaya. The proposed framework was observed to outperform (accuracy 96.79%) the current state-of-art machine learning algorithms such as artificial neural network, support vector machine, and random forest. Accuracy assessment was performed by means of several statistics measures (precision, accuracy, recall, and specificity). © 2018, Indian Society of Remote Sensing.","Classification; CNN; Debris; Glaciers; Random forest; Texture","algorithm; detection method; glacier; image classification; Landsat; numerical model; pixel; river basin; satellite imagery; Shuttle Radar Topography Mission; Alaknanda River; Himalayas; India; Uttarakhand"
"Machinery health indicator construction based on convolutional neural networks considering trend burr","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043466517&doi=10.1016%2fj.neucom.2018.02.083&partnerID=40&md5=399cc94d1772b870ca8bebbcb13a5c25","In the study of data-driven prognostic methods of machinery, much attention has been paid to constructing health indicators (HIs). Most of the existing HIs, however, are manually constructed for a specific degradation process and need the prior knowledge of experts. Additionally, for the existing HIs, there are usually some outlier regions deviating to an expected degradation trend and reducing the performance of HIs. We refer to this phenomenon as trend burr. To deal with these problems, this paper proposes a convolutional neural network based HI construction method considering trend burr. The proposed method first learns features through convolution and pooling operations, and then these learned features are constructed into a HI through a nonlinear mapping operation. Furthermore, an outlier region correction technique is applied to detect and remove outlier regions existing in the HIs. Unlike traditional methods in which HIs are manually constructed, the proposed method aims to automatically construct HIs. Moreover, the outlier region correction technique enables the constructed HIs to be more effective. The effectiveness of the proposed method is verified using a bearing dataset. Through comparing with commonly used HI construction methods, it is demonstrated that the proposed method achieves better results in terms of trendability, monotonicity and scale similarity. © 2018 Elsevier B.V.","Convolutional neural network; Deep learning; Machinery health indicator; Outlier region correction; Trend burr","Convolution; Deep learning; Health; Machinery; Neural networks; Construction method; Convolutional neural network; Correction techniques; Data-driven prognostics; Degradation process; Machinery health; Nonlinear mappings; Trend burr; Statistics; algorithm; Article; artificial neural network; classification; data processing; health status indicator; machine; machine learning; mathematical analysis; nonlinear system; priority journal; prognosis; remote sensing; signal processing"
"Automatic large-scalae 3D building shape refinement using conditional generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048370080&doi=10.5194%2fisprs-archives-XLII-2-103-2018&partnerID=40&md5=efdfb0c917f776dc2b4fd05baac8333c","Three-dimensional building reconstruction from remote sensing imagery is one of the most difficult and important 3D modeling problems for complex urban environments. The main data sources provided the digital representation of the Earths surface and related natural, cultural, and man-made objects of the urban areas in remote sensing are the digital surface models (DSMs). The DSMs can be obtained either by light detection and ranging (LIDAR), SAR interferometry or from stereo images. Our approach relies on automatic global 3D building shape refinement from stereo DSMs using deep learning techniques. This refinement is necessary as the DSMs, which are extracted from image matching point clouds, suffer from occlusions, outliers, and noise. Though most previous works have shown promising results for building modeling, this topic remains an open research area. We present a new methodology which not only generates images with continuous values representing the elevation models but, at the same time, enhances the 3D object shapes, buildings in our case. Mainly, we train a conditional generative adversarial network (cGAN) to generate accurate LIDAR-like DSM height images from the noisy stereo DSM input. The obtained results demonstrate the strong potential of creating large areas remote sensing depth images where the buildings exhibit better-quality shapes and roof forms. © Authors 2018.","3D building shape; 3D scene refinement; Conditional generative adversarial networks (cGANs); Digital surface model","Buildings; Deep learning; Image enhancement; Optical radar; Remote sensing; Three dimensional computer graphics; 3d buildings; 3D scenes; Adversarial networks; Building reconstruction; Complex urban environments; Digital representations; Digital surface models; Light detection and ranging; Stereo image processing"
"Efficient camera self-calibration method for remote sensing photogrammetry","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047611426&doi=10.1364%2fOE.26.014213&partnerID=40&md5=e59858200619f51a958c4f8fa07d6fd4","Internal parameter calibration of remote sensing cameras (RSCs) is a necessary step in remote sensing photogrammetry. On-orbit camera calibration widely adopts external ground control points (GCPs) to measure its internal parameters. However, accessible and available GCPs are not easy to achieve when cameras work on a satellite platform. In this paper, we propose an efficient camera self-calibration method using a micro-transceiver in conjunction with deep learning. A supervised learning set is produced by the micro-transceiver, where multiple two-dimensional diffraction grids are produced and transformed into multiple auto-collimating sub-beams equivalent to infinite target-point training examples. A deep learning network is used to invert the learnable internal parameters. The micro-transceiver can be easily integrated into the internal structure of RSCs allowing to calibrate them independently on external ground control targets. © 2018 Optical Society of America.",,"Calibration; Cameras; Deep learning; Orbits; Photogrammetry; Radio transceivers; Rock mechanics; Transceivers; Camera calibration; Camera self calibration; Ground control points; Internal parameters; Internal structure; Remote sensing cameras; Satellite platforms; Two dimensional diffraction; Remote sensing; article; calibration; deep learning; diffraction; photogrammetry; remote sensing"
"SATELLITE IMAGE CLASSIFICATION of BUILDING DAMAGES USING AIRBORNE and SATELLITE IMAGE SAMPLES in A DEEP LEARNING APPROACH","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048391659&doi=10.5194%2fisprs-annals-IV-2-89-2018&partnerID=40&md5=cc62ca393fdf8abab982e0fcc18a81e1","The localization and detailed assessment of damaged buildings after a disastrous event is of utmost importance to guide response operations, recovery tasks or for insurance purposes. Several remote sensing platforms and sensors are currently used for the manual detection of building damages. However, there is an overall interest in the use of automated methods to perform this task, regardless of the used platform. Owing to its synoptic coverage and predictable availability, satellite imagery is currently used as input for the identification of building damages by the International Charter, as well as the Copernicus Emergency Management Service for the production of damage grading and reference maps. Recently proposed methods to perform image classification of building damages rely on convolutional neural networks (CNN). These are usually trained with only satellite image samples in a binary classification problem, however the number of samples derived from these images is often limited, affecting the quality of the classification results. The use of up/down-sampling image samples during the training of a CNN, has demonstrated to improve several image recognition tasks in remote sensing. However, it is currently unclear if this multi resolution information can also be captured from images with different spatial resolutions like satellite and airborne imagery (from both manned and unmanned platforms). In this paper, a CNN framework using residual connections and dilated convolutions is used considering both manned and unmanned aerial image samples to perform the satellite image classification of building damages. Three network configurations, trained with multi-resolution image samples are compared against two benchmark networks where only satellite image samples are used. Combining feature maps generated from airborne and satellite image samples, and refining these using only the satellite image samples, improved nearly 4 % the overall satellite image classification of building damages. © 2018 Copernicus GmbH. All rights reserved.","dilated convolutions; earthquake; machine learning; multi-platform; multi-resolution; multi-scale; residual connections; UAV",
"Earth science [Big] data analytics","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073094395&doi=10.1007%2f978-3-319-89923-7_4&partnerID=40&md5=0bfcb750514c00630569d238645079e0","Tremendous research has been done and is still in progress in the domain of earth science. With the advent of Big Data and availability of datasets on Earth science, the study of Earth sciences has reached new dimensions. The diversity and high dimensional remote sensing data have provided with complex data sets capable of giving insights and intelligence that was not possible in last decades. With Computing progress made in ingesting and inferring data from myriad of sources including high resolution cameras mounted on satellites and sensors giving access to unconventional Big Data and also with the GPU computing and Data science advances we are today able to leverage machine learning and deep learning in extensively complex datasets derived from remote sensing about Earth Sciences. Our focus is to analyze what exactly does big data mean in earth science applications and how can big data provide added value in this context. Furthermore, this chapter demonstrates various big data tools which can be mapped with various techniques to be used for experimenting earth science datasets, processed, and exploited for different earth science applications. In order to illustrate the aforementioned aspects, instances are presented in order to demonstrate the use of big data in remote sensing. Firstly, this chapter presents earth science studies, application areas/research fields and a brief insight on earth science data. Then various techniques implemented in this domain are elaborated viz. classification, clustering, regression, deep learning, pattern recognition, machine learning, earth data analysis and processing. Later this chapter introduces big data analytics and various tools/ platforms in big data viz. BigInsights, GIS, Jupyter notebook, Matlab, Python. Finally, it is shown how these tools are mapped to Earth science datasets using ArcGIS to illustrate with experimental instances the inferences and patterns generated. © Springer International Publishing AG, part of Springer Nature 2019. All Rights Reserved.","BigInsights ArcGIS; Earth data analysis and processing; Geosciences; GIS Big data analytics; Jupyter notebook; MATLAB; Python",
"Deep learning for sequence pattern recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048225086&doi=10.1109%2fICNSC.2018.8361281&partnerID=40&md5=ea9a5c919c951b605f9db3023495b4eb","Deep Learning is a superb way to solve remote sensing related problems, which mainly cover four perspectives: image processing, pixel-based classification, target recognition and scene understanding. In this paper, we focus on target recognition by building deep learning models, and our target is sequence pattern. Accurate prediction of sequence pattern would help identify significant characters from text sequence. Despite considerable advances in using machine learning techniques for sequence pattern recognition problem, its efficiency is still limited because of its involving extensive manual feature engineering in the process of features extraction from raw sequences. Thus, we apply a deep learning approach in sequence pattern recognition problem. The sequences of the datasets we used are self-generated genomic format sequences, and each dataset is generated based on a kind of pattern. We then investigate and construct various deep neural network models (such as convolutional networks, recurrent networks and a hybrid of convolutional and recurrent networks). The one-hot encoding method that preserves the vital position information of each character is presented to represent sequences as inputs to the models. The sequence patterns are then extracted from the input and output the probabilities of the existence of sequence patterns. Experimental results demonstrate that the deep learning approaches can achieve high accuracy and high precision in sequence pattern recognition. In addition, a saliency-map-based method is applied to visualize the learned sequence patterns. In view of the simulation results, we believe that we can find an appropriate deep learning model for a certain sequence sensing problem. © 2018 IEEE.","deep learning; feature extraction; machine learning; sensing; sequence pattern","Convolution; Deep learning; Deep neural networks; Extraction; Feature extraction; Image processing; Learning systems; Recurrent neural networks; Remote sensing; State assignment; Convolutional networks; Feature engineerings; Machine learning techniques; Neural network model; Pixel based classifications; Position information; sensing; Sequence patterns; Pattern recognition"
"Residential scene classification for gridded population sampling in developing countries using deep convolutional neural networks on satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046654849&doi=10.1186%2fs12942-018-0132-1&partnerID=40&md5=20f848008d969c1db6ee2d80470927f3","Background: Conducting surveys in low- and middle-income countries is often challenging because many areas lack a complete sampling frame, have outdated census information, or have limited data available for designing and selecting a representative sample. Geosampling is a probability-based, gridded population sampling method that addresses some of these issues by using geographic information system (GIS) tools to create logistically manageable area units for sampling. GIS grid cells are overlaid to partition a country's existing administrative boundaries into area units that vary in size from 50 m × 50 m to 150 m × 150 m. To avoid sending interviewers to unoccupied areas, researchers manually classify grid cells as ""residential"" or ""nonresidential"" through visual inspection of aerial images. ""Nonresidential"" units are then excluded from sampling and data collection. This process of manually classifying sampling units has drawbacks since it is labor intensive, prone to human error, and creates the need for simplifying assumptions during calculation of design-based sampling weights. In this paper, we discuss the development of a deep learning classification model to predict whether aerial images are residential or nonresidential, thus reducing manual labor and eliminating the need for simplifying assumptions. Results: On our test sets, the model performs comparable to a human-level baseline in both Nigeria (94.5% accuracy) and Guatemala (96.4% accuracy), and outperforms baseline machine learning models trained on crowdsourced or remote-sensed geospatial features. Additionally, our findings suggest that this approach can work well in new areas with relatively modest amounts of training data. Conclusions: Gridded population sampling methods like geosampling are becoming increasingly popular in countries with outdated or inaccurate census data because of their timeliness, flexibility, and cost. Using deep learning models directly on satellite images, we provide a novel method for sample frame construction that identifies residential gridded aerial units. In cases where manual classification of satellite images is used to (1) correct for errors in gridded population data sets or (2) classify grids where population estimates are unavailable, this methodology can help reduce annotation burden with comparable quality to human analysts. © 2018 The Author(s).","Clustering; Complex sample design; Deep learning; GIS; Machine learning; Probability based; Remote sensing; Scene classification","artificial neural network; cluster analysis; developing world; GIS; machine learning; population estimation; probability; remote sensing; satellite imagery; article; calculation; developing country; error; geographic information system; grid cell; Guatemala; human; human cell; human experiment; machine learning; manual labor; Nigeria; probability; satellite imagery; scientist; artificial neural network; classification; demography; developing country; epidemiology; information processing; procedures; satellite imagery; Guatemala [Central America]; Nigeria; Data Collection; Demography; Developing Countries; Guatemala; Humans; Neural Networks (Computer); Nigeria; Residence Characteristics; Satellite Imagery"
"Extraction of urban water bodies from high-resolution remote-sensing imagery using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046655775&doi=10.3390%2fw10050585&partnerID=40&md5=167750436953136b11a77273641b32ae","Accurate information on urban surface water is important for assessing the role it plays in urban ecosystem services in the context of human survival and climate change. The precise extraction of urban water bodies from images is of great significance for urban planning and socioeconomic development. In this paper, a novel deep-learning architecture is proposed for the extraction of urban water bodies from high-resolution remote sensing (HRRS) imagery. First, an adaptive simple linear iterative clustering algorithm is applied for segmentation of the remote-sensing image into high-quality superpixels. Then, a new convolutional neural network (CNN) architecture is designed that can extract useful high-level features of water bodies from input data in a complex urban background and mark the superpixel as one of two classes: an including water or no-water pixel. Finally, a high-resolution image of water-extracted superpixels is generated. Experimental results show that the proposed method achieved higher accuracy for water extraction from the high-resolution remote-sensing images than traditional approaches, and the average overall accuracy is 99.14%. © 2018 by the authors.","Convolutional neural networks; Deep learning; High-resolution remote-sensing images; Superpixel; Urban water bodies","Climate change; Clustering algorithms; Convolution; Deep learning; Ecosystems; Image segmentation; Iterative methods; Network architecture; Neural networks; Pixels; Remote sensing; Superpixels; Surface waters; Convolutional neural network; Convolutional Neural Networks (CNN); High resolution remote sensing; High resolution remote sensing imagery; High resolution remote sensing images; Simple Linear Iterative Clustering; Socio-economic development; Urban water bodies; Extraction; algorithm; artificial neural network; image resolution; pixel; remote sensing; satellite imagery; segmentation; surface water; urban ecosystem; urban planning; urban region; water resource"
"When Deep Learning Meets Metric Learning: Remote Sensing Image Scene Classification via Learning Discriminative CNNs","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044717760&doi=10.1109%2fTGRS.2017.2783902&partnerID=40&md5=07c2913feaa8daa11fbca024cefc4f66","Remote sensing image scene classification is an active and challenging task driven by many applications. More recently, with the advances of deep learning models especially convolutional neural networks (CNNs), the performance of remote sensing image scene classification has been significantly improved due to the powerful feature representations learnt through CNNs. Although great success has been obtained so far, the problems of within-class diversity and between-class similarity are still two big challenges. To address these problems, in this paper, we propose a simple but effective method to learn discriminative CNNs (D-CNNs) to boost the performance of remote sensing image scene classification. Different from the traditional CNN models that minimize only the cross entropy loss, our proposed D-CNN models are trained by optimizing a new discriminative objective function. To this end, apart from minimizing the classification error, we also explicitly impose a metric learning regularization term on the CNN features. The metric learning regularization enforces the D-CNN models to be more discriminative so that, in the new D-CNN feature spaces, the images from the same scene class are mapped closely to each other and the images of different classes are mapped as farther apart as possible. In the experiments, we comprehensively evaluate the proposed method on three publicly available benchmark data sets using three off-the-shelf CNN models. Experimental results demonstrate that our proposed D-CNN methods outperform the existing baseline methods and achieve state-of-the-art results on all three data sets. © 1980-2012 IEEE.","Convolutional neural networks (CNNs); deep learning; discriminative CNNs (D-CNNs); metric learning; remote sensing image scene classification","Computer architecture; Convolution; Feature extraction; Image classification; Image enhancement; Learning systems; Measurement; Neural networks; Remote sensing; Convolutional neural network; discriminative CNNs (D-CNNs); Image color analysis; Metric learning; Remote sensing images; Deep learning; algorithm; artificial neural network; data set; experimental design; image classification; numerical method; numerical model; remote sensing"
"Dense net-based depth-width double reinforced deep learning neural network for high-resolution remote sensing image per-pixel classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047526637&doi=10.3390%2frs10050779&partnerID=40&md5=c3af89f64efe7858fb3a8d7f97039260","Deep neural networks (DNNs) face many problems in the very high resolution remote sensing (VHRRS) per-pixel classification field. Among the problems is the fact that as the depth of the network increases, gradient disappearance influences classification accuracy and the corresponding increasing number of parameters to be learned increases the possibility of overfitting, especially when only a small amount of VHRRS labeled samples are acquired for training. Further, the hidden layers in DNNs are not transparent enough, which results in extracted features not being sufficiently discriminative and significant amounts of redundancy. This paper proposes a novel depth-width-reinforced DNN that solves these problems to produce better per-pixel classification results in VHRRS. In the proposed method, densely connected neural networks and internal classifiers are combined to build a deeper network and balance the network depth and performance. This strengthens the gradients, decreases negative effects from gradient disappearance as the network depth increases and enhances the transparency of hidden layers, making extracted features more discriminative and reducing the risk of overfitting. In addition, the proposed method uses multi-scale filters to create a wider neural network. The depth of the filters from each scale is controlled to decrease redundancy and the multi-scale filters enable utilization of joint spatio-spectral information and diverse local spatial structure simultaneously. Furthermore, the concept of network in network is applied to better fuse the deeper and wider designs, making the network operate more smoothly. The results of experiments conducted on BJ02, GF02, geoeye and quickbird satellite images verify the efficacy of the proposed method. The proposed method not only achieves competitive classification results but also proves that the network can continue to be robust and perform well even while the amount of labeled training samples is decreasing, which fits the small training samples situation faced by VHRRS per-pixel classification. © 2018 by the authors.","Densely connected neural network; Image per-pixel classification; Internal classifier; Multi-scale filters; Network in network; Remote sensing","Deep neural networks; Pixels; Redundancy; Reinforcement; Remote sensing; Sampling; Classification accuracy; Classification results; High resolution remote sensing images; In networks; Learning neural networks; Multi-scale; Pixel classification; Spectral information; Image classification"
"Training small networks for scene classification of remote sensing images via knowledge distillation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047523713&doi=10.3390%2frs10050719&partnerID=40&md5=bb6678c49e00ad9143b0ecaadefb0e3c","Scene classification, aiming to identify the land-cover categories of remotely sensed image patches, is now a fundamental task in the remote sensing image analysis field. Deep-learning-model-based algorithms are widely applied in scene classification and achieve remarkable performance, but these high-level methods are computationally expensive and time-consuming. Consequently in this paper, we introduce a knowledge distillation framework, currently a mainstream model compression method, into remote sensing scene classification to improve the performance of smaller and shallower network models. Our knowledge distillation training method makes the high-temperature softmax output of a small and shallow student model match the large and deep teacher model. In our experiments, we evaluate knowledge distillation training method for remote sensing scene classification on four public datasets: AID dataset, UCMerced dataset, NWPU-RESISC dataset, and EuroSAT dataset. Results show that our proposed training method was effective and increased overall accuracy (3% in AID experiments, 5% in UCMerced experiments, 1% in NWPU-RESISC and EuroSAT experiments) for small and shallow models. We further explored the performance of the student model on small and unbalanced datasets. Our findings indicate that knowledge distillation can improve the performance of small network models on datasets with lower spatial resolution images, numerous categories, as well as fewer training samples. © 2018 by the authors.","Convolutional neural networks (CNNs); Deep learning; Knowledge distillation; Remote sensing; Scene classification","Classification (of information); Deep learning; Distillation; Image classification; Image enhancement; Neural networks; Personnel training; Teaching; Convolutional neural network; High-level methods; Overall accuracies; Remote sensing images; Remotely sensed images; Scene classification; Spatial resolution images; Unbalanced datasets; Remote sensing"
"Symmetrical Dense-Shortcut Deep Fully Convolutional Networks for Semantic Segmentation of Very-High-Resolution Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044851591&doi=10.1109%2fJSTARS.2018.2810320&partnerID=40&md5=bc1034923d64ba865ab2d38177048cf5","Semantic segmentation has emerged as a mainstream method in very-high-resolution remote sensing land-use/land-cover applications. In this paper, we first review the state-of-the-art semantic segmentation models in both computer vision and remote sensing fields. Subsequently, we introduce two semantic segmentation frameworks: SNFCN and SDFCN, both of which contain deep fully convolutional networks with shortcut blocks. We adopt an overlay strategy as the postprocessing method. Based on our frameworks, we conducted experiments on two online ISPRS datasets: Vaihingen and Potsdam. The results indicate that our frameworks achieve higher overall accuracy than the classic FCN-8s and SegNet models. In addition, our postprocessing method can increase the overall accuracy by about 1%-2% and help to eliminate 'salt and pepper' phenomena and block effects. © 2008-2012 IEEE.","Convolutional neural networks (CNN); deep learning (DL); fully convolutional networks (FCN); remote sensing; SDFCN; semantic segmentation","Convolution; Decoding; Deep learning; Feature extraction; Image segmentation; Labeling; Land use; Neural networks; Personnel training; Semantic Web; Semantics; Convolutional networks; Convolutional Neural Networks (CNN); Overall accuracies; Postprocessing methods; SDFCN; Semantic segmentation; State of the art; Very high resolution; Remote sensing; accuracy assessment; artificial neural network; computer vision; data set; image resolution; land cover; land use change; remote sensing; segmentation; Baden-Wurttemberg; Brandenburg [Germany]; Germany; Potsdam; Vaihingen an der Enz"
"Road Extraction by Deep Residual U-Net","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043403057&doi=10.1109%2fLGRS.2018.2802944&partnerID=40&md5=d7009cf2f5569ed73b60fec58aea2124","Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis. In this letter, a semantic segmentation neural network, which combines the strengths of residual learning and U-Net, is proposed for road area extraction. The network is built with residual units and has similar architecture to that of U-Net. The benefits of this model are twofold: first, residual units ease training of deep networks. Second, the rich skip connections within the network could facilitate information propagation, allowing us to design networks with fewer parameters, however, better performance. We test our network on a public road data set and compare it with U-Net and other two state-of-the-art deep-learning-based road extraction methods. The proposed approach outperforms all the comparing methods, which demonstrates its superiority over recently developed state of the arts. © 2004-2012 IEEE.","Convolutional neural network; deep residual U-Net; road extraction","Antennas; Data mining; Deep learning; Extraction; Feature extraction; Information dissemination; Neural networks; Personnel training; Remote sensing; Roads and streets; Semantics; Statistical tests; Convolutional neural network; deep residual U-Net; Information propagation; Remote sensing images; Road extraction; Road extraction method; Roads; Semantic segmentation; Image segmentation; aerial survey; artificial neural network; extraction method; image analysis; parameter estimation; performance assessment; remote sensing; segmentation"
"Pansharpening by interspectral similarity and edge information using improved deep residual network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047720984&doi=10.1117%2f1.JEI.27.3.033013&partnerID=40&md5=c72761d1b9d0dc0cc3f49fd8a343fe6b","Remote sensing image pansharpening involves the fusing of multispectral (MS) images with a panchromatic (PAN) image to produce an image with high-spatial as well as high-spectral resolution. We propose an improved pansharpening algorithm based on deep learning. A four-layer residual network is used as a reconstructed model to enable the accurate estimation of high-frequency details. We consider two priors to take advantage of MS information. The first prior indicates interspectral similarity, wherein the relationship between high- and low-resolution PAN images is used in the estimation of high-resolution MS images. The second prior provides the location of edges and textures according to the gradient of the PAN image. Consistency in the spectral characteristic is used as the basis in creating a pretrained model with the aim of accelerating convergence. Multiple evaluation metrics were applied to simulated and real images in order to compare the efficacy of the proposed method with that of state-of-the-art image fusion methods. © 2018 SPIE and IS&T.","convolutional neural network; deep learning; pansharpening","Frequency estimation; Image fusion; Network layers; Neural networks; Remote sensing; Spectral resolution; Accurate estimation; Convolutional neural network; High spectral resolution; Multispectral images; Pan-sharpening; Panchromatic (Pan) image; Remote sensing images; Spectral characteristics; Deep learning"
"LiDAR Data Classification Using Morphological Profiles and Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044047624&doi=10.1109%2fLGRS.2018.2810276&partnerID=40&md5=86c0d5863920618ade7131b9c6c17a1f","In recent years, deep learning-based methods, especially convolutional neural networks (CNNs), have shown their capabilities in remote sensing data processing. The efficacy of light detection and ranging (LiDAR) has been already proven in a wide variety of research areas. Most of the existing methods do not extract the informative features from LiDAR-derived rasterized digital surface models (LiDAR-DSM) data in a deep manner. In order to utilize the advantages of deep models for the classification of LiDAR-derived features, deep CNN is proposed here to hierarchically extract the robust and discriminant features of the input data. Moreover, morphological profiles and multiattribute profiles (MAPs) are investigated to enrich the inputs of the CNN and further to improve the ultimate classification performance. Furthermore, a new activation function, sigmoid-weighted linear units (SiLUs), is introduced. The proposed frameworks are tested on two LiDAR-DSMs (i.e., Bayview Park and Houston data sets). The MAP-CNNs with SiLU outperform original CNNs by 6.62% and 6.88% in terms of overall accuracy on Bayview Park and Houston data sets, respectively, when the number of training samples of each class is 40. © 2004-2012 IEEE.","Convolutional neural network (CNN); deep learning; feature extraction (FE); light detection and ranging (LiDAR); morphological profile (MP); multiattribute profile (MAP); sigmoid-weighted linear units (SiLUs)","Classification (of information); Convolution; Data handling; Data mining; Data structures; Deep learning; Extraction; Feature extraction; Iron; Lithium compounds; Neural networks; Remote sensing; Robustness (control systems); Tracking radar; Convolutional Neural Networks (CNN); Light detection and ranging; Linear units; Morphological profile; Multi-attributes; Shape; Optical radar; accuracy assessment; algorithm; artificial neural network; data set; lidar; numerical method; satellite data"
"Soil moisture retrieval based on convolutional neural network and AMSR2 microwave remote sensing [基于卷积神经网络和AMSR2微波遥感的土壤水分反演研究]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059617748&doi=10.3772%2fj.issn.1002-0470.2018.05.003&partnerID=40&md5=3568e18250899bd40b6192ce3e2d8765","Soil moisture is an important parameter in the research of hydrology, meteorology and agriculture, especially in crop yield estimation model and agricultural drought monitoring research. A great deal of research work has been done on the research of soil moisture retrieval algorithm. However, due to many factors affecting the soil moisture on the surface, part of algorithms still have some shortcomings. Based on the analysis of previous inversion algorithms, this paper summarizes the advantages and limitations of different methods, and proposes to use the convolutional neural network (CNN) to perform soil moisture retrieval to overcome the limitation of traditional soil moisture retrieval methods and improve the accuracy. Taking the passive microwave AMSR2 data as an example, the model framework of soil moisture retrieval based on CNN and the analysis of AMSR2 soil moisture product algorithm is constructed to improve the versatility and accuracy of soil moisture retrieval method. The accuracy of deep learning CNN depends primarily on the training and testing sample databases. However,passive microwave pixels have low resolution and it is difficult to obtain ground-based measured data synchronized with the satellite. We selected a number of surface soil moisture observation stations in different regions and AMSR2 soil moisture products as a reference to obtain the ground synchronization data, thus overcoming the problem of ground synchronous observation data. The AMSR2 brightness temperature data and corresponding soil moisture data at ground-based observations are randomly divided into training and testing databases. When the CNN chooses a combination of two layers of two convolution layers with 3000 iterations in iterative training, the overall accuracy of network inversion is the highest. The test sample show that the root mean square error (RMSE) of CNN retrieval soil moisture and ground observation data is 1.1178%, which show a high correlation (R=0.8685). The CNN retrieving results are verified by the in-suit data from the ground stations, which show that the average relative error is 39.25%. The average relative error of CNN is 10.24% lower than JAXA and ground-based value, which shows that the accuracy of passive microwave soil moisture retrieval based on CNN is obviously improved © 2018, Executive Office of the Journal. All right reserved.","AMSR2; Convolution neural network (CNN); Deep learning; Passive microwave; Soil moisture",
"Single satellite imagery simultaneous super-resolution and colorization using multi-task deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043229610&doi=10.1016%2fj.jvcir.2018.02.016&partnerID=40&md5=45ff504eadf171a2896b41df3ed4ec7f","Satellite imagery is a kind of typical remote sensing data, which holds preponderance in large area imaging and strong macro integrity. However, for most commercial space usages, such as virtual display of urban traffic flow, virtual interaction of environmental resources, one drawback of satellite imagery is its low spatial resolution, failing to provide the clear image details. Moreover, in recent years, synthesizing the color for grayscale satellite imagery or recovering the original color of camouflage sensitive regions becomes an urgent requirement for large spatial objects virtual reality interaction. In this work, unlike existing works which solve these two problems separately, we focus on achieving image super-resolution (SR) and image colorization synchronously. Based on multi-task learning, we provide a novel deep neural network model to fulfill single satellite imagery SR and colorization simultaneously. By feeding back the color feature representations into the SR network and jointly optimizing such two tasks, our deep model successfully achieves the mutual cooperation between imagery reconstruction and image colorization. To avoid color bias, we not only adopt the non-satellite imagery to enrich the color diversity of satellite image, but also recalculate the prior color distribution and the valid color range based on the mixed data. We evaluate the proposed model on satellite images from different data sets, such as RSSCN7 and AID. Both the evaluations and comparisons reveal that the proposed multi-task deep learning approach is superior to the state-of-the-art methods, where image SR and colorization can be accomplished simultaneously and efficiently. © 2018 Elsevier Inc.","Deep neural networks; Image super-resolution; Multi-task learning; Satellite image colorization","Color; Deep learning; Deep neural networks; Learning systems; Optical resolving power; Remote sensing; Satellites; Virtual reality; Environmental resources; Image super resolutions; Multitask learning; Neural network model; Satellite images; State-of-the-art methods; Virtual interactions; Virtual reality interactions; Satellite imagery"
"Hyperspectral imagery classification based on semi-supervised broad learning system","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047533595&doi=10.3390%2frs10050685&partnerID=40&md5=99e73c686b417415f31c06621b27c540","Recently, deep learning-based methods have drawn increasing attention in hyperspectral imagery (HSI) classification, due to their strong nonlinear mapping capability. However, these methods suffer from a time-consuming training process because of many network parameters. In this paper, the concept of broad learning is introduced into HSI classification. Firstly, to make full use of abundant spectral and spatial information of hyperspectral imagery, hierarchical guidance filtering is performing on the original HSI to get its spectral-spatial representation. Then, the class-probability structure is incorporated into the broad learning model to obtain a semi-supervised broad learning version, so that limited labeled samples and many unlabeled samples can be utilized simultaneously. Finally, the connecting weights of broad structure can be easily computed through the ridge regression approximation. Experimental results on three popular hyperspectral imagery datasets demonstrate that the proposed method can achieve better performance than deep learning-based methods and conventional classifiers. © 2018 by the authors.","Broad learning; Class-probability structure; Classification; Hyperspectral imagery; Semi-supervised","Classification (of information); Deep learning; Information filtering; Regression analysis; Remote sensing; Spectroscopy; Broad learning; Class probabilities; Conventional classifier; Hyper-spectral imageries; Hyperspectral imagery classifications; Learning-based methods; Semi-supervised; Spatial representations; Image classification"
"Open Data for Global Multimodal Land Use Classification: Outcome of the 2017 IEEE GRSS Data Fusion Contest","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045645809&doi=10.1109%2fJSTARS.2018.2799698&partnerID=40&md5=97dc7a51a9b14985276caa3256ddad8d","In this paper, we present the scientific outcomes of the 2017 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2017 Contest was aimed at addressing the problem of local climate zones classification based on a multitemporal and multimodal dataset, including image (Landsat 8 and Sentinel-2) and vector data (from OpenStreetMap). The competition, based on separate geographical locations for the training and testing of the proposed solution, aimed at models that were accurate (assessed by accuracy metrics on an undisclosed reference for the test cities), general (assessed by spreading the test cities across the globe), and computationally feasible (assessed by having a test phase of limited time). The techniques proposed by the participants to the Contest spanned across a rather broad range of topics, and of mixed ideas and methodologies deriving from computer vision and machine learning but also deeply rooted in the specificities of remote sensing. In particular, rigorous atmospheric correction, the use of multidate images, and the use of ensemble methods fusing results obtained from different data sources/time instants made the difference. © 2008-2012 IEEE.","Convolutional neural networks (CNNs); crowdsourcing; deep learning (DL); ensemble learning; image analysis and data fusion (IADF); multimodal; multiresolution; multisource; OpenStreetMap (OSM); random fields","Classification (of information); Crowdsourcing; Deep learning; Image analysis; Land use; Neural networks; Remote sensing; Convolutional neural network; Ensemble learning; Multi resolutions; Multi-modal; Multisources; Openstreetmap; Random fields; Image fusion; accuracy assessment; artificial neural network; crowdsourcing; data processing; ensemble forecasting; image analysis; land classification; land use; Landsat; map; remote sensing"
"Deep cube-pair network for hyperspectral imagery classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047512150&doi=10.3390%2frs10050783&partnerID=40&md5=58630b62a644495d81ced6b2b408d2b5","Advanced classification methods, which can fully utilize the 3D characteristic of hyperspectral image (HSI) and generalize well to the test data given only limited labeled training samples (i.e., small training dataset), have long been the research objective for HSI classification problem. Witnessing the success of deep-learning-based methods, a cube-pair-based convolutional neural networks (CNN) classification architecture is proposed to cope this objective in this study, where cube-pair is used to address the small training dataset problem as well as preserve the 3D local structure of HSI data. Within this architecture, a 3D fully convolutional network is further modeled, which has less parameters compared with traditional CNN. Provided the same amount of training samples, the modeled network can go deeper than traditional CNN and thus has superior generalization ability. Experimental results on several HSI datasets demonstrate that the proposed method has superior classification results compared with other state-of-the-art competing methods. © 2018 by the authors.","Convolutional neural network; Datacube; Deep learning; Hyperspectral imagery; Spatial-spectral","Convolution; Deep learning; Geometry; Image classification; Network architecture; Neural networks; Remote sensing; Sampling; Spectroscopy; Statistical tests; Classification results; Convolutional networks; Convolutional neural network; Convolutional Neural Networks (CNN); Data cube; Hyper-spectral imageries; Hyperspectral imagery classifications; Spatial-spectral; Classification (of information)"
"Semantic labeling of high resolution aerial imagery and LiDAR data with fine segmentation network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047525682&doi=10.3390%2frs10050743&partnerID=40&md5=127d6564778cb714b1dccabfb46d6f6a","In this paper, a novel convolutional neural network (CNN)-based architecture, named fine segmentation network (FSN), is proposed for semantic segmentation of high resolution aerial images and light detection and ranging (LiDAR) data. The proposed architecture follows the encoder-decoder paradigm and the multi-sensor fusion is accomplished in the feature-level using multi-layer perceptron (MLP). The encoder consists of two parts: the main encoder based on the convolutional layers of Vgg-16 network for color-infrared images and a lightweight branch for LiDAR data. In the decoder stage, to adaptively upscale the coarse outputs from encoder, the Sub-Pixel convolution layers replace the transposed convolutional layers or other common up-sampling layers. Based on this design, the features from different stages and sensors are integrated for a MLP-based high-level learning. In the training phase, transfer learning is employed to infer the features learned from generic dataset to remote sensing data. The proposed FSN is evaluated by using the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen 2D Semantic Labeling datasets. Experimental results demonstrate that the proposed framework can bring considerable improvement to other related networks. © 2018 by the authors.","Convolutional neural network (CNN); Deep learning; High resolution aerial imagery; LiDAR; Semantic segmentation; Spectral image","Aerial photography; Antennas; Convolution; Decoding; Deep learning; Infrared imaging; Network architecture; Neural networks; Optical radar; Remote sensing; Semantic Web; Semantics; Signal encoding; Spectroscopy; Convolutional Neural Networks (CNN); High resolution aerial imagery; High-resolution aerial images; Light detection and ranging; Multi layer perceptron; Proposed architectures; Semantic segmentation; Spectral images; Image segmentation"
"Deep convolutional neural network training enrichment using multi-view object-based analysis of Unmanned Aerial systems imagery for wetlands classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044025784&doi=10.1016%2fj.isprsjprs.2018.03.006&partnerID=40&md5=521ce7527f7abd6a499b3d464c7f6f35","Deep convolutional neural network (DCNN) requires massive training datasets to trigger its image classification power, while collecting training samples for remote sensing application is usually an expensive process. When DCNN is simply implemented with traditional object-based image analysis (OBIA) for classification of Unmanned Aerial systems (UAS) orthoimage, its power may be undermined if the number training samples is relatively small. This research aims to develop a novel OBIA classification approach that can take advantage of DCNN by enriching the training dataset automatically using multi-view data. Specifically, this study introduces a Multi-View Object-based classification using Deep convolutional neural network (MODe) method to process UAS images for land cover classification. MODe conducts the classification on multi-view UAS images instead of directly on the orthoimage, and gets the final results via a voting procedure. 10-fold cross validation results show the mean overall classification accuracy increasing substantially from 65.32%, when DCNN was applied on the orthoimage to 82.08% achieved when MODe was implemented. This study also compared the performances of the support vector machine (SVM) and random forest (RF) classifiers with DCNN under traditional OBIA and the proposed multi-view OBIA frameworks. The results indicate that the advantage of DCNN over traditional classifiers in terms of accuracy is more obvious when these classifiers were applied with the proposed multi-view OBIA framework than when these classifiers were applied within the traditional OBIA framework. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural network; Deep learning; OBIA; Object-based classification; Random forest; small UAS; Support vector machine","Antennas; Classification (of information); Convolution; Decision trees; Deep learning; Deep neural networks; Image analysis; Image processing; Neural networks; Remote sensing; Sampling; Support vector machines; Unmanned aerial vehicles (UAV); Convolutional neural network; OBIA; Object-based classifications; Random forests; Small UAS; Image classification; accuracy assessment; artificial neural network; data set; image analysis; image classification; land cover; model validation; remote sensing; support vector machine; unmanned vehicle; wetland"
"IMG2DSM: Height Simulation from Single Imagery Using Conditional Generative Adversarial Net","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042846217&doi=10.1109%2fLGRS.2018.2806945&partnerID=40&md5=566c8a6d765dac73c83d8451f80d6cca","This letter proposes a groundbreaking approach in the remote-sensing community to simulating the digital surface model (DSM) from a single optical image. This novel technique uses conditional generative adversarial networks whose architecture is based on an encoder-decoder network with skip connections (generator) and penalizing structures at the scale of image patches (discriminator). The network is trained on scenes where both the DSM and optical data are available to establish an image-to-DSM translation rule. The trained network is then utilized to simulate elevation information on target scenes where no corresponding elevation information exists. The capability of the approach is evaluated both visually (in terms of photographic interpretation) and quantitatively (in terms of reconstruction errors and classification accuracies) on subdecimeter spatial resolution data sets captured over Vaihingen, Potsdam, and Stockholm. The results confirm the promising performance of the proposed framework. © 2004-2012 IEEE.","Conditional generative adversarial networks (cGANs); convolutional neural network (CNN); deep learning; digital surface model (DSM); encoder-decoder networks; optical images","Aerial photography; Classification (of information); Data visualization; Decoding; Deep learning; Gallium nitride; Gas generators; Geometrical optics; III-V semiconductors; Network coding; Neural networks; Optical sensors; Personnel training; Remote sensing; Adversarial networks; Convolutional Neural Networks (CNN); Digital surface model (DSM); Encoder-decoder; Optical image; Optical imaging; Adaptive optics; accuracy assessment; algorithm; artificial neural network; data set; detection method; image analysis; network analysis; numerical model; reconstruction; satellite data; satellite imagery; spatial resolution; Baden-Wurttemberg; Brandenburg [Germany]; Germany; Potsdam; Stockholm [Sweden]; Sweden; Vaihingen an der Enz"
"Identifying Corresponding Patches in SAR and Optical Images with a Pseudo-Siamese CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043467411&doi=10.1109%2fLGRS.2018.2799232&partnerID=40&md5=29e3afecea5bb46d384fa267a2517331","In this letter, we propose a pseudo-siamese convolutional neural network architecture that enables to solve the task of identifying corresponding patches in very high-resolution optical and synthetic aperture radar (SAR) remote sensing imagery. Using eight convolutional layers each in two parallel network streams, a fully connected layer for the fusion of the features learned in each stream, and a loss function based on binary cross entropy, we achieve a one-hot indication if two patches correspond or not. The network is trained and tested on an automatically generated data set that is based on a deterministic alignment of SAR and optical imagery via previously reconstructed and subsequently coregistered 3-D point clouds. The satellite images, from which the patches comprising our data set are extracted, show a complex urban scene containing many elevated objects (i.e., buildings), thus providing one of the most difficult experimental environments. The achieved results show that the network is able to predict corresponding patches with high accuracy, thus indicating great potential for further development toward a generalized multisensor key-point matching procedure. © 2004-2012 IEEE.","Convolutional neural networks (CNNs); data fusion; deep learning; deep matching; image matching; optical imagery; synthetic aperture radar (SAR)","Convolution; Data fusion; Deep learning; Geometrical optics; Image matching; Network architecture; Neural networks; Optical fibers; Optical sensors; Radar imaging; Remote sensing; Synthetic aperture radar; Convolutional neural network; deep matching; Optical distortion; Optical fiber networks; Optical imagery; Optical imaging; Optical interferometry; Adaptive optics; artificial neural network; cloud; data set; identification method; image analysis; imagery; optical method; remote sensing; satellite imagery; spectral resolution; synthetic aperture radar"
"Large-scale supervised learning for 3D point cloud labeling: Semantic3d.net","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047402490&doi=10.14358%2fPERS.84.5.297&partnerID=40&md5=d02677fc489743386ec2bbd11dfeeaf0","In this paper we review current state-of-the-art in 3D point cloud classification, present a new 3D point cloud classification benchmark data set of single scans with over four billion manually labeled points, and discuss first available results on the benchmark. Much of the stunning recent progress in 2D image interpretation can be attributed to the availability of large amounts of training data, which have enabled the (supervised) learning of deep neural networks. With the data set presented in this paper, we aim to boost the performance of CNNs also for 3D point cloud labeling. Our hope is that this will lead to a breakthrough of deep learning also for 3D (geo-) data. The semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains eight semantic classes and covers a wide range of urban outdoor scenes, including churches, streets, railroad tracks, squares, villages, soccer fields, and castles. We describe our labeling interface and show that, compared to those already available to the research community, our data set provides denser and more complete point clouds, with a much higher overall number of labeled points. We further provide descriptions of baseline methods and of the first independent submissions, which are indeed based on CNNs, and already show remarkable improvements over prior art. We hope that semantic3D. net will pave the way for deep learning in 3D point cloud analysis, and for 3D representation learning in general. © 2018 American Society for Photogrammetry and Remote Sensing.",,"Deep neural networks; Semantics; 3d representations; Baseline methods; Benchmark data; Recent progress; Research communities; Semantic class; State of the art; Terrestrial laser scanners; Classification (of information); artificial neural network; benchmarking; cloud classification; data set; image analysis; research work; supervised learning"
"Robust Traffic-Sign Detection and Classification Using Mobile LiDAR Data with Digital Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044345006&doi=10.1109%2fJSTARS.2018.2810143&partnerID=40&md5=a2b2a189a47cd79952721a34010e5dad","This study aims at building a robust method for detecting and classifying traffic signs from mobile LiDAR point clouds and digital images. First, this method detects traffic signs from mobile LiDAR point clouds with regard to a prior knowledge of road width, pole height, reflectance, geometrical structure, and traffic-sign size. Then, traffic-sign images are segmented by projecting the detected traffic-sign points onto the digital images. Afterward, the segmented traffic-sign images are normalized for automatic classification with a given image size. Finally, a traffic-sign classifier is proposed based on a supervised Gaussian-Bernoulli deep Boltzmann machine model. We evaluated the proposed method using datasets acquired by a RIEGL VMX-450 system. The traffic-sign detection accuracy of 86.8% was achieved; through parameter sensitivity analysis, the overall performance of traffic-sign classification achieved a recognition rate of 93.3%. The computational performance showed that our method provides a promising solution to traffic-sign detection and classification using mobile LiDAR point clouds and digital images. © 2008-2012 IEEE.","deep learning; Digital images; geometrical features; intensity; mobile LiDAR point clouds; traffic signs","Deep learning; Image classification; Optical radar; Sensitivity analysis; Automatic classification; Computational performance; Deep boltzmann machines; Digital image; Geometrical features; intensity; Mobile lidar; Parameter sensitivity analysis; Traffic signs; accuracy assessment; data set; detection method; digital image; image classification; lidar; performance assessment; remote sensing; signal"
"Intelligent detection of structure from remote sensing images based on deep learning method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046938188&doi=10.5194%2fisprs-archives-XLII-3-1959-2018&partnerID=40&md5=c758491c99ef19ca4bb5e262d0ea734d","Utilizing high-resolution remote sensing images for earth observation has become the common method of land use monitoring. It requires great human participation when dealing with traditional image interpretation, which is inefficient and difficult to guarantee the accuracy. At present, the artificial intelligent method such as deep learning has a large number of advantages in the aspect of image recognition. By means of a large amount of remote sensing image samples and deep neural network models, we can rapidly decipher the objects of interest such as buildings, etc. Whether in terms of efficiency or accuracy, deep learning method is more preponderant. This paper explains the research of deep learning method by a great mount of remote sensing image samples and verifies the feasibility of building extraction via experiments. © Authors 2018. CC BY 4.0 License.","Building; Deep learning; Land use monitoring; Neural network; Remote sensing image","Buildings; Deep learning; Deep neural networks; Image recognition; Land use; Neural networks; Artificial intelligent; Building extraction; Earth observations; High resolution remote sensing images; Image interpretation; Intelligent detection; Neural network model; Remote sensing images; Remote sensing"
"A Coarse-to-fine model for airplane detection from large remote sensing images using saliency modle and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046965554&doi=10.5194%2fisprs-archives-XLII-3-1571-2018&partnerID=40&md5=8f47c54ed887b60fe3244055d131e5fc","High resolution remote sensing images are bearing the important strategic information, especially finding some time-sensitive-targets quickly, like airplanes, ships, and cars. Most of time the problem firstly we face is how to rapidly judge whether a particular target is included in a large random remote sensing image, instead of detecting them on a given image. The problem of time-sensitive-targets target finding in a huge image is a great challenge: 1) Complex background leads to high loss and false alarms in tiny object detection in a large-scale images. 2) Unlike traditional image retrieval, what we need to do is not just compare the similarity of image blocks, but quickly find specific targets in a huge image. In this paper, taking the target of airplane as an example, presents an effective method for searching aircraft targets in large scale optical remote sensing images. Firstly, we used an improved visual attention model utilizes salience detection and line segment detector to quickly locate suspected regions in a large and complicated remote sensing image. Then for each region, without region proposal method, a single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation is adopted to search small airplane objects. Unlike sliding window and region proposal-based techniques, we can do entire image (region) during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Experimental results show the proposed method is quickly identify airplanes in large-scale images. © Authors 2018. CC BY 4.0 License.","Airplane detection; Large-scale; Object of interest; Optical remote sensing image; Saliency detection; Single neural network","Aircraft detection; Behavioral research; Deep learning; Image enhancement; Image retrieval; Image segmentation; Object detection; Object recognition; Training aircraft; Airplane detections; Large-scale; Object of interest; Optical remote sensing; Saliency detection; Remote sensing"
"Soil moisture retrieval using convolutional neural networks: Application to passive microwave remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046973025&doi=10.5194%2fisprs-archives-XLII-3-583-2018&partnerID=40&md5=c271b5cdeb73b2950bac041ca9c1260f","A empirical model is established to analyse the daily retrieval of soil moisture from passive microwave remote sensing using convolutional neural networks (CNN). soil moisture plays an important role in the water cycle. However, with the rapidly increasing of the acquiring technology for remotely sensed data, it's a hard task for remote sensing practitioners to find a fast and convenient model to deal with the massive data. In this paper, the AMSR-E brightness temperatures are used to train CNN for the prediction of the European centre for medium-range weather forecasts (ECMWF) model. Compared with the classical inversion methods, the deep learning-based method is more suitable for global soil moisture retrieval. It is very well supported by graphics processing unit (GPU) acceleration, which can meet the demand of massive data inversion. Once the model trained, a global soil moisture map can be predicted in less than 10 seconds. What's more, the method of soil moisture retrieval based on deep learning can learn the complex texture features from the big remote sensing data. In this experiment, the results demonstrates that the CNN deployed to retrieve global soil moisture can achieve a better performance than the support vector regression (SVR) for soil moisture retrieval. © Authors 2018.","Convolutional neural networks; Deep learning; GPU; Microwave remote sensing; Soil moisture retrieval","Computer graphics; Computer graphics equipment; Convolution; Deep learning; Graphics processing unit; Metadata; Neural networks; Program processors; Soil moisture; Weather forecasting; Convolutional neural network; Convolutional Neural Networks (CNN); European centre for medium-range weather forecasts; Graphics Processing Unit (GPU); Microwave remote sensing; Passive microwave remote sensing; Soil moisture retrievals; Support vector regression (SVR); Remote sensing"
"Application of deep learning in GLOBELAND30-2010 product refinement","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046965107&doi=10.5194%2fisprs-archives-XLII-3-1111-2018&partnerID=40&md5=8f93b08104776706ab681f412748a278","GlobeLand30, as one of the best Global Land Cover (GLC) product at 30-m resolution, has been widely used in many research fields. Due to the significant spectral confusion among different land cover types and limited textual information of Landsat data, the overall accuracy of GlobeLand30 is about 80%. Although such accuracy is much higher than most other global land cover products, it cannot satisfy various applications. There is still a great need of an effective method to improve the quality of GlobeLand30. The explosive high-resolution satellite images and remarkable performance of Deep Learning on image classification provide a new opportunity to refine GlobeLand30. However, the performance of deep leaning depends on quality and quantity of training samples as well as model training strategy. Therefore, this paper 1) proposed an automatic training sample generation method via Google earth to build a large training sample set; and 2) explore the best training strategy for land cover classification using GoogleNet (Inception V3), one of the most widely used deep learning network. The result shows that the fine-tuning from first layer of Inception V3 using rough large sample set is the best strategy. The retrained network was then applied in one selected area from Xi'an city as a case study of GlobeLand30 refinement. The experiment results indicate that the proposed approach with Deep Learning and google earth imagery is a promising solution for further improving accuracy of GlobeLand30. © Authors 2018. CC BY 4.0 License.","Deep learning; Globeland30-2010; Google Earth; High resolution images; Training sample set","Image enhancement; Remote sensing; Sampling; Globeland30-2010; Google earths; High resolution image; High resolution satellite images; Land cover classification; Overall accuracies; Textual information; Training sample; Deep learning"
"Research on the construction of remote sensing automatic interpretation symbol big data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046963814&doi=10.5194%2fisprs-archives-XLII-3-381-2018&partnerID=40&md5=ce3414cc772e5ef0dfe8e6f75acc7f60","Remote sensing automatic interpretation symbol (RSAIS) is an inexpensive and fast method in providing precise in-situ information for image interpretation and accuracy. This study designed a scientific and precise RSAIS data characterization method, as well as a distributed and cloud architecture massive data storage method. Additionally, it introduced an offline and online data update mode and a dynamic data evaluation mechanism, with the aim to create an efficient approach for RSAIS big data construction. Finally, a national RSAIS database with more than 3 million samples covering 86 land types was constructed during 2013-2015 based on the National Geographic Conditions Monitoring Project of China and then annually updated since the 2016 period. The RSAIS big data has proven to be a good method for large scale image interpretation and field validation. It is also notable that it has the potential to solve image automatic interpretation with the assistance of deep learning technology in the remote sensing big data era. © Authors 2018.","Automatic interpretation symbol; Big data; Crowdsourcing update mode; Database construction; Geographical conditions monitoring; National wide application; Open evaluation; Remote sensing","Deep learning; Digital storage; Image analysis; Remote sensing; Automatic interpretation symbol; Data characterization; Database construction; Dynamic data evaluation; Geographic conditions; Geographical conditions; Image interpretation; Open evaluation; Big data"
"Application of deep learning of multi-temporal Sentinel-1 images for the classification of coastal vegetation zone of the danube delta","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046971893&doi=10.5194%2fisprs-archives-XLII-3-1311-2018&partnerID=40&md5=fedc34adce8bddb3df251ee3f26da268","Land cover is a fundamental variable for regional planning, as well as for the study and understanding of the environment. This work propose a multi-temporal approach relying on a fusion of radar multi-sensor data and information collected by the latest sensor (Sentinel-1) with a view to obtaining better results than traditional image processing techniques. The Danube Delta is the site for this work. The spatial approach relies on new spatial analysis technologies and methodologies: Deep Learning of multi-temporal Sentinel-1. We propose a deep learning network for image classification which exploits the multi-temporal characteristic of Sentinel-1 data. The model we employ is a Gated Recurrent Unit (GRU) Network, a recurrent neural network that explicitly takes into account the time dimension via a gated mechanism to perform the final prediction. The main quality of the GRU network is its ability to consider only the important part of the information coming from the temporal data discarding the irrelevant information via a forgetting mechanism. We propose to use such network structure to classify a series of images Sentinel-1 (20 Sentinel-1 images acquired between 9.10.2014 and 01.04.2016). The results are compared with results of the classification of Random Forest. © Authors 2018. CC BY 4.0 License.","Danube Delta; Deep Learning; Temporal series Sentinel-1; Vegetation","Decision trees; Image classification; Recurrent neural networks; Regional planning; Remote sensing; Sensor data fusion; Vegetation; Coastal vegetation; Danube Delta; Forgetting mechanisms; Image processing technique; Learning network; Multi-sensor data; Network structures; Sentinel-1; Deep learning"
"Detecting water bodies in Landsat8 OLI image using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046944199&doi=10.5194%2fisprs-archives-XLII-3-669-2018&partnerID=40&md5=dbc236d8774d0143bb05c01601306e4f","Water body identifying is critical to climate change, water resources, ecosystem service and hydrological cycle. Multi-layer perceptron(MLP) is the popular and classic method under deep learning framework to detect target and classify image. Therefore, this study adopts this method to identify the water body of Landsat8. To compare the performance of classification, the maximum likelihood and water index are employed for each study area. The classification results are evaluated from accuracy indices and local comparison. Evaluation result shows that multi-layer perceptron(MLP) can achieve better performance than the other two methods. Moreover, the thin water also can be clearly identified by the multi-layer perceptron. The proposed method has the application potential in mapping global scale surface water with multi-source medium-high resolution satellite data. © Authors 2018.","Deep learning; Landsat 8; Maximum likelihood; Multi-layer perceptron; Water body","Climate change; Ecosystems; Maximum likelihood; Remote sensing; Surface waters; Classification results; Evaluation results; High resolution satellite data; Hydrological cycles; LANDSAT; Learning frameworks; Multi layer perceptron; Waterbodies; Deep learning"
"Research on high accuracy detection of red tide hyperspecrral based on deep learning CNN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046947704&doi=10.5194%2fisprs-archives-XLII-3-573-2018&partnerID=40&md5=2d4747936e0e7a3c63444323688f4090","Increasing frequency in red tide outbreaks has been reported around the world. It is of great concern due to not only their adverse effects on human health and marine organisms, but also their impacts on the economy of the affected areas. this paper put forward a high accuracy detection method based on a fully-connected deep CNN detection model with 8-layers to monitor red tide in hyperspectral remote sensing images, then make a discussion of the glint suppression method for improving the accuracy of red tide detection. The results show that the proposed CNN hyperspectral detection model can detect red tide accurately and effectively. The red tide detection accuracy of the proposed CNN model based on original image and filter-image is 95.58% and 97.45%, respectively, and compared with the SVM method, the CNN detection accuracy is increased by 7.52% and 2.25%. Compared with SVM method base on original image, the red tide CNN detection accuracy based on filter-image increased by 8.62% and 6.37%. It also indicates that the image glint affects the accuracy of red tide detection seriously. © Authors 2018.","CNN; Glint; Hyperspectral; Red Tide; Remote Sensing","Economic and social effects; Image enhancement; Marine biology; Remote sensing; Detection accuracy; Detection methods; Glint; HyperSpectral; Hyperspectral detections; Hyperspectral Remote Sensing Image; Red tide; Suppression method; Deep learning"
"A novel framework for remote sensing image scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046977060&doi=10.5194%2fisprs-archives-XLII-3-657-2018&partnerID=40&md5=15e1f0727871716242e7e199c7c8c0e4","High resolution remote sensing (HRRS) images scene classification aims to label an image with a specific semantic category. HRRS images contain more details of the ground objects and their spatial distribution patterns than low spatial resolution images. Scene classification can bridge the gap between low-level features and high-level semantics. It can be applied in urban planning, target detection and other fields. This paper proposes a novel framework for HRRS images scene classification. This framework combines the convolutional neural network (CNN) and XGBoost, which utilizes CNN as feature extractor and XGBoost as a classifier. Then, this framework is evaluated on two different HRRS images datasets: UC-Merced dataset and NWPU-RESISC45 dataset. Our framework achieved satisfying accuracies on two datasets, which is 95.57% and 83.35% respectively. From the experiments result, our framework has been proven to be effective for remote sensing images classification. Furthermore, we believe this framework will be more practical for further HRRS scene classification, since it costs less time on training stage. © Authors 2018.","Convolutional neural network; Deep learning; Fully-connected layer; Scene classification; XGBoost","Convolution; Deep learning; Neural networks; Remote sensing; Semantics; Uranium compounds; Convolutional neural network; Convolutional Neural Networks (CNN); Fully-connected layers; High resolution remote sensing; Remote sensing images classification; Scene classification; Spatial distribution patterns; XGBoost; Image classification"
"Rapid target detection in high resolution remote sensing images using YOLO Model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046976898&doi=10.5194%2fisprs-archives-XLII-3-1915-2018&partnerID=40&md5=18ad25aff002c5c3c7c06a8b1adfcced","Object detection in high resolution remote sensing images is a fundamental and challenging problem in the field of remote sensing imagery analysis for civil and military application due to the complex neighboring environments, which can cause the recognition algorithms to mistake irrelevant ground objects for target objects. Deep Convolution Neural Network(DCNN) is the hotspot in object detection for its powerful ability of feature extraction and has achieved state-of-the-art results in Computer Vision. Common pipeline of object detection based on DCNN consists of region proposal, CNN feature extraction, region classification and post processing. YOLO model frames object detection as a regression problem, using a single CNN predicts bounding boxes and class probabilities in an end-to-end way and make the predict faster. In this paper, a YOLO based model is used for object detection in high resolution sensing images. The experiments on NWPU VHR-10 dataset and our airport/airplane dataset gain from GoogleEarth show that, compare with the common pipeline, the proposed model speeds up the detection process and have good accuracy. © Authors 2018. CC BY 4.0 License.","Deep learning; High resolution; Object detection; Remote sensing; YOLO","Deep learning; Extraction; Feature extraction; Military photography; Object detection; Object recognition; Pipelines; Class probabilities; Convolution neural network; High resolution; High resolution remote sensing images; Recognition algorithm; Region classifications; Remote sensing imagery; YOLO; Remote sensing"
"Extracting 3D semantic information from video surveillance system using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046971472&doi=10.5194%2fisprs-archives-XLII-3-2257-2018&partnerID=40&md5=781d16b090a158774da3f95d6ed39994","At present, intelligent video analysis technology has been widely used in various fields. Object tracking is one of the important part of intelligent video surveillance, but the traditional target tracking technology based on the pixel coordinate system in images still exists some unavoidable problems. Target tracking based on pixel can't reflect the real position information of targets, and it is difficult to track objects across scenes. Based on the analysis of Zhengyou Zhang's camera calibration method, this paper presents a method of target tracking based on the target's space coordinate system after converting the 2-D coordinate of the target into 3-D coordinate. It can be seen from the experimental results: Our method can restore the real position change information of targets well, and can also accurately get the trajectory of the target in space. © Authors 2018. CC BY 4.0 License.","3-D space; Camera calibration; Target recognition; Target tracking","Calibration; Cameras; Clutter (information theory); Deep learning; Pixels; Remote sensing; Security systems; Semantics; 3-D space; Camera calibration; Intelligent Video Analysis; Intelligent video surveillance; Position information; Semantic information; Target recognition; Video surveillance systems; Target tracking"
"Quality evaluation of land-cover classification using convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046953671&doi=10.5194%2fisprs-archives-XLII-3-257-2018&partnerID=40&md5=4a1e0a8f1c79bdf14d67889d39678458","Land-cover classification is one of the most important products of earth observation, which focuses mainly on profiling the physical characters of the land surface with temporal and distribution attributes and contains the information of both natural and man-made coverage elements, such as vegetation, soil, glaciers, rivers, lakes, marsh wetlands and various man-made structures. In recent years, the amount of high-resolution remote sensing data has increased sharply. Accordingly, the volume of land-cover classification products increases, as well as the need to evaluate such frequently updated products that is a big challenge. Conventionally, the automatic quality evaluation of land-cover classification is made through pixel-based classifying algorithms, which lead to a much trickier task and consequently hard to keep peace with the required updating frequency. In this paper, we propose a novel quality evaluation approach for evaluating the land-cover classification by a scene classification method Convolutional Neural Network (CNN) model. By learning from remote sensing data, those randomly generated kernels that serve as filter matrixes evolved to some operators that has similar functions to man-crafted operators, like Sobel operator or Canny operator, and there are other kernels learned by the CNN model that are much more complex and can't be understood as existing filters. The method using CNN approach as the core algorithm serves quality-evaluation tasks well since it calculates a bunch of outputs which directly represent the image's membership grade to certain classes. An automatic quality evaluation approach for the land-cover DLG-DOM coupling data (DLG for Digital Line Graphic, DOM for Digital Orthophoto Map) will be introduced in this paper. The CNN model as an robustness method for image evaluation, then brought out the idea of an automatic quality evaluation approach for land-cover classification. Based on this experiment, new ideas of quality evaluation of DLG-DOM coupling land-cover classification or other kinds of labelled remote sensing data can be further studied. © Authors 2018.","Convolutional neural network; Deep learning; Land cover classification; Quality evaluation; Remote sensing","Convolution; Damage detection; Deep learning; Graph theory; Image quality; Neural networks; Petroleum reservoir evaluation; Remote sensing; Wetlands; Convolutional neural network; Convolutional Neural Networks (CNN); High resolution remote sensing; Land cover classification; Man-made structures; Quality evaluation; Remote sensing data; Scene classification; Classification (of information)"
"The extraction of post-earthquake building damage informatiom based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046949557&doi=10.5194%2fisprs-archives-XLII-3-161-2018&partnerID=40&md5=496a16027cb7c298f971a5774e5cd871","The seismic damage information of buildings extracted from remote sensing (RS) imagery is meaningful for supporting relief and effective reduction of losses caused by earthquake. Both traditional pixel-based and object-oriented methods have some shortcoming in extracting information of object. Pixel-based method can't make fully use of contextual information of objects. Object-oriented method faces problem that segmentation of image is not ideal, and the choice of feature space is difficult. In this paper, a new stratage is proposed which combines Convolution Neural Network (CNN) with imagery segmentation to extract building damage information from remote sensing imagery. the key idea of this method includes two steps. First to use CNN to predicate the probability of each pixel and then integrate the probability within each segmentation spot. The method is tested through extracting the collapsed building and uncollapsed building from the aerial image which is acquired in Longtoushan Town after Ms 6.5 Ludian County, Yunnan Province earthquake. The results show that the proposed method indicates its effectiveness in extracting damage information of buildings after earthquake. © Authors 2018.","Convolutional neural network; Deep learning; Earthquake; Extraction; Seismic damage information","Antennas; Buildings; Convolution; Deep learning; Extraction; Image segmentation; Neural networks; Pixels; Remote sensing; Space optics; Structural analysis; Contextual information; Convolution neural network; Convolutional neural network; Earthquake building damage; Extracting information; Object oriented method; Remote sensing imagery; Seismic damage; Earthquakes"
"Fully convolutional network based shadow extraction from GF-2 imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046934524&doi=10.5194%2fisprs-archives-XLII-3-985-2018&partnerID=40&md5=fd1aec788baed3099a07383b372c9fa1","There are many shadows on the high spatial resolution satellite images, especially in the urban areas. Although shadows on imagery severely affect the information extraction of land cover or land use, they provide auxiliary information for building extraction which is hard to achieve a satisfactory accuracy through image classification itself. This paper focused on the method of building shadow extraction by designing a fully convolutional network and training samples collected from GF-2 satellite imagery in the urban region of Changchun city. By means of spatial filtering and calculation of adjacent relationship along the sunlight direction, the small patches from vegetation or bridges have been eliminated from the preliminary extracted shadows. Finally, the building shadows were separated. The extracted building shadow information from the proposed method in this paper was compared with the results from the traditional object-oriented supervised classification algorihtms. It showed that the deep learning network approach can improve the accuracy to a large extent. © Authors 2018. CC BY 4.0 License.","Building shadows; Deep learning; Fully convolutional network; GF-2; Shadow extraction","Buildings; Classification (of information); Convolution; Deep learning; Land use; Remote sensing; Auxiliary information; Building extraction; Convolutional networks; GF-2; High spatial resolution; Shadow extractions; Spatial filterings; Supervised classification; Satellite imagery"
"Extraction of built-up areas using convolutional neural networks and transfer learning from sentinel-2 satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046941787&doi=10.5194%2fisprs-archives-XLII-3-79-2018&partnerID=40&md5=092e347dc8f2268c6c05570bd6c8e3de","With rapid globalization, the extent of built-up areas is continuously increasing. Extraction of features for classifying built-up areas that are more robust and abstract is a leading research topic from past many years. Although, various studies have been carried out where spatial information along with spectral features has been utilized to enhance the accuracy of classification. Still, these feature extraction techniques require a large number of user-specific parameters and generally application specific. On the other hand, recently introduced Deep Learning (DL) techniques requires less number of parameters to represent more abstract aspects of the data without any manual effort. Since, it is difficult to acquire high-resolution datasets for applications that require large scale monitoring of areas. Therefore, in this study Sentinel-2 image has been used for built-up areas extraction. In this work, pre-trained Convolutional Neural Networks (ConvNets) i.e. Inception v3 and VGGNet are employed for transfer learning. Since these networks are trained on generic images of ImageNet dataset which are having very different characteristics from satellite images. Therefore, weights of networks are fine-tuned using data derived from Sentinel-2 images. To compare the accuracies with existing shallow networks, two state of art classifiers i.e. Gaussian Support Vector Machine (SVM) and Back-Propagation Neural Network (BP-NN) are also implemented. Both SVM and BP-NN gives 84.31% and 82.86% overall accuracies respectively. Inception-v3 and VGGNet gives 89.43% of overall accuracy using fine-tuned VGGNet and 92.10% when using Inception-v3. The results indicate high accuracy of proposed fine-tuned ConvNets on a 4-channel Sentinel-2 dataset for built-up area extraction. © Authors 2018.","Built-up area extraction; Convolutional neural networks; Deep learning; Sentinel-2 images; Transfer learning","Arts computing; Backpropagation; Convolution; Data mining; Deep learning; Extraction; Image processing; Neural networks; Remote sensing; Support vector machines; Accuracy of classifications; Backpropagation neural networks; Built-up areas; Convolutional neural network; Feature extraction techniques; High-resolution datasets; Sentinel-2 images; Transfer learning; Classification (of information)"
"Study on the classification of Gaofen-3 polarimetric SAR images using deep neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046936735&doi=10.5194%2fisprs-archives-XLII-3-2263-2018&partnerID=40&md5=541121ca557a82b86c8582086f662ab6","Polarimetric Synthetic Aperture Radar(POLSAR) imaging principle determines that the image quality will be affected by speckle noise. So the recognition accuracy of traditional image classification methods will be reduced by the effect of this interference. Since the date of submission, Deep Convolutional Neural Network impacts on the traditional image processing methods and brings the field of computer vision to a new stage with the advantages of a strong ability to learn deep features and excellent ability to fit large datasets. Based on the basic characteristics of polarimetric SAR images, the paper studied the types of the surface cover by using the method of Deep Learning. We used the fully polarimetric SAR features of different scales to fuse RGB images to the GoogLeNet model based on convolution neural network Iterative training, and then use the trained model to test the classification of data validation. First of all, referring to the optical image, we mark the surface coverage type of GF-3 POLSAR image with 8m resolution, and then collect the samples according to different categories. To meet the GoogLeNet model requirements of 256 × 256 pixel image input and taking into account the lack of full-resolution SAR resolution, the original image should be pre-processed in the process of resampling. In this paper, POLSAR image slice samples of different scales with sampling intervals of 2m and 1m to be trained separately and validated by the verification dataset. Among them, the training accuracy of GoogLeNet model trained with resampled 2-m polarimetric SAR image is 94.89%, and that of the trained SAR image with resampled 1 m is 92.65%. © Authors 2018. CC BY 4.0 License.","Deep learning; Gaofen-3 satellite; GoogLeNet; Landcover; Sar classification","Convolution; Deep learning; Deep neural networks; Geometrical optics; Image classification; Iterative methods; Neural networks; Optical data processing; Polarimeters; Remote sensing; Synthetic aperture radar; Classification of data; Convolution neural network; Deep convolutional neural networks; Fully polarimetric SAR; GoogLeNet; Image processing - methods; Land cover; Polarimetric synthetic aperture radars; Radar imaging"
"Fusing panchromatic and SWIR bands based On CNN - A preliminary study over worldview-3 datasets","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046974454&doi=10.5194%2fisprs-archives-XLII-3-437-2018&partnerID=40&md5=995c4fbc93d79a953aa32ffeb87159d8","The traditional fusion methods are based on the fact that the spectral ranges of the Panchromatic (PAN) and multispectral bands (MS) are almost overlapping. In this paper, we propose a new pan-sharpening method for the fusion of PAN and SWIR (short-wave infrared) bands, whose spectral coverages are not overlapping. This problem is addressed with a convolutional neural network (CNN), which is trained by WorldView-3 dataset. CNN can learn the complex relationship among bands, and thus alleviate spectral distortion. Consequently, in our network, we use the simple three-layer basic architecture with 16×16 kernels to conduct the experiment. Every layer use different receptive field. The first two layers compute 512 feature maps by using the 16×16 and 1×1 receptive field respectively and the third layer with a 8×8 receptive field. The fusion results are optimized by continuous training. As for assessment, four evaluation indexes including Entropy, CC, SAM and UIQI are selected built on subjective visual effect and quantitative evaluation. The preliminary experimental results demonstrate that the fusion algorithms can effectively enhance the spatial information. Unfortunately, the fusion image has spectral distortion, it cannot maintain the spectral information of the SWIR image. © Authors 2018.","Convolutional neural network; Deep learning; Image fusion; Pan-sharpening; Remote sensing; Short-wave infrared","Convolution; Deep learning; Infrared radiation; Neural networks; Remote sensing; Complex relationships; Convolutional neural network; Convolutional Neural Networks (CNN); Pan-sharpening; Quantitative evaluation; Short wave infrared; Spatial informations; Spectral distortions; Image fusion"
"REAL-TIME and SEAMLESS MONITORING of GROUND-LEVEL PM2.5 USING SATELLITE REMOTE SENSING","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046814147&doi=10.5194%2fisprs-annals-IV-3-143-2018&partnerID=40&md5=3864d5d1021273f09a0168bf0f082703","Satellite remote sensing has been reported to be a promising approach for the monitoring of atmospheric PM2.5. However, the satellite-based monitoring of ground-level PM2.5 is still challenging. First, the previously used polar-orbiting satellite observations, which can be usually acquired only once per day, are hard to monitor PM2.5 in real time. Second, many data gaps exist in satellitederived PM2.5 due to the cloud contamination. In this paper, the hourly geostationary satellite (i.e., Harawari-8) observations were adopted for the real-time monitoring of PM2.5 in a deep learning architecture. On this basis, the satellite-derived PM2.5 in conjunction with ground PM2.5 measurements are incorporated into a spatio-temporal fusion model to fill the data gaps. Using Wuhan Urban Agglomeration as an example, we have successfully derived the real-time and seamless PM2.5 distributions. The results demonstrate that Harawari-8 satellite-based deep learning model achieves a satisfactory performance (out-of-sample cross-validation R2 Combining double low line 0.80, RMSE Combining double low line 17.49 μg/m3) for the estimation of PM2.5. The missing data in satellite-derive PM2.5 are accurately recovered, with R2 between recoveries and ground measurements of 0.75. Overall, this study has inherently provided an effective strategy for the realtime and seamless monitoring of ground-level PM2.5. © Authors 2018.","Deep learning; PM2.5; Real-time; Satellite remote sensing; Seamless; Spatio-temporal fusion",
"Machine learning classification methods in hyperspectral data processing for agricultural applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055287900&doi=10.1145%2f3220228.3220242&partnerID=40&md5=82c6433008b1d8cd084eb08fb56d4d3a","In agricultural applications hyperspectral imaging is used in cases where differences in spectral reflectance of the examined objects are small. However, the large amount of data generated by hyperspectral sensors requires advance processing methods. Machine learning approaches may play an important role in this task. They are known for decades, but they need high volume of data to compute accurate results. Until recently, the availability of hyperspectral data was a big drawback. It was first used in satellites, later in manned aircrafts and data availability from those platforms was limited because of logistics complexity and high price. Nowadays, hyperspectral sensors are available for unmanned aerial vehicles, which enabled to reach a high volume of data, thus overcoming these issues. This way, the aim of this paper is to present the status of the usage of machine learning approaches in the hyperspectral data processing, with a focus on agriculture applications. Nevertheless, there are not many studies available applying machine learning approach to hyperspectral data for agricultural applications. This apparent limitation was in fact the inspiration for making this survey. Preliminary results using UAV-based data are presented, showing the suitability of machine learning techniques in remote sensed data. © 2018 Association for Computing Machinery.","Agriculture; Deep learning; Hyperspectral data; Machine learning; Remote sensing","Agriculture; Antennas; Artificial intelligence; Deep learning; Hyperspectral imaging; Information analysis; Learning systems; Remote sensing; Spectroscopy; Unmanned aerial vehicles (UAV); Agriculture applications; Hyperspectral Data; Hyperspectral sensors; Machine learning approaches; Machine learning classification; Machine learning techniques; Remote sensed data; Spectral reflectances; Data handling"
"Single-frame super resolution of remote-sensing images by convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041435269&doi=10.1080%2f01431161.2018.1425561&partnerID=40&md5=92ef93afd8e287d8e3d6491aaac77722","Super resolution (SR) refers to generation of a high-resolution (HR) image from a decimated, blurred, low-resolution (LR) image set, which can be either a single-frame or multi-frame that contains a collection of images acquired from slightly different views of the same observation area. In this study, two convolutional neural network (CNN)-based deep learning techniques are adapted in single-frame SR to increase the resolution of remote sensing (RS) images by a factor of 2, 3, and 4. In order to both preserve the colour information and speed up the algorithm, first an intensity hue saturation (IHS) transform is utilized and the SR techniques are only applied to the intensity channel of the images. Colour information is then restored with an inverse IHS transformation. We demonstrate the results of the proposed method on RS images acquired from Satellites Pour l’Observation de la Terre (SPOT) or Earth-observing satellites and Pleiades satellites with different spatial resolution. First synthetic LR images are created by downsampling, then structural similarity (SSIM) Index, peak signal-to-noise ratio (PSNR), Spectral Angle Mapper (SAM) and Erreur Relative Globale Adimensionnelle de Synthese (ERGAS) values are calculated for a quantitative evaluation of the methods. Finally, the method, with better performance results, is tested within a real scenario, that is, with original LR images as the input. The obtained HR images demonstrated visible qualitative enhancements. © 2018 Informa UK Limited, trading as Taylor & Francis Group.",,"Convolution; Deep learning; Image acquisition; Image segmentation; Inverse problems; Neural networks; Optical resolving power; Remote sensing; Satellites; Signal to noise ratio; Convolutional neural network; Convolutional Neural Networks (CNN); Earth observing satellite; Intensity hue saturations; Peak Signal to Noise Ratio (PSNR); Quantitative evaluation; Single frame super resolutions; Structural similarity indices (SSIM); Image enhancement; algorithm; artificial neural network; image resolution; remote sensing; spatial resolution"
"Emerging trends in geospatial artificial intelligence (geoAI): Potential applications for environmental epidemiology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045538289&doi=10.1186%2fs12940-018-0386-x&partnerID=40&md5=8a3f4ea574c3e83197ba0840e9eaf205","Geospatial artificial intelligence (geoAI) is an emerging scientific discipline that combines innovations in spatial science, artificial intelligence methods in machine learning (e.g., deep learning), data mining, and high-performance computing to extract knowledge from spatial big data. In environmental epidemiology, exposure modeling is a commonly used approach to conduct exposure assessment to determine the distribution of exposures in study populations. geoAI technologies provide important advantages for exposure modeling in environmental epidemiology, including the ability to incorporate large amounts of big spatial and temporal data in a variety of formats; computational efficiency; flexibility in algorithms and workflows to accommodate relevant characteristics of spatial (environmental) processes including spatial nonstationarity; and scalability to model other environmental exposures across different geographic areas. The objectives of this commentary are to provide an overview of key concepts surrounding the evolving and interdisciplinary field of geoAI including spatial data science, machine learning, deep learning, and data mining; recent geoAI applications in research; and potential future directions for geoAI in environmental epidemiology. © 2018 The Author(s).","Data mining; Deep learning; Environmental epidemiology; Exposure modeling; geoAI; Geospatial artificial intelligence; Machine learning; Remote sensing; Spatial data science","artificial intelligence; data mining; ecosystem health; environmental quality; innovation; machine learning; remote sensing; spatial data; spatiotemporal analysis; air pollution; artificial intelligence; data extraction; data mining; environmental exposure; geographic information system; geospatial artificial intelligence; health hazard; human; information processing; machine learning; priority journal; remote sensing; Review; environmental exposure; environmental health; environmental monitoring; procedures; Artificial Intelligence; Environmental Exposure; Environmental Health; Environmental Monitoring"
"2017 International Conference on Big Data, IoT and Data Science, BID 2017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048254482&partnerID=40&md5=d1b9e5935c616a2d09f5cdba1469240c","The proceedings contain 35 papers. The topics discussed include: a hybrid approach for cost effective routing for WSNs using PSO and GSO algorithms; news and events aware stock price forecasting technique; an automotive diagnostics, fuel efficiency and emission monitoring system using CAN; texture based interstitial lung disease detection using convolutional neural network; automatic text summarization of news articles; content-based auto-tagging of audios using deep learning; path planning based QoS routing in VANET; and remote sensing and controlling of greenhouse agriculture parameters based on IoT.",,
"Segmentation of atmospheric clouds images obtained by remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047533236&doi=10.1109%2fTCSET.2018.8336189&partnerID=40&md5=684223b62f19933f6a72df3f67d67f97","Forecasting of clouds is not a new task, but its relevance is increasing every year. The work proposes new algorithm for segmentation of cloud images. The analysis of proposed approaches for the cloudiness segmentation problem is carried out. The advantages of the described approaches are presented. The research was carried out within the framework of the project provided by the State Fund for Fundamental Research. © 2018 IEEE.","cloud segmentation; deep learning; image processing; remote sensing; thresholding","Deep learning; Image processing; Remote sensing; Cloud image; Cloud segmentation; Fundamental research; Thresholding; Image segmentation"
"Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042711925&doi=10.1016%2fj.isprsjprs.2018.02.014&partnerID=40&md5=d5357505f795dcd35fb4d8b0d0530423","Multi-class geospatial object detection from high spatial resolution (HSR) remote sensing imagery is attracting increasing attention in a wide range of object-related civil and engineering applications. However, the distribution of objects in HSR remote sensing imagery is location-variable and complicated, and how to accurately detect the objects in HSR remote sensing imagery is a critical problem. Due to the powerful feature extraction and representation capability of deep learning, the deep learning based region proposal generation and object detection integrated framework has greatly promoted the performance of multi-class geospatial object detection for HSR remote sensing imagery. However, due to the translation caused by the convolution operation in the convolutional neural network (CNN), although the performance of the classification stage is seldom influenced, the localization accuracies of the predicted bounding boxes in the detection stage are easily influenced. The dilemma between translation-invariance in the classification stage and translation-variance in the object detection stage has not been addressed for HSR remote sensing imagery, and causes position accuracy problems for multi-class geospatial object detection with region proposal generation and object detection. In order to further improve the performance of the region proposal generation and object detection integrated framework for HSR remote sensing imagery object detection, a position-sensitive balancing (PSB) framework is proposed in this paper for multi-class geospatial object detection from HSR remote sensing imagery. The proposed PSB framework takes full advantage of the fully convolutional network (FCN), on the basis of a residual network, and adopts the PSB framework to solve the dilemma between translation-invariance in the classification stage and translation-variance in the object detection stage. In addition, a pre-training mechanism is utilized to accelerate the training procedure and increase the robustness of the proposed algorithm. The proposed algorithm is validated with a publicly available 10-class object detection dataset. © 2018","Geospatial object detection; High spatial resolution (HSR) remote sensing imagery; Position-sensitive balancing; Pre-training mechanism; Residual network","Convolution; Deep learning; Feature extraction; Image enhancement; Image resolution; Neural networks; Object recognition; Remote sensing; Convolutional networks; Convolutional Neural Networks (CNN); Engineering applications; Geo-spatial objects; High spatial resolution; Position sensitive; Pre-training; Remote sensing imagery; Object detection; algorithm; artificial neural network; classification; detection method; imagery; learning; remote sensing; spatial resolution; training"
"Ensemble Stacked Auto-encoder Classification on LIDAR Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037638530&doi=10.1007%2fs12524-017-0712-8&partnerID=40&md5=6fdb56dd4f2f81e0e6f628d339a581a3","Light detection and ranging system (LIDAR) can obtain diverse remote sensing datasets which contains different land cover information. The datasets offer vital and significant features for land cover classification. As a new and effective deep learning algorithm, stacked auto-encoders (SAE) consists of multiple auto-encoders in which the code of each auto-encoder is the input of the successive one. The classification precision is closely related to hidden layers, and the number of samples in fine-tuning step also affects classification results. In this paper we study the classifiers based on different number of samples and hidden layers. According to appropriate parameters, we promote SAE with adaptive boosting ensemble strategy to build new classification method. Two tests which are based on LIDAR datasets are implemented. The experiment results prove that the fusion of deep learning and ensemble learning is effective to LIDAR remote sensing images. The proposed method is robust to similar scenes classification. The overall accuracy increases 6% compared with bagging method on test 1. © 2017, Indian Society of Remote Sensing.","Deep learning; Ensemble learning; LIDAR; Stacked auto-encoder","algorithm; image classification; land cover; lidar; numerical method; precision; remote sensing; satellite imagery"
"Rotation-insensitive and context-augmented object detection in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040060192&doi=10.1109%2fTGRS.2017.2778300&partnerID=40&md5=6d4b98b6ab98963c0d3b9e2347c01dd3","Most of the existing deep-learning-based methods are difficult to effectively deal with the challenges faced for geospatial object detection such as rotation variations and appearance ambiguity. To address these problems, this paper proposes a novel deep-learning-based object detection framework including region proposal network (RPN) and local-contextual feature fusion network designed for remote sensing images. Specifically, the RPN includes additional multiangle anchors besides the conventional multiscale and multiaspect-ratio ones, and thus can deal with the multiangle and multiscale characteristics of geospatial objects. To address the appearance ambiguity problem, we propose a double-channel feature fusion network that can learn local and contextual properties along two independent pathways. The two kinds of features are later combined in the final layers of processing in order to form a powerful joint representation. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method. © 1980-2012 IEEE.","Convolutional neural networks (CNNs); Object detection; Remote sensing images; Restricted Boltzmann machine (RBM)","Deep learning; Neural networks; Object recognition; Remote sensing; Comprehensive evaluation; Contextual properties; Convolutional neural network; Detection framework; Geo-spatial objects; Learning-based methods; Remote sensing images; Restricted boltzmann machine; Object detection; detection method; image processing; machinery; numerical method; remote sensing; satellite imagery"
"Comparison of partial least square regression, support vector machine, and deep-learning techniques for estimating soil salinity from hyperspectral data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041188992&doi=10.1117%2f1.JRS.12.022204&partnerID=40&md5=ce435be9dfcc329a51b239834e41c145","This study explored three techniques for estimating the soil salt content from Landsat data. First, the 127 items of in situ measured hyperspectral reflectance data were collected and resampled to the spectral resolution of the reflectance bands of Landsat 5 and Landsat 8, respectively. Second, 12 soil salt indices (SSI) summarized from previous literature were determined based on the simulated Landsat bands. Third, 127 measurement groups with Landsat bands and SSI were randomly divided into training (102) and testing subgroups (25). Three techniques including partial least square regression (PLSR), support vector machine (SVM), and deep learning (DL) were used to establish a soil salinity model using SSI and the simulated Landsat bands as independent variables (IV), respectively. Results indicated that PLSR with SSI performed best for both simulated Landsat 5 and Landsat 8 data. Compared with PLSR, SVM underestimated soil salt content, whereas DL obtained centralized simulations and failed to capture the lower and upper observations. We recommend the PLSR model with SSI as IV to estimate soil salt content because it can identify >66% moderate-to-high-saline soils, which indicates its great potential for soil salt monitoring in arid or semiarid regions. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Hetao Irrigation District; hyperspectral data; Landsat; remote sensing; Salinity","Arid regions; Reflection; Remote sensing; Soils; Support vector machines; Centralized simulation; Hetao irrigation districts; Hyperspectral Data; Hyperspectral reflectance; Independent variables; LANDSAT; Partial least square regression; Salinity; Deep learning"
"Binary patterns encoded convolutional neural networks for texture recognition and remote sensing scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042190618&doi=10.1016%2fj.isprsjprs.2018.01.023&partnerID=40&md5=72fc5ee38dec1e44cb8b2c9d3e5c0805","Designing discriminative powerful texture features robust to realistic imaging conditions is a challenging computer vision problem with many applications, including material recognition and analysis of satellite or aerial imagery. In the past, most texture description approaches were based on dense orderless statistical distribution of local features. However, most recent approaches to texture recognition and remote sensing scene classification are based on Convolutional Neural Networks (CNNs). The de facto practice when learning these CNN models is to use RGB patches as input with training performed on large amounts of labeled data (ImageNet). In this paper, we show that Local Binary Patterns (LBP) encoded CNN models, codenamed TEX-Nets, trained using mapped coded images with explicit LBP based texture information provide complementary information to the standard RGB deep models. Additionally, two deep architectures, namely early and late fusion, are investigated to combine the texture and color information. To the best of our knowledge, we are the first to investigate Binary Patterns encoded CNNs and different deep network fusion architectures for texture recognition and remote sensing scene classification. We perform comprehensive experiments on four texture recognition datasets and four remote sensing scene classification benchmarks: UC-Merced with 21 scene categories, WHU-RS19 with 19 scene classes, RSSCN7 with 7 categories and the recently introduced large scale aerial image dataset (AID) with 30 aerial scene types. We demonstrate that TEX-Nets provide complementary information to standard RGB deep model of the same network architecture. Our late fusion TEX-Net architecture always improves the overall performance compared to the standard RGB network on both recognition problems. Furthermore, our final combination leads to consistent improvement over the state-of-the-art for remote sensing scene classification. © 2018","Deep learning; Local Binary Patterns; Remote sensing; Scene classification; Texture analysis","Aerial photography; Antennas; Classification (of information); Convolution; Deep learning; Image processing; Image texture; Network architecture; Network coding; Neural networks; Satellite imagery; Uranium compounds; Computer vision problems; Convolutional neural network; Local binary pattern (LBP); Local binary patterns; Material recognition; Scene classification; Statistical distribution; Texture analysis; Remote sensing; aerial photography; artificial neural network; classification; data set; learning; performance assessment; remote sensing; satellite imagery"
"Automatic Water-Body Segmentation from High-Resolution Satellite Images via Deep Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041494028&doi=10.1109%2fLGRS.2018.2794545&partnerID=40&md5=2c78a66e1bc468198690675ec30ca2c7","Water-body segmentation is an important issue in remote sensing and image interpretation. Classic methods for counteracting this problem usually include the construction of index features by combining different spectra, however, these methods are essentially rule-based and fail to take advantage of context information. Additionally, as the quality of image resolution improves, these methods are proved to be inadequate. With the rise of convolutional neural networks (CNN), the level of research about segmentation has taken a huge leap, but the field is still facing an increasing demand for data and the problem of blurring boundaries. In this letter, a new segmentation network called restricted receptive field deconvolution network (RRF DeconvNet) is proposed, with which to extract water bodies from high-resolution remote sensing images. Compared with natural images, remote sensing images have a weaker pixel neighborhood relativity; in consideration of this challenge, an RRF DeconvNet compresses the redundant layers in the original DeconvNet and no longer relies on a pretrained model. In addition, to tackle the blurring boundaries that occur in CNN, a new loss function called edges weighting loss is proposed to train segmentation networks, which has been shown to significantly sharpen the segmentation boundaries in results. Experiments, based on Google Earth images for water-body segmentation, are presented in this letter to prove our method. © 2012 IEEE.","Deep learning; neural network; remote sensing; segmentation; water body","Deep learning; Image enhancement; Image resolution; Neural networks; Remote sensing; Context information; Convolutional Neural Networks (CNN); High resolution remote sensing images; High resolution satellite images; Image interpretation; Remote sensing images; Segmentation boundaries; Waterbodies; Image segmentation; algorithm; artificial neural network; image classification; image resolution; pixel; remote sensing; satellite imagery; segmentation"
"Optimization of OpenStreetMap building footprints based on semantic information of oblique UAV images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045994589&doi=10.3390%2frs10040624&partnerID=40&md5=8f5d2a1522351486567b1e754d331d88","Building footprint information is vital for 3D building modeling. Traditionally, in remote sensing, building footprints are extracted and delineated from aerial imagery and/or LiDAR point cloud. Taking a different approach, this paper is dedicated to the optimization ofOpenStreetMap (OSM) building footprints exploiting the contour information, which is derived from deep learning-based semantic segmentation of oblique images acquired by the Unmanned Aerial Vehicle (UAV). First, a simplified 3D buildingmodel of Level ofDetail 1 (LoD1) is initialized using the footprint information fromOSMand the elevation information fromDigital SurfaceModel (DSM). In parallel, a deep neural network for pixel-wise semantic image segmentation is trained in order to extract the building boundaries as contour evidence. Subsequently, an optimization integrating the contour evidence from multi-view images as a constraint results in a refined 3D building model with optimized footprints and height. Our method is leveraged to optimize OSM building footprints for four datasets with different building types, demonstrating robust performance for both individual buildings and multiple buildings regardless of image resolution. Finally, we compare our result with reference data from German Authority Topographic-Cartographic Information System (ATKIS). Quantitative and qualitative evaluations reveal that the original OSM building footprints have large offset, but can be significantly improved from meter level to decimeter level after optimization. © 2018 by the authors.","Building footprint; Deep neural network; Oblique UAV images; Semantic segmentation","Aerial photography; Antennas; Computer software maintenance; Deep neural networks; Image resolution; Image segmentation; Information use; Remote sensing; Semantics; Unmanned aerial vehicles (UAV); 3D building models; Building footprint; Contour information; Oblique UAV images; Qualitative evaluations; Semantic image segmentations; Semantic information; Semantic segmentation; Buildings"
"Large-scale urban point cloud labeling and reconstruction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042192599&doi=10.1016%2fj.isprsjprs.2018.02.008&partnerID=40&md5=85a41d4e4a9e294a7e0b17101a6db041","The large number of object categories and many overlapping or closely neighboring objects in large-scale urban scenes pose great challenges in point cloud classification. In this paper, a novel framework is proposed for classification and reconstruction of airborne laser scanning point cloud data. To label point clouds, we present a rectified linear units neural network named ReLu-NN where the rectified linear units (ReLu) instead of the traditional sigmoid are taken as the activation function in order to speed up the convergence. Since the features of the point cloud are sparse, we reduce the number of neurons by the dropout to avoid over-fitting of the training process. The set of feature descriptors for each 3D point is encoded through self-taught learning, and forms a discriminative feature representation which is taken as the input of the ReLu-NN. The segmented building points are consolidated through an edge-aware point set resampling algorithm, and then they are reconstructed into 3D lightweight models using the 2.5D contouring method (Zhou and Neumann, 2010). Compared with deep learning approaches, the ReLu-NN introduced can easily classify unorganized point clouds without rasterizing the data, and it does not need a large number of training samples. Most of the parameters in the network are learned, and thus the intensive parameter tuning cost is significantly reduced. Experimental results on various datasets demonstrate that the proposed framework achieves better performance than other related algorithms in terms of classification accuracy and reconstruction quality. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Building reconstruction; Point cloud parsing; ReLu-NN","Classification (of information); Deep learning; Airborne Laser scanning; Building reconstruction; Classification accuracy; Discriminative features; Point cloud; Reconstruction quality; ReLu-NN; Unorganized point clouds; Three dimensional computer graphics; accuracy assessment; algorithm; artificial neural network; data set; experimental study; learning; reconstruction; three-dimensional modeling"
"Spatio-temporal prediction of crop disease severity for agricultural emergency management based on recurrent neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037345144&doi=10.1007%2fs10707-017-0314-1&partnerID=40&md5=54160c19ee25583dc5a7088783326621","As crop diseases bring huge losses every year in both developed and developing countries, determining how to precisely predict crop disease severity to facilitate agricultural emergency management is really a worldwide problem. Previous studies have introduced machine learning (ML) techniques into crop disease prediction and achieved better experimental results. However, the architectures of these ML models are unsuitable to model time series data. Moreover, the dependences among observations over time and across space have not been taken into account in model construction. By applying data-mining techniques to dynamic spatial panels of remote sensing data and considering features of bioclimatic, topographic and soil conditions as a supplement, we propose a novel crop disease prediction framework for agricultural emergency management based on ensemble learning techniques and spatio-temporal recurrent neural network (STRNN) which is an extension of recurrent neural network (RNN) in time and space. Empirical experiments are conducted on a specific dataset which is built based on reported cases of wheat yellow rust outbreaks in the Longnan city. Experimental results indicate that our proposed method outperforms all baseline models in crop disease severity prediction. The managerial implication of our work is that by applying the proposed methodology, some preparedness measures can be implemented in advance to prevent or mitigate the possible disasters according to predicted results. Notable economic and ecological benefits can be achieved by optimizing the frequency and timing of application of fungicide, pesticides and other preventative measures. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.","Crop disease prediction; Deep learning; Emergency management; Ensemble learning; Recurrent neural network","Agricultural machinery; Agriculture; Civil defense; Crops; Data mining; Deep learning; Developing countries; Disaster prevention; Disasters; Forecasting; Information management; Learning systems; Remote sensing; Risk management; Crop disease; Ecological benefits; Emergency management; Empirical experiments; Ensemble learning; Managerial implications; Recurrent neural network (RNN); Spatio-temporal prediction; Recurrent neural networks; agricultural management; artificial neural network; crop; developing world; disease severity; prediction; spatiotemporal analysis; China; Gansu; Longnan; Puccinia striiformis f. sp. tritici"
"Foreword to the special issue on machine learning for geospatial data analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046431952&doi=10.3390%2fijgi7040147&partnerID=40&md5=1375b3e9b5a5f1fb8e6040b6493511f5","Advances in machine learning research are pushing the limits of geographical information sciences (GIScience) by offering accurate procedures to analyze small-to-big GeoData. This Special Issue groups together six original contributions in the field of GeoData-driven GIScience that focus mainly on three different areas: extraction of semantic information from satellite imagery, image recommendation, and map generalization. Different technical approaches are chosen for each sub-topic, from deep learning to latent topic models. © 2018 by the authors.","Big data; Classification; Geospatial machine learning; GIS; GIScience; Remote sensing",
"Automatic ship classification from optical aerial images with convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045970125&doi=10.3390%2frs10040511&partnerID=40&md5=b8f774ddc070c07e6642ebca2ab69582","The automatic classification of ships from aerial images is a considerable challenge. Previous works have usually applied image processing and computer vision techniques to extract meaningful features from visible spectrum images in order to use them as the input for traditional supervised classifiers. We present a method for determining if an aerial image of visible spectrum contains a ship or not. The proposed architecture is based on Convolutional Neural Networks (CNN), and it combines neural codes extracted from a CNN with a k-Nearest Neighbor method so as to improve performance. The kNN results are compared to those obtained with the CNN Softmax output. Several CNN models have been configured and evaluated in order to seek the best hyperparameters, and the most suitable setting for this task was found by using transfer learning at different levels. A new dataset (named MASATI) composed of aerial imagery with more than 6000 samples has also been created to train and evaluate our architecture. The experimentation shows a success rate of over 99% for our approach, in contrast with the 79% obtained with traditional methods in classification of ship images, also outperforming other methods based on CNNs. A dataset of images (MWPU VHR-10) used in previous works was additionally used to evaluate the proposed approach. Our best setup achieves a success ratio of 86% with these data, significantly outperforming previous state-of-the-art ship classification methods. © 2018 by the authors.","Aerial image classification; Convolutional neural networks; Deep learning; Maritime surveillance; Optical remote sensing; Ships classification","Aerial photography; Antennas; Convolution; Deep learning; Nearest neighbor search; Network architecture; Neural networks; Optical data processing; Remote sensing; Ships; Aerial images; Automatic classification; Convolutional neural network; Convolutional Neural Networks (CNN); Image processing and computer vision; K-nearest neighbor method; Maritime surveillance; Optical remote sensing; Image classification"
"Mapping fine-scale urban housing prices by fusing remotely sensed imagery and social media data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044608640&doi=10.1111%2ftgis.12330&partnerID=40&md5=7b86e7c68d1acc703e3dc81b83a0c6fe","The accurate mapping of urban housing prices at a fine scale is essential to policymaking and urban studies, such as adjusting economic factors and determining reasonable levels of residential subsidies. Previous studies focus mainly on housing price analysis at a macro scale, without fine-scale study due to a lack of available data and effective models. By integrating a convolutional neural network for united mining (UMCNN) and random forest (RF), this study proposes an effective deep-learning-based framework for fusing multi-source geospatial data, including high spatial resolution (HSR) remotely sensed imagery and several types of social media data, and maps urban housing prices at a very fine scale. With the collected housing price data from China's biggest online real estate market, we produced the spatial distribution of housing prices at a spatial resolution of 5 m in Shenzhen, China. By comparing with eight other multi-source data mining techniques, the UMCNN obtained the highest housing price simulation accuracy (Pearson R = 0.922, OA = 85.82%). The results also demonstrated a complex spatial heterogeneity inside Shenzhen's housing price distribution. In future studies, we will work continuously on housing price policymaking and residential issues by including additional sources of spatial data. © 2018 John Wiley & Sons Ltd",,"artificial neural network; data mining; GIS; image analysis; mapping; price dynamics; remote sensing; social media; spatial resolution; subsidy system; urban housing; China; Guangdong; Shenzhen"
"Two-stream deep architecture for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040079705&doi=10.1109%2fTGRS.2017.2778343&partnerID=40&md5=fe7fad1918dac3b50a0a005c39d63876","Most traditional approaches classify hyperspectral image (HSI) pixels relying only on the spectral values of the input channels. However, the spatial context around a pixel is also very important and can enhance the classification performance. In order to effectively exploit and fuse both the spatial context and spectral structure, we propose a novel two-stream deep architecture for HSI classification. The proposed method consists of a two-stream architecture and a novel fusion scheme. In the two-stream architecture, one stream employs the stacked denoising autoencoder to encode the spectral values of each input pixel, and the other stream takes as input the corresponding image patch and deep convolutional neural networks are employed to process the image patch. In the fusion scheme, the prediction probabilities from two streams are fused by adaptive class-specific weights, which can be obtained by a fully connected layer. Finally, a weight regularizer is added to the loss function to alleviate the overfitting of the class-specific fusion weights. Experimental results on real HSIs demonstrate that the proposed two-stream deep architecture can achieve competitive performance compared with the state-of-the-art methods. © 1980-2012 IEEE.","Class-specific fusion; Convolutional neural networks (CNNs); Deep learning; Hyperspectral image (HSI) classification; Remote sensing; Stacked denoising autoencoder (SdAE); Two-stream architecture","Architecture; Convolution; Deep learning; Deep neural networks; Feature extraction; Hyperspectral imaging; Image processing; Learning systems; Network architecture; Neural networks; Personnel training; Pixels; Remote sensing; Spectroscopy; Auto encoders; Classification performance; Competitive performance; Convolutional neural network; Prediction probabilities; State-of-the-art methods; Traditional approaches; Two-stream; Image classification; artificial neural network; image classification; learning; numerical method; pixel; remote sensing; sensor; spectral analysis"
"Vegetation density estimation in the wild","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052023297&doi=10.1145%2f3190645.3190690&partnerID=40&md5=3ad62d206d0d911cf5a2dc6b94a73ea1","Remote sensing has revolutionized the efficiency of vegetation mapping, but such techniques remain impractical for mapping some types of flora over relatively limited spatial extents. We propose a deep-learning based framework for automated detection and planar mapping of an epiphytic plant in a forest from geotagged static imagery using inexpensive cameras. Our pipeline consists of two steps: segmentation and spatial distribution estimation. We evaluate several segmentation methods on a novel dataset of roughly 375 outdoor images with per-pixel labels indicating the presence of Spanish moss. We also evaluate the accuracy of the spatial distribution estimates with respect to field measurements by ecologists for Spanish moss. © 2018 Association for Computing Machinery.","Deep Learning; In the Wild; Machine Vision; Plants; Segmentation","Computer vision; Image segmentation; Remote sensing; Spatial distribution; Vegetation; Automated detection; Distribution estimation; Field measurement; Outdoor images; Plants; Segmentation methods; Vegetation density; Vegetation mapping; Deep learning"
"Stacking approach for CNN transfer learning ensemble for remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049367645&doi=10.1109%2fIntelliSys.2017.8324356&partnerID=40&md5=5f744984c350fa0eacf48094163a7ec8","In this paper we propose a stacking approach for Convolutional Neural Network (CNN) transfer learning ensemble for remote sensing imagery, in particular for the task of scene classification. We propose to use a combination of features produced by an ensemble of CNNs as one feature vector for classification. At the same time the original data set can be processed with different up-sampling and image enhancement methods and then used to obtain more features from pretrained networks. We investigate both fine-Tuning and non fine-Tuning approaches for transfer learning. We have selected Brazilian Coffee Scenes data set as a benchmark to measure the classification accuracy. Proposed method in case of a non fine-Tuned model shows 89.18% classification accuracy. For a fine-Tuned model the best classification rate is 96.11%. We analyzed how networks that have appeared recently (VGG-19 and SqueezeNet), can be applied to the task of transfer learning for remote sensing. Also we describe a method of decreasing processing time and memory consumption while preserving classification accuracy by using feature selection based on feature importance. © 2017 IEEE.","CNN; deep learning; Image classification; remote sensing; transfer learning","Deep learning; Image classification; Image enhancement; Intelligent systems; Neural networks; Remote sensing; Brazilian coffees; Classification accuracy; Classification rates; Convolutional Neural Networks (CNN); Memory consumption; Remote sensing imagery; Scene classification; Transfer learning; Classification (of information)"
"A review on image classification of remote sensing using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049777739&doi=10.1109%2fCompComm.2017.8322878&partnerID=40&md5=5c03e976b4227b1cde9295b0e13e682c","Deep learning is the state-of-the-art of machine learning. Previous literature demonstrates that deep learning gains the excellent performance in image classification of remote sensing. To begin with, data sources of remote sensing and current classification methods are briefly introduced. Then, common data sets and typical models of deep learning are presented, including deep belief network, convolutional neural network, stacked auto encoder. Furthermore, optimal configuration of these methods of deep learning is summarized according to the overall accuracy and Kappa coefficient. Finally, the existing problems and future work of satellite images classification by deep learning are pointed out. The review shows that deep learning is promised to be dominant method of image classification of remote sensing. © 2017 IEEE.","Deep Learning; Image Classification; Remote Sensing","Image classification; Neural networks; Remote sensing; Classification methods; Convolutional neural network; Deep belief networks; Existing problems; Kappa coefficient; Overall accuracies; Satellite images; State of the art; Deep learning"
"Classification of imbalanced land-use/land-cover data using variational semi-supervised learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046935339&doi=10.1109%2fINNOCIT.2017.8319149&partnerID=40&md5=ba6d7e000ffc855873d1a7da3831068d","Classification of Land Use/Land Cover (LULC) data is a typical task in remote-sensing domain. However, because the classes distribution in LULC data is naturally imbalance, it is difficult to do the classification. In this paper, we employ Variational Semi-Supervised Learning (VSSL) to solve imbalance problem in LULC of Jakarta City. This VSSL exploits the use of semi-supervised learning on deep learning model. Therefore, it is suitable for classifying data with abundant unlabeled like LULC. The result shows that VSSL achieves 80.17% of overall accuracy, outperforming other algorithms in comparison. © 2017 IEEE.","Imbalanced Learning; Remote Sensing; Variational Autoencoder; Variational Semi-Supervised Learning","Artificial intelligence; Classification (of information); Deep learning; Internet of things; Land use; Remote sensing; Auto encoders; Imbalance problem; Imbalanced Learning; Jakarta; Land use/land cover; Learning models; Overall accuracies; Semi- supervised learning; Supervised learning"
"Modern Trends in Hyperspectral Image Analysis: A Review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043766341&doi=10.1109%2fACCESS.2018.2812999&partnerID=40&md5=4c61f9cf67f9e67d5abbb49d06e3b60b","Over the past three decades, significant developments have been made in hyperspectral imaging due to which it has emerged as an effective tool in numerous civil, environmental, and military applications. Modern sensor technologies are capable of covering large surfaces of earth with exceptional spatial, spectral, and temporal resolutions. Due to these features, hyperspectral imaging has been effectively used in numerous remote sensing applications requiring estimation of physical parameters of many complex surfaces and identification of visually similar materials having fine spectral signatures. In the recent years, ground based hyperspectral imaging has gained immense interest in the research on electronic imaging for food inspection, forensic science, medical surgery and diagnosis, and military applications. This review focuses on the fundamentals of hyperspectral image analysis and its modern applications such as food quality and safety assessment, medical diagnosis and image guided surgery, forensic document examination, defense and homeland security, remote sensing applications such as precision agriculture and water resource management and material identification and mapping of artworks. Moreover, recent research on the use of hyperspectral imaging for examination of forgery detection in questioned documents, aided by deep learning, is also presented. This review can be a useful baseline for future research in hyperspectral image analysis. © 2018 IEEE.","Agriculture; document images; food quality and safety; hyperspectral imaging; medical imaging; remote sensing","Accident prevention; Agriculture; Deep learning; Diagnosis; Digital forensics; Food safety; Image analysis; Image processing; Imaging techniques; Medical imaging; Military applications; Network security; Precision agriculture; Quality control; Remote sensing; Spectroscopy; Surgery; Water management; Water resources; Document examinations; Document images; Food quality and safeties; Image guided surgery; Material identification; Remote sensing applications; Spatial resolution; Waterresource management; Hyperspectral imaging"
"Robust single stage detector based on two-stage regression for SAR ship detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053608318&doi=10.1145%2f3194206.3194223&partnerID=40&md5=091b3e318c58966d3e2f9c721845bd67","Automatic ship detection in SAR imagery plays an indispensable role in the surveillance of maritime activity. With the spring of the deep neural network and the rapid development of SAR imaging technique, SAR ship detection based on deep convolutional neural networks has attracted increasing attention in remote sensing imagery interpretation. However, the ships of variant sizes (multiscale) and complicated background result in the essential challenge of using deep learning based methods. The initial motivation of this paper is to design a structure that can achieve better accuracy as well as maintain compatible time efficiency on small object detection. Aiming at solving this problem, we develop anelaborately designed network based on single-shot detector and two-stage regression called Robust two-stage regression network (R2RN). Our network comprises an anchor modified first module and an object detection second module as well as the connective block between them which inherits the essence of feature pyramid. Experiments on SAR ship dataset have demonstrated the effectiveness and efficiency of the proposed method. © 2018 Association for Computing Machinery.","Convolutional Neural Networks; SAR; Ship Detection; Single shot Detector","Convolution; Deep neural networks; Efficiency; Neural networks; Object detection; Object recognition; Regression analysis; Remote sensing; Ships; Convolutional neural network; Deep convolutional neural networks; Effectiveness and efficiencies; Learning-based methods; Remote sensing imagery; Ship detection; Single shots; Small object detection; Radar imaging"
"How to deal with multi-source data for tree detection based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048071182&doi=10.1109%2fGlobalSIP.2017.8309141&partnerID=40&md5=afe44019670a4d6b524648bc12f05117","In the field of remote sensing, it is very common to use data from several sensors in order to make classification or segmentation. Most of the standard Remote Sensing analysis use machine learning methods based on image descriptions as HOG or SIFT and a classifier as SVM. In recent years neural networks have emerged as a key tool regarding the detection of objects. Due to the heterogeneity of information (optical, infrared, LiDAR), the combination of multi-source data is still an open issue in the Remote Sensing field. In this paper, we focus on managing data from multiple sources for the task of localization of urban trees in multi-source (optical, infrared, DSM) aerial images and we evaluate the different effects of preprocessing on the input data of a CNN. © 2017 IEEE.","Data Fusion; Deep Learning; Localization; Multi-source Data; Remote Sensing","Antennas; Data fusion; Forestry; Object detection; Optical radar; Remote sensing; Trees (mathematics); Different effects; Image descriptions; Localization; Machine learning methods; Multiple source; Multisource data; Remote sensing analysis; Tree detections; Deep learning"
"Parallel multi-stage features fusion of deep convolutional neural networks for aerial scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063295152&doi=10.1080%2f2150704X.2017.1415477&partnerID=40&md5=00ce4234f4b331f3fe0cc495954ecd4c","Aerial scene classification is a challenging task in the remote sensing image processing field. Owing to some similar scene, there are only differences in density. To challenge this problem, this paper proposes a novel parallel multi-stage (PMS) architecture formed by a low, middle, and high deep convolutional neural network (DCNN) sub-model. PMS model automatically learns representative and discriminative hierarchical features, which include three 512 dimension vectors, respectively, and the final representative feature created by linear connection. PMS model describes a robust feature of aerial image through three stages feature. Unlike previous methods, we only use transfer learning and deep learning methods to obtain more discriminative features from scene images while improving performance. Experimental results demonstrate that the proposed PMS model has a more superior performance than the state-of-the-art methods, obtaining average classification accuracies of 98.81% and 95.56%, respectively, on UC Merced (UCM) and aerial image dataset (AID) benchmark datasets. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Antennas; Benchmarking; Classification (of information); Convolution; Image enhancement; Neural networks; Remote sensing; Classification accuracy; Convolutional neural network; Discriminative features; Hierarchical features; Improving performance; Remote sensing image processing; Scene classification; State-of-the-art methods; Deep neural networks; aerial survey; artificial neural network; benchmarking; data set; experimental study; image classification; image processing; numerical method; numerical model; remote sensing"
"Hyperspectral remote sensing image classification based on deep extreme learning machine [基于深度极限学习机的高光谱遥感影像分类研究]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056780975&doi=10.7511%2fdllgxb201802009&partnerID=40&md5=040bca822da919698709bec19d8ab408","Hyperspectral remote sensing data are more and more popular and widely used. The accurate classification of surface objects based on hyperspectral remote sensing data is one of the core applications of hyperspectral remote sensing technology. For the classification problem of hyperspectral remote sensing image, a classification method is proposed based on deep extreme learning machine (D-ELM). This method uses a new depth learning model, that is D-ELM to classify hyperspectral remote sensing images. It is compared with extreme learning machine(ELM), support vector machine (SVM), kernal extreme learning machine(ELMK) classification methods. The results show that D-ELM classification method can excavate the spatial distribution regularity of the hyperspectral remote sensing image more accurately, and improve the accuracy of classification with respect to ELM, SVM and ELMK classification methods. © 2018, Editorial Office of Journal of Dalian University of Technology. All right reserved.","Deep learning; Extreme learning machine; Hyperspectral remote sensing image; Remote sensing image classification",
"Semi-Supervised Deep Learning Using Pseudo Labels for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034267169&doi=10.1109%2fTIP.2017.2772836&partnerID=40&md5=73c99192f5211dc3bdd32111d8acc918","Deep learning has gained popularity in a variety of computer vision tasks. Recently, it has also been successfully applied for hyperspectral image classification tasks. Training deep neural networks, such as a convolutional neural network for classification requires a large number of labeled samples. However, in remote sensing applications, we usually only have a small amount of labeled data for training because they are expensive to collect, although we still have abundant unlabeled data. In this paper, we propose semi-supervised deep learning for hyperspectral image classification-our approach uses limited labeled data and abundant unlabeled data to train a deep neural network. More specifically, we use deep convolutional recurrent neural networks (CRNN) for hyperspectral image classification by treating each hyperspectral pixel as a spectral sequence. In the proposed semi-supervised learning framework, the abundant unlabeled data are utilized with their pseudo labels (cluster labels). We propose to use all the training data together with their pseudo labels to pre-train a deep CRNN, and then fine-tune using the limited available labeled data. Further, to utilize spatial information in the hyperspectral images, we propose a constrained Dirichlet process mixture model (C-DPMM), a non-parametric Bayesian clustering algorithm, for semi-supervised clustering which includes pairwise must-link and cannot-link constraints-this produces high-quality pseudo-labels, resulting in improved initialization of the deep neural network. We also derived a variational inference model for the C-DPMM for efficient inference. Experimental results with real hyperspectral image data sets demonstrate that the proposed semi-supervised method outperforms state-of-the-art supervised and semi-supervised learning methods for hyperspectral classification. © 1992-2012 IEEE.","constrained Dirichlet process mixture model; Convolutional recurrent neural networks; pseudo labels","Classification (of information); Clustering algorithms; Convolution; Deep learning; Feature extraction; Hyperspectral imaging; Image classification; Image enhancement; Image processing; Independent component analysis; Learning algorithms; Learning systems; Mixtures; Neural networks; Personnel training; Recurrent neural networks; Remote sensing; Spectroscopy; Supervised learning; Convolutional neural network; Dirichlet process mixture model; Hyper-spectral classification; Non parametric bayesian clustering; Remote sensing applications; Semi- supervised learning; Semi-supervised Clustering; Semi-supervised learning methods; Deep neural networks"
"Pixel-wise classification method for high resolution remote sensing imagery using deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044511354&doi=10.3390%2fijgi7030110&partnerID=40&md5=0d750075a58b8d93675bad63e03d1b73","Considering the classification of high spatial resolution remote sensing imagery, this paper presents a novel classification method for such imagery using deep neural networks. Deep learning methods, such as a fully convolutional network (FCN) model, achieve state-of-the-art performance in natural image semantic segmentation when provided with large-scale datasets and respective labels. To use data efficiently in the training stage, we first pre-segment training images and their labels into small patches as supplements of training data using graph-based segmentation and the selective search method. Subsequently, FCN with atrous convolution is used to perform pixel-wise classification. In the testing stage, post-processing with fully connected conditional random fields (CRFs) is used to refine results. Extensive experiments based on the Vaihingen dataset demonstrate that our method performs better than the reference state-of-the-art networks when applied to high-resolution remote sensing imagery classification. © 2018 by the authors.","Convolution neural network; Data augmentation; Deep learning; High resolution imagery; Remote sensing; Semantic segmentation",
"A Massively Parallel Deep Rule-Based Ensemble Classifier for Remote Sensing Scenes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041527392&doi=10.1109%2fLGRS.2017.2787421&partnerID=40&md5=f8d55a22ba5605e10aadf65776dbccfd","In this letter, we propose a new approach for remote sensing scene classification by creating an ensemble of the recently introduced massively parallel deep (fuzzy) rule-based (DRB) classifiers trained with different levels of spatial information separately. Each DRB classifier consists of a massively parallel set of human-interpretable, transparent zero-order fuzzy IF...THEN... rules with a prototype-based nature. The DRB classifier can self-organize 'from scratch' and self-evolve its structure. By employing the pretrained deep convolution neural network as the feature descriptor, the proposed DRB ensemble is able to exhibit human-level performance through a transparent and parallelizable training process. Numerical examples using benchmark data set demonstrate the superior accuracy of the proposed approach together with human-interpretable fuzzy rules autonomously generated by the DRB classifier. © 2004-2012 IEEE.","Deep learning (DL); fuzzy rules; rule-based classifier; scene classification","Deep learning; Fuzzy inference; Fuzzy rules; Remote sensing; Convolution neural network; Ensemble classifiers; Human-level performance; Interpretable fuzzy rules; Massively parallels; Rule-based classifier; Scene classification; Spatial informations; Classification (of information); accuracy assessment; benchmarking; data set; fuzzy mathematics; image classification; remote sensing"
"A Multiscale and Multidepth Convolutional Neural Network for Remote Sensing Imagery Pan-Sharpening","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041545449&doi=10.1109%2fJSTARS.2018.2794888&partnerID=40&md5=1e72abdf3aacc5ae2928089b389e2d7b","Pan-sharpening is a fundamental and significant task in the field of remote sensing imagery processing, in which high-resolution spatial details from panchromatic images are employed to enhance the spatial resolution of multispectral (MS) images. As the transformation from low spatial resolution MS image to high-resolution MS image is complex and highly nonlinear, inspired by the powerful representation for nonlinear relationships of deep neural networks, we introduce multiscale feature extraction and residual learning into the basic convolutional neural network (CNN) architecture and propose the multiscale and multidepth CNN for the pan-sharpening of remote sensing imagery. Both the quantitative assessment results and the visual assessment confirm that the proposed network yields high-resolution MS images that are superior to the images produced by the compared state-of-the-art methods. © 2008-2012 IEEE.","Convolutional neural network (CNN); multiscale feature learning; pan-sharpening; remote sensing","Convolution; Deep learning; Deep neural networks; Extraction; Feature extraction; Image enhancement; Image processing; Image resolution; Job analysis; Learning systems; Mathematical transformations; Neural networks; Convolutional neural network; Multi-scale features; Pan-sharpening; Spatial resolution; Task analysis; Remote sensing; artificial neural network; image processing; imagery; multispectral image; panchromatic image; remote sensing; spatial resolution"
"Long-term annual mapping of four cities on different continents by applying a deep information learning method to Landsat data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044202715&doi=10.3390%2frs10030471&partnerID=40&md5=792d91035cbea02feab05c2569963d83","Urbanization is a substantial contributor to anthropogenic environmental change, and often occurs at a rapid pace that demands frequent and accurate monitoring. Time series of satellite imagery collected at fine spatial resolution using stable spectral bands over decades are most desirable for this purpose. In practice, however, temporal spectral variance arising from variations in atmospheric conditions, sensor calibration, cloud cover, and other factors complicates extraction of consistent information on changes in urban land cover. Moreover, the construction and application of effective training samples is time-consuming, especially at continental and global scales. Here, we propose a new framework for satellite-based mapping of urban areas based on transfer learning and deep learning techniques. We apply this method to Landsat observations collected during 1984-2016 and extract annual records of urban areas in four cities in the temperate zone (Beijing, New York, Melbourne, andMunich). The method is trained using observations of Beijing collected in 1999, and then used to map urban areas in all target cities for the entire 1984-2016 period. The method addresses two central challenges in long term detection of urban change: temporal spectral variance and a scarcity of training samples. First, we use a recurrent neural network to minimize seasonal urban spectral variance. Second, we introduce an automated transfer strategy tomaximize information gain fromlimited training samples when applied to new target cities in similar climate zones. Compared with other state-of-the-art methods, our method achieved comparable or even better accuracy: the average change detection accuracy during 1984-2016 is 89% for Beijing, 94% for New York, 93% forMelbourne, and 89% forMunich, and the overall accuracy of single-year urban maps is approximately 96 ± 3% among the four target cities. The results demonstrate the practical potential and suitability of the proposed framework. The method is a promising tool for detecting urban change in massive remote sensing data sets with limited training data. © 2018 by the authors.","Deep learning; Long time series; Recurrent neural network; Transfer learning; Urban mapping","Learning systems; Mapping; Recurrent neural networks; Remote sensing; Sampling; Satellite imagery; Time series; Atmospheric conditions; Limited training data; Long time series; Mapping of urban areas; Massive remote sensing datum; State-of-the-art methods; Transfer learning; Urban mapping; Deep learning"
"General deep transfer features based high resolution remote scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046762803&doi=10.3969%2fj.issn.1001-506X.2018.03.30&partnerID=40&md5=cf3df3a7c3e1bb0aaf705ee8cbcca159","Transferring deep convolutional neural network (DCNN) for high resolution remote scene classification is discussed. A promising architecture is proposed to enhance the generalization power of pre-trained DCNN for high resolution remote scene classification. Firstly, a linear principle component analysis network is designed to synthesize spatial information of high resolution remote sensing images. This design shortens the spatial distance between the target and source datasets for DCNN. Then pre-trained DCNN extract more general global features from the synthesized image. Experimental results of the two independent remote sensing datasets demonstrate that compared with state-of-the-art results, the proposed framework improves the accuracy of the remote scene classification without changing parameters in DCNN. © 2018, Editorial Office of Systems Engineering and Electronics. All right reserved.","Convolutional neural network (CNN); Deep learning; Generalization power; High resolution remote sensing image; Scene classification","Convolution; Deep learning; Deep neural networks; Image processing; Neural networks; Principal component analysis; Remote sensing; Changing parameter; Convolutional Neural Networks (CNN); Deep convolutional neural networks; Generalization power; High resolution remote sensing images; Principle component analysis; Scene classification; Spatial informations; Classification (of information)"
"Deep Recurrent Neural Networks for Winter Vegetation Quality Mapping via Multitemporal SAR Sentinel-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041386709&doi=10.1109%2fLGRS.2018.2794581&partnerID=40&md5=17dace48dd6941ba4f2087c83bf27566","Mapping winter vegetation quality is a challenging problem in remote sensing. This is due to cloud coverage in winter periods, leading to a more intensive use of radar rather than optical images. The aim of this letter is to provide a better understanding of the capabilities of Sentinel-1 radar images for winter vegetation quality mapping through the use of deep learning techniques. Analysis is carried out on a multitemporal Sentinel-1 data over an area around Charentes-Maritimes, France. This data set was processed in order to produce an intensity radar data stack from October 2016 to February 2017. Two deep recurrent neural network (RNN)-based classifiers were employed. Our work revealed that the results of the proposed RNN models clearly outperformed classical machine learning approaches (support vector machine and random forest). © 2004-2012 IEEE.","Gated recurrent unit (GRU); long short term memory (LSTM); multitemporal; recurrent neural network (RNN); Sentinel-1; synthetic aperture radar (SAR); vegetation quality","Decision trees; Deep neural networks; Geometrical optics; Mapping; Radar imaging; Remote sensing; Synthetic aperture radar; Vegetation; Gated recurrent unit (GRU); Learning techniques; Machine learning approaches; Multi-temporal; Multi-temporal SAR; Quality mapping; Recurrent neural network (RNN); Sentinel-1; Long short-term memory; artificial neural network; cloud cover; machine learning; optical property; remote sensing; Sentinel; synthetic aperture radar; vegetation cover; Canada; France; Maritime Provinces"
"A comparative study of LSTM neural networks in forecasting day-ahead global horizontal irradiance with satellite data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041651713&doi=10.1016%2fj.solener.2018.01.005&partnerID=40&md5=ab71097f41dcdcbfcb74db52fb0d5f6b","Accurate forecasts of solar energy are important for photovoltaic (PV) based energy plants to facilitate an early participation in energy auction markets and efficient resource planning. The study concentrates on Long Short Term Memory (LSTM), a novel forecasting method from the family of deep neural networks, and compares its forecasting accuracy to alternative methods with a proven track record in solar energy forecasting. To provide a comprehensive and reliable assessment of LSTM, the study employs remote-sensing data for testing predictive accuracy at 21 locations, 16 of which are in mainland Europe and 5 in the US. To that end, a novel framework to conduct empirical forecasting comparisons is introduced, which includes the generation of virtual PV plants. The framework enables richer comparisons with higher coverage of geographical regions. Empirical results suggest that LSTM outperforms a large number of alternative methods with substantial margin and an average forecast skill of 52.2% over the persistence model. An implication for energy management practice is that LSTM is a promising technique, which deserves a place in forecasters’ toolbox. From an academic point of view, LSTM and the proposed framework for experimental design provide a valuable environment for future studies that assess new forecasting technology. © 2018 Elsevier Ltd","Deep learning; Long short term memory; Remote sensing data; Solar energy forecasting","Brain; Deep learning; Deep neural networks; Education; Forecasting; Geographical regions; Remote sensing; Solar energy; Solar power generation; Comparative studies; Energy forecasting; Forecasting accuracy; Forecasting methods; Management practices; Predictive accuracy; Reliable assessment; Remote sensing data; Long short-term memory; artificial neural network; comparative study; forecasting method; irradiation; learning; photovoltaic system; reliability analysis; remote sensing; satellite data; solar power; Europe; United States"
"Generative street addresses from satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044511332&doi=10.3390%2fijgi7030084&partnerID=40&md5=9fe4d56d7de4c0033ce32bb57ca8a620","We describe our automatic generative algorithm to create street addresses from satellite images by learning and labeling roads, regions, and address cells. Currently, 75% of the world's roads lack adequate street addressing systems. Recent geocoding initiatives tend to convert pure latitude and longitude information into a memorable form for unknown areas. However, settlements are identified by streets, and such addressing schemes are not coherent with the road topology. Instead, we propose a generative address design that maps the globe in accordance with streets. Our algorithm starts with extracting roads from satellite imagery by utilizing deep learning. Then, it uniquely labels the regions, roads, and structures using some graph- and proximity-based algorithms. We also extend our addressing scheme to (i) cover inaccessible areas following similar design principles; (ii) be inclusive and flexible for changes on the ground; and (iii) lead as a pioneer for a unified street-based global geodatabase. We present our results on an example of a developed city and multiple undeveloped cities. We also compare productivity on the basis of current ad hoc and new complete addresses. We conclude by contrasting our generative addresses to current industrial and open solutions. © 2018 by the authors.","Generative schemes automatic geocoding; Machine learning; Remote sensing; Road extraction; Satellite imagery; Supervised learning",
"Fully Convolutional Networks for Semantic Segmentation of Very High Resolution Remotely Sensed Images Combined with DSM","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041687234&doi=10.1109%2fLGRS.2018.2795531&partnerID=40&md5=ebbfff9c9789faf7e0b26fe39668e98d","Recently, approaches based on fully convolutional networks (FCN) have achieved state-of-The-art performance in the semantic segmentation of very high resolution (VHR) remotely sensed images. One central issue in this method is the loss of detailed information due to downsampling operations in FCN. To solve this problem, we introduce the maximum fusion strategy that effectively combines semantic information from deep layers and detailed information from shallow layers. Furthermore, this letter develops a powerful backend to enhance the result of FCN by leveraging the digital surface model, which provides height information for VHR images. The proposed semantic segmentation scheme has achieved an overall accuracy of 90.6% on the ISPRS Vaihingen benchmark. © 2004-2012 IEEE.","deep learning; Fully convolutional networks (FCN); remote sensing; semantic segmentation; very high resolution (VHR)","Convolution; Deep learning; Image segmentation; Remote sensing; Semantic Web; Semantics; Convolutional networks; Digital surface models; Overall accuracies; Remotely sensed images; Semantic information; Semantic segmentation; State-of-the-art performance; Very high resolution; Image enhancement; accuracy assessment; artificial neural network; image resolution; remote sensing; segmentation"
"Monitoring Method for UAV Image of Greenhouse and Plastic-mulched Landcover Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048262902&doi=10.6041%2fj.issn.1000-1298.2018.02.018&partnerID=40&md5=7861681ea243f3dedf48e74185ac52b8","With the development of precision agriculture, the demand on rapidly obtaining the area and geographical distribution of greenhouses, plastic-mulched landcover is increased. However, using the interpretation method for satellite remote sensing images to process unmanned aerial vehicle (UAV) images is not ideal, due to the complex feature extraction, low recognition accuracy, long processing time and so on. To circumvent this issue, a UAV aerial monitoring method was proposed based on deep learning for greenhouses and plastic-mulched landcover monitoring. The six-rotor UAV equipped with Sony NEX-5k camera captured aerial photographs in the Wangyefu town of Chifeng City. The 558 UAV images were orthographically corrected and stitched. The five fully convolutional network (FCN) variants, i.e. the FCN-32s, FCN-16s, FCN-8s, FCN-4s and FCN-2s models were built by multi-scale fusion. The modes were trained end-to-end by the stochastic gradient descent algorithm with momentum. The features were extracted and learned from the photographs automatically. The FCN models were compared with two economic softwares, i.e. the pixel-based classification method of ENVI and the object-oriented classification method of eCognition. The results showed that the FCN-4s was the best model on the identification of greenhouses and plastic-mulched landcover. The average overall accuracy of test area was 97%, while that of pixel-based classification method and the object-oriented classification method was 74.1% and 81.78%, respectively. The average runtime of the FCN-4s was 16.85 s, which was 0.06% and 5.62% of those of pixel-based classification method and the object-oriented classification method, respectively. The proposed method demonstrated high recognition accuracy and fast speed, which can meet the demand on UAV monitoring of facilities agriculture. © 2018, Chinese Society of Agricultural Machinery. All right reserved.","Agricultural monitoring; Deep learning; Fully convolutional network; Semantic segmentation; Unmanned aerial vehicle","Agriculture; Antennas; Convolution; Geographical distribution; Greenhouses; Monitoring; Photography; Pixels; Remote sensing; Semantics; Stochastic systems; Unmanned aerial vehicles (UAV); Agricultural monitoring; Convolutional networks; Interpretation methods; Object oriented classification; Pixel based classifications; Satellite remote sensing; Semantic segmentation; Stochastic gradient descent algorithm; Deep learning"
"Proposal for a method of extracting road layers from remote sensing images using conditional GANs","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048385485&doi=10.1145%2f3193025.3193051&partnerID=40&md5=7bf11ceaa3238041f7d7bb13ed3514aa","With the recent advances in unmanned aerial vehicle (UAV) technology, remote sensing images have become relatively easy to obtain and their accuracy has increased enough to be able to handle land information. Therefore, there is a growing demand to utilize remote sensing images for extracting semantic objects Conventional methods are mainly focused on pixel-based classification and recently people commonly use convolutional neural networks, which post processing is required to linearize roads that are cut off and accurately shape the contours of buildings. We propose the use of a generative model to carry out this post processing in the networks. Using conditional Generative Adversarial Network (GANs), we translate remote sensing images into map-based images from which roads are easily extracted, while retaining the underlying structure. Next, we extract road layers from the generated images. Through this approach, it is possible to achieve the same effect as if complicating post processing were done in the networks during the object extraction process. © 2018 Association for Computing Machinery.","Deep learning; Extracting roads; Generative adversarial networks; Remote sensing; Satellite images","Antennas; Deep learning; Digital signal processing; Neural networks; Remote sensing; Roads and streets; Semantics; Unmanned aerial vehicles (UAV); Adversarial networks; Conventional methods; Convolutional neural network; Extracting roads; Object extraction; Pixel based classifications; Remote sensing images; Satellite images; Image processing"
"Aircraft detection of high-resolution remote sensing image based on faster R-CNN model and SSD model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047337448&doi=10.1145%2f3191442.3191443&partnerID=40&md5=f5a37b8d09a6e74711a840012e59c223","With the continuous improvement of the space resolution in remote sensing images, the rapid and accurate detection in high-resolution remote sensing images has become a hotspot in the field of remote sensing application. For nearly 10 years, deep learning has made outstanding achievements in the feature extraction of original image and received attention of a large number of scholars. Among them, the convolutional neural network (CNN) has made breakthrough progress in the field of image classification and detection, and has overcome three shortcomings of the original remote sensing image detection method: low detection efficiency, redundant human resource input, and flawed feature selection. In this paper, Faster R-CNN model and SSD model are trained by high-resolution remote sensing images. The appropriate training time is determined by the detection results of verification set and the loss function. When we get trained models, it will be used to detect the test set images, and the accuracy rate and recall rate of two models were calculated by visual interpretation method. The experimental results show that both the Faster R-CNN model and the SSD model can be applied to aircraft detection in corresponding high-resolution remote sensing images. The SSD model can detect the single scene aircraft quickly and accurately. The Faster R-CNN model has a high accuracy but cannot reach the requirement of real-time detection. Besides, the accuracy rate and recall rate of Faster RCNN model was significantly higher than the SSD model in the complex scenes, and the Faster R-CNN model has a great advantage for the detection of small aircraft. © 2018 Association for Computing Machinery.","Convolutional neural network; Faster-RCNN; Object detection; SSD","Aircraft detection; Convolution; Deep learning; Feature extraction; Image enhancement; Neural networks; Object detection; Object recognition; Space optics; Training aircraft; Continuous improvements; Convolutional neural network; Convolutional Neural Networks (CNN); Faster-RCNN; High resolution remote sensing images; Remote sensing applications; Remote sensing images; Visual interpretation; Remote sensing"
"Remote sensing image object recognition based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048824655&doi=10.1109%2fEIIS.2017.8298722&partnerID=40&md5=91d679fdf7a898f036a5e39f1a18edef","The development of space remote sensing technology brings a lot of remote sensing image data. The traditional target detection method is difficult to adapt to the large amount of high-resolution remote sensing image data. It is necessary to find a way to automatically learn the most effective features from the image data, and to fully recover the correlation between the data. Based on the recognition of the typical targets in remote sensing image data, this paper proposes a method of remote sensing target recognition based on deep learning. In this paper, the pre-train method is introduced to improve the simulation of the model. The experiment of the test set proves the validity of the method. © 2017 IEEE.","deep learning; object recognition; pre-train","Deep learning; Information systems; Information use; Neural networks; Object recognition; Space optics; Convolutional neural network; High resolution remote sensing images; Large amounts; pre-train; Remote sensing images; Space remote sensing technology; Target recognition; Typical targets; Remote sensing"
"Hyperspectral image classification via shape-adaptive deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045317422&doi=10.1109%2fICIP.2017.8296306&partnerID=40&md5=8ad95b5071d9083e57157a1ab4f299bb","Hyperspectral image(HSI) Classification is one of the most prevalent issue in remote sensing area. Recently, application of deep learning in HSI classification has emerged. However, merging spatial features with spectral properties in deep learning is a pervasive problem. This paper presents, a discriminative spatial updated deep belief network (SDBN) which effectively utilizes spatial information within spectrally identical contiguous pixels for HSI classification. In the proposed approach, HSI is first segmented into adaptive boundary adjustment based spatially similar regions with similar spectral features, following which an object-level feature extraction and classification is undertaken using deep belief network (DBN) based decision fusion approach that incorporate spatial-segmented contextual and spectral information into a DBN framework for effective spectral-spatial HSI classification. Moreover, for improved accuracy, band preference/correlation based feature selection approach is used to select the most informative bands without compromising the original content in HSI. Usage of local contextual features and spectral similarity from adaptive boundary adjustment based approach, and integration of spatial and spectral features into DBN results into improved accuracy of the final HSI classification. Experimental results on well known hyperspectral data indicates the classification accuracy of the proposed method over several existing techniques. © 2017 IEEE.","Deep belief network; Hyperpsectral image; Image classification; Segmentation","Deep learning; Feature extraction; Image classification; Image segmentation; Remote sensing; Spectroscopy; Classification accuracy; Deep belief network (DBN); Deep belief networks; Feature extraction and classification; Hyperpsectral image; Spatial informations; Spectral information; Spectral similarity; Classification (of information)"
"Hyper-voxel based deep learning for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045319743&doi=10.1109%2fICIP.2017.8296399&partnerID=40&md5=308a28422f961586f689932824c408b7","Classification of distinct classes in hyperspectral images (HSI) is one of the most pervasive problem in remote sensing field. Deep learning has recently proved its efficiency in HSI classification. However, incorporating spatial/contextual features along with spectral information in deep network is still a challenging task. In this paper, for an effective spectral-spatial feature extraction, an improved deep network, spatial updated hyper-voxel stacked auto-encoder (HVSAE) approach is proposed which exploits spatial context within spectrally similar contiguous pixels for effective HSI classification. The proposed approach involves two key steps-firstly, we compute adaptive boundary adjustment based segmentation whose size and shape can be adapted according to the spatial structures and which consists of spatially contiguous pixels with similar spectral features, followed by an object-level classification using stacked auto-encoder (SAE) based decision fusion approach that merges spatial-segmented outcome and spectral information into a SAE framework for robust spectral-spatial HSI classification. In addition, instead of directly using a large number of spectral bands, band preference and correlation based band selection approach is used to select the most informative bands without compromising the original content in HSI. Use of local spatial structural regularity and spectral similarity information from adaptive boundary adjustment based process, and fusion of spatial context and spectral features into SAE has significant effect on the accuracy of the final HSI classification. Experimental results on real divergent hyperspectral imagery with different contexts and resolutions validates the classification accuracy of the proposed method over several existing techniques. © 2017 IEEE.","Hyperpsectral image; Image classification; Segmentation; Stacked auto-encoder","Classification (of information); Deep learning; Image segmentation; Independent component analysis; Pixels; Remote sensing; Signal encoding; Spectroscopy; Auto encoders; Classification accuracy; Hyper-spectral imageries; Hyperpsectral image; Spatial structure; Spectral information; Spectral similarity; Structural regularity; Image classification"
"Hyperspectral image super-resolution via convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045329596&doi=10.1109%2fICIP.2017.8297093&partnerID=40&md5=f159a54ceeab80210cfc0f90d2bca5be","Due to the tradeoff between spatial and spectral resolution in remote sensing imaging, hyperspectral images are often acquired with a relative low spatial resolution, which limits their applications in many areas. Inspired by recent achievements in convolutional neural network (CNN) based super resolution (SR), a novel CNN based framework is constructed for SR of hyperspectral images by considering both spatial context and spectral correlation. As a result, the spectral distortion incurred by directly applying traditional SR algorithms to hyperspectral images is alleviated. Experimental results on several benchmark hyperspectral datasets have demonstrated that higher quality of reconstruction and spectral fidelity can be achieved, compared to band-wise manner based algorithms. © 2017 IEEE.","Convolutional neural network; Deep learning; Hyperspectral; Super-resolution","Benchmarking; Convolution; Deep learning; Hyperspectral imaging; Independent component analysis; Neural networks; Optical resolving power; Remote sensing; Spectroscopy; Convolutional neural network; Convolutional Neural Networks (CNN); HyperSpectral; Image super resolutions; Remote sensing imaging; Spectral correlation; Spectral distortions; Super resolution; Image processing"
"A Plane Target Detection Algorithm in Remote Sensing Images based on Deep Learning Network Technology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042849371&doi=10.1088%2f1742-6596%2f960%2f1%2f012025&partnerID=40&md5=ce7fc70fbea97705323cdf606977fc5e","Plane is an important target category in remote sensing targets and it is of great value to detect the plane targets automatically. As remote imaging technology developing continuously, the resolution of the remote sensing image has been very high and we can get more detailed information for detecting the remote sensing targets automatically. Deep learning network technology is the most advanced technology in image target detection and recognition, which provided great performance improvement in the field of target detection and recognition in the everyday scenes. We combined the technology with the application in the remote sensing target detection and proposed an algorithm with end to end deep network, which can learn from the remote sensing images to detect the targets in the new images automatically and robustly. Our experiments shows that the algorithm can capture the feature information of the plane target and has better performance in target detection with the old methods. © Published under licence by IOP Publishing Ltd.",,"Deep learning; E-learning; Engineering education; Image enhancement; Image processing; Learning systems; Object recognition; Signal detection; Signal processing; Advanced technology; Deep networks; Feature information; Learning network; Performance improvements; Remote imaging; Remote sensing images; Target detection and recognition; Remote sensing"
"A deep-learning model for the amplitude inversion of internal waves based on optical remote-sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032862828&doi=10.1080%2f01431161.2017.1390269&partnerID=40&md5=8b3cecb270cab3b6b89e81bfe6df4df6","The amplitude of internal waves is an important parameter for the remote-sensing detection. However, there is no analytic method developed for the amplitude inversion of internal waves based on optical remote-sensing images. In this article, the deep-learning model is introduced to inverse the amplitude of internal waves based on a large number of optical remote-sensing images. The peak-to-peak distance and 15 types of texture characteristic parameters of images are computed, and the relationship between the amplitude of internal waves and the characteristic parameters of remote-sensing images are also investigated. In addition, based on the correlation difference between these parameters and the amplitude, three types of deep-learning model are established by selecting different parameters as input variables. Results show that the inverted amplitude of internal waves from our models shows good agreement with the in-situ data of internal waves, and the average errors between them are about 6.7%, 4.9%, and 3.6%, respectively, which indicates that the deep-learning model is effective for amplitude inversion of internal waves based on optical remote-sensing images. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Remote sensing; Analytic method; Average errors; Input variables; Internal waves; Learning models; Optical remote sensing; Remote sensing images; Texture characteristics; Deep learning; amplitude; in situ measurement; internal wave; optical method; remote sensing; satellite imagery; texture"
"Visually-enabled active deep learning for (geo) text and image classification: A review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043275308&doi=10.3390%2fijgi7020065&partnerID=40&md5=8da9ef2e78ee8ae07b160f2b7b61cd64","This paper investigates recent research on active learning for (geo) text and image classification, with an emphasis on methods that combine visual analytics and/or deep learning. Deep learning has attracted substantial attention across many domains of science and practice, because it can find intricate patterns in big data; but successful application of the methods requires a big set of labeled data. Active learning, which has the potential to address the data labeling challenge, has already had success in geospatial applications such as trajectory classification from movement data and (geo) text and image classification. This review is intended to be particularly relevant for extension of these methods to GISience, to support work in domains such as geographic information retrieval from text and image repositories, interpretation of spatial language, and related geo-semantics challenges. Specifically, to provide a structure for leveraging recent advances, we group the relevant work into five categories: Active learning, visual analytics, active learning with visual analytics, active deep learning, plus GIScience and Remote Sensing (RS) using active learning and active deep learning. Each category is exemplified by recent influential work. Based on this framing and our systematic review of key research, we then discuss some of the main challenges of integrating active learning with visual analytics and deep learning, and point out research opportunities from technical and application perspectives-for application-based opportunities, with emphasis on those that address big data with geospatial components. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Active learning; Deep learning; Geographic information retrieval; Human-centered computing; Image classification; Machine learning; Multi-class classification; Multi-label classification; Text classification; Visual analytics",
"Matching of remote sensing images with complex background variations via Siamese convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042550730&doi=10.3390%2frs10020355&partnerID=40&md5=a90367363ca1fa730090d290b5799119","Feature-based matching methods have been widely used in remote sensing image matching given their capability to achieve excellent performance despite image geometric and radiometric distortions. However, most of the feature-based methods are unreliable for complex background variations, because the gradient or other image grayscale information used to construct the feature descriptor is sensitive to image background variations. Recently, deep learning-based methods have been proven suitable for high-level feature representation and comparison in image matching. Inspired by the progresses made in deep learning, a new technical framework for remote sensing image matching based on the Siamese convolutional neural network is presented in this paper. First, a Siamese-type network architecture is designed to simultaneously learn the features and the corresponding similarity metric from labeled training examples of matching and non-matching true-color patch pairs. In the proposed network, two streams of convolutional and pooling layers sharing identical weights are arranged without the manually designed features. The number of convolutional layers is determined based on the factors that affect image matching. The sigmoid function is employed to compute the matching and non-matching probabilities in the output layer. Second, a gridding sub-pixel Harris algorithm is used to obtain the accurate localization of candidate matches. Third, a Gaussian pyramid coupling quadtree is adopted to gradually narrow down the searching space of the candidate matches, and multiscale patches are compared synchronously. Subsequently, a similarity measure based on the output of the sigmoid is adopted to find the initial matches. Finally, the random sample consensus algorithm and the whole-to-local quadratic polynomial constraints are used to remove false matches. In the experiments, different types of satellite datasets, such as ZY3, GF1, IKONOS, and Google Earth images, with complex background variations are used to evaluate the performance of the proposed method. The experimental results demonstrate that the proposed method, which can significantly improve the matching performance of multi-temporal remote sensing images with complex background variations, is better than the state-of-the-art matching methods. In our experiments, the proposed method obtained a large number of evenly distributed matches (at least 10 times more than other methods) and achieved a high accuracy (less than 1 pixel in terms of root mean square error). © 2018 by the authors.","Background variation; Gaussian pyramid coupling quadtree; Image matching; Siamese convolutional neural network; Sub-pixel Harris algorithm","Color matching; Complex networks; Convolution; Deep learning; Image matching; Least squares approximations; Mean square error; Network architecture; Neural networks; Pixels; Remote sensing; Space optics; Background variation; Convolutional neural network; Feature based matching; Harris algorithm; Multi-temporal remote sensing; Quad trees; Random sample consensus; Root mean square errors; Image enhancement"
"Large-Scale Remote Sensing Image Retrieval by Deep Hashing Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041331909&doi=10.1109%2fTGRS.2017.2756911&partnerID=40&md5=143b39b0e5e7259ac015c915e22f520f","As one of the most challenging tasks of remote sensing big data mining, large-scale remote sensing image retrieval has attracted increasing attention from researchers. Existing large-scale remote sensing image retrieval approaches are generally implemented by using hashing learning methods, which take handcrafted features as inputs and map the high-dimensional feature vector to the low-dimensional binary feature vector to reduce feature-searching complexity levels. As a means of applying the merits of deep learning, this paper proposes a novel large-scale remote sensing image retrieval approach based on deep hashing neural networks (DHNNs). More specifically, DHNNs are composed of deep feature learning neural networks and hashing learning neural networks and can be optimized in an end-to-end manner. Rather than requiring to dedicate expertise and effort to the design of feature descriptors, we can automatically learn good feature extraction operations and feature hashing mapping under the supervision of labeled samples. To broaden the application field, DHNNs are evaluated under two representative remote sensing cases: scarce and sufficient labeled samples. To make up for a lack of labeled samples, DHNNs can be trained via transfer learning for the former case. For the latter case, DHNNs can be trained via supervised learning from scratch with the aid of a vast number of labeled samples. Extensive experiments on one public remote sensing image data set with a limited number of labeled samples and on another public data set with plenty of labeled samples show that the proposed remote sensing image retrieval approach based on DHNNs can remarkably outperform state-of-the-art methods under both of the examined conditions. © 1980-2012 IEEE.","Deep hashing neural networks (DHNNs); large-scale remote sensing image retrieval; remote sensing big data (RSBD) mining; supervised learning from scratch; transfer learning","Big data; Data mining; Deep neural networks; Learning systems; Neural networks; Remote sensing; Search engines; Supervised learning; Deep feature learning; Deep hashing neural networks (DHNNs); High dimensional feature; Learning neural networks; Manuals; Remote sensing image retrieval; State-of-the-art methods; Transfer learning; Image retrieval; analytical method; artificial neural network; complexity; data mining; data set; design; image processing; map; mapping; remote sensing; supervised learning"
"Multisource Remote Sensing Data Classification Based on Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041325770&doi=10.1109%2fTGRS.2017.2756851&partnerID=40&md5=f022c93eded84d4d98f1105f6c2bc44a","As a list of remotely sensed data sources is available, how to efficiently exploit useful information from multisource data for better Earth observation becomes an interesting but challenging problem. In this paper, the classification fusion of hyperspectral imagery (HSI) and data from other multiple sensors, such as light detection and ranging (LiDAR) data, is investigated with the state-of-the-art deep learning, named the two-branch convolution neural network (CNN). More specific, a two-tunnel CNN framework is first developed to extract spectral-spatial features from HSI; besides, the CNN with cascade block is designed for feature extraction from LiDAR or high-resolution visual image. In the feature fusion stage, the spatial and spectral features of HSI are first integrated in a dual-tunnel branch, and then combined with other data features extracted from a cascade network. Experimental results based on several multisource data demonstrate the proposed two-branch CNN that can achieve more excellent classification performance than some existing methods. © 1980-2012 IEEE.","Convolutional neural network (CNN); data fusion; deep learning; feature extraction; hyperspectral imagery (HSI)","Convolution; Data fusion; Deep learning; Extraction; Feature extraction; Neural networks; Optical radar; Spectroscopy; Classification fusion; Classification performance; Convolution neural network; Convolutional neural network; Hyperspectral imagery; Light detection and ranging; Multisource remote sensing data; Remotely sensed data; Remote sensing; artificial neural network; classification; data assimilation; lidar; remote sensing; satellite data; satellite imagery; spectral analysis"
"Remote sensing scene classification based on convolutional neural networks pre-trained using attention-guided sparse filters","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042524529&doi=10.3390%2frs10020290&partnerID=40&md5=688e52e3eee871ef4f7978526dbd62f7","Semantic-level land-use scene classification is a challenging problem, in which deep learning methods, e.g., convolutional neural networks (CNNs), have shown remarkable capacity. However, a lack of sufficient labeled images has proved a hindrance to increasing the land-use scene classification accuracy of CNNs. Aiming at this problem, this paper proposes a CNN pre-training method under the guidance of a human visual attention mechanism. Specifically, a computational visual attention model is used to automatically extract salient regions in unlabeled images. Then, sparse filters are adopted to learn features from these salient regions, with the learnt parameters used to initialize the convolutional layers of the CNN. Finally, the CNN is further fine-tuned on labeled images. Experiments are performed on the UCMerced and AID datasets, which show that when combined with a demonstrative CNN, our method can achieve 2.24% higher accuracy than a plain CNN and can obtain an overall accuracy of 92.43% when combined with AlexNet. The results indicate that the proposed method can effectively improve CNN performance using easy-to-access unlabeled images and thus will enhance the performance of land-use scene classification especially when a large-scale labeled dataset is unavailable. © 2018 by the authors.","Convolutional neural networks; Scene classification; Sparse filters; Unsupervised learning; Visual attention mechanism","Bandpass filters; Behavioral research; Classification (of information); Convolution; Deep learning; Land use; Neural networks; Remote sensing; Semantics; Unsupervised learning; Convolutional neural network; Human visual attention; Learning methods; Overall accuracies; Scene classification; Sparse filters; Visual attention mechanisms; Visual attention model; Image enhancement"
"A CNN-based fusion method for feature extraction from sentinel data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042529724&doi=10.3390%2frs10020236&partnerID=40&md5=bd1801df44abe2294ff21414c321510b","Sensitivity to weather conditions, and specially to clouds, is a severe limiting factor to the use of optical remote sensing for Earth monitoring applications. A possible alternative is to benefit from weather-insensitive synthetic aperture radar (SAR) images. In many real-world applications, critical decisions are made based on some informative optical or radar features related to items such as water, vegetation or soil. Under cloudy conditions, however, optical-based features are not available, and they are commonly reconstructed through linear interpolation between data available at temporally-close time instants. In this work, we propose to estimate missing optical features through data fusion and deep-learning. Several sources of information are taken into account-optical sequences, SAR sequences, digital elevation model-so as to exploit both temporal and cross-sensor dependencies. Based on these data and a tiny cloud-free fraction of the target image, a compact convolutional neural network (CNN) is trained to perform the desired estimation. To validate the proposed approach, we focus on the estimation of the normalized difference vegetation index (NDVI), using coupled Sentinel-1 and Sentinel-2 time-series acquired over an agricultural region of Burkina Faso from May-November 2016. Several fusion schemes are considered, causal and non-causal, single-sensor or joint-sensor, corresponding to different operating conditions. Experimental results are very promising, showing a significant gain over baseline methods according to all performance indicators. © 2018 by the authors.","Coregistration; Deep learning; Multi-sensor fusion; Multitemporal images; Normalized difference vegetation index (NDVI); Pansharpening","Data fusion; Deep learning; Neural networks; Radar; Radar imaging; Remote sensing; Vegetation; Coregistration; Multi-sensor fusion; Multi-temporal image; Normalized difference vegetation index; Pan-sharpening; Synthetic aperture radar"
"Semantic Segmentation of Aerial Images With Shuffling Convolutional Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040604395&doi=10.1109%2fLGRS.2017.2778181&partnerID=40&md5=b53a33ab7c77bde29dd2d37cffef31f6","Semantic segmentation of aerial images refers to assigning one land cover category to each pixel. This is a challenging task due to the great differences in the appearances of ground objects. Many attempts have been made during the past decades. In recent years, convolutional neural networks (CNNs) have been introduced in the remote sensing field, and various solutions have been proposed to realize dense semantic labeling with CNNs. In this letter, we propose shuffling CNNs to realize semantic segmentation of aerial images in a periodic shuffling manner. This approach is a supplement to current methods for semantic segmentation of aerial images. We propose a naive version and a deeper version of this method, and both are adept at detecting small objects. Additionally, we propose a method called field-of-view (FoV) enhancement that can enhance the predictions. This method can be applied to various networks, and our experiments verify its effectiveness. The final results are further improved through an ensemble method that averages the score maps generated by the models at different checkpoints of the same network. We evaluate our models using the ISPRS Vaihingen and Potsdam data sets, and we acquire promising results using these two data sets. © 2017 IEEE.","Aerial images; convolutional neural networks (CNNs); deep learning; remote sensing; semantic segmentation","Antennas; Convolution; Deep learning; Neural networks; Object detection; Remote sensing; Semantic Web; Semantics; Aerial images; Convolutional neural network; Ensemble methods; Field of views; Ground objects; Semantic labeling; Semantic segmentation; Small objects; Image segmentation; artificial neural network; data set; field of view; image processing; land cover; pixel; remote sensing; segmentation"
"A two-stage deep neural network framework for precipitation estimation from bispectral satellite information","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042644755&doi=10.1175%2fJHM-D-17-0077.1&partnerID=40&md5=19c9be33f301231ab06f0f26d9ebf9ce","Compared to ground precipitation measurements, satellite-based precipitation estimation products have the advantage of global coverage and high spatiotemporal resolutions. However, the accuracy of satellitebased precipitation products is still insufficient to serve many weather, climate, and hydrologic applications at high resolutions. In this paper, the authors develop a state-of-the-art deep learning framework for precipitation estimation using bispectral satellite information, infrared (IR), and water vapor (WV) channels. Specifically, a two-stage framework for precipitation estimation from bispectral information is designed, consisting of an initial rain/no-rain (R/NR) binary classification, followed by a second stage estimating the nonzero precipitation amount. In the first stage, the model aims to eliminate the large fraction of NR pixels and to delineate precipitation regions precisely. In the second stage, the model aims to estimate the pointwise precipitation amount accurately while preserving its heavily skewed distribution. Stacked denoising autoencoders (SDAEs), a commonly used deep learning method, are applied in both stages. Performance is evaluated along a number of common performance measures, including both R/NR and real-valued precipitation accuracy, and compared with an operational product, Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks-Cloud Classification System (PERSIANN-CCS). For R/NR binary classification, the proposed two-stage model outperforms PERSIANN-CCS by 32.56% in the critical success index (CSI). For real-valued precipitation estimation, the two-stage model is 23.40% lower in average bias, is 44.52% lower in average mean squared error, and has a 27.21% higher correlation coefficient. Hence, the two-stage deep learning framework has the potential to serve as a more accurate and more reliable satellite-based precipitation estimation product. The authors also provide some future directions for development of satellite-based precipitation estimation products in both incorporating auxiliary information and improving retrieval algorithms. © 2018 American Meteorological Society.",,"algorithm; artificial neural network; estimation method; pixel; precipitation (climatology); remote sensing; satellite data"
"When low rank representation based hyperspectral imagery classification meets segmented stacked denoising auto-encoder based spatial-spectral feature","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042531857&doi=10.3390%2frs10020284&partnerID=40&md5=de9e4f5eb646774e300f1248a0f9329f","When confronted with limited labelled samples, most studies adopt an unsupervised feature learning scheme and incorporate the extracted features into a traditional classifier (e.g., support vector machine, SVM) to deal with hyperspectral imagery classification. However, these methods have limitations in generalizing well in challenging cases due to the limited representative capacity of the shallowfeature learningmodel, aswell as the insufficient robustness of the classifierwhich only depends on the supervision of labelled samples. To address these two problems simultaneously, we present an effective low-rank representation-based classification framework for hyperspectral imagery. In particular, a novel unsupervised segmented stacked denoising auto-encoder-based feature learning model is proposed to depict the spatial-spectral characteristics of each pixel in the imagery with deep hierarchical structure. With the extracted features, a low-rank representation based robust classifier is then developed which takes advantage of both the supervision provided by labelled samples and unsupervised correlation (e.g., intra-class similarity and inter-class dissimilarity, etc.) among those unlabelled samples. Both the deep unsupervised feature learning and the robust classifier benefit, improving the classification accuracy with limited labelled samples. Extensive experiments on hyperspectral imagery classification demonstrate the effectiveness of the proposed framework. © 2018 by the authors.","Deep unsupervised feature learning; Hyperspectral imagery classification; Low rank representation; Segmented stacked denoising auto-encoder","Classification (of information); Deep learning; Extraction; Learning systems; Remote sensing; Signal encoding; Spectroscopy; Support vector machines; Auto encoders; Classification framework; Hierarchical structures; Hyper-spectral imageries; Hyperspectral imagery classifications; Low-rank representations; Spectral characteristics; Unsupervised feature learning; Image classification"
"Assessment of convolution neural networks for surficial geology mapping in the South Rae geological region, Northwest Territories, Canada","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042525882&doi=10.3390%2frs10020307&partnerID=40&md5=ffbfb81466c9b62e6486beb8ca87656c","Mapping of surficial geology is an important requirement for broadening the geoscience database of northern Canada. Surficial geology maps are an integral data source for mineral and energy exploration. Moreover, they provide information such as the location of gravels and sands, which are important for infrastructure development. Currently, surficial geology maps are produced through expert interpretation of aerial photography and field data. However, interpretation is known to be subjective, labour-intensive and difficult to repeat. The expert knowledge required for interpretation can be challenging to maintain and transfer. In this research, we seek to assess the potential of deep neural networks to aid surficial geology mapping by providing an objective surficial materials initial layer that experts can modify to speed map development and improve consistency between mapped areas. Such an approach may also harness expert knowledge in a way that is transferable to unmapped areas. For this purpose, we assess the ability of convolution neural networks (CNN) to predict surficial geology classes under two sampling scenarios. In the first scenario, a CNN uses samples collected over the area to be mapped. In the second, a CNN trained over one area is then applied to locations where the available samples were not used in training the network. The latter case is important, as a collection of in situ training data can be costly. The evaluation of the CNN was carried out using aerial photos, Landsat reflectance, and high-resolution digital elevation data over five areas within the South Rae geological region of Northwest Territories, Canada. The results are encouraging, with the CNN generating average accuracy of 76% when locally trained. For independent test areas (i.e., trained over one area and applied over other), accuracy dropped to 59-70% depending on the classes selected for mapping. In the South Rae region, significant confusion was found between till veneer and till blanket as well as glaciofluvial subclasses (esker, terraced, and hummocky ice-contact). Merging these classes respectively increased accuracy for independent test area to 68% on average. Relative to the more widely used Random Forest machine learning algorithm, this represents an improvement in accuracy of 4%. Furthermore, the CNN produced better results for less frequent classes with distinct spatial structure. © 2018 by the authors.","Deep learning; Remote sensing; Surficial geology; Surficial materials mapping","Aerial photography; Antennas; Convolution; Decision trees; Deep learning; Deep neural networks; Learning algorithms; Learning systems; Mapping; Remote sensing; Surficial sediments; Convolution neural network; Digital elevation data; Energy exploration; Expert knowledge; Infrastructure development; Labour-intensive; Spatial structure; Surficial geology; Geology"
"Semisupervised Hyperspectral Image Classification Based on Generative Adversarial Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041227580&doi=10.1109%2fLGRS.2017.2780890&partnerID=40&md5=f1b8b078938310cc83d7dcb6c3e1984b","Because the collection of ground-truth labels is difficult, expensive, and time-consuming, classifying hyperspectral images (HSIs) with few training samples is a challenging problem. In this letter, we propose a novel semisupervised algorithm for the classification of hyperspectral data by training a customized generative adversarial network (GAN) for hyperspectral data. The GAN constructs an adversarial game between a discriminator and a generator. The generator generates samples that are not distinguishable by the discriminator, and the discriminator determines whether or not a sample is composed of real data. We design a semisupervised framework for HSI data based on a 1-D GAN (HSGAN). This framework enables the automatic extraction of spectral features for HSI classification. When HSGAN is trained using unlabeled hyperspectral data, the generator can generate hyperspectral samples that are similar to the real data, while the discriminator contains the features, which can be used to classify hyperspectral data with only a small number of labeled samples. The performance of the HSGAN is evaluated on the Airborne Visible Infrared Imaging Spectrometer image data, and the results show that the proposed framework achieves very promising results with a small number of labeled samples. © 2017 IEEE.","Deep learning; generative adversarial network (GAN); hyperspectral image (HSI) classification; remote sensing; semisupervised learning (SSL)","Deep learning; Image classification; Independent component analysis; Remote sensing; Spectroscopy; Thermography (imaging); Adversarial networks; Airborne visible infrared imaging spectrometer; Automatic extraction; Hyperspectral Data; Semi-supervised; Semi-supervised algorithm; Semi-supervised learning (SSL); Spectral feature; Classification (of information); algorithm; AVIRIS; image classification; machine learning; remote sensing; spectral analysis; supervised classification"
"Semantic segmentation for high-resolution aerial imagery using multi-skip network and Markov random fields","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050877748&doi=10.1109%2fICUS.2017.8278309&partnerID=40&md5=1cc122698f519a7acb077805708874c2","Semantic segmentation for aerial imagery is a significant work for remote sensing applications especially for unmanned aerial vehicles (UAVs). In recent years, with the success of deep learning methods, convolutional neural network (CNN) based model plays an important role in both image classification and segmentation. However, due to the presence of small objects in the imagery and imbalance of classes distribution, the pixel-wise semantic segmentation remains a challenge for high-resolution remote sensing imagery. In this paper, a novel CNN based semantic segmentation method is proposed to solve the mentioned problem. To provide more context information for the decoding stage, multi-scale skip connections are designed to feed the pooling layers output from encoding stage to the decoding part. Inception modules are also used to replace the convolutional layers providing multi-scale reception areas. Finally, to enhance the result visually, we re-correct the result basing on prediction confidence in post processing procedure, and then a Markov random fields model is built to refine the label map using simulated annealing algorithm. Experiments on Vaihingen dataset show accuracy improvement on overall performance and car class segmentations. © 2017 IEEE.","convolutional neural network; Markov random fields; remote sensing; semantic segmentation; unmanned aerial vehicles","Aerial photography; Antennas; Convolution; Decoding; Deep learning; Markov processes; Neural networks; Remote sensing; Semantic Web; Semantics; Simulated annealing; Unmanned aerial vehicles (UAV); Convolutional neural network; Convolutional Neural Networks (CNN); High resolution aerial imagery; High resolution remote sensing imagery; Markov Random Fields; Remote sensing applications; Semantic segmentation; Simulated annealing algorithms; Image segmentation"
"Hierarchical Segmentation of Remote Sensing Images by Unsupervised Deep Learning Features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046971355&doi=10.1109%2fISCID.2017.79&partnerID=40&md5=3f06347b59a3efb97aab57cea1eadc25","Due to the scale diversity of geographical objects, hierarchical remote sensing image segmentation plays an important role in object-oriented image analysis. In this paper, a hierarchical remote sensing images segmentation method with unsupervised deep learning features is proposed. The unsupervised deep learning features of an image are extracted with a sparse convolutional auto-encoder. Both deep features and the clustering information of deep features are utilized in hierarchical image segmentation which is a bottom-up region merging process. The iterative region merging process starts from an initial partition of an image under a merging criterion. Finally, a tree-like image segmentations hierarchy which contains ground objects of different scales is obtained. The proposed method integrates the advantages of unsupervised deep learning features with region merging-based hierarchical image segmentation. The experiment results have shown the proposed method is superior to the methods using only spectral or spatial features. © 2017 IEEE.","hierarchical image segmentation; nearest neighbor graph; region adjacency graph; remote sensing images; sparse convolutional auto-encoder","Artificial intelligence; Convolution; Deep learning; Iterative methods; Merging; Remote sensing; Signal encoding; Viscosity measurement; Auto encoders; Clustering information; Geographical objects; Hierarchical segmentation; Nearest neighbors; Object-oriented image analysis; Region adjacency graphs; Remote sensing images; Image segmentation"
"Combined Deep Learning and Multiscale Segmentation for Rapid High Resolution Damage Mapping","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047328727&doi=10.1109%2fiThings-GreenCom-CPSCom-SmartData.2017.238&partnerID=40&md5=b4d1a5b33ffb4549963b1db998cbf207","High resolution seismic remote sensing image classification plays a key role in rapid post-earthquake damage assessment and emergency rescue. Convolution neural networks (CNNs) exhibit good performance in image classification, especially for automatic extraction of high-level features and accurate identification of target objects. However, it suffers from difficulty in localization accuracy due to the reduced feature resolution in the repeated combination of max-pooling and downsampling in the network, which may lead to poor objects delineation. To tackle this problem, multiscale segmentation is considered as a promising solution. It extracts the objects information based on the regions contained in the multiscale segmentation results, so that the original boundary information can be well-preserved. In this paper, we propose a combined convolutional neural network and multiscale segmentation (CMSCNN) method for high resolution seismic image classification. First, CNN is directly trained to classify the original image. The initial classification map is combined with the multiscale segmentation results to obtain the class-based segmented images with different scales. Especially, the multiscale segmentation considers the scale variance of different objects, thus capturing objects as well as image context at the same time. Finally, these classification images are voted to determine the final results of segmented images in classes. The experimental results show that the proposed approach can reflect the multiscale information of complex scenes and obtain good classification results for mapping seismic damage using high resolution remote sensing images. © 2017 IEEE.","convolutional neural network; damage mapping; deep learning; high resolution seismic image; multiscale segmentation","Classification (of information); Convolution; Damage detection; Deep learning; Earthquakes; Green computing; Image classification; Internet of things; Mapping; Neural networks; Remote sensing; Seismic waves; Structural analysis; Convolution neural network; Convolutional neural network; Damage mapping; High resolution remote sensing images; High resolution seismic; Multi-scale informations; Multiscale segmentation; Remote sensing image classification; Image segmentation"
"Cloud Detection of ZY-3 Satellite Remote Sensing Images Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045763045&doi=10.3788%2fAOS201838.0128005&partnerID=40&md5=5aa74294ddd63d70e5262067f7c4623f","The cloud detection method of ZY-3 satellite remote sensing images based on deep learning is proposed to solve the problem of the images with few image bands and limited spectral range. Firstly, we obtain the feature of remote sensing images measured with the unsupervised pre-training network structure of principal component analysis. Secondly, we put forward the adaptive pooling model, which can well mine images in order to reduce the loss of image feature information in the pooling process. Finally, the image features are input into the support vector machine classifier to obtain the cloud detection results. The typical regions are selected for cloud detection experiments, and the detection results are compared with that of the traditional Otsu method. The results show that the proposed method has high detection precision and is not limited by the spectral range, and it can be used for the multi-spectral and panchromatic images cloud detection of ZY-3 satellite. © 2018, Chinese Lasers Press. All right reserved.","Cloud detection; Deep learning; Principal component analysis; Remote sensing; ZY-3 satellite image","Image analysis; Principal component analysis; Remote sensing; Satellites; Cloud detection; Cloud detection method; Detection precision; Panchromatic images; Remote sensing images; Satellite images; Satellite remote sensing; Support vector machine classifiers; Deep learning"
"Valid Aircraft Detection System for Remote Sensing Images Based on Cognitive Models","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045761647&doi=10.3788%2fAOS201838.0111005&partnerID=40&md5=087397105eb3e4d7b633cc51fc01b840","In view of the problems in the traditional aircraft detection algorithms and the existing machine learning detection algorithms, one concept of valid aircraft detection is proposed for remote sensing images. With the full convolution detection and segmentation network based on the deep learning in the cognitive model, one valid aircraft detection system is designed and simulated. A cognitive model for detection is constructed, and the function of each module is designed. The experimental results certify the effectiveness of this system, and this system provides a new thinking way and method for the development of intelligent detection of multiple objectives. © 2018, Chinese Lasers Press. All right reserved.","Cognitive model; Deep learning; Image segmentation; Imaging systems; Remote sensing images; Valid aircraft detection","Aircraft; Cognitive systems; Deep learning; Image segmentation; Imaging systems; Remote sensing; Signal detection; Aircraft detection system; Cognitive model; Detection algorithm; Intelligent detection; Multiple-objectives; Network-based; Remote sensing images; Aircraft detection"
"Building extraction in very high resolution remote sensing imagery using deep learning and guided filters","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040839625&doi=10.3390%2frs10010144&partnerID=40&md5=a65ae47e8edd3db33e1fd430c20859aa","Very high resolution (VHR) remote sensing imagery has been used for land cover classification, and it tends to a transition from land-use classification to pixel-level semantic segmentation. Inspired by the recent success of deep learning and the filter method in computer vision, this work provides a segmentation model, which designs an image segmentation neural network based on the deep residual networks and uses a guided filter to extract buildings in remote sensing imagery. Our method includes the following steps: first, the VHR remote sensing imagery is preprocessed and some hand-crafted features are calculated. Second, a designed deep network architecture is trained with the urban district remote sensing image to extract buildings at the pixel level. Third, a guided filter is employed to optimize the classification map produced by deep learning; at the same time, some salt-and-pepper noise is removed. Experimental results based on the Vaihingen and Potsdam datasets demonstrate that our method, which benefits from neural networks and guided filtering, achieves a higher overall accuracy when compared with other machine learning and deep learning methods. The method proposed shows outstanding performance in terms of the building extraction from diversified objects in the urban district. © 2018 by the authors.","Building extraction; Deep learning; Guided filter; Very high resolution","Bandpass filters; Buildings; Extraction; Image processing; Image segmentation; Land use; Learning systems; Network architecture; Pixels; Remote sensing; Semantics; Building extraction; Guided filters; Land cover classification; Landuse classifications; Remote sensing imagery; Remote sensing images; Salt-and-pepper noise; Very high resolution; Deep learning"
"High-level semantic information extraction of remote-sensing images based on deep-learning image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059390742&doi=10.1117%2f12.2502545&partnerID=40&md5=41c3487dba029e1895318ab62901b273","In recent years, remote sensing imaging technology has developed rapidly. A growing number of high resolution remote sensing images become available, which largely facilitates the research and applications of remote sensing images. Landcover classification is one of the most important tasks of remote sensing image applications [1] . However, traditional classification methods rely on manual feature design, which is time-consuming and requires expertise. It is difficult to apply to massive data. Compared with the traditional classification methods, deep learning [2] can automatically acquire the most intrinsic and discriminative features of the image. Based on the deep learning image classification, this paper designs a high-level semantic information extraction system with high efficiency and robustness. A deep fully convolutional networks (FCN) is designed to extract the features from remote sensing images and to predict the landcover classes of each image, which include building, tree, road, and grass. On the basis of the classification results, we use binarization to highlight the building objects. Then the noise of the binarized image is removed by Gaussian filtering and morphological image processing. After that we set a threshold to delete small misdiagnosis areas. At last the connected domain algorithm is applied to detect the buildings and calculate the building number in each image. The forest coverage is then obtained by computing the proportion of the pixels with 'tree' class label to the total number of the pixels in each image. Different from the traditional image interpretation method, this systematic high-level semantic information extraction framework not only detects the number of buildings in the scene but also extracts forest coverage. Moreover, more high-level information extraction can be easily supplemented to this framework, such as road localization or interested object detection. © 2018 SPIE.","Classification; Deep learning; Fully convolutional networks; High-level semantic information extraction; Remote sensing images","Buildings; Convolution; Data mining; Deep learning; Forestry; Image classification; Information retrieval; Multimedia systems; Object detection; Object recognition; Pixels; Remote sensing; Roads and streets; Semantics; Convolutional networks; Discriminative features; High level semantics; High resolution remote sensing images; Land-cover classification; Morphological image processing; Remote sensing images; Research and application; Classification (of information)"
"Deep learning decision fusion for the classification of urban remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044071893&doi=10.1117%2f1.JRS.12.016038&partnerID=40&md5=1ec81846576a47e666095d6fbd3b006b","Multisensor data fusion is one of the most common and popular remote sensing data classification topics by considering a robust and complete description about the objects of interest. Furthermore, deep feature extraction has recently attracted significant interest and has become a hot research topic in the geoscience and remote sensing research community. A deep learning decision fusion approach is presented to perform multisensor urban remote sensing data classification. After deep features are extracted by utilizing joint spectral-spatial information, a soft-decision made classifier is applied to train high-level feature representations and to fine-Tune the deep learning framework. Next, a decision-level fusion classifies objects of interest by the joint use of sensors. Finally, a context-Aware object-based postprocessing is used to enhance the classification results. A series of comparative experiments are conducted on the widely used dataset of 2014 IEEE GRSS data fusion contest. The obtained results illustrate the considerable advantages of the proposed deep learning decision fusion over the traditional classifiers. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Convolutional neural network; Decision-level fusion; Deep features; Deep learning; Stacked sparse autoencoder; Thermal hyperspectral","Data fusion; Deep learning; Deep neural networks; Neural networks; Remote sensing; Auto encoders; Convolutional neural network; Decision level fusion; Deep features; HyperSpectral; Classification (of information)"
"Deep Learning and Machine Learning for Object Detection in Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040107034&doi=10.1007%2f978-981-10-7521-6_30&partnerID=40&md5=a385da4ad02fa4a1e49415c042bb048d","Object detection is one of the most effective ways to analyze the remote sensing (RS) images. In this paper, we focus on the prevalent object detection framework based on deep learning technology for RS images which contains three different stages, namely the region proposals generation, feature extraction, and classification. The review provides a clear picture of the challenges and possible development trends in this field. Typical methods under this framework are extensively reviewed and analyzed. Comparisons among traditional methods with deep learning methods are presented, in which supervised and unsupervised methods for RS scene target detection are deeply discussed. © 2018, Springer Nature Singapore Pte Ltd.","Deep learning; Object detection; Remote sensing","Computer vision; Deep learning; Feature extraction; Learning systems; Object recognition; Remote sensing; Detection framework; Development trends; Different stages; Learning methods; Learning technology; Remote sensing images; RS image; Unsupervised method; Object detection"
"Cloud detection of remote sensing images on landsat-8 by deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052612409&doi=10.1117%2f12.2503034&partnerID=40&md5=024fc54463581a329faa14ac51fd5982","Cloud is always the weak and even uninformative area inevitably existing in the remote sensing images, and greatly limits the development of remote sensing applications. Accurate and automatic detection of clouds in satellite scenes is a key problem for the application of remote sensing images. Most of the previous methods use the low-level feature of the cloud, which often generate error results especially with thin cloud or in complex scenes. In this paper, we propose a novel cloud detection method based on deep learning framework for remote sensing images. The designed deep Convolution Neural Network (CNN) which can mine the deep features of cloud consists of three convolution layers and three fully-connected layers. Using the designed network model, we can predict the probability of each image that belongs to cloud region, and then generate the cloud probability map of the image. To demonstrate the effectiveness of the method, we test it on Landsat-8 satellite images. The overall accuracy of our proposed method for cloud detection is higher than 95%. Experimental results indicate that both thin and thick cloud can be well detected with higher accuracy and robustness using our method. © 2018 SPIE.","Cloud detection; Convolution neural network; Deep learning; Landsat-8","Convolution; Image processing; Remote sensing; Automatic Detection; Cloud detection; Cloud detection method; Convolution neural network; Fully-connected layers; LANDSAT; Remote sensing applications; Remote sensing images; Deep learning"
"Deep Learning for Fusion of APEX Hyperspectral and Full-Waveform LiDAR Remote Sensing Data for Tree Species Mapping","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056537761&doi=10.1109%2fACCESS.2018.2880083&partnerID=40&md5=4b9e15ab09cf7c10015a996c0d24a801","Deep learning has been widely used to fuse multi-sensor data for classification. However, current deep learning architecture for multi-sensor data fusion might not always perform better than single data source, especially for the fusion of hyperspectral and light detection and ranging (LiDAR) remote sensing data for tree species mapping in complex, closed forest canopies. In this paper, we propose a new deep fusion framework to integrate the complementary information from hyperspectral and LiDAR data for tree species mapping. We also investigate the fusion of either 'single-band' or multi-band (i.e., full-waveform) LiDAR with hyperspectral data for tree species mapping. Additionally, we provide a solution to estimate the crown size of tree species by the fusion of multi-sensor data. Experimental results on fusing real APEX hyperspectral and LiDAR data demonstrate the effectiveness of the proposed deep fusion framework. Compared to using only single data source or current deep fusion architecture, our proposed method yields improvements in overall and average classification accuracies ranging from 82.21% to 87.10% and 76.71% to 83.45%, respectively. © 2013 IEEE.","data fusion; Deep learning; hyperspectral; LiDAR; remote sensing","Data fusion; Deep learning; Feature extraction; Forestry; Hyperspectral imaging; Mapping; Optical radar; Remote sensing; Spectroscopy; Vegetation; Classification accuracy; Fusion architecture; HyperSpectral; Hyperspectral Data; Learning architectures; LIDAR (light detection and ranging); Multisensor data fusion; Remote sensing data; Sensor data fusion"
"Estimation of remote sensing imagery atmospheric conditions using deep learning and image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060499323&doi=10.1007%2f978-3-030-01057-7_93&partnerID=40&md5=980ce4fbdf7521a20896ef8d35f4157d","Estimation of atmospheric conditions is an important problem for remote sensing imagery analysis and processing. Especially it is useful to have a fast and accurate method when collecting weekly or daily imagery of the entire land surface of the earth with high resolution. This task appears in many remote sensing applications such as tracking changes of the landscape, agricultural image analysis, landscape anomaly detection. In this paper, we propose a method of atmospheric conditions estimation based on RGB image classification using fine-tunned CNN ensemble and image classifiers. We investigate usage of CNNs (Alexnet and a pretrained CNN ensemble) as feature extractors in combination with different classifiers such as XGBoost and ExtraTrees. We have tested the proposed method on a data set provided in the kaggle contest “Planet: Understanding the Amazon from Space” where the application task is to analyze deforestation in the Amazon Basin. © Springer Nature Switzerland AG 2019.","Classification and regression trees; Deep learning; Neural networks; Transfer learning","Deep learning; Deforestation; Image analysis; Neural networks; Remote sensing; Space optics; Anomaly detection; Application tasks; Atmospheric conditions; Classification and regression tree; Image Classifiers; Remote sensing applications; Remote sensing imagery; Transfer learning; Image classification"
"Ship Detection from Remote Sensing Images Based on Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049026186&doi=10.1007%2f978-981-13-0893-2_36&partnerID=40&md5=6dcd543abd2f12628b9054a5ba29017a","Due to the complicated maritime climate environment, the detection of marine Ship by using Remote sensing images is faced with many challenges in the field of object detection. In this paper, a ship detection method based on dark channel priority haze removal and Faster RCNN is proposed to solve this problem. We label and experiment with thousands of ships images on the sea. Compared with the using of object detection model directly and some traditional methods, the detection accuracy of the new method is obviously improved. © 2018, Springer Nature Singapore Pte Ltd.","Deep learn; Faster RCNN; Haze removal; Remote sensing","Object detection; Object recognition; Remote sensing; Ships; Detection accuracy; Faster RCNN; Haze removal; Maritime climate; Remote sensing images; Ship detection; Deep learning"
"Impact of Deep Learning in Image Processing and Computer Vision","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041806134&doi=10.1007%2f978-981-10-7329-8_48&partnerID=40&md5=dccb07e9ec392a040a5ff56dcc0e73a0","With deep learning techniques, a revolution has taken place in the field of image processing and computer vision. The survey paper emphasizes the importance of representation learning methods for machine learning tasks. Deep learning, the modern machine learning is commonly used in the vision tasks—semantic segmentation, image captioning, object detection, recognition, and image classification. The paper focuses on the recent developments in the domain of remote sensing, retinal image understanding, and scene understanding based on newly proposed deep architectures. The author finds it quite intriguing of the classical building blocks of image segmentation (Gabor, K-means), shifting gear, and contributing to image recognition tasks based on deep learning (Gabor convolutional network, K-means dictionary learning). The survey makes an attempt to serve as a concise guide in providing latest works in computer vision applications based on deep learning and giving futuristic insights. © Springer Nature Singapore Pte Ltd. 2018.","Computer vision; Deep learning; Image processing; Representation learning","Artificial intelligence; Computer vision; Image processing; Image recognition; Image segmentation; Learning systems; Microelectronics; Object detection; Remote sensing; Semantics; Surveys; Computer vision applications; Convolutional networks; Dictionary learning; Image processing and computer vision; Learning techniques; Representation learning; Scene understanding; Semantic segmentation; Deep learning"
"Greedy Annotation of Remote Sensing Image Scenes Based on Automatic Aggregation via Hierarchical Similarity Diffusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054481065&doi=10.1109%2fACCESS.2018.2873761&partnerID=40&md5=a91bc85967abe8105d118cbb26c07859","As a basic and key problem in the remote sensing community, remote sensing image scene understanding (RSISU) has attracted increasing research interest. In recent years, deep learning has revolutionized RSISU. However, the great success of deep learning strongly depends on the availability of a large-scale data set with explicit labels. Although remote sensing image scene data sets have been publicly released for a limited number of remote sensing image types, data sets of many types of remote sensing images are still not available, which limits the applicability of deep learning. Generally, exhaustively labeling remote sensing image scene data sets via manual labor is time consuming, and it becomes impossible when the data set volume is very large. Hence, it is necessary to develop an intelligent annotation approach to efficiently and accurately label these data sets. Based on a prior assumption of consistency, namely, the assumption that samples within the same cluster are likely to have the same label, this paper proposes a novel annotation method for remote sensing image scene data sets called automatic aggregation via hierarchical similarity diffusion (AA-HSD). More specifically, each remote sensing image scene is represented by multiple features. To make full use of these complementary features, this paper proposes a new hierarchical similarity diffusion method for robustly measuring the similarity matrix of the scenes in the data set. Based on this similarity matrix, the scenes are automatically aggregated into clusters. Instead of annotating the data set scene by scene, as in the traditional manual annotation solution, we annotate the data set cluster by cluster, which dramatically increases the annotation speed while achieving a very high accuracy. Extensive experiments on two public remote sensing image scene data sets demonstrate the validity of our proposed AA-HSD method, which outperforms all competing baselines. © 2013 IEEE.","automatic aggregation; efficient and accurate image scene dataset annotation; hierarchical similarity diffusion; Remote sensing image scene understanding (RSISU)","Agglomeration; Deep learning; Diffusion; Image annotation; Annotation methods; Automatic aggregations; Complementary features; Image scene; Large-scale dataset; Manual annotation; Remote sensing images; Research interests; Remote sensing"
"A novel rotation invariance hashing network for fast remote sensing image retrieval","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052653610&doi=10.1117%2f12.2503185&partnerID=40&md5=41f03d283b28ae6175614c3cc591c5fa","With the increasing amount of high-resolution remote sensing images, large-scale remote sensing image retrieval(RSIR) becomes more and more significant and has attracted great attention. Traditional image retrieval methods generally use hand-crafted features which are not only time-consuming but also always get poor performance. Deep learning recently achieves remarkable performance due to its powerful ability to learn high-level semantic features, so researchers attempt to take advantage of features derived from Convolutional Neural Networks(CNNs) in RSIR. But remote sensing image is different from natural scene image, its background is more complicated with a lot of noise and existing deep learning method didn't handle this well. Both the speed and the accuracy achieve unsatisfactory performance. In this paper, we propose a rotation invariant hashing network that represents an image as a binary hash code to retrieve image faster while considering the rotation invariance of the same target. The results of the experiments on some available remote sensing datasets show that our method is effective and outperforms than other features which is usually used in RSIR. © 2018 SPIE.","Binary Hashing; Deep Learning; Remote Sensing Image Retrieval; Rotation Invariance","Deep learning; Hash functions; Image processing; Neural networks; Remote sensing; Rotation; Semantics; Binary Hashing; Convolutional neural network; High resolution remote sensing images; High-level semantic features; Natural scene images; Remote sensing image retrieval; Remote sensing images; Rotation invariance; Image retrieval"
"Ship classification from SAR images based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057103797&doi=10.1007%2f978-3-030-01054-6_2&partnerID=40&md5=2bdb71d49321bd709955f13d3bf7c08e","Ship classification based on remote sensing data is an important task of maritime/sea border security and surveillance applications. Therefore, in this research, ship classification was performed by using deep learning aiming for improving the accuracy of ship classification with respect to conventional approaches. The research focuses on not only ship or non-ship classification but also ship classification by its type and length, which is difficult using such filtering methods as CFAR and a standard deviation filter. The ship type classification resulted in promising accuracy for specific ship types with distinguishable features from SAR images. The ship length classification error resulted in 26% on average. Moreover, this research could classify ship or not with equivalent accuracy as GPU by using FPGA. © Springer Nature Switzerland AG 2019.","Convolutional neural network; Deep learning; FPGA; SAR image; Ship classification","Deep learning; Field programmable gate arrays (FPGA); Intelligent systems; Neural networks; Radar imaging; Remote sensing; Ships; Synthetic aperture radar; Classification errors; Conventional approach; Convolutional neural network; Remote sensing data; SAR Images; Ship classification; Standard deviation; Type classifications; Image classification"
"Intensive positioning network for remote sensing image captioning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057137307&doi=10.1007%2f978-3-030-02698-1_49&partnerID=40&md5=f1ff98c94af2c10305827b669ce5c9d8","This paper focuses on solving the problem of information loss during the generation of remote sensing image captions. In the field of artificial intelligence, the automatic description of remote sensing images is an important but rarely studied task. In the traditional framework, due to the higher pixels of the remote sensing image and the smaller target, when the image is processed and classified, the information is largely lost. In this case, we propose a new remote sensing image captioning framework using deep learning technology and attention mechanism. The experimental results show that the model can generate a full sentence description for remote sensing images. © Springer Nature Switzerland AG 2018.","Attention mechanism; Deep learning; Image processing; Intensive positioning network (IPN); Remote sensing image captioning","Big data; Deep learning; Image processing; Attention mechanisms; Information loss; Intensive positioning network (IPN); Learning technology; Remote sensing images; Remote sensing"
"AniWatch: Camera trap data processor for deep learning-based automatic identification of wildlife species","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071874969&partnerID=40&md5=bd858b7818bd349ab32119104a032998","Camera trap equipment is mainly used to monitor the status of wildlife in protected areas. The existing data survey identifies wild animals through visual interpretation. This process not only requires a long time, but also has the problem that the expertise of the investigator determines the reliability of the data. Recently, deep Learning in the field of image recognition has been detecting the object identification, object count, and the image description in the image with high accuracy. In this paper, we introduce the camera trap data processor (AniWatch) which can automatically database wildlife identification, animal count, and motion information by deep learning. To test the software performance, the Sobaeksan national park's Jukryong eco-corridor was selected as a study area. First, we collected the camera trap data in the area. Since we need to detect moving objects in a fixed position, we performed data preprocessing through computer vision algorithms. Through the image tracking algorithm, the minimum bounding rectangle of the wild animal object was detected, and each frame was saved as an image. Because each image is of different size and resolution, we adjusted it to 100 × 100-pixel sizes to recognize it as training data. For deep learning, we applied a convolutional neural network (CNN) technique which is used in the image recognition field. Open source libraries (OpenCV, TensorFlow, and Keras) were used to implement the model, and the software was developed as a GUI application through Python. In the test results, AniWatch confirmed that it could reduce the time required for visual interpretation and minimize human errors. In the future, we will provide an automatic calculator of monitoring statistics by inputting camera trap data. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Convolutional neural network; Image recognition; Jukryong eco-corridor; Species identification","Animals; Application programs; Automation; Cameras; Conservation; Convolution; Image recognition; Neural networks; Object detection; Open source software; Open systems; Remote sensing; Software testing; Automatic identification; Computer vision algorithms; Convolutional neural network; Jukryong eco-corridor; Minimum bounding rectangle; Open-source libraries; Species identification; Visual interpretation; Deep learning"
"Automatic building footprints extraction of Yangon city from geoeye monocular optical satellite image by using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071861850&partnerID=40&md5=5a0229462966c42f21b3e70f925583c6","In this paper, building footprints in Yangon city are automatically extracted only from high resolution optical satellite image by using deep learning algorithm. Automatic extraction of building footprints only from optical satellite images is a very challenging task because of spatial and spectral complexities of urban objects, especially trees, buildings occluded by trees, shadow and water bodies. Moreover, very high spatial resolution of satellite image is necessary for building level extraction. In this work, pix2pix is used for automatic building footprint extraction in Yangon City. Pix2pix is based on conditional generative adversarial network which is an unsupervised deep learning algorithm that only needs fewer labelled samples. First, four different models of pix2pix are designed and trained based on different number of epochs, and different types and number of training images. Second, the trained pix2pix models are tested on the images of the study area. After that, the output images of pix2pix are digitized, compared with manually digitized reference image, and evaluated by using performance metrics such as completeness, correctness, and quality, and area assessment. According to the results, pix2pix can successfully delineate building boundaries out of other urban objects up to 71.43% of completeness, 62.5% of extraction quality, and 56.8% of overlap for extracted area and reference area on some parts of the study area. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Building footprint extraction; Deep learning; High resolution optical satellite image; Pix2pix","Buildings; Extraction; Forestry; Image processing; Learning algorithms; Optical resolving power; Remote sensing; Satellites; Adversarial networks; Automatic buildings; Automatic extraction; Building footprint; High-resolution optical satellite images; Optical satellite images; Pix2pix; Very high spatial resolutions; Deep learning"
"A land-cover classification method of high-resolution remote sensing imagery based on convolution neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058274927&doi=10.1117%2f12.2318930&partnerID=40&md5=eb731846f46d09368d5dd12dd367ad53","With the development of space satellites, a large number of high-resolution remote sensing images have been produced, so the analysis and application of high-resolution remote sensing images are very important. Recently deep learning provides a new method to increase the accuracy of land-cover classification. This study aims to propose a classification framework based on convolutional neural network (CNN) to carry out remote sensing scene classification. After remote sensing images are trained by CNN, a model which can extract complex characteristic from the image for classification is created. In this paper, GaoFen-2(GF-2) satellite data is used as data sources and Jilin province of China is selected as the study area. Firstly, the preprocessed images are made into a GF-2 satellite data sets. Secondly, CaffeNet is used to train the data sets through Caffe platform and the classification result is obtained. The CNN overall accuracy is 89.88%, the Kappa coefficient is 0.8026. Compared with the traditional BP neural network classification result, it is obviously find the CNN is more suitable for remote sensing image classification. © 2018 SPIE.","CaffeNet; CNN; Deep learning; GF-2; High-resolution remote sensing image","Classification (of information); Convolution; Deep learning; Image classification; Neural networks; Satellites; Space optics; CaffeNet; Convolution neural network; Convolutional Neural Networks (CNN); GF-2; High resolution remote sensing imagery; High resolution remote sensing images; Land cover classification; Remote sensing image classification; Remote sensing"
"Cloud detection in high-resolution multispectral satellite imagery using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054806879&doi=10.1007%2f978-3-030-01424-7_28&partnerID=40&md5=79bfee12c74c48b24ae450a0255ce9f8","Cloud detection in high-resolution satellite images is a critical step for many remote sensing applications, but also a challenge, as such images have limited spectral bands. The contribution of this paper is twofold: We present a dataset called CloudPeru as well as a methodology for cloud detection in multispectral satellite images (approximately 2.8 meters per pixel) using deep learning. We prove that an agile Convolutional Neural Network (CNN) is able to distinguish between non-clouds and different types of clouds, including thin and very small ones, and achieve a classification accuracy of 99.94%. Each image is subdivided into superpixels by the SLICO algorithm, which are then processed by the trained CNN. Finally, we obtain the cloud mask by applying a threshold of 0.5 on the probability map. The results are compared with manually annotated images, showing a Kappa coefficient of 0.944, which is higher than that of compared methods. © Springer Nature Switzerland AG 2018.","Cloud detection; Convolutional neural networks; Deep learning; High-resolution","Convolution; Neural networks; Pixels; Remote sensing; Satellite imagery; Cloud detection; Convolutional neural network; Convolutional Neural Networks (CNN); High resolution; High resolution satellite images; Multispectral satellite image; Multispectral satellite imagery; Remote sensing applications; Deep learning"
"A comparison of shadow detection methods for high spatial resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052621038&doi=10.1117%2f12.2503093&partnerID=40&md5=dae9c343eb3177d8463b1c4b3ce605d4","Shadow detection is one of major research problems in processing high spatial resolution remote sensing images. Developing effective shadow detection methods is one of the essential topics in remote sensing image processing, particularly for urban regions and mountainous forest. Accurate detection of shadow areas in remote sensing images is vital for subsequent image classification and analysis. In this paper, the current shadow detection algorithms are reviewed and classified into 4 types: geometric model-based methods, physical model-based methods, color spacebased model methods and threshold. The research progress, advantages and disadvantages of these methods are compared, analyzed and discussed. According to the comparison, the potential promising research topics includes:(1) making the shadow detection process more robust and accurate, (2) solving the problem of automatic threshold selection. (3) utilizing machine learning algorithms, especially deep learning methods. © 2018 SPIE.","Image shadow detection;remote sensing images","Deep learning; Image resolution; Learning algorithms; Remote sensing; Automatic threshold selection; Geometric modeling; High spatial resolution; Learning methods; Remote sensing image processing; Remote sensing images; Research problems; Shadow detections; Image processing"
"End-to-end airplane detection using transfer learning in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040836357&doi=10.3390%2frs10010139&partnerID=40&md5=f33f2af0de6cc23a0827d8d1ee25d8ba","Airplane detection in remote sensing images remains a challenging problem due to the complexity of backgrounds. In recent years, with the development of deep learning, object detection has also obtained great breakthroughs. For object detection tasks in natural images, such as the PASCAL (Pattern Analysis, Statistical Modelling and Computational Learning) VOC (Visual Object Classes) Challenge, the major trend of current development is to use a large amount of labeled classification data to pre-train the deep neural network as a base network, and then use a small amount of annotated detection data to fine-tune the network for detection. In this paper, we use object detection technology based on deep learning for airplane detection in remote sensing images. In addition to using some characteristics of remote sensing images, some new data augmentation techniques have been proposed. We also use transfer learning and adopt a single deep convolutional neural network and limited training samples to implement end-to-end trainable airplane detection. Classification and positioning are no longer divided into multistage tasks; end-to-end detection attempts to combine them for optimization, which ensures an optimal solution for the final stage. In our experiment, we use remote sensing images of airports collected from Google Earth. The experimental results show that the proposed algorithm is highly accurate and meaningful for remote sensing object detection. © 2018 by the authors.","Airplane detection; Convolutional neural networks; End to end; Transfer learning","Aircraft; Convolution; Deep learning; Deep neural networks; Neural networks; Object detection; Object recognition; Remote sensing; Airplane detections; Computational learning; Convolutional neural network; Detection technology; End to end; Remote sensing images; Statistical modelling; Transfer learning; Aircraft detection"
"Remote sensing image classification based on Stacked Denoising Autoencoder","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040666806&doi=10.3390%2frs10010016&partnerID=40&md5=f1bbf1083bc97d0c2bf29894901742a9","Focused on the issue that conventional remote sensing image classification methods have run into the bottlenecks in accuracy, a new remote sensing image classification method inspired by deep learning is proposed, which is based on Stacked Denoising Autoencoder. First, the deep network model is built through the stacked layers of Denoising Autoencoder. Then, with noised input, the unsupervised Greedy layer-wise training algorithm is used to train each layer in turn for more robust expressing, characteristics are obtained in supervised learning by Back Propagation (BP) neural network, and the whole network is optimized by error back propagation. Finally, Gaofen-1 satellite (GF-1) remote sensing data are used for evaluation, and the total accuracy and kappa accuracy reach 95.7% and 0.955, respectively, which are higher than that of the Support Vector Machine and Back Propagation neural network. The experiment results show that the proposed method can effectively improve the accuracy of remote sensing image classification.","Back Propagation neural network; Deep learning; Land cover classification; Stacked denoising autoencoder","Backpropagation algorithms; Deep learning; Image classification; Image enhancement; Learning systems; Neural networks; Remote sensing; Torsional stress; Auto encoders; Back propagation neural networks; Error back propagation; Land cover classification; Network modeling; Remote sensing data; Remote sensing image classification; Training algorithms; Backpropagation"
"Residential roof condition assessment system using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044593363&doi=10.1117%2f1.JRS.12.016040&partnerID=40&md5=d67793e4d039b1608ea1560dd66e05ed","The emergence of high resolution (HR) and ultra high resolution (UHR) airborne remote sensing imagery is enabling humans to move beyond traditional land cover analysis applications to the detailed characterization of surface objects. A residential roof condition assessment method using techniques from deep learning is presented. The proposed method operates on individual roofs and divides the task into two stages: (1) roof segmentation, followed by (2) condition classification of the segmented roof regions. As the first step in this process, a self-tuning method is proposed to segment the images into small homogeneous areas. The segmentation is initialized with simple linear iterative clustering followed by deep learned feature extraction and region merging, with the optimal result selected by an unsupervised index, Q. After the segmentation, a pretrained residual network is fine-tuned on the augmented roof segments using a proposed k-pixel extension technique for classification. The effectiveness of the proposed algorithm was demonstrated on both HR and UHR imagery collected by EagleView over different study sites. The proposed algorithm has yielded promising results and has outperformed traditional machine learning methods using hand-crafted features. © 2018 The Authors.","classification; deep learning; roof condition assessment; segmentation","Classification (of information); Housing; Image segmentation; Iterative methods; Machine learning; Remote sensing; Roofs; Turnaround time; Airborne remote sensing; Condition assessment systems; Condition assessments; High resolution; Iterative clustering; Land cover analysis; Machine learning methods; Ultrahigh resolution; Deep learning"
"A Systematic Scheme for Automatic Airplane Detection from High-Resolution Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048582120&doi=10.1007%2f978-3-319-92753-4_36&partnerID=40&md5=3459a3fe837bba71173d931dc11c700d","Airport and airplane are typical objects in remote sensing research field. However, there are rare methods to detect airport and airplane in a unit system. In this paper, we propose a systematic scheme for airport detection and airplane detection from high-resolution remote sensing images. The airport detection part is mainly based on the parallel line features of runway, containing six main stages: down-sampling, Frequency-Tuned (FT) saliency detection, Line Segment Detector (LSD) line detection, line growing, parallel lines detection and line clustering. The airplane detection part is mainly based on Circle Frequency Filter (CF-filter) and a Fast R-CNN deep learning model. Experimental results on 500 high-resolution remote sensing images acquired more than 95% accuracy, and the average detection time was about 14 s, which proved that the proposed system was effective and efficient. © 2018, Springer International Publishing AG, part of Springer Nature.","Airplane detection; Circle Frequency Filter; Deep learning; High-resolution remote sensing images","Aircraft; Airports; Deep learning; Remote sensing; Airplane detections; Airport detections; Frequency filters; High resolution remote sensing images; Learning models; Line detection; Research fields; Saliency detection; Aircraft detection"
"Multi-scale residual convolutional neural network for shadow detection in high resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071885535&partnerID=40&md5=c035e80f7993aec918b3ccf112214472","The presence of shadows degrades the image quality and reduces the interpretation accuracy. With the improvement of the spatial resolution of the remote sensing images, the shadow effect has become more and more obvious. Therefore shadow detection is an essential step for remote sensing image analysis. Convolutional neural networks have recently demonstrated outstanding performance for image processing, but few works have been done for shadow detection in remote sensing images with this technology. In this paper, we propose a novel multi-scale residual convolutional neural network for shadow detection (MSR-SDCN) in high resolution remote sensing images. The core of the proposed network is composed of a multi-scale encoder network and a corresponding decoder network with residual structure. The encoder network is used to extract the image features and downsample the input feature maps gradually, while the decoder network learns to upsample its input feature maps and generates the shadow detection result which has the same size as the input image. The residual structure is used in two aspects, i.e., internal same scale connection and encoder-decoder connection. The training data is from UCF database and data augmentation is used to increase the amount of the training data with random downscale, flipping and rotation. The overall accuracy of the proposed method on the test images is above 90%. The shadow detection results from the proposed method are compared with the three shadow detection algorithms in visual analysis and quantitative assessment. It shows that the proposed deep learning approach can achieve more accurate result than the traditional methods. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Deep learning; Remote sensing images; Residual convolution neural network; Shadow detection","Convolution; Decoding; Deep learning; Deep neural networks; Image enhancement; Image segmentation; Network coding; Neural networks; Convolution neural network; Convolutional neural network; High resolution remote sensing images; Quantitative assessments; Remote sensing images; Residual structure; Shadow detections; Spatial resolution; Remote sensing"
"Object-oriented crops classification for remote sensing images based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059019246&doi=10.1117%2f12.2317448&partnerID=40&md5=9a955f1955f8d58b666612cc069ad31a","Deep learning technology such as convolutional neural networks (CNN) has achieved outstanding results in the field of crops classification for remote sensing images. The way of land cover or crop types remote sensing classification using CNN is mainly pixel-based classification which is often affected by the phenomenon of ""salt and pepper"". In order to reduce this effect, an object-oriented crops classification method based on CNN is proposed in this paper. By combining image segmentation technology and CNN model, we use this method to obtain the results of crops classification from Sentinel-2A multi-spectral remote sensing images in Yuanyang County, Henan Province, China. The experiment show that, compared with the pixel level classification based on CNN which only consider the spectral and temporal characteristics of the crops, the method we proposed comprehensively utilizes more detailed information such as spectral feature, texture feature, spatial relationship, and color space. Thus, it gains a better discriminability for some specific crop and achieves higher classification accuracy. © 2018 SPIE.","CNN; Crops classification; Multi-spectral; Object-oriented; Remote sensing","Classification (of information); Convolution; Crops; Deep learning; Image classification; Image segmentation; Neural networks; Pixels; Space optics; Classification accuracy; Convolutional neural network; Convolutional Neural Networks (CNN); Multi-spectral; Object oriented; Pixel based classifications; Remote sensing classification; Temporal characteristics; Remote sensing"
"Using deep learning in image hyper spectral segmentation, classification, and detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049657027&doi=10.1117%2f12.2307376&partnerID=40&md5=5333dceeee62ee0e9378372cf191f383","Recent years have shown that deep learning neural networks are a valuable tool in the field of computer vision. Deep learning method can be used in applications like remote sensing such as Land cover Classification, Detection of Vehicle in Satellite Images, Hyper spectral Image classification. This paper addresses the use of the deep learning artificial neural network in Satellite image segmentation. Image segmentation plays an important role in image processing. The hue of the remote sensing image often has a large hue difference, which will result in the poor display of the images in the VR environment. Image segmentation is a pre processing technique applied to the original images and splits the image into many parts which have different hue to unify the color. Several computational models based on supervised, unsupervised, parametric, probabilistic region based image segmentation techniques have been proposed. Recently, one of the machine learning technique known as, deep learning with convolution neural network has been widely used for development of efficient and automatic image segmentation models. In this paper, we focus on study of deep neural convolution network and its variants for automatic image segmentation rather than traditional image segmentation strategies. © 2018 SPIE. Downloading of the abstract is permitted for personal use only.","Convolution Neural Network; Deep Neural Network; hyper-spectral data classification; Image segmentation; Restricted Boltzmann Machines (RBMs)","Convolution; Deep neural networks; Image classification; Neural networks; Remote sensing; Spectroscopy; Automatic image segmentation; Convolution neural network; Hyperspectral Data; Land cover classification; Machine learning techniques; Region based image segmentation; Restricted boltzmann machine; Satellite image segmentations; Image segmentation"
"Machine learning methods for remote sensing applications: An overview","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057300926&doi=10.1117%2f12.2503653&partnerID=40&md5=666e2b7102093a6075ab84703283ed7c","Machine learning algorithms have shown a surprisingly successful development within the last years. Several data intensive technical and scientific fields - like search engines, speech recognition, and robotics - have an enormous benefit of these developments. Remote sensing tasks belong to data intensive applications as well. Today, remote sensing provides data over a wide range of the electromagnetic spectrum (UV, VIS, NIR, IR, and Radar). The capabilities of the sensors include single band images as well as multi- and even hyperspectral data. Due to the fact that remote sensing applications are often monitoring tasks, long time series data are in the focus of image exploitation. Several machine learning algorithms have been used in the remote sensing community since decades, ranging from basic algorithms such as PCA and K-Means to more sophisticated classification and regression frameworks like SVMs, decision trees, Random Forests, and artificial neural networks. Through a combination of data availability, algorithmic progress, and specialized hardware, deep learning methods and convolutional networks (ConvNets) came in the focus of the image exploitation community during the last years and are now on the verge between revolutionary success and illusionary hype. This overview aims to explore in which situations these new approaches are useful in remote sensing applications, which problems are actually solved, and which are still open. © 2018 SPIE.","Convolutional networks; Deep learning; Machine learning; Remote sensing; Training data","Binary alloys; Convolution; Decision trees; Deep learning; Learning algorithms; Learning systems; Neural networks; Search engines; Speech recognition; Uranium alloys; Vanadium alloys; Convolutional networks; Data-intensive application; Electromagnetic spectra; Hyperspectral Data; Machine learning methods; Remote sensing applications; Specialized hardware; Training data; Remote sensing"
"Applying deep learning for agricultural classification using multitemporal SAR Sentinel-1 for Camargue, France","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059018566&doi=10.1117%2f12.2325160&partnerID=40&md5=837dff6a5e158d61c1850046ac69c055","The aim of this paper is to provide a better understanding of potentialities of the new Sentinel-1 radar images for mapping the different crops in the Camargue region in the South France. The originality relies on deep learning techniques. The analysis is carried out on multitemporal Sentinel-1 data over an area in Camargue,France.50 Sentinel-1 images processed in order to produce an intensity radar data stack from May 2017 to September 2017. We revealed that even with classical machine learning approaches (K nearest neighbors, random forest, and support vector machine), good performance classification could be achieved with F-measure/Accuracy greater than 86 % and Kappa coefficient better than 0.82. We found that the results of the two deep recurrent neural network (RNN)-based classifiers clearly outperformed the classical approaches. Finally, our analyses of Camargue area results show that the same performance was obtained with two different RNN-based classifiers on the Rice class, which is the most dominant crop of this region, with a F-measure metric of 96 %. These results thus highlight that in the near future, these RNN-based techniques will play an important role in the analysis of remote sensing time series. © 2018 SPIE.","Land cover map; Multi-temporal; Recurrent neural network; SAR; Sentinel-1","Crops; Decision trees; Image processing; Nearest neighbor search; Radar imaging; Recurrent neural networks; Remote sensing; Synthetic aperture radar; Time series analysis; Classical approach; K-nearest neighbors; Land cover; Learning techniques; Machine learning approaches; Multi-temporal; Recurrent neural network (RNN); Sentinel-1; Deep learning"
"Fusion of shallow and deep features for classification of high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043754840&doi=10.1117%2f12.2284777&partnerID=40&md5=56546458f80c7d34fcae56d28e711b3d","Effective spectral and spatial pixel description plays a significant role for the classification of high resolution remote sensing images. Current approaches of pixel-based feature extraction are of two main kinds: one includes the widelyused principal component analysis (PCA) and gray level co-occurrence matrix (GLCM) as the representative of the shallow spectral and shape features, and the other refers to the deep learning-based methods which employ deep neural networks and have made great promotion on classification accuracy. However, the former traditional features are insufficient to depict complex distribution of high resolution images, while the deep features demand plenty of samples to train the network otherwise over fitting easily occurs if only limited samples are involved in the training. In view of the above, we propose a GLCM-based convolution neural network (CNN) approach to extract features and implement classification for high resolution remote sensing images. The employment of GLCM is able to represent the original images and eliminate redundant information and undesired noises. Meanwhile, taking shallow features as the input of deep network will contribute to a better guidance and interpretability. In consideration of the amount of samples, some strategies such as L2 regularization and dropout methods are used to prevent over-fitting. The fine-tuning strategy is also used in our study to reduce training time and further enhance the generalization performance of the network. Experiments with popular data sets such as PaviaU data validate that our proposed method leads to a performance improvement compared to individual involved approaches. © 2018 SPIE.","convolutional neural network; gray-level co-occurrence matrix; high-resolution remote sensing imagery; image classification","Classification (of information); Convolution; Deep learning; Deep neural networks; Image processing; Matrix algebra; Neural networks; Pattern recognition; Pixels; Principal component analysis; Remote sensing; Convolution neural network; Convolutional neural network; Generalization performance; Gray level co occurrence matrix(GLCM); Gray level co-occurrence matrix; High resolution remote sensing imagery; High resolution remote sensing images; Performance improvements; Image classification"
"Deep learning performance for digital terrain model generation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059025776&doi=10.1117%2f12.2325768&partnerID=40&md5=0594afa4632c6f6f9ae10b3521f4ebdc","Traditional methods of photogrammetric image processing allow reconstructing a digital terrain model with high accuracy needed for producing different geographic information system (GIS) products such as maps, orthophoto etc. The accuracy of correspondence problem solution for good imaging conditions can reach the level of hundredth of a pixel. Recently deep learning techniques have been applied for dense stereo matching of images. They allow receiving depth information directly from stereo images and show impressive results for such tasks as scene segmentation, object detection and classification. The aim of the study was to evaluate the performance of deep learning techniques for accurate digital terrain models generation. Some of the state-of-the-art deep learning stereo matching models were evaluated along with original convolutional network. To get reliable accuracy estimation of the considered techniques the results of 3D reconstruction were compared with the reference surface obtained by 3D scanning. Also the imagery acquired from unmanned aerial vehicle during the mission on digitizing an archaeological site was used for performance evaluation by comparing with digital terrain model generated by photogrammetric technique. © 2018 SPIE.","3D reconstruction; Accuracy; Deep learning; Digital terrain model; Performance evaluation; Stereo matching","Antennas; Classification (of information); Deep learning; E-learning; Image reconstruction; Image segmentation; Landforms; Learning algorithms; Object detection; Photogrammetry; Remote sensing; Three dimensional computer graphics; 3D reconstruction; Accuracy; Digital terrain model; Performance evaluations; Stereo matching; Stereo image processing"
"Training deep learning based denoisers without ground truth data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064833402&partnerID=40&md5=0fa7dc95c5f631695028972aed692e22","Recently developed deep-learning-based denoisers often outperform state-of-the-art conventional denoisers, such as the BM3D. They are typically trained to minimize the mean squared error (MSE) between the output image of a deep neural network and a ground truth image. In deep learning based denoisers, it is important to use high quality noiseless ground truth data for high performance, but it is often challenging or even infeasible to obtain noiseless images in application areas such as hyperspectral remote sensing and medical imaging. In this article, we propose a method based on Stein's unbiased risk estimator (SURE) for training deep neural network denoisers only based on the use of noisy images. We demonstrate that our SURE-based method, without the use of ground truth data, is able to train deep neural network denoisers to yield performances close to those networks trained with ground truth, and to outperform the state-of-the-art denoiser BM3D. Further improvements were achieved when noisy test images were used for training of denoiser networks using our proposed SURE-based method. Code is available at https://github.com/Shakarim94/Net-SURE. © 2018 Curran Associates Inc.All rights reserved.",,"Hyperspectral imaging; Image enhancement; Mean square error; Medical imaging; Remote sensing; Application area; Ground truth data; High quality; Hyperspectral remote sensing; Mean squared error; Noiseless images; State of the art; Stein's unbiased risk estimators (SURE); Deep neural networks"
"Diversity-promoting deep structural metric learning for remote sensing scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033725344&doi=10.1109%2fTGRS.2017.2748120&partnerID=40&md5=c884f785baf0c9974b10673ce6b5d5ef","Deep models with multiple layers have demonstrated their potential in learning abstract and invariant features for better representation and classification of remote sensing images. Moreover, metric learning (ML) is usually introduced into the deep models to further increase the discrimination of deep representations. However, the usual deep ML methods treat the training samples in each training batch in the stochastic gradient descent-based learning procedure independently, and thus, they neglect the important contextual (structural) information in the training samples. In this paper, we first introduce deep structural ML (DSML) into the literature of remote sensing scene classification and specifically capture and use the structural information during the training on the remote sensing images. Further analysis demonstrates that DSML usually makes many learned metric parameters similar. This similarity leads to obvious model redundancy and thus decreases the representational ability of the model. To address this problem, this paper proposes a new diversity-promoting DSML (D-DSML) method by regularizing the learning procedure by a diversity-promoting prior over the parameter factors. The proposed D-DSML encourages the parameter factors to be uncorrelated, such that each factor can model unique information, and thus, the model’s description ability and classification performance would be significantly improved. Experiments over six real-world remote sensing scene data sets demonstrate that the proposed method obtains much better results than those obtained by the original deep models and has comparable or even better performances when compared with state-of-the-art methods. © 2017 IEEE.","Convolutional neural network (CNN); Deep structural metric learning (DSML); Diversity; Scene classification","Classification (of information); Neural networks; Remote sensing; Sampling; Stochastic systems; Classification of remote sensing image; Classification performance; Convolutional Neural Networks (CNN); diversity; Metric learning; Scene classification; State-of-the-art methods; Stochastic gradient descent; Deep learning"
"Automatic cloud detection based on deep learning from AVHRR data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052156431&doi=10.1007%2f978-981-13-2122-1_10&partnerID=40&md5=3d080a8f1614a10ce08b494b07501c42","Thanks to the development of satellite remote sensing technology, more observing data are acquired and can be used for various purposes. However, statistical data show that half of the earth’s surface is covered by clouds, which may seriously influence the usability of remote sensing data. Most existing cloud detection methods are manual or semi-automatic methods with low efficiency. This paper focuses on automatic cloud detection over sea surface from Advanced Very High Resolution Radiometer (AVHRR) data. A novel cloud detection framework named DBN-Otsu Hybrid Model (DOHM) has been proposed, which combines Deep Belief Networks (DBN) and Otsu’s method for the first time. DOHM adopts adaptive thresholds to replace manual interventions, implementing full automation. Experimental results show that DOHM achieves the highest average accuracy ratio among the six detection methods. Moreover, DOHM makes a good balance between False Alarm Rate (FAR) and Miss Rate (MR). © Springer Nature Singapore Pte Ltd. 2018.","Advanced Very High Resolution Radiometer (AVHRR); Cloud detection; Deep Belief Network; Deep learning","Advanced very high resolution radiometers (AVHRR); Artificial intelligence; Bayesian networks; Remote sensing; Surface waters; Automatic cloud detection; Cloud detection; Cloud detection method; Deep belief network (DBN); Deep belief networks; Manual intervention; Satellite remote sensing; Semiautomatic methods; Deep learning"
"Feature extraction based on extended multi-attribute profiles and sparse autoencoder for remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043785381&doi=10.1117%2f12.2288350&partnerID=40&md5=c4f898ee369d80cce3c8d4d6984b3a50","The satellite images with very high spatial resolution have been recently widely used in image classification topic as it has become challenging task in remote sensing field. Due to a number of limitations such as the redundancy of features and the high dimensionality of the data, different classification methods have been proposed for remote sensing images classification particularly the methods using feature extraction techniques. This paper propose a simple efficient method exploiting the capability of extended multi-attribute profiles (EMAP) with sparse autoencoder (SAE) for remote sensing image classification. The proposed method is used to classify various remote sensing datasets including hyperspectral and multispectral images by extracting spatial and spectral features based on the combination of EMAP and SAE by linking them to kernel support vector machine (SVM) for classification. Experiments on new hyperspectral image ""Huston data"" and multispectral image ""Washington DC data"" shows that this new scheme can achieve better performance of feature learning than the primitive features, traditional classifiers and ordinary autoencoder and has huge potential to achieve higher accuracy for classification in short running time. © 2018 SPIE.","deep learning (DL); dimensionality reduction; extend multi-attribute profiles (EMAP); Hyperspectral image; image classification; multispectral image; sparse autoencoder (SAE); spatial feature extraction; support vector machine (SVM)","Deep learning; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Image processing; Learning systems; Pattern recognition; Remote sensing; Satellite imagery; Spectroscopy; Support vector machines; Auto encoders; Dimensionality reduction; Feature extraction techniques; Multi-attributes; Multispectral images; Remote sensing image classification; Remote sensing images classification; Very high spatial resolutions; Classification (of information)"
"A novel technique based on deep learning and a synthetic target database for classification of urban areas in PolSAR data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040906277&doi=10.1109%2fJSTARS.2017.2752282&partnerID=40&md5=c7b4b3620a0ffc1416a6641aeb03e7c3","The classification of urban areas in polarimetric synthetic aperture radar (PolSAR) data is a challenging task. Moreover, urban structures oriented away from the radar line of sight pose an additional complexity in the classification process. The characterization of such areas is important for disaster relief and urban sprawl monitoring applications. In this paper, a novel technique based on deep learning is proposed, which leverages a synthetic target database for data augmentation. The PolSAR dataset is rotated by uniform steps and collated to form a reference database. A stacked autoencoder network is used to transform the information in the augmented dataset into a compact representation. This significantly improves the generalization capabilities of the network. Finally, the classification is performed by a multilayer perceptron network. The modular architecture allows for easy optimization of the hyperparameters. The synthetic target database is created and the classification performance is evaluated on an L-band airborne UAVSAR dataset and L-band space-borne ALOS-2 dataset acquired over San Francisco, USA. The proposed technique shows an overall accuracy of 91.3{\%}. An improvement over state-of-the-art techniques is achieved, especially in urban areas rotated away from the radar line of sight. © 2017 IEEE.","Autoencoder (AE); Classification; Deep learning; Deep neural networks; Polarimetric synthetic aperture radar (PolSAR); Representation learning; Urban remote sensing","Database systems; Deep learning; Deep neural networks; Disaster prevention; Feature extraction; Learning systems; Neural networks; Polarimeters; Radar; Remote sensing; Scattering; Synthetic aperture radar; Auto encoders; Polarimetric synthetic aperture radars; representation learning; Urban areas; Urban remote sensing; Classification (of information); artificial neural network; data processing; database; image classification; machine learning; remote sensing; synthetic aperture radar; urban area"
"High Performance Geological Disaster Recognition using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062030356&doi=10.1016%2fj.procs.2018.10.237&partnerID=40&md5=5146e363e497ad5e68c57d6e23b56485","Geological disaster recognition on optical image is one of the key techniques in disaster control and disaster relief. Comparing with optical images, remote sensing images contain much higher resolution and more visualized contents. In this paper, we propose a landslide recognition framework which trains a deep auto-encoder network on the compressed domain. ANN or SVM is used as the classifier for decision making. In addition, in order to meet the requirement of some real-time applications, a high performance training network on CUDA-enabled GPUs is designed and implemented. Experiments are conducted on optical images from Google Earth. © 2018 The Authors. Published by Elsevier B.V.","CUDA; deep learning; geological disaster recognition; parallel computing","Decision making; Disaster prevention; Disasters; Geology; Geometrical optics; Parallel processing systems; Program processors; Remote sensing; Compressed domain; CUDA; Disaster relief; Geological disaster; Higher resolution; Real-time application; Remote sensing images; Training network; Deep learning"
"100-years of land use change analysis with old topographic map using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071883326&partnerID=40&md5=29bb4512d3310d6242b771d34a3faafb","In Japan, although databases capable of quantitative assessment of land use have been developed since the latter half of the 1970s, databases that are prior to that time have not been developed. In this research, land use classification data were created from the old topographical map dating back to past 100 years by using pix2pix. Their accuracy was also evaluated by reconverting the color data of them and then it was shown that, by using pix2pix, it was able to be classified with the accuracy of 54.6%. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Map tile; Pix2pix; Pixel data","Land use; Maps; Remote sensing; Land use change analysis; Landuse classifications; Pix2pix; Quantitative assessments; Topographic map; Deep learning"
"Spatial interpolation of surface ozone observations using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057347199&doi=10.1117%2f12.2320755&partnerID=40&md5=757fd06fd4b4bbda94357232eea181ec","Surface ozone can trigger many health problems for human (e.g. coughing, bronchitis, emphysema, and asthma), especially for children and the elderly. It also has harmful effects on plants (e.g. chlorosis, necrosis, and yield reduction). The United State (U.S.) Environmental Protection Agency (EPA) has been monitoring surface ozone concentrations across the U.S. since 1980s. However, their stations are sparsely distributed and mainly in urban areas. Evaluation of surface ozone effects at any given locations in the U.S. requires spatial interpolation of ozone observations. In this study, we implemented two traditional spatial interpolation methods (i.e. triangulation-based linear interpolation and geostatistics-based method). One limitation of these two methods is their reliance on single-scene observations in constructing the spatial relationship, which is prone to influence of noisy observations and has large uncertainty. Deep learning, on the other hand, is capable of simulating common patterns (including complex spatial patterns) from a large amount of training samples. Therefore, we also implemented three deep learning algorithms for the spatial interpolation problem: mixture model network (MoNet), Convolutional Neural Network for Graphs (ChebNet), and Recurrent Neural Network (RNN). The training and validation data of this study are the 2016 EPA hourly surface ozone observations within ±3-degree box centered at the Billings, Oklahoma station (USDA UV-B Monitoring and Research Program). The results showed that among the five methods, RNN and MoNet outperformed the two traditional spatial interpolation methods and RNN has the lowest validation error (mean absolute error: 2.82 ppb; standard deviation: 2.76 ppb). Finally, we used the integrated gradients method to analyze the attribution of RNN inputs on the surface ozone prediction. The results showed that surface ozone observation is the most important input feature followed by distance and absolute locations (i.e. elevations, longitudes, and latitudes). © 2018 SPIE.","Convolutional neural network for graphs (ChebNet); Long short-term memory (LSTM); Mixture model network (MoNet); Recurrent neural network (RNN); Spatial interpolation; Spatial process model; Surface ozone","Chlorophyll; Convolution; Deep learning; Ecosystems; Environmental Protection Agency; Interpolation; Learning algorithms; Mixtures; Ozone; Remote sensing; Sustainable development; Convolutional neural network; Mixture model; Recurrent neural network (RNN); Spatial interpolation; Spatial process model; Surface ozone; Long short-term memory"
"Humanitarian applications of machine learning with remote-sensing data: Review and case study in refugee settlement mapping","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060199940&doi=10.1098%2frsta.2017.0363&partnerID=40&md5=93233f886d9ad1fbb5725d598cccca9e","The coordination of humanitarian relief, e.g. in a natural disaster or a conflict situation, is often complicated by a scarcity of data to inform planning. Remote sensing imagery, from satellites or drones, can give important insights into conditions on the ground, including in areas which are difficult to access. Applications include situation awareness after natural disasters, structural damage assessment in conflict, monitoring human rights violations or population estimation in settlements. We review machine learning approaches for automating these problems, and discuss their potential and limitations. We also provide a case study of experiments using deep learning methods to count the numbers of structures in multiple refugee settlements in Africa and the Middle East. We find that while high levels of accuracy are possible, there is considerable variation in the characteristics of imagery collected from different sensors and regions. In this, as in the other applications discussed in the paper, critical inferences must be made from a relatively small amount of pixel data. We, therefore, consider that using machine learning systems as an augmentation of human analysts is a reasonable strategy to transition from current fully manual operational pipelines to ones which are both more efficient and have the necessary levels of quality control. This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'. © 2018 The Author(s) Published by the Royal Society. All rights reserved.","Humanitarian aid; Object detection; Remote sensing; Satellite imaging","Artificial intelligence; Damage detection; Deep learning; Disasters; Object detection; Structural analysis; Humanitarian aid; Humanitarian relief; Machine learning approaches; Operational pipeline; Population estimations; Remote sensing imagery; Satellite imaging; Structural damage assessments; Remote sensing"
"Optical remote sensing image retrieval based on convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044635889&doi=10.3788%2fOPE.20182601.0200&partnerID=40&md5=917b57a8908f8c532bdd044ba6e23639","A method for remote sensing image retrieval based on convolutional neural networks was proposed. First, the convolution and pooling of remote sensing images were conducted by multi-layer convolutional neural networks. The feature maps of each image were obtained, and the high-level features were extracted to build the image feature database. In this process, the training of networks' parameters and the Softmax classifier were completed using feature maps. Then, in the image retrieval stage, classification was introduced by the softmax classifier which will improve the accuracy of image retrieval. Lastly, the remote sensing image retrieval was sorted based on the similarity between the query image and database. Retrieval experiments were performed on the high-resolution optical remote sensing images. The average retrieval precision on five kinds including water, plant, building, farmland and land is 98.4%, and the retrieval precision on seven types (adding plane and ship) is 95.9%. The introduction of class information improves the retrieval precision and speed, saving time by 17.6% approximately. The proposed method behaves better than the methods that based on color feature, texture feature and the bag of words model, and the results show that the high-level feature from deep convolutional neural networks can represent image content effectively. Experimeat indicates that retrieval speed and accuracy of optical remote-sensing images can be effectively increased in this method. © 2018, Science Press. All right reserved.","Convolutional neural networks; Deep learning; Image classification; Remote sensing image retrieval; Softmax classifier","Convolution; Deep learning; Deep neural networks; Image classification; Image enhancement; Network layers; Neural networks; Query processing; Remote sensing; Bag-of-words models; Convolutional neural network; Deep convolutional neural networks; High-level features; Optical remote sensing; Remote sensing image retrieval; Remote sensing images; Softmax classifiers; Image retrieval"
"Research on target detection of SAR images based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059038391&doi=10.1117%2f12.2500089&partnerID=40&md5=83981dcec4320897d4e484a3352f197d","In this paper the target detection based on deep convolution neural network (DCNN) and transfer learning has been developed for synthetic aperture radar (SAR) images inspired by recent successful deep learning methods. DCNN has excellent performance in optical images, while its application for SAR images is restricted by the limited quantity of SAR imagery training data. Transfer learning has been introduced into the target detection of a small quantity of SAR images. Firstly, by some contrast experiments to transfer convolution weights layer by layer and analyze its impact, the combination of fine-tuned and frozen weights is used to improve the generalization and stability of the network. Then, the network model is improved according to the target detection task, it increases the network detection speed and reduces the network parameters. Finally, combining with the complicated scene clutter slices to train the network, the false alarm targets number of background clutter is reduced. The detection results of complex multi-target scenes show that the proposed method has good generality while ensuring good detection performance. © 2018 SPIE.","Deep convolution neural network; Deep learning; SAR; Target detection; Transfer learning","Clutter (information theory); Convolution; Deep learning; Deep neural networks; Geometrical optics; Image processing; Remote sensing; Synthetic aperture radar; Target tracking; Background clutter; Contrast experiment; Convolution neural network; Detection performance; Network detections; Network parameters; Synthetic aperture radar (SAR) images; Transfer learning; Radar imaging"
"Surface water mapping of remote sensing data using pre-trained fully convolutional network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066602573&doi=10.7848%2fksgpc.2018.36.5.423&partnerID=40&md5=771b053f52cc9843c96b3aad0755c114","Surface water mapping has been widely used in various remote sensing applications. Water indices have been commonly used to distinguish water bodies from land; however, determining the optimal threshold and discriminating water bodies from similar objects such as shadows and snow is difficult. Deep learning algorithms have greatly advanced image segmentation and classification. In particular, FCN (Fully Convolutional Network) is state-of-the-art in per-pixel image segmentation and are used in most benchmarks such as PASCAL VOC2012 and Microsoft COCO (Common Objects in Context). However, these data sets are designed for daily scenarios and a few studies have conducted on applications of FCN using large scale remotely sensed data set. This paper aims to fine-tune the pre-trained FCN network using the CRMS (Coastwide Reference Monitoring System) data set for surface water mapping. The CRMS provides color infrared aerial photos and ground truth maps for the monitoring and restoration of wetlands in Louisiana, USA. To effectively learn the characteristics of surface water, we used pre-trained the DeepWaterMap network, which classifies water, land, snow, ice, clouds, and shadows using Landsat satellite images. Furthermore, the DeepWaterMap network was fine-tuned for the CRMS data set using two classes: water and land. The fine-tuned network finally classifies surface water without any additional learning process. The experimental results show that the proposed method enables high-quality surface mapping from CRMS data set and show the suitability of pre-trained FCN networks using remote sensing data for surface water mapping. © 2018 Korean Society of Surveying. All rights reserved.","Coastwide Reference Monitoring System; Deep Learning; DeepWaterMap; Fully Convolutional Networks; Surface Water Mapping","data set; image classification; mapping method; pixel; remote sensing; satellite data; surface water"
"UFCN: A fully convolutional neural network for road extraction in RGB imagery acquired by remote sensing from an unmanned aerial vehicle","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042209374&doi=10.1117%2f1.JRS.12.016020&partnerID=40&md5=d99d88b9fc3dd4bbaf87dbbdd0c0b9bd","Road extraction in imagery acquired by low altitude remote sensing (LARS) carried out using an unmanned aerial vehicle (UAV) is presented. LARS is carried out using a fixed wing UAV with a high spatial resolution vision spectrum (RGB) camera as the payload. Deep learning techniques, particularly fully convolutional network (FCN), are adopted to extract roads by dense semantic segmentation. The proposed model, UFCN (U-shaped FCN) is an FCN architecture, which is comprised of a stack of convolutions followed by corresponding stack of mirrored deconvolutions with the usage of skip connections in between for preserving the local information. The limited dataset (76 images and their ground truths) is subjected to realtime data augmentation during training phase to increase the size effectively. Classification performance is evaluated using precision, recall, accuracy, F1 score, and brier score parameters. The performance is compared with support vector machine (SVM) classifier, a one-dimensional convolutional neural network (1D-CNN) model, and a standard two-dimensional CNN (2D-CNN). The UFCN model outperforms the SVM, 1D-CNN, and 2D-CNN models across all the performance parameters. Further, the prediction time of the proposed UFCN model is comparable with SVM, 1D-CNN, and 2D-CNN models. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","computer vision; deep learning; fully convolutional neural networks; image segmentation; remote sensing; road extraction","Antennas; Color image processing; Computer vision; Convolution; Deep learning; Deep neural networks; Extraction; Feature extraction; Fixed wings; Image segmentation; Neural networks; One dimensional; Roads and streets; Semantics; Support vector machines; Unmanned aerial vehicles (UAV); Classification performance; Convolutional networks; Convolutional neural network; High spatial resolution; Learning techniques; Performance parameters; Road extraction; Semantic segmentation; Remote sensing"
"Classification of high resolution hyperspectral remote sensing data using deep neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048362142&doi=10.3233%2fJIFS-171307&partnerID=40&md5=27f23b2523203bde0eb87a60ff596561","The high resolution hyperspectral remote sensing data collected from urban and landscape areas have been extensively studied over the past decades. Recent applications pose an emerging need of analyzing the land cover types based on high resolution hyperspectral remote sensing data originating from remote sensory devices. Toward this goal, we propose a deep neural network (DNN) classifier in this paper. The DNN is constructed by combining a stacked autoencoder with desired numbers of autoencoders and a softmax classifier. Our experimental results based on the hyperspectral remote sensing data demonstrate that the presented DNN classifier can accurately distinguish different land covers including the mixed deciduous broadleaf natural forest and different land covers such as agriculture, roads, buildings, etc. We test the proposed method by using three different benchmark data sets. The proposed method showcases the huge potential of deep neural networks for hyperspectral data analysis. © 2018 - IOS Press and the authors. All rights reserved.","deep learning; deep neural network; Hyperspectral remote sensing; softmax classifier; stacked autoencoder","Classification (of information); Deep learning; Remote sensing; High resolution; Hyperspectral data analysis; Hyperspectral remote sensing; Hyperspectral remote sensing data; Land-cover types; Natural forests; Softmax classifiers; Stacked autoencoder; Deep neural networks"
"Remote sensing image analysis by aggregation of segmentation-classification collaborative agents","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028728912&doi=10.1016%2fj.patcog.2017.08.030&partnerID=40&md5=799e4d7745506e95b478f4077a8e0bf4","In this article we present two different approaches for automatic remote sensing image interpretation which are based on a multi-paradigm collaborative framework which uses classification in order to guide the segmentation process. The first approach applies sequentially many one-vs-all class extractors in a manner inspired by cascading techniques in machine learning. The second approach applies many collaborating one-vs-all class extractors in parallel. We show that the collaboration of the segmentation and classification paradigms result in a remarkable reduction of segmentation errors but also in better object classification in comparison to a hybrid pixel-object approach as well as a deep learning approach. © 2017 Elsevier Ltd","Classification; Collaborative approaches; Remote sensing; Segmentation","Classification (of information); Image analysis; Image classification; Image reconstruction; Image segmentation; Learning systems; Cascading technique; Collaborative agents; Collaborative approach; Collaborative framework; Object classification; Remote sensing image interpretations; Remote sensing images; Segmentation process; Remote sensing"
"Two-stage object detection based on deep pruning for remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052228675&doi=10.1007%2f978-3-319-99365-2_12&partnerID=40&md5=8e3a5566bee9ec6d547940963675513e","In this paper, we concentrate on tackling the problems of object detection in very-high-resolution (VHR) remote sensing images. The main challenges of object detection in VHR remote sensing images are: (1) VHR images are usually too large and it will consume too much time when locating objects; (2) high false alarm because background dominate and is complex in VHR images. To address the above challenges, a new method is proposed to build two-stage object detection model. Our proposed method can be divided into two processes: (1) we use twice pruning to get region proposal convolutional neural network which is used to predict region proposals; (2) and we use once pruning to get classification convolutional neural network which is used to analyze the result of the first stage and output the class labels of proposals. The experimental results show that the proposed method has high precision and is significantly faster than the state-of-the-art methods on NWPU VHR-10 remote sensing dataset. © 2018, Springer Nature Switzerland AG.","Computer vision; Convolutional neural network; Deep learning; Object detection; Very-high-resolution remote sensing image","Computer vision; Convolution; Deep learning; Neural networks; Object detection; Object recognition; Class labels; Convolutional neural network; False alarms; High-precision; Region proposals; Remote sensing images; State-of-the-art methods; Very high resolution; Remote sensing"
"Image translation between high-resolution remote sensing optical and SAR data using conditional GAN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054552945&doi=10.1007%2f978-3-030-00764-5_23&partnerID=40&md5=d7649203f8a3f0edf092fe532d50b3f1","This paper presents a study on a new problem: applying machine learning approaches to translate remote sensing images between high-resolution optical and Synthetic Aperture Radar (SAR) data. To this end, conditional Generative Adversarial Networks (GAN) have been explored. Efficiency of the conditional GAN have been verified with different SAR parameters on three regions from the world: Toronto, Vancouver in Canada and Shanghai in China. The generated SAR images have been evaluated by pixel-based image classification with detailed land cover types including: low and high density residential area, industry area, construction site, golf course, water, forest, pasture and crops. In comparison with an unsupervised GAN translation approach, the proposed conditional GAN could effectively keep many land cover types with compatible classification accuracy to the ground truth SAR data. This is one of first study on multi-source remote sensing data translation by machine learning. © Springer Nature Switzerland AG 2018.","Deep learning; Generative Adversarial Network; Remote sensing","Artificial intelligence; Deep learning; Radar imaging; Synthetic aperture radar; Adversarial networks; Classification accuracy; Construction sites; High resolution remote sensing; High-density residential areas; Machine learning approaches; Remote sensing data; Remote sensing images; Remote sensing"
"CNN with coefficient of variation-based dimensionality reduction for hyperspectral remote sensing images classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042194942&doi=10.11834%2fjrs.20187075&partnerID=40&md5=afdac8035ee937eeef85af07d9cb46d9","Hyperspectral remote Sensing Images (HSIs), which contain rich spectral and spatial information, are important in the precise classification of earth objects. However, HSIs are usually highly dimensional and non-linear, and they contain large amounts of data. These characteristics increase the difficulty of data processing and bring about the Hughes phenomenon. Conventional methods such as Neural Networks (NN) and support vector machine have solved these problems by reducing the dimensions of HSIs with PCA, ICA, or MNF. Although these methods are effective in the classification of HSIs, they may cause information loss in the original data. Therefore, improving the accuracy of HSI classification with inadequate data is difficult. Recently, deep learning method, especially Convolutional Neural Network (CNN), has achieved remarkable performance in many fields. Therefore, the application of CNNs to HSI classification shows immense potential. To avoid the Hughes phenomenon and improve the accuracy of his classification, this study proposes a Coefficient of Variation-Convolution Neural Network (CV-CNN) method for the classification of HSIs. After the calculation of the Coefficient of Variation of the IntrAclass (CVIA) and the Coefficient of Variation of the IntEr-class (CVIE) of each band, the bands with low (CVIE)2/CVIA values are excluded. Then, a target pixel with the spectral information of its eight neighbors is organized as multi-layer spectral-spatial information. The spectral-spatial information of the target pixel should then be converted into matrix form. The two-dimensional image suitable for the input of CNN was subsequently obtained. Furthermore, a seven-layer CNN model was constructed with two convolution layers, two max-pooling layers, two full-connection layers, and one softmax layer. Using the seven-layer CNN can effectively improve the accuracy of HSI classification. Experiments were conducted on the Indian Pines dataset and Pavia University dataset to evaluate the performance of the presented CVCNN method. The results are as follows: (1) Compared with the method that only considers spectral information for HSI classification, the use of spectral-spatial information can actually improve the accuracy of HSI classification. (2) Compared with other CNN methods, the seven-layer CNN model with no band removed can increase the overall accuracy by 2.61% for the Indian Pines dataset and 0.04% for the Pavia University dataset. (3) Based on the same seven-layer model, the experiments show that the classification of HSI with poor bands excluded shows increased accuracy from 97.9% to 98.69% for the Indian Pines dataset and from 99.6% to 99.66% for the Pavia University dataset. This outcome verifies the validity of excluding poor bands on the basis of the CV method. (4) Based on the same seven-layer model, the experiments show that the CV method, compared with the PCA and MNF methods, can increase the overall accuracy by 1.38% and 0.44%, respectively, for the Indian Pines dataset and by 3.26% and 1.83%, respectively, for the Pavia University dataset. The CNN model developed in this study and the method of removing poor bands with the CV technique can improve the accuracy of HSI classification. © 2018, Science Press. All right reserved.","Classification; Coefficient of variation; Convolutional Neural Networks(CNN); Deep learning; Hyperspectral remote sensing images","Computerized tomography; Convolution; Data handling; Deep learning; Image classification; Neural networks; Pixels; Remote sensing; Coefficient of variation; Conventional methods; Convolution neural network; Convolutional Neural Networks (CNN); Dimensionality reduction; Hyperspectral Remote Sensing Image; Large amounts of data; Two dimensional images; Classification (of information)"
"Remote Sensing Image Fusion Based on Two-Stream Fusion Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042122813&doi=10.1007%2f978-3-319-73603-7_35&partnerID=40&md5=4b8e074451051447d2fb76a6d3bd6c53","Remote sensing image fusion (or pan-sharpening) aims at generating high resolution multi-spectral (MS) image from inputs of a high spatial resolution single band panchromatic (PAN) image and a low spatial resolution multi-spectral image. In this paper, a deep convolutional neural network with two-stream inputs respectively for PAN and MS images is proposed for remote sensing image pan-sharpening. Firstly the network extracts features from PAN and MS images, then it fuses them to form compact feature maps that can represent both spatial and spectral information of PAN and MS images, simultaneously. Finally, the desired high spatial resolution MS image is recovered from the fused features using an encoding-decoding scheme. Experiments on Quickbird satellite images demonstrate that the proposed method can fuse the PAN and MS image effectively. © 2018, Springer International Publishing AG.","Convolutional neural networks; Deep learning; Image fusion; Pan-sharpening; Remote sensing","Convolution; Deep learning; Deep neural networks; Image resolution; Neural networks; Remote sensing; Satellite imagery; Spectroscopy; Convolutional neural network; High spatial resolution; Multispectral images; Pan-sharpening; Panchromatic (Pan) image; QuickBird satellite; Remote sensing images; Spectral information; Image fusion"
"Dual-convolutional enhanced residual network for single super-resolution of remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059004137&doi=10.1007%2f978-3-030-04224-0_31&partnerID=40&md5=d3deda920aa3e355773b73b32be3f3f6","The image super-resolution aims to recover a high-resolution image using a single or sequential low-resolution images. The super resolution methods based on deep learning, especially the deep convolutional neural network, have achieved good results. In this paper,we propose Dual-Convolutional Enhanced Residual Network (DCER) for remote sensing images based on residual learning, which concatenates the feature maps of different convolutional kernel sizes (3 × 3, 5 × 5). On the one hand, it can learn more high-frequency detail information by combining the local details of different scales; on the other hand, it reduces network parameters and greatly shorten the training time. The experimental results show that DCER achieves favorable performance of accuracy and visual performance against the state-of-the-art methods with the scale factor 2x, 4x and 8x. © Springer Nature Switzerland AG 2018.","Dual-Convolutional Enhanced Residual Network (DCER); Remote sensing images; Single super-resolution","Convolution; Deep neural networks; Neural networks; Optical resolving power; Remote sensing; Deep convolutional neural networks; Dual-Convolutional Enhanced Residual Network (DCER); High resolution image; Image super resolutions; Remote sensing images; State-of-the-art methods; Super resolution; Superresolution methods; Image enhancement"
"A review of fine-scale land use and land cover classification in open-pit mining areas by remote sensing techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040683351&doi=10.3390%2frs10010015&partnerID=40&md5=27f3b8fd00c8fb7e7b404a1a78790299","Over recent decades, fine-scale land use and land cover classification in open-pit mine areas (LCCMA) has become very important for understanding the influence of mining activities on the regional geo-environment, and for environmental impact assessment procedure. This research reviews advances in fine-scale LCCMA from the following aspects. Firstly, it analyzes and proposes classification thematic resolution for LCCMA. Secondly, remote sensing data sources, features, feature selection methods, and classification algorithms for LCCMA are summarized. Thirdly, three major factors that affect LCCMA are discussed: significant three-dimensional terrain features, strong LCCMA feature variability, and homogeneity of spectral-spatial features. Correspondingly, three key scientific issues that limit the accuracy of LCCMA are presented. Finally, several future research directions are discussed: (1) unitization of new sensors, particularly those with stereo survey ability; (2) procurement of sensitive features by new sensors and combinations of sensitive features using novel feature selection methods; (3) development of robust and self-adjusted classification algorithms, such as ensemble learning and deep learning for LCCMA; and (4) application of fine-scale mining information for regularity and management of mines.","Fine-scale; Land cover classification; Open-pit mining area; Remote sensing; Review","Classification (of information); Data mining; Environmental impact; Environmental impact assessments; Feature extraction; Land use; Remote sensing; Research and development management; Reviews; Feature selection methods; Fine-scale; Future research directions; Land cover classification; Land-use and land cover classifications; Remote sensing data source; Remote sensing techniques; Three dimensional terrain; Open pit mining"
"Automatic semantic segmentation for change detection in remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047987201&doi=10.1007%2f978-981-10-8569-7_34&partnerID=40&md5=ff2ada6ec0f8b6efa12346a0132bd922","Change detection (CD) mainly focuses on the extraction of change information from multispectral remote sensing images of the same geographical location for environmental monitoring, natural disaster evaluation, urban studies, and deforestation monitoring. While capturing the Landsat imagery, there may occur data missing issues such as occlusion of cloud, camera sensor, and aperture artifacts. The existing machine learning approaches do not provide significant results. This paper proposes a DeepLab Dilated convolutional neural network (DL-DCNN) for semantic segmentation with the goal to occur the change map for earth observation applications. Experimental results reveal that the accuracy of the proposed change detection results provides improved results as compared with the existing algorithms and maps the semantic objects within the predefined class as change or no change. © Springer Nature Singapore Pte Ltd. 2018.","Change detection; Deep learning; Multispectral; Remote sensing","Deep learning; Deforestation; Disasters; Image segmentation; Neural networks; Semantics; Change detection; Convolutional neural network; Environmental Monitoring; Geographical locations; Machine learning approaches; Multi-spectral; Multispectral remote sensing image; Remote sensing images; Remote sensing"
"Aerial image semantic segmentation using neural search network architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057086993&doi=10.1007%2f978-3-030-03014-8_10&partnerID=40&md5=839b587539de663ff3db4849a39e1d40","In remote sensing data analysis and computer vision, aerial image segmentation is a crucial research topic, which has many applications in environmental and urban planning. Recently, deep learning is using to tackle many computer vision problem, including aerial image segmentation. Results have shown that deep learning gains much higher accuracy than other methods on many benchmark data sets. In this work, we propose a neural network called NASNet-FCN, which based on Fully Convolutional Network - a frame work for solving semantic segmentation problem and image feature extractor derived from state-of-the-art object recognition network called Neural Search Network Architecture. Our networks are trained and judged by using benchmark dataset from ISPRS Vaihingen challenge. Results show that our methods achieved state-of-the-art accuracy with potential improvements. © Springer Nature Switzerland AG 2018.","Aerial image; Neural search network architecture; Semantic segmentation","Antennas; Artificial intelligence; Computer vision; Deep learning; Image segmentation; Object recognition; Remote sensing; Semantic Web; Semantics; Aerial images; Benchmark datasets; Computer vision problems; Convolutional networks; Remote sensing data; Research topics; Semantic segmentation; State of the art; Network architecture"
"Fuzzy choquet integration of deep convolutional neural networks for remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046335657&doi=10.1007%2f978-3-319-89629-8_1&partnerID=40&md5=efcd1c2d35fcbebcfa50b4c6cee9a31f","What deep learning lacks at the moment is the heterogeneous and dynamic capabilities of the human system. In part, this is because a single architecture is not currently capable of the level of modeling and representation of the complex human system. Therefore, a heterogeneous set of pathways from sensory stimulus to cognitive function needs to be developed in a richer computational model. Herein, we explore the learning of multiple pathways–as different deep neural network architectures–coupled with appropriate data/information fusion. Specifically, we explore the advantage of data-driven optimization of fusing different deep nets–GoogleNet, CaffeNet and ResNet–at a per class (neuron) or shared weight (single data fusion across classes) fashion. In addition, we explore indices that tell us the importance of each network, how they interact and what aggregation was learned. Experiments are provided in the context of remote sensing on the UC Merced and WHU-RS19 data sets. In particular, we show that fusion is the top performer, each network is needed across the various target classes, and unique aggregations (i.e., not common operators) are learned. © Springer International Publishing AG, part of Springer Nature 2018.","Convolutional neural network; Fuzzy integral; Remote sensing",
"Automatic identification of plant species through a convolutional neural network model for UAV mounted digital cameras","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071841223&partnerID=40&md5=f33389e58cd67da4296c73a6b9211555","Species Identification is a significant part of biodiversity conservation and protection. Traditional techniques of plant species identification are slow, complicated and require expertise in the field of biosciences. Plant species identification is a challenging task for a novice, interested in obtaining knowledge for various applications such as Bio-diversity monitoring, remote sensing. Hence, with the advent of cost-effective unmanned aerial vehicle technologies, deep learning and computer-vision have given rise to an interest in their use for plant species identification, with minimum knowledge of the expert. In this study, a seven-layer convolutional neural network (CNN) based on deep learning feedforward artificial neural networks is proposed for automatic identification of plant species from the images or videos acquired by UAVs, using feature learning in real time. Small UAVs are suited for this model implementation as they can capture data at a very low altitude. Three types of plant species leaf's i.e. Eucalyptus, Corylus (Hazel), Maple (Acer) where used for training of the network (90% of acquired data) and for testing (10% of acquired data) for identification by the networks. Model performance and efficiency are studied using the accuracy, loss curves and confusion matrix. The model showed an outstanding performance of 93% recognition rate. Performance of model is notable because only basic RGB images are used. It is observed that the increase in training data increases the accuracy of identification with decreased loss rate. The model could be trained to recognize as many species with basic RGB images or videos, without the need for developing a new system. Future work include increase in robustness of the model by training it with a greater number of species. This model can be a powerful tool for automated identification of plant species in very low altitude UAV imageries, videos and could be used for many forest, agricultural research and management processes. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Agriculture and plantation; Automatic-classification; CNN; Deep-learning; UAV","Agriculture; Antennas; Automation; Biodiversity; Conservation; Convolution; Cost effectiveness; Deep learning; Feedforward neural networks; Multilayer neural networks; Network layers; Unmanned aerial vehicles (UAV); Automated identification; Automatic classification; Automatic identification; Biodiversity conservation; Convolutional neural network; Feed-forward artificial neural networks; Plant species identification; Species identification; Remote sensing"
"An efficient cloud detection algorithm based on CNN and SVM with its prospect in aerospace camera","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055018072&doi=10.1007%2f978-3-319-96707-3_26&partnerID=40&md5=396233cb32ac89ee7e25db28c407050f","Terrain information is difficult to obtain due to cloud cover, these regions will become blind in remote sensing images, which led to the remote sensing images can not play its due value. So we need to use some detection technology to detect the cloud in remote sensing image to avoid a lot of useless images captured by the satellite wastes a large amount of storage space and transmission bandwidth. This paper relay on a delicate high-resolution earth observation satellite in synchronous orbit, combined the CNN with SVM for cloud detection, and finally designed a new algorithm which can be realized in remote sensing satellite video processing system to statistic the cloud coverage ratio in orbit. Firstly, the large image is cut into small pieces, which will be send into the algorithm module. The new algorithm uses the deep learning algorithm to automatically abstract the effective features, and after that SVM classifier is used to recognize the thick cloud. The CNN net is designed with a matlab toolbox Matconvnet, which is easy to use and provides many pre-trained model. The most important is that the CNN network is simple enough so that it can be realized in embedded system like Zynq platform. Final 1000 cloud subimages and 1000 no-cloud subimages are used as training dataset, and another 1000 cloud subimages and 1000 no-cloud subimages are used as testing dataset, and the illation labels are compared to the true labels, the result shows that this algorithm is much more accurate than traditional cloud detection such as threshold method or SVM. In the future, this new algorithm can be easily combined with dynamic partial reconfiguration of FPGA, then the algorithm codes can be upload from earth to the satellite and reconfigure the algorithm module in FPGA. So the satellite can advance with the times. © Springer Nature Switzerland AG 2018.","Cloud detection; CNN; Deep learning; SVM","Deep learning; Field programmable gate arrays (FPGA); Learning algorithms; Optical instruments; Orbits; Satellites; Signal detection; Space optics; Statistical tests; Video signal processing; Cloud detection; Cloud detection algorithms; Detection technology; Dynamic partial reconfiguration; Earth observation satellites; Remote sensing images; Remote sensing satellites; Transmission bandwidth; Remote sensing"
"Deep multiple instance learning-based spatial–spectral classification for PAN and MS imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030682333&doi=10.1109%2fTGRS.2017.2750220&partnerID=40&md5=800951bc45b5264234e191c354e800ce","Panchromatic (PAN) and multispectral (MS) imagery classification is one of the hottest topics in the field of remote sensing. In recent years, deep learning techniques have been widely applied in many areas of image processing. In this paper, an end-to-end learning framework based on deep multiple instance learning (DMIL) is proposed for MS and PAN images’ classification using the joint spectral and spatial information based on feature fusion. There are two instances in the proposed framework: one instance is used to capture the spatial information of PAN and the other is used to describe the spectral information of MS. The features obtained by the two instances are concatenated directly, which can be treated as simple fusion features. To fully fuse the spatial–spectral information for further classification, the simple fusion features are fed into a fusion network with three fully connected layers to learn the high-level fusion features. Classification experiments carried out on four different airborne MS and PAN images indicate that the classifier provides feasible and efficient solution. It demonstrates that DMIL performs better than using a convolutional neural network and a stacked autoencoder network separately. In addition, this paper shows that the DMIL model can learn and fuse spectral and spatial information effectively, and has huge potential for MS and PAN imagery classification. © 2017 IEEE.","Deep learning; Feature fusion; Image classification; Joint features; Multiple instance learning","Convolution; Deep learning; Electric fuses; Feature extraction; Image classification; Image fusion; Learning systems; Neural networks; Remote sensing; Convolutional neural network; Feature fusion; Fully-connected layers; Multiple instance learning; Spatial informations; Spatial resolution; Spectral classification; Spectral information; Classification (of information)"
"Retrieving nitrogen concentration with hyperspectral data and long short-Term memory model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071927894&partnerID=40&md5=559492b8247e3e4cf1a7a9ddf28d14ac","Nitrogen is one of the key nutrient elements in growth of crops. Quantitative retrieval of nitrogen by hyperspectral remote sensing data provides technical guidance for irrigation and fertilizer management of crops, and is of great significance to China's digital agriculture strategy. In this paper, nitrogen retrieval experiments were carried out by a deep learning model of long short-Term memory (LSTM). The partial least squares regression (PLSR) and support vector machine (SVM) algorithms were also conducted to make comparative analysis. The 248 samples of canopy spectra and corresponding leaf nitrogen contents, covering the whole growth period of winter wheat and under different coverage levels, were collected and used to validate the algorithms. The samples were allocated to calibration and validation data sets according to Nitrogen concentration values. Three groups of spectral features were extracted by successive projections algorithm from the original and the transformed spectra, and by spectral position selection and vegetation indices. The principal component analysis was used to select the optimal features. Then the optimized and unoptimized spectral features were fed respectively into LSTM model to retrieve nitrogen concentration. Compared with the retrieval using unoptimized features, the R2 and RPD were improved from 0.7077 and 1.6776 to 0.8927 and 3.2598 by the optimized features and LSTM model. The R2 and RPD were 0.8475 and 2.7887 for PLSR, and 0.7903 and 2.6253 for SVM respectively by the optimized features. The retrieval accuracy of Nitrogen concentration using feature optimized LSTM algorithm was superior to those of PLSR and SVM algorithms. The results demonstrate that LSTM model is able to retrieve nitrogen concentration of winter wheat effectively and has the potential to predict biophysical and biochemical parameters of other crops. © 2018 Asian Association on Remote Sensing. All Rights Reserved.","Feature selection; Hyperspectral remote sensing; Long-short term memory model; nitrogen retrieval; Winter wheat","Brain; Crops; Deep learning; Feature extraction; Least squares approximations; Nitrogen fertilizers; Principal component analysis; Remote sensing; Support vector machines; Calibration and validations; Hyperspectral remote sensing; Hyperspectral remote sensing data; Partial least squares regressions (PLSR); Short term memory; Successive projections algorithm; Support vector machine algorithm; Winter wheat; Long short-term memory"
"Repeat multiview panchromatic super-resolution restoration using the UCL MAGiGAN system","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059024601&doi=10.1117%2f12.2500196&partnerID=40&md5=eb5c8dab554c5cc6473c3417b6e63921","High spatial resolution imaging data is always considered desirable in the field of remote sensing, particularly Earth observation. However, given the physical constraints of the imaging instruments themselves, one needs to be able to trade-off spatial resolution against launch mass as well as telecommunications bandwidth for transmitting data back to the Earth. In this paper, we present a newly developed super-resolution restoration system, called MAGiGAN, based on our original GPT-SRR system combined with deep learning image networks to be able to restore up to 4x higher resolution enhancement using multi-angle repeat images as input. © 2018 SPIE.","Deep learning; Earth observation; Generative adversarial network; MAGiGAN; Multi-angle; Super-resolution restoration","Deep learning; Economic and social effects; Image enhancement; Image resolution; Observatories; Optical resolving power; Restoration; Adversarial networks; Earth observations; MAGiGAN; Multi angle; Super-resolution restoration; Remote sensing"
"Semantic segmentation of aerial image using fully convolutional network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052222758&doi=10.1007%2f978-981-13-1702-6_54&partnerID=40&md5=8ccc3ba4ddf80337da914e58867b1aec","Dense semantic segmentation is an important task for remote sensing image analyzing and understanding. Recently deep learning has been applied to pixel-level labeling tasks in computer vision and produces state-of-the-art results. In this work, a fully convolutional network (FCN), which is a variant of convolutional neural network (CNN), is employed to address the semantic segmentation of high resolution aerial images. We design a skip-layer architecture that combines different layers of features in aerial images. This structure integrates the semantic information from deep layer and appearance information from shallow layer to make better use of the aerial image features. Moreover, the FCN can be trained end-to-end and produce segmentation output correspondingly-sized as the input image. Our model is trained on the extended GE-4 aerial image dataset to adapt FCN to the aerial image segmentation task. A full-resolution semantic segmentation is produced for each testing aerial image. Experiments show that our method obtains improvement in accuracy compared with several other methods. © Springer Nature Singapore Pte Ltd., 2018.","Aerial images; Convolutional neural network; Deep learning; Fully convolutional network; Semantic segmentation","Antennas; Convolution; Deep learning; Neural networks; Remote sensing; Semantic Web; Semantics; Aerial images; Convolutional networks; Convolutional neural network; Convolutional Neural Networks (CNN); High-resolution aerial images; Remote sensing images; Semantic information; Semantic segmentation; Image segmentation"
"Adaptive image filtering based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054842437&doi=10.1007%2f978-3-030-00021-9_34&partnerID=40&md5=432ada053f6348205b1c378f067a1831","The process of digital image acquisition and transmission is easy to be polluted by noise. Noises can also cause disturbances, or even misjudgements in the remote sensing image, face recognition, image classification of machine learning and deep learning. Therefore the correctness and safety of image usage is greatly reduced. Different types of noise may occur under various conditions, and the same filtering method has different effects on different types of noise processing, which makes it difficult to select the best way to filtering the image. So the detection and recognition of noise type has always been a hot topic in the field of information security. However, there are lacking solutions to the current noise type identification problem, and the complexity is very high. In this paper, a convolutional neural network(CNN) model which is able to automatically identify salt and pepper noise, Gauss noise and random noise based on deep learning training is proposed. After that, median filter, mean filter and wiener filter are used to filter the corresponding images. The purpose of ensuring the correctness and security of the image application is achieved. By simulating the images of different noise and analyzing PSNR, it is proved that this method able to distinguish the noise and filter obviously. © Springer Nature Switzerland AG 2018.","Convolution neural network; Filtering; Image feature extraction; Noise type; PSNR","Adaptive filtering; Adaptive filters; Cloud computing; Convolution; Deep learning; Face recognition; Filtration; Neural networks; Remote sensing; Security of data; Convolution neural network; Convolutional neural network; Convolutional Neural Networks (CNN); Digital image acquisition; Identification problem; Image feature extractions; Noise types; PSNR; Median filters"
"Snow cover mapping for mountainous areas by fusion of MODIS L1B and geographic data based on stacked denoising auto-encoders","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055667060&doi=10.32604%2fcmc.2018.02376&partnerID=40&md5=cd42cc3d9baef66e2910432efea70f67","Snow cover plays an important role in meteorological and hydrological researches. However, the accuracies of currently available snow cover products are significantly lower in mountainous areas than in plains, due to the serious snow/cloud confusion problem caused by high altitude and complex topography. Aiming at this problem, an improved snow cover mapping approach for mountainous areas was proposed and applied in Qinghai-Tibetan Plateau. In this work, a deep learning framework named Stacked Denoising Auto-Encoders (SDAE) was employed to fuse the MODIS multispectral images and various geographic datasets, which are then classified into three categories: Snow, cloud and snow-free land. Moreover, two independent SDAE models were trained for snow mapping in snow and snow-free seasons respectively in response to the seasonal variations of meteorological conditions. The proposed approach was verified using in-situ snow depth records, and compared to the most widely used snow products MOD10A1 and MYD10A1. The comparison results show that our method got the best performance: Overall accuracy of 98.95% and F-measure of 73.84%. The results indicated that our method can effectively improve the snow recognition accuracy, and it can be further extended to other multi-source remote sensing image classification issues. Copyright © 2018 Tech Science Press.","Deep learning; MODIS L1B.; Qinghai-Tibetan Plateau; Remote sensing; Snow cover","Deep learning; Image enhancement; Mapping; Radiometers; Remote sensing; Signal encoding; Complex topographies; Meteorological condition; MODIS L1B; Multispectral images; Qinghai-Tibetan plateau; Recognition accuracy; Remote sensing image classification; Snow covers; Snow"
"The role of Hyperspectral Imaging: A literature review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061439049&doi=10.14569%2fijacsa.2018.090808&partnerID=40&md5=29334de7622944a1b34dc4434f22e64e","Optical analysis techniques are used recently to detect and identify the objects from a large scale of images. Hyperspectral imaging technique is also one of them. Vision of human eye is based on three basic color (red, green and blue) bands, but spectral imaging divides the vision into many more bands. Hyperspectral remote sensors achieve imagery data in the form of hundreds of adjoining spectral bands. In this paper, our purpose is to illustrate the fundamental concept, hyperspectral remote sensing, remotely sensed information, methods for hyperspectral imaging and applications based on hyperspectral imaging. Moreover, in the forensic context, the novel methods involving deep neural networks are elaborated in this paper. The proposed idea can be useful for further research in the field of hyperspectral imaging using deep learning. © 2018 International Journal of Advanced Computer Science and Applications.","Deep learning; Electromagnetic spectrum; Hyperspectral imaging; Imaging spectroscopy; Multispectral imaging; Remote sensing",
"Identifying urban canopy coverage from satellite imagery using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058234152&partnerID=40&md5=4c0ffe992fb14be1f5ff871bf179d1e0","The availability of high resolution satellite imagery offers a compelling opportunity for the utilisation of state-of-the-art deep learning techniques in the applications of remote sensing. This research investigates the application of different Convolution Neural Network (CNN) architectures for pixel-level segmentation of canopy coverage in urban areas. The performance of two established patch-based CNN architectures (LeNet and a pre-trained VGG16) and two encoder-decoder architectures (a simple 4-layer convolutional encoder-decoder and Unet) was compared using two datasets (a large set of images of the Geerman town of Vaihingen and smaller set of the US city of Denver). Results show that the patch-based methods outperform the encoder-decoder methods. It is also shown that pre-training is only effective with the smaller dataset. © 2018 CEUR Workshop Proceedings. All rights reserved.","Canopy coverage; Convolutional neural network; Deep learning; Google earth engine; Remote sensing","Convolution; Decoding; Deep learning; Network architecture; Neural networks; Remote sensing; Signal encoding; Canopy coverage; Convolution neural network; Convolutional encoders; Convolutional neural network; Encoder-decoder architecture; Google earths; High resolution satellite imagery; Patch-based methods; Satellite imagery"
"Digging more in neural world: An efficient approach for hyperspectral image classification using convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049009605&doi=10.1007%2f978-981-13-0896-3_12&partnerID=40&md5=fa4294790547bc1ff6bd53a4f380508c","Classification of hyperspectral images (HSI) can benefit from deep learning models with deep architecture in remote sensing. In this letter, a novel method based on Convolutional Neural Network (CNN) is proposed for the classification of hyperspectral images. Due to using more spatio-spectral features for the classification of hyperspectral images, the proposed method outperforms the existing state-of-the-art classification techniques. Our proposed method first reduces the dimension of hyperspectral images using Principle component analysis (PCA). The spatial and spectral features are then exploited by a fixed size convolutional filter to generate the combine spatio-spectral feature maps. Finally, these feature maps are fed into a Multi-Layer Perceptron (MLP) classifier that predicts the class of the pixel vector. To validate the effectiveness of our proposed method, computer simulations are conducted using three datasets namely Indian Pines, Salinas and Pavia University and comparisons with existing techniques are made. © Springer Nature Singapore Pte Ltd. 2018.","CNN; Hyperspectral classification; Multi-Layer Perceptron (MLP); PCA; Remote sensing","Convolution; Deep learning; Independent component analysis; Neural networks; Principal component analysis; Remote sensing; Spectroscopy; Classification technique; Convolutional neural network; Convolutional Neural Networks (CNN); Deep architectures; Hyper-spectral classification; Multi layer perceptron; Principle component analysis; Spectral feature; Image classification"
"Understanding Historical Cityscapes from Aerial Imagery Through Machine Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055444492&doi=10.1007%2f978-3-030-01762-0_17&partnerID=40&md5=0d1197be87bd72d958957fc250cef700","Understanding cityscapes using remote sensing data has been an active research field for more than two decades. Meanwhile, machine learning provides generalization capabilities compared to hierarchical and rule-based methods. This paper evaluates several machine learning algorithms in order to fuse shadow detection and shadow compensation methods for building detection using high resolution aerial imagery. Three complex and real-life urban study areas were used as test datasets with various: (i) kinds of buildings structures of special architecture, (ii) pixel resolutions and, (iii) types of data. Objective evaluation metrics have been used for assessing the compared algorithms such recall, precision and F1-score as well as rates of completeness, correctness and quality. For both approaches, i.e., shadow detection and building detection, the computational complexity of each machine learning algorithm was examined. The results indicate that deep learning schemes, such a Convolutional Neural Network (CNN), provides the best classification performance in terms of shadow detection and building detection. © 2018, Springer Nature Switzerland AG.","Building detection; Machine learning; Point cloud; Shadow compensation; Shadow detection","Aerial photography; Antennas; Buildings; Complex networks; Deep learning; Historic preservation; Learning systems; Neural networks; Petroleum reservoir evaluation; Quality control; Remote sensing; Speech processing; Building detection; Classification performance; Convolutional Neural Networks (CNN); Generalization capability; High resolution aerial imagery; Point cloud; Shadow compensations; Shadow detections; Learning algorithms"
"Comparing U-Net convolutional network with mask R-CNN in the performances of pomegranate tree canopy segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058223848&doi=10.1117%2f12.2325570&partnerID=40&md5=3ded2ae9acd15511f96df4883524d8ca","In the last decade, technologies of unmanned aerial vehicles (UAVs) and small imaging sensors have achieved a significant improvement in terms of equipment cost, operation cost and image quality. These low-cost platforms provide flexible access to high resolution visible and multispectral images. As a result, many studies have been conducted regarding the applications in precision agriculture, such as water stress detection, nutrient status detection, yield prediction, etc. Different from traditional satellite low-resolution images, high-resolution UAVbased images allow much more freedom in image post-processing. For example, the very first procedure in post-processing is pixel classification, or image segmentation for extracting region of interest(ROI). With the very high resolution, it becomes possible to classify pixels from a UAV-based image, yet it is still a challenge to conduct pixel classification using traditional remote sensing features such as vegetation indices (VIs), especially considering various changes during the growing season such as light intensity, crop size, crop color etc. Thanks to the development of deep learning technologies, it provides a general framework to solve this problem. In this study, we proposed to use deep learning methods to conduct image segmentation. We created our data set of pomegranate trees by flying an off-shelf commercial camera at 30 meters above the ground around noon, during the whole growing season from the beginning of April to the middle of October 2017. We then trained and tested two convolutional network based methods U-Net and Mask R-CNN using this data set. Finally, we compared their performances with our dataset aerial images of pomegranate trees. [Tiebiao- add a sentence to summarize the findings and their implications to precision agriculture]. © 2018 SPIE.","Canopy Segmentation; Instance-aware Segmentation; Mask R-CNN; Pomegranate Trees; U-net CNN","Aircraft detection; Antennas; Convolution; Crops; Deep learning; Forestry; Image enhancement; Pixels; Plants (botany); Precision agriculture; Remote sensing; Trees (mathematics); Unmanned aerial vehicles (UAV); Convolutional networks; High resolution visible; Learning technology; Low resolution images; Multispectral images; Pixel classification; Pomegranate Trees; Very high resolution; Image segmentation"
"Application of the VP8-Codec-based Format for Efficient Disaster Information Propagation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071874423&partnerID=40&md5=6515d88da1d9175ae3b4cb564a7d0888","The propagation of GIS-based disaster information can facilitate more effective and intuitive disaster response compared to the existing emergency disaster information texting service, and can thus lead to reduced disaster rates. In disaster situations, however, the data traffic surges, which can create server overload in a short period of time. This problem can be resolved by employing physical methods like increased server installation, but as it is very difficult to forecast the data traffic in disaster situations, and given the characteristics of disaster situations, such method is deemed very inefficient. This study sought to efficiently resolve the aforementioned limitations of GIS-based disaster information propagation by reducing the disaster information volume based on images and videos. Towards this end, this study used the VP8-codec-based format. The VP8 codec is designed to reduce the data traffic in the Web environment. When data is compressed, it has an excellent color and excellent borderline preservation power through the mutual forecasting mode of macroblocks; as such, readable compression can be performed. The same information and video have different volumes by Web format and codec, and the format that has less volume in the same information is deemed a good compression format. The universally used JPEG and MP4 formats, and the VP8-codec-based formats Webp and Webm, were compressed according to certain sections, and it was found that the VP8-codec-based format is more efficient. This suggests that the VP8-codec-based format can effectively prevent server overload. In the future, compression using the deep learning technology should be performed. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Disaster information; Image compression; Python; Risk map; VP8","Data reduction; Deep learning; Geographic information systems; Image compression; Information dissemination; Maps; Remote sensing; Disaster Information; Disaster response; Disaster situations; Learning technology; Physical methods; Python; Risk maps; Server overload; Emergency services"
"Ship detection in satellite imagery via convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071845291&partnerID=40&md5=2862f7a19e5560b06981c2e21fa424ef","Ship detection from satellite imagery is a valuable tool for maritime traffic surveillance, detecting illegal fishing, oil discharge control, and sea pollution monitoring. Over the last decade, deep learning using convolutional neural networks (CNNs) has become the dominant paradigm for various computer vision tasks such as image classification, object detection, and segmentation, far surpassing traditional techniques. However, research and development are often focused on popular large-scale datasets, consisting of images that are vastly different from satellite imagery. We therefore trained and evaluated the Single-Shot MultiBox Detector (SSD) model for detection of ships in satellite imagery on a small-scale custom dataset, which consisted of 794 images of ships collected near harbors and coastlines. Two slightly different approaches were trialed for comparison: (1) Only the base architecture layers from the VGG-16 network were initialised with pre-trained weights produced from the ImageNet dataset and then the subsequent layers trained in a typical transfer learning setting. (2) All layers of the network were initialised with pre-trained weights produced from the Microsoft COCO dataset and then fine-tuned by retraining every layer with our dataset. The evaluation algorithms provided by the Pascal VOC competition were used to produce precision-recall (PR) curves and determine the overall mean average precision (mAP). Both approaches achieved competitive results in mAP. However the latter proved slightly more effective in both the rate of training and the final performance. The results confirm the effectiveness of the SSD model for object detection tasks, particularly in the case of satellite imagery. Our results also indicate that training with the SSD model can be greatly improved by fine-tuning pre-trained weights rather than training the entire network from scratch or transfer learning on the later layers of the network. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Convolutional neural networks; Maritime surveillance; Object detection; Ships; Transfer-learning","Convolution; Deep learning; Image segmentation; Large dataset; Marine pollution; Network layers; Neural networks; Object detection; Object recognition; Pollution control; Remote sensing; Ships; Small satellites; Convolutional neural network; Evaluation algorithm; Large-scale datasets; Maritime surveillance; Pollution monitoring; Research and development; Traditional techniques; Transfer learning; Satellite imagery"
"Scene classification via triplet networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032748357&doi=10.1109%2fJSTARS.2017.2761800&partnerID=40&md5=eaaf12fb7e7876dfc3b6fbe96739495b","Scene classification is a fundamental task for automatic remote sensing image understanding. In recent years, convolutional neural networks have become a hot research topic in the remote sensing community, and have made great achievements in scene classification. Deep convolutional networks are primarily trained in a supervised way, requiring huge volumes of labeled training samples. However, clearly labeled remote sensing data are usually limited. To address this issue, in this paper, we propose a novel scene classification method via triplet networks, which use weakly labeled images as network inputs. Besides, we initiate a theoretical study on the three existing loss functions for triplet networks, analyzing their different underlying mechanisms for dealing with 'hard' and/or 'easy' triplets during training. Furthermore, four new loss functions are constructed, aiming at laying more stress on 'hard' triplets to improve classification accuracy. Extensive experiments have been conducted, and the experimental results show that triplet networks coupled with our proposed losses achieve a state-of-the-art performance in scene classification tasks. © 2017 IEEE.","Deep learning; Difference loss; Ratio loss; Scene classification; Triplet networks","Convolution; Deep learning; Feature extraction; Learning systems; Neural networks; Personnel training; Semantics; Classification accuracy; Convolutional networks; Convolutional neural network; Hot research topics; Remote sensing data; Remote sensing images; Scene classification; State-of-the-art performance; Remote sensing; artificial neural network; image analysis; image classification; remote sensing"
"Scalable image informatics","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054289819&doi=10.1016%2fB978-0-12-811889-4.00009-9&partnerID=40&md5=fda336923f7d1252b0fbddcb2536c9c7","Images and video play a major role in scientific discoveries. Significant new advances in imaging science over the past two decades have resulted in new devices and technologies that are able to probe the world at nanoscales to planetary scales. These instruments generate massive amounts of multimodal imaging data. In addition to the raw imaging data, these instruments capture additional critical information-the metadata-that include the imaging context. Further, the experimental conditions are often added manually to such metadata that describe processes that are not implicit in the instrumentation metadata. Despite these technological advances in imaging sciences, resources for curation, distribution, sharing, and analysis of such data at scale are still lacking. Robust image analysis workflows have the potential to transform image-based sciences such as biology, ecology, remote sensing, materials science, and medical imaging. In this context, this chapter presents BisQue, a novel eco-system where scientific image analysis methods can be discovered, tested, verified, refined, and shared among users on a shared, cloud-based infrastructure. The vision of BisQue is to enable large-scale, data-driven scientific explorations. The following sections will discuss the core requirements of such an architecture, challenges in developing and deploying the methods, and will conclude with an application to image recognition using deep learning. © 2018 Elsevier Ltd All rights reserved.","Bisque; Data-driven Discovery; Deep Learning; Image Data Management; Reproducible Science; Scientific Image Databases","Deep learning; Image analysis; Image recognition; Informatics; Information management; Metadata; Remote sensing; Bisque; Data driven; Experimental conditions; Reproducible Science; Scientific discovery; Scientific exploration; Scientific images; Technological advances; Medical imaging"
"Data organization in video surveillance systems using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055446480&doi=10.18287%2f1613-0073-2018-2210-243-250&partnerID=40&md5=c214c88e9ffd0d78bbf3fe7f9fc044cd","In this paper we propose to organize information in video surveillance systems by grouping the video tracks, which contain identical faces. Aggregation of the features of individual frames extracted using deep convolutional neural networks are used in order to obtain a descriptor of video track. The tracks with identical faces are grouped using the known face verification algorithms and clustering methods. We experimentally compare frame aggregation methods using the YouTubeFaces dataset and contemporary neural networks (VGGFace, VGGFace2, LightenedCNN). It is shown that the most accurate video-based face verification is achieved with the L2-normalization of average unnormalized features of individual frames of each video track. Finally, we demonstrate that the best video grouping is obtained by sequential and rank-order clustering methods. © 2018 CEUR-WS. All rights reserved.",,"Cluster analysis; Clustering algorithms; Deep neural networks; Face recognition; Nanotechnology; Neural networks; Remote sensing; Clustering methods; Convolutional neural network; Data organization; Face Verification; Face verification algorithms; Frame aggregation; Rank Order Clustering; Video surveillance systems; Security systems"
"Systematic evaluation of CNN on land cover classification from remotely sensed images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059017183&doi=10.1117%2f12.2501968&partnerID=40&md5=c61e7affbbd5df1e5073ea778d616200","In using the convolutional neural network (CNN) for classification, there is a set of hyperparameters available for the configuration purpose. This study aims to validate the effectiveness of the CNN architecture, i.e. AlexNet on land cover classification, based on four remotely sensed land-use land-cover (LULC) datasets. In addition to the evaluation of the impact of a range of parameters in the evaluation tests, the influence of a set of hyperparameters on the classification performance will be assessed. The parameters include the epoch values, batch size, convolutional filter size and input image size. Thus, a set of experiments were conducted to specify the effectiveness of the selected parameters. We first explore the number of epochs under several selected batch size values. The impact of the first layer kernel size of the convolutional filters also was evaluated. Moreover, testing assorted sizes of the input images provided insight of the influence of the size of the convolutional filters and the image sizes. To generalize the validation, four remote sensing datasets, AID, RDS, UCMerced and RSCCN7, which have different land coverage and are publicly available, were used in the experiments. These datasets have a wide diversity of input data, such as the number of classes, amount of labelled data and texture patterns. A specifically-designed, interactive, deep-learning GPU training platform for image classification (Nvidia Digit) was employed in the experiments. It has shown efficiency in both training, evaluation and testing. These results provide opportunities toward better classification performance in various applications, such as hyperspectral multi-temporal agricultural LULC. © 2018 SPIE.","Classification; Convolutional neural network; Land use land cover; Remote sensing","Classification (of information); Convolution; Deep learning; Land use; Neural networks; Remote sensing; Classification performance; Convolutional neural network; Convolutional Neural Networks (CNN); Land cover classification; Land use/ land covers; Remotely sensed images; Systematic evaluation; Training platform; Image classification"
"Satellite imagery analysis for operational damage assessment in emergency situations","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050644228&doi=10.1007%2f978-3-319-93931-5_25&partnerID=40&md5=025011b9a00a7e1dfe42e3793e0b0ff7","When major disaster occurs the questions are raised how to estimate the damage in time to support the decision making process and relief efforts by local authorities or humanitarian teams. In this paper we consider the use of Machine Learning and Computer Vision on remote sensing imagery to improve time efficiency of assessment of damaged buildings in disaster affected area. We propose a general workflow that can be useful in various disaster management applications, and demonstrate the use of the proposed workflow for the assessment of the damage caused by the wildfires in California in 2017. © Springer International Publishing AG, part of Springer Nature 2018.","Damage assessment; Deep learning; Emergency mapping; Emergency response; Remote sensing; Satellite imagery","Damage detection; Decision making; Deep learning; Disaster prevention; Emergency services; Image enhancement; Information systems; Information use; Remote sensing; Damage assessments; Decision making process; Disaster management; Emergency response; Emergency situation; Local authorities; Remote sensing imagery; Time efficiencies; Satellite imagery"
"Comparison of four machine learning methods for object-oriented change detection in high-resolution satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045147995&doi=10.1117%2f12.2285493&partnerID=40&md5=87f4ce761001a9239a5a676e80a96c27","High resolution image change detection is one of the key technologies of remote sensing application, which is of great significance for resource survey, environmental monitoring, fine agriculture, military mapping and battlefield environment detection. In this paper, for high-resolution satellite imagery, Random Forest (RF), Support Vector Machine (SVM), Deep belief network (DBN), and Adaboost models were established to verify the possibility of different machine learning applications in change detection. In order to compare detection accuracy of four machine learning Method, we applied these four machine learning methods for two high-resolution images. The results shows that SVM has higher overall accuracy at small samples compared to RF, Adaboost, and DBN for binary and from-to change detection. With the increase in the number of samples, RF has higher overall accuracy compared to Adaboost, SVM and DBN. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Change detection; high resolution; machine learning","Adaptive boosting; Artificial intelligence; Decision trees; Deep learning; Environmental technology; Learning systems; Military mapping; Military photography; Object detection; Remote sensing; Support vector machines; Battlefield environments; Change detection; Deep belief network (DBN); Environmental Monitoring; High resolution; High resolution satellite imagery; Machine learning applications; Remote sensing applications; Satellite imagery"
"Seagrass detection in coastal water through deep capsule networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057209524&doi=10.1007%2f978-3-030-03335-4_28&partnerID=40&md5=d1138374a149ba5a78f17842daeccbd6","Seagrass is an important factor to balance marine ecological systems, and there is a great interest in monitoring its distribution in different parts of the world. This paper presents a deep capsule network for classification of seagrass in high-resolution multispectral satellite images. We tested our method on three satellite images of the coastal areas in Florida and obtained better performances than those achieved by the traditional deep convolutional neural network (CNN) model. We also propose a few-shot deep learning strategy to transfer knowledge learned by the capsule network from one location to another for seagrass detection, in which the capsule network’s reconstruction capability is utilized to generate new artificial data for fine-tuning the model at new locations. Our experimental results show that the proposed model achieves superb performances in cross-validation on three satellite images collected in Florida as compared to support vector machine (SVM) and CNN. © Springer Nature Switzerland AG 2018.","Capsule network; Convolutional neural network; Deep learning; Remote sensing; Seagrass detection; Transfer learning","Computer vision; Convolution; Deep learning; Neural networks; Plants (botany); Remote sensing; Satellites; Support vector machines; Convolutional neural network; Deep convolutional neural networks; Ecological systems; Learning strategy; Multispectral satellite image; Reconstruction capability; Seagrasses; Transfer learning; Deep neural networks"
"An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022177717&doi=10.1007%2f978-3-319-60663-7_18&partnerID=40&md5=c143c4e792fd2254552d0ac8e5f25697","Object classification from images is among the many practical examples where deep learning algorithms have successfully been applied. In this paper, we present an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Several aspects of the proposed method are enhanced, incl. incorporation of ELU (exponential linear unit)—as opposed to ReLU (rectified linear unit) that typically outperforms ELU in most object classification cases; amplification of datasets by adding incrementally-rotated images with eight different angles in the training corpus (this eliminates the limitation that the number of training aerial images is usually limited), thus the number of training datasets is increased by eight times; and lastly, adoption of landscape metrics to further improve the overall quality of results by removing false road objects. The most recent DCED approach for object segmentation, namely SegNet, is used as one of the benchmarks in evaluating our method. The experiments were conducted on a well-known aerial imagery, Massachusetts roads dataset (Mass. Roads), which is publicly available. The results showed that our method outperforms all of the baselines in terms of precision, recall, and F1 scores. © Springer International Publishing AG 2018.","Deep convolutional neural network; Deep learning; Image processing; Remote sensing; Road segmentation","Aerial photography; Classification (of information); Convolution; Decoding; Deep neural networks; Education; Image processing; Image segmentation; Learning algorithms; Neural networks; Remote sensing; Roads and streets; Transportation; Convolutional encoders; Convolutional neural network; Landscape metrics; Object classification; Object segmentation; Overall quality; Road segmentation; Training data sets; Deep learning"
"Exploring recurrent and feedback CNNs for multi-spectral satellite image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061965475&doi=10.1016%2fj.procs.2018.10.325&partnerID=40&md5=8becc355cebcac7a31fe2f2d32a8782f","The emergence of deep learning applications such as convolutional neural networks (CNNs) have resulted in huge improvements on computer vision applications in a wide variety of fields. However, several works demonstrated that low-quality or noisy data (even including perceptually not visible noises) may have a huge impact on the accuracy of CNN models. Therefore, as inspired by biological perception systems, some recent works proposed the use of recurrent and feedback features in CNNs as an improvement to the existing feed-forward CNNs. These recent works on the integration of recurrence and/or feedback to CNNs mostly tested deep networks on natural scenes with relatively perceptually good resolution color images. In this work, we explored the effectiveness of CNNs with recurrent and feedback features for the solar-power plant classification task on mid-resolution (1 pixel - 30×30 square meters per pixel) multi-spectral satellite images. Experiments show promising results when using top-down signals (especially recurrent and feedback features together) on CNNs for multi-spectral image classification tasks, outperforming the baseline CNN model without any recurrent and feedback structure and other approaches in the literature including deep models. © 2018 The Authors. Published by Elsevier B.V.","Classification; CNNs with Recurrent; Convolutional Neural Network; Feedback; Multi-Spectral Images; Remote Sensing","Adaptive systems; Classification (of information); Complex networks; Convolution; Deep learning; Embedded systems; Feedback; Neural networks; Pixels; Remote sensing; Solar energy; Solar power plants; Solar power satellites; Spectroscopy; CNNs with Recurrent; Computer vision applications; Convolutional neural network; Multispectral image classification; Multispectral images; Multispectral satellite image; Perception systems; Plant classification; Image classification"
"Data fusion for improved camera-based detection of respiration in neonates","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045207465&doi=10.1117%2f12.2290139&partnerID=40&md5=b329974e66358d3be30cd9b1f6051e41","Monitoring respiration during neonatal sleep is notoriously difficult due to the nonstationary nature of the signals and the presence of spurious noise. Current approaches rely on the use of adhesive sensors, which can damage the fragile skin of premature infants. Recently, non-contact methods using low-cost RGB cameras have been proposed to acquire this vital sign from (a) motion or (b) photoplethysmographic signals extracted from the video recordings. Recent developments in deep learning have yielded robust methods for subject detection in video data. In the analysis described here, we present a novel technique for combining respiratory information from high-level visual descriptors provided by a multi-task convolutional neural network. Using blind source separation, we find the combination of signals which best suppresses pulse and motion distortions and subsequently use this to extract a respiratory signal. Evaluation results were obtained from recordings on 5 neonatal patients nursed in the Neonatal Intensive Care Unit (NICU) at the John Radcliffe Hospital, Oxford, UK. We compared respiratory rates derived from this fused breathing signal against those measured using the gold standard provided by the attending clinical staff. We show that respiratory rate (RR) be accurately estimated over the entire range of respiratory frequencies. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","data fusion; deep learning; ICA; neonates; NICU.; PCA; remote sensing; respiration","Cameras; Data fusion; Deep learning; Diagnosis; Independent component analysis; Intensive care units; Neonatal monitoring; Neural networks; Remote sensing; Video recording; Convolutional neural network; Neonatal intensive care units; neonates; NICU; Photoplethysmographic signals; respiration; Respiratory frequency; Respiratory signals; Blind source separation"
"Synthetic aperture radar for pipeline right-of-way monitoring","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057332371&doi=10.1117%2f12.2504980&partnerID=40&md5=6de7d998a32757de21b4b2bed43a4a3f","The Ball Aerospace Pipeline Damage Prevention Radar (PDPR) project evaluated the use of airborne synthetic aperture radar (SAR) to detect vehicles and equipment located within buried pipeline right-of-way areas but obscured from visual detection. The project included the configuration of a commercial dual-band SAR/EO system for airborne operations, hardware and software modifications to optimize SAR change detection processing, and the execution of multiple flight tests to characterize SAR performance for the detection of equipment obscured by vegetation. Flight tests were conducted in 2016 and 2017 using X-band, Ku-band and ultra-wide band (UWB) SAR in urban and rural environments. Targets in the open showed close to 100% detection performance while covered target results depended on the amount of vegetative canopy. Detection ""through"" vegetation was generally better using the UWB system, but vegetation gaps frequently allowed higher spatial resolution detections with the Ku-band system. While large equipment was frequently identifiable in the Ku-band SAR images, having coincident EO imagery proved critical for context and automated deep learning based object identification. The detection performance difference between open and covered conditions clearly illustrates how a collection plan that optimizes open viewing conditions increases the overall probability of detection. This research was performed in response to the Damage Prevention topic through the Technology Development in the Pipeline Safety Research and Development Announcement DTPH5615RA00001. © 2018 SPIE.","CCD; Change detection; MCD; Pipeline monitoring; SAR; Synthetic aperture radar","Broadband networks; Charge coupled devices; Damage detection; Deep learning; Ecosystems; Object detection; Pipelines; Radar imaging; Remote sensing; Rights of way; Software testing; Sustainable development; Ultra-wideband (UWB); Vegetation; Airborne synthetic aperture radars; Change detection; Detection performance; Hardware and software; Object identification; Pipeline monitoring; Probability of detection; Technology development; Synthetic aperture radar"
"Evaluation of distance measures for feature based image registration using AlexNet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057220924&doi=10.14569%2fIJACSA.2018.091034&partnerID=40&md5=5597e60bd63bcfe9673b09b24fab0efc","Image registration is a classic problem of computer vision with several applications across areas like defence, remote sensing, medicine etc. Feature based image registration methods traditionally used hand-crafted feature extraction algorithms, which detect key points in an image and describe them using a region around the point. Such features are matched using a threshold either on distances or ratio of distances computed between the feature descriptors. Evolution of deep learning, in particular convolution neural networks, has enabled researchers to address several problems of vision such as recognition, tracking, localization etc. Outputs of convolution layers or fully connected layers of CNN which has been trained for applications like visual recognition are proved to be effective when used as features in other applications such as retrieval. In this work, a deep CNN, AlexNet, is used in the place of handcrafted features for feature extraction in the first stage of image registration. However, there is a need to identify a suitable distance measure and a matching method for effective results. Several distance metrics have been evaluated in the framework of nearest neighbour and nearest neighbour ratio matching methods using benchmark dataset. Evaluation is done by comparing matching and registration performance using metrics computed from ground truth. © 2015 The Science and Information (SAI) Organization Limited.","Deep learning; Distance measures; Feature descriptor; Feature detection; Image matching",
"Change detection of land use from pairs of satellite images via convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071866813&partnerID=40&md5=a1510a159aaf4072c0ac432f8e34429d","Monitoring changes in land use is employed in land price analysis and urban planning. Land use refers to categories of land classified according to the purpose of its use such as residential area, road, crop land, paddy land, farmland, or waterway. Currently, land-use classifier is updated manually, which is time-consuming. In this research, we propose a method for automatic land-use change detection from multiple temporal satellite images to support manual updating of land-use classifier. Our proposed method contributes to reduction of the time required for manual analysis by detecting potential changed areas. Changes are detected from pairs of high-spatial-resolution satellite images using convolutional neural network (CNN). To detect actual changes in land use, distinguishing between actual changes and seasonal changes is required. One example of seasonal changes is paddy land that looks different every season. Data for supervised training is generated by assigning a label of class of change for each mesh of 25 meters. The input data to CNN are pairs of small patches cropped from two temporal images. In case of comparing multiple temporal land-cover maps, which have been generated from each image for detecting changes, some factors derived from satellite images such as position aberration are detected as changes. Our method attempts to improve CNN model performance by learning the changes that are required to be detected using a pair of satellite images each representing a mesh of 25 meters. The detected changes are validated by comparing with the actually labeled images In some areas, existence of shadows affects the analysis results. Changes due to shadows are classified as candidates for the changed areas in land-use classification. To reduce misclassification due to shadows, the number of variations in the supervised data set should be increased in the future. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Change detection; Convolutional neural network; Deep learning; Satellite image; Urban planning","Convolution; Deep learning; Deep neural networks; Land use; Mesh generation; Neural networks; Remote sensing; Satellites; Urban planning; Change detection; Convolutional neural network; High spatial resolution; Landuse classifications; Misclassifications; Monitoring change; Satellite images; Supervised trainings; Image enhancement"
"Image denoising using a deep encoder-decoder network with skip connections","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059025113&doi=10.1007%2f978-3-030-04224-0_48&partnerID=40&md5=1705cb7421afcb268e5e94543f27dab5","In many areas images can be corrupted by various types of noise and therefore image denoising is a prerequisite. For example, medical images like the 4D-CT or ultrasound ones, are prone to noise and artifacts that can affect diagnostic confidence. Remote sensing is another field for which image preprocessing is mandatory to improve the quality of source images. Synthetic Aperture Radar (SAR) images are typically corrupted by multiplicative speckle noise. In this paper, a deep neural network able to deal with both additive white Gaussian and multiplicative speckle noises is developed, showing also some blind denoising capacity. The experiments on noisy images show that the proposal, which consists in a encoder-decoder, is efficient and competitive in comparison with state-of-the-art methods. © Springer Nature Switzerland AG 2018.","Additive and multiplicative noises; Deep learning; Encoder-decoder; Image denoising","Additives; Computerized tomography; Decoding; Deep learning; Deep neural networks; Diagnosis; Image enhancement; Medical imaging; Network coding; Radar imaging; Remote sensing; Speckle; Synthetic aperture radar; Additive and multiplicative noise; Blind denoising; Encoder-decoder; Image preprocessing; Multiplicative speckle noise; Source images; State-of-the-art methods; Synthetic aperture radar (SAR) images; Image denoising"
"DeepCS: Deep convolutional neural network and SVM based single image super-resolution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054865703&doi=10.1007%2f978-3-030-00807-9_1&partnerID=40&md5=76a34c29560b0259b952b532a0e8f27c","Computer based patient monitoring systems help in keeping track of the patients’ responsiveness to the treatment over the course of the treatment. Further, development of these kind of healthcare systems that require minimal or no human intervention form one of the most essential elements of smart cities. In order to make it a reality, the computer vision and machine learning techniques provide numerous ways to improve the efficiency of the automated healthcare systems. Image super-resolution (SR) has been an active area of research in the field of computer vision for the past couple of decades. The SR algorithms are offline and independent of image capturing devices making them suitable for various applications such as video surveillance, medical image analysis, remote sensing etc. This paper proposes a learning based SR algorithm for generating high resolution (HR) images from low resolution (LR) images. The proposed approach uses the fusion of deep convolutional neural network (CNN) and support vector machines (SVM) with regression for learning and reconstruction. Learning with deep neural networks exhibit better approximation and support vector machines work well in decision making. The experiments with the retinal images from RIMONE and CHASEDB have shown that the proposed approach outperforms the existing image super-resolution approaches in terms of peak signal to noise ratio (PSNR) as well as mean squared error (MSE). © Springer Nature Switzerland AG 2018.","Deep learning; Deep neural networks; Image super-resolution; Rectifier linear units","Computer aided analysis; Computer vision; Convolution; Decision making; Deep learning; Image analysis; Image segmentation; Learning algorithms; Mean square error; Medical computing; Medical imaging; Neural networks; Optical resolving power; Patient monitoring; Patient treatment; Pediatrics; Remote sensing; Security systems; Signal to noise ratio; Support vector machines; Deep convolutional neural networks; Image capturing devices; Image super resolutions; Linear units; Low resolution images; Machine learning techniques; Patient monitoring systems; Peak Signal to Noise Ratio (PSNR); Deep neural networks"
"Hybrid Adaptive Prediction Mechanisms with Multilayer Propagation Neural Network for Hyperspectral Image Compression","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042468507&doi=10.1007%2f978-3-319-75786-5_14&partnerID=40&md5=7b3026b5a0034496bc1072514b6b1758","Hyperspectral (HS) image is a three dimensional data image where the 3rd dimension carries the wealth of spectrum information. HS image compression is one of the areas that has attracted increasing attention for big data processing and analysis. HS data has its own distinguishing feature which differs with video because without motion, also different with a still image because of redundancy along the wavelength axis. The prediction based method is playing an important role in the compression and research area. Reflectance distribution of HS based on our analysis indicates that there is some nonlinear relationship in intra-band. The Multilayer Propagation Neural Networks (MLPNN) with backpropagation training are particularly well suited for addressing the approximation function. In this paper, an MLPNN based predictive image compression method is presented. We propose a hybrid Adaptive Prediction Mechanism (APM) with MLPNN model (APM-MLPNN). MLPNN is trained to predict the succeeding bands by using current band information. The purpose is to explore whether MLPNN can provide better image compression results in HS images. Besides, it uses less computation cost than a deep learning model so we can easily validate the model. We encoded the weights vector and the bias vector of MLPNN as well as the residuals. That is the only few bytes it then sends to the decoder side. The decoder will reconstruct a band by using the same structure of the network. We call it an MLPNN decoder. The MLPNN decoder does not need to be trained as the weights and biases have already been transmitted. We can easily reconstruct the succeeding bands by the MLPNN decoder. APM constrained the correction offset between the succeeding band and the current spectral band in order to prevent HS image being affected by large predictive biases. The performance of the proposed algorithm is verified by several HS images from Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) reflectance dataset. MLPNN simulation results can improve prediction accuracy; reduce residual of intra-band with high compression ratio and relatively lower bitrates. © 2018, Springer International Publishing AG, part of Springer Nature.","BP neural network; Data compression; Hyperspectral image; Image coding; Inter-band prediction; MLP neural network; Remote sensing","Backpropagation; Big data; Data compression; Data handling; Decoding; Deep learning; Forecasting; Hydraulic structures; Hyperspectral imaging; Image coding; Multilayer neural networks; Multilayers; Neural networks; Reflection; Remote sensing; Spectroscopy; Airborne visible/infrared imaging spectrometers; Approximation function; Backpropagation training; BP neural networks; Image compression methods; MLP neural networks; Non-linear relationships; Three-dimensional data; Image compression"
"Village detection based on deep semantic segmentation network in google earth satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052663582&doi=10.1117%2f12.2502849&partnerID=40&md5=05848e94e32b76a03fc96d538c878e24","Segmenting remote sensing images into no fly zone and open zone is the base of digital geo-fencing construction which is important for the development of unmanned aerial vehicle (UAV) management and control system. The human based segmentation is time consuming, labor intensive and cannot satisfy the real time requirement. Therefore, the automatic segmentation is strongly desired. This study, starting with the rural area in Google Earth satellite images, defines the village as the no fly zone, and other regions are defined as the open zone. The proposed method utilized a deep semantic segmentation network to detect village. A convolution encoder and decoder architecture of SegNet are applied to extract features, and a weighted soft-max classifier, in order to solve the imbalance of sample numbers, is chosen to label the image in pixel level. The results show that it is an effective way to detect the village and exhibit a high accuracy (98%). The method not only can be used as the first step of the automatic construction of digital geo-fencing, but also in the environmental analysis, disaster monitoring of rural area and many other applications. © 2018 SPIE.","Deep learning; Digital geo-fencing; SegNet; Semantic segmentation; Village detection","Antennas; Deep learning; Remote sensing; Rural areas; Semantic Web; Semantics; Unmanned aerial vehicles (UAV); Automatic construction; Automatic segmentations; Digital geo-fencing; Environmental analysis; Management and controls; Real time requirement; SegNet; Semantic segmentation; Image segmentation"
"One-dimensional convolution neural networks for object-based feature selection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058983601&doi=10.1117%2f12.2325640&partnerID=40&md5=8f291ab4e9ad908044bbca5870c7bcd0","Recently, the new Geographic object-based image analysis (GEOBIA) was proposed as an alternative classification approach to pixel based ones. In GEOBIA, image segments can be depicted with various attributes such as spectral, texture, shape, deep features and context, and hence final classification can produce better land cover/use map. The presence of such a large number of features poses significant challenges to standard machine learning methods and has rendered many existing classification techniques impractical. In this work, we are interested to feature selection techniques, which are employed to reduce the dimensionality of the data while keeping the most of its expressive power. Inspired by recent works in remote sensing using Convolutional Neural Networks (CNNs), especially for hyperspectral band selection, a feature selection approach based on One-Dimensional Convolutional Neural Networks (1-D CNN) is proposed in this study. All object-based features are used to train the 1-D CNN to obtain well trained model. After testing different feature combinations, we use the well trained model to obtain their test classification accuracies, and finally we select the subset of features with the highest precision. In our experiments, we evaluate our feature selection approach on 30-cm resolution colour infrared (CIR) aerial orthoimagery. A multi-resolution segmentation is performed to segment the images into regions, which are characterized later using various spectral, textural and spatial attributes to form the final object-based feature dataset. The obtained experimental results show that the proposed method can achieve satisfactory results when compared with traditional feature selection approaches. © 2018 SPIE.","1-D convolutional neural network; Deep learning; Feature selection; Geographic object-based image analysis; Very high spatial resolution imagery","Antennas; Convolution; Deep learning; Image analysis; Image segmentation; Neural networks; Remote sensing; Classification accuracy; Classification approach; Classification technique; Convolution neural network; Convolutional neural network; Feature selection techniques; Geographic object-based image analysis; Very high spatial resolutions; Feature extraction"
"Performance Enhancement of Satellite Image Classification Using a Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029518718&doi=10.1007%2f978-3-319-64861-3_63&partnerID=40&md5=89c3d8c3eee1bf6ec7358302f721f28f","With dramatically increasing of very resolution of satellite imaging sensors and the daily increasing of remote sensing databases, image classification has been gaining prominence in remote sensing applications. Convolutional neural networks (CNNs) techniques have already been outperforming other classification approaches in various domains. In this paper, we propose an enhance classification of satellite image using CNNs. high information content of satellite images alongside high computational calculations needed by CNNs, that make performance issues very crucial. The enhancement process is based on an efficient selection of adequate image scales that perform respectively, high classification accuracy with least computational burdens. We evaluate the proposed method on three state-of-the-art datasets: UC Merced Land Use Dataset, WHU-RS Dataset and Brazilian Coffee Scenes Dataset. The proposed method leads to a performance enhancement, as opposed to using original scales directly. © 2018, Springer International Publishing AG.","Convolutional neural network; Deep learning; Performance enhancement; Satellite image classification",
"Non-dominant object recognition using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072899047&partnerID=40&md5=d1468207fe2de0f445cbdc0d041c8e67","We identify a challenging problem of nondominant object recognition with applications in medical/remote sensing images, and investigate convolutional Neural Network (CNN)'s learning capability towards solving this problem through carefully designed experiments. In particular, we trained multiple CNN models with different layer structures and model sizes in terms of the depth and the width in each layer, and compared their testing performances on recognizing non-dominant objects. Insights about the CNN's architecture and performance gained from the experimental results are analyzed, and can be used to guide the design of CNN models in related applications such as tumor detection in medical images. © 2018 International Conference on Image Processing, Computer Vision, and Pattern Recognition, IPCV 2018.All right reserved.","Convolutional neural networks; Deep learning; Image classification; Object detection","Computer vision; Convolution; Deep learning; Deep neural networks; Image classification; Medical imaging; Neural networks; Object detection; Architecture and performance; Convolutional neural network; Designed experiments; Different layers; Learning capabilities; Model size; Testing performance; Tumor detection; Object recognition"
"Improvement of persistent tracking in wide area motion imagery by CNN-based motion detections","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059037375&doi=10.1117%2f12.2325367&partnerID=40&md5=da4585abf95436f46104ebf6dfe11976","Reliable vehicle detection and tracking in wide area motion imagery (WAMI), a novel class of imagery captured by airborne sensor arrays and characterized by large ground coverage and low frame rate, are the basis for higher-level image analysis tasks in wide area aerial surveillance. Possible applications include real-time traffic monitoring, driver behavior analysis, and anomaly detection. Most frameworks for detection and tracking in WAMI data rely on motion-based input detections generated by frame differencing or background subtraction. Subsequently employed tracking approaches aim at recovering missing motion detections to enable persistent tracking, i.e. continuous tracking also for vehicles that become stationary. Recently, a moving object detection method based on convolutional neural networks (CNNs) showed promising results on WAMI data. Therefore, in this work we analyze how CNN-based detection methods can improve persistent WAMI tracking compared to detection methods based on difference images. To find detections, we employ a network that uses consecutive frames as input and computes detection heatmaps as output. The high quality of the output heatmaps allows for detection localization by non-maximum suppression without further post processing. For quantitative evaluation, we use several regions of interest defined on the publicly available, annotated WPAFB 2009 dataset. We employ the common metrics precision, recall, and f-score to evaluate detection performance, and additionally consider track identity switches and multiple object tracking accuracy to assess tracking performance. We first evaluate the moving object detection performance of our deep network in comparison to a previous analysis of difference-image based detection methods. Subsequently, we apply a persistent multiple hypothesis tracker with WAMI-specific adaptations to the CNN-based motion detections, and evaluate the tracking results with respect to a persistent tracking ground truth. We yield significant improvement of both the motion-based input detections and the output tracking quality, demonstrating the potential of CNNs in the context of persistent WAMI tracking. © 2018 SPIE.","Deep learning; Moving object detection; Multi-target tracking; Wide area aerial surveillance; Wide area motion imagery","Antennas; Deep learning; Image coding; Image enhancement; Motion analysis; Neural networks; Object recognition; Remote sensing; Target tracking; Aerial surveillance; Convolutional neural network; Moving-object detection; Multi-target tracking; Multiple hypothesis tracker; Multiple object tracking; Non-maximum suppression; Wide-area motion imageries; Object detection"
"SAR ATR in the phase history domain using deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058983550&doi=10.1117%2f12.2325365&partnerID=40&md5=44e383f88b4a233c5f8a42192de49d6d","Synthetic aperture radar (SAR) automatic target recognition (ATR) has been an interesting topic of research for decades. Existing methods perform the ATR task after image formation. However, in principle, image formation does not provide any new information regarding the classification task and it may even cause some information loss. Motivated by this, in this paper, we examine two SAR ATR frameworks that work in the phase history domain. In the first framework, we feed the complex-valued phase histories to a deep convolutional neural network (CNN) directly, and in the second one, we perform image formation, phase removal, and phase history generation before feeding the data to the CNN. CNNs are known for their superior performance on image classification tasks. The effectiveness of CNNs is based on dependency patterns in a given input. Thus, the input of CNNs is not limited to images but any input exhibiting such dependencies. Since complex-valued phase histories also have such a structure, they can be the input of a CNN. We perform ATR experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) database and compare the results of image-based and phase history-based classification. © 2018 SPIE.","Automatic target recognition; Convolutional neural networks; Deep learning; Synthetic aperture radar","Automatic target recognition; Classification (of information); Complex networks; Convolution; Deep learning; Deep neural networks; Image processing; Neural networks; Optical character recognition; Radar imaging; Remote sensing; Synthetic aperture radar; Classification tasks; Complex-valued; Convolutional neural network; Deep convolutional neural networks; Image-based; Information loss; Phase history; Stationary targets; Radar target recognition"
"Electric pole detection using deep network based object detector","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057322715&doi=10.1117%2f12.2323773&partnerID=40&md5=90b5defae25098aaea5ca8b4d66dd9ef","Efficient and safe facility maintenance has been a serious social problem due to the decline in labor force, facility deterioration over the years, and the rise of large-scale natural disasters. For electric power companies, maintaining and inspecting power equipment spread in wide areas is an important management issues to deal with. Identifying the electric poles that require maintenance is one of the essential inspection tasks. To identify the electric poles in an image, several methods focusing on their unique features such as color and shape have been proposed. However, this feature-based approach suffers from noise caused by shooting conditions. Another approach using a laser scanning technique requires high computational cost for handling the obtained point cloud data. We explored methods to efficiently detect the electric poles in a large number of images taken by a vehicle-mounted camera run in an urban area and its suburbs. Here, we show that a single shot MultiBox detector (SSD), which has been successfully used for object detection in an image, can be effectively applied to the task. We trained SSD models using around 600 supervised image data and evaluated the performance with 100 test images. In the evaluation, we examined whether pole-like objects such as telephone poles, traffic light poles, or trunks of trees can be distinguished from the electric poles. We also evaluated the influence of the background and exteriors attached to the pole. We found that the electric poles can be detected with an average precision (AP) of 72.2%. Our results demonstrate operational feasibility of the autonomous electric pole inspection system that implements a deep network based object detector. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep learning; Electric pole detection; Facility maintenance; Vehicle-mounted camera","Cameras; Deep learning; Deterioration; Disasters; Electric utilities; Inspection; Maintenance; Poles; Remote sensing; Urban planning; Computational costs; Electric poles; Electric power company; Facility maintenance; Feature based approaches; Management issues; Operational feasibility; Shooting conditions; Object detection"
"Palm trees detecting and counting from high-resolution WorldView-3 satellite images in United Arab Emirates","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056451436&doi=10.1117%2f12.2325733&partnerID=40&md5=679aea928ea844db61c2d8a4fdaa0870","The United Arab Emirates (UAE) is one of the fastest agriculture economical growing country in the world. One aspect of this agriculture growth is the development of the date palm trees sector in the UAE. The date palm tree is considered one of the oldest and most widely cultivated tree, which is commercially the most important tree in the life of its people and their heritage. Moreover, the date palm tree was believed to be a part of the UAE strategy to control desertification. With this huge investment and interest in palm trees in the UAE, there is limited knowledge of the actual tree counts and their exact spatial locations, which is a requirement for any agricultural census. WorldView-3 satellite images were used to develop an algorithm to detect and count palm trees in the UAE. The processing was done in two steps: The first step is to detect palm trees which involved supervised classification using maximum likelihood with four feature classes: Red, Blue, Green and Near infrared (NIR) bands associated with palm trees objects taken by the labeling. The second step is to count palm trees which involved extracting local spatial maxima of Laplacian blob from Normalized Difference Vegetation Index (NDVI) masking. The algorithm was tested in different regions of interest in AlAin city, part of the capital Emirate Abu Dhabi. The algorithm and final results are compared with ground truth images for accuracy assessment. The results were satisfactory with an accuracy of 89% and higher and very minimum negligible misclassification. © SPIE. Downloading of the abstract is permitted for personal use only.","Classification; Counting; Deep learning; Object detection; Palm trees; Vegetation","Classification (of information); Cultivation; Deep learning; Ecosystems; Forestry; Hydrology; Infrared devices; Maximum likelihood; Object detection; Palmprint recognition; Remote sensing; Vegetation; Accuracy assessment; Counting; Near-infrared bands; Normalized difference vegetation index; PaLM-tree; Regions of interest; Supervised classification; United Arab Emirates; Trees (mathematics)"
"Semi-supervised deep autoencoder network for graph-based dimensionality reduction of hyperspectral imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050872194&doi=10.1117%2f12.2303912&partnerID=40&md5=64b3b243b9a5bc32a21b9be45f23d17f","Nonlinear graph-based dimensionality reduction algorithms have been shown to be very effective at yielding low-dimensional representations of hyperspectral image data. However, the steps of graph construction and eigenvector computation often suffer from prohibitive computational and memory requirements. In the paper, we develop a semi-supervised deep auto-encoder network (SSDAN) that is capable of generating mappings that approximate the embeddings computed by the nonlinear DR methods. The SSDAN is trained with only a small subset of the original data and enables an expert user to provide constraints that can bias data points from the same class towards being mapped closely together. Once the SSDAN is trained on a small subset of the data, it can be used to map the rest of the data to the lower dimensional space, without requiring complicated out-of-sample extension procedures that are often necessary in nonlinear DR methods. Experiments on publicly available hyperspectral imagery (Indian Pines and Salinas) demonstrate that SSDANs compute low-dimensional embeddings that yield good results when input to pixel-wise classification algorithms. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Autoencoder; Deep learning; Dimensionality reduction; Graph-based method; Hyperspectral image","Deep learning; Hyperspectral imaging; Remote sensing; Spectroscopy; Auto encoders; Dimensionality reduction; Dimensionality reduction algorithms; Eigenvector computations; Graph-based methods; Hyper-spectral imageries; Hyperspectral image datas; Low-dimensional representation; Graphic methods"
"A large-scale data-oriented intelligent system for urban growth simulation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077004330&doi=10.1007%2f978-981-13-2330-0_12&partnerID=40&md5=d8392d860fbf16bbe58009b886c603ee","With more than 50% of world population living in cities (as per UN report), urbanization is one of the pressing problems, which has been recognized by the United Nations. One of the outcomes of large-scale urbanization in cities is uncontrolled urban growth which has many potentially adverse effects on the environment. Due to this, various studies have been conducted on methods of simulating urban growth which assists in making appropriate decisions and plans. However, the studies involving urban growth simulation are very much dependent on human ingenuity because factors behind urban growth and their relationships are varied and complex. Therefore, scholars generally do not have consensus on these aspects which leads to difficulty in designing large-scale systems. Furthermore, the ability of machines to extract patterns from data has improved significantly in the past few decades with the advent of deep neural networks and high-performance computing architectures. In this work, we propose an intelligent system to model urban growth by minimizing the human ingenuity constraint from the simulation process. The system is based on machine learning, deep learning methods, and high-performance computing systems. Presently, the system uses the satellite remote sensing imagery and transportation network data for the urban growth simulation. We have experimented our proposed architecture on data gathered for the city of Mumbai, India, and have found encouraging performance compared to the existing learning-based methods. © Springer Nature Singapore Pte Ltd. 2018.",,
"Automated remote monitoring of offshore assets using satellite imagery and machine learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050526909&partnerID=40&md5=b0be284782c2bc3b47ff43ef1ee37996","Satellite imaging technology continues to progress toward providing reliable, real-time remote monitoring of offshore asset health and safety. The fast growth of satellite constellations which offer high-resolution observation data over all weather conditions generates opportunities to monitor, predict, and report on a variety of incidents. This paper discusses a new satellite imaging monitoring technology with a case study focus on marine oil slick detection. Currently, typical applications of this technology face several limitations common to many remote sensors, such as reliance on a human-in-the-loop and the limitations imposed by individual sensors onboard a weight- and downlink-limited spacecraft. This paper discusses recent work in which deep learning was applied to multiple satellite imagery streams operating in different spectral bands to automatically detect oil slicks on the ocean surface. Different imagery streams were registered in space and time to create multidimensional composite images. These images were then used to train a deep neural network classifier to detect if oil is present or not present. The results of this work demonstrate that it is now possible to overcome several key limitations of satellite sensing technology. First, by leveraging multiple satellite sensors in parallel, both primary sensing modalities can be used to overcome the shortcomings of either. Second, initial results from the output of the machine learning classifier will be presented, showing that it is both feasible and advantageous to trust an algorithm for automatically monitoring offshore assets for leak events. When the human is removed from the loop, the time and cost it would have taken to manually review satellite imagery are significantly reduced, in turn increasing efficiency and operability. Lastly, additional potential applications that this work enables will be discussed. Multispectral imagery fusion overcomes the individual limitations of the sensors for oil slick detection and asset monitoring. Machine learning overcomes the human-in-the-loop limitations, allowing for reliable, automated, and large-scale monitoring necessary for offshore applications. © 2018, Offshore Technology Conference.",,"Artificial intelligence; Deep neural networks; Monitoring; Remote control; Remote sensing; High resolution observations; Large-scale monitoring; Monitoring technologies; Multi-spectral imagery; Neural network classifier; Offshore applications; Real-time remote monitoring; Satellite constellations; Satellite imagery"
"3rd International Conference on Signal and Information Processing, Networking and Computers, ICSINC 2017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040120821&partnerID=40&md5=24e5eb915f85ec18d3b5c39ab7e0f6ff","The proceedings contain 61 papers. The special focus in this conference is on Signal, Information Processing, Networking and Computers. The topics include: An Energy Detection Based on Coefficient of Variation for Spectrum Sensing in Cognitive Radio; performance Analysis for User-Centric Cloud Radio Access Network in Millimeter Wave; millimeter Wave Cloud Radio Access Network Coverage and Capacity; Optimal Downtilts for 3D Beamforming Based on Greedy Algorithm in Massive MIMO Networks with Imperfect CSI; Dynamic User Scheduling Algorithms for Massive MIMO Multicast System; a Bat Algorithm Based on Centroid Strategy; an Optimized Daisy-chain Topology for Multi-load Interconnection in High–speed and High–density Electronic Systems; Realization and Optimization of Pulse Compression Algorithm on OpenCL-Based FPGA Heterogeneous Computing Platform; The Research of SAR Processing Performance Based on Multi-core GPU; A PHY-Based Secret Sharing Scheme in MIMO Systems; ship Detection in Optical Satellite Images Based on Sparse Representation; application of Back Propagation Neural Network with Simulated Annealing Algorithm in Network Intrusion Detection Systems; an Improved Binary Bee Colony Algorithm for Satellite Resource Scheduling Method; Automatic Liver Segmentation on CT Images; An Improved Blind Spectrum Sensing Algorithm Based on QR Decomposition and SVM; Sentiment Analysis Using Modified LDA; co-training Based on Multi-type Text Features; GNSS Spoofing Jamming Recognition Based on Machine Learning; TinyPEP: Tiny Pairwise-Key Establishment Protocol for Wireless Sensor Networks; An Innovative Indoor Location Algorithm Based on Supervised Learning and WIFI Fingerprint Classification; long Short-Term Memory Network for Wireless Channel Prediction; deep Learning and Machine Learning for Object Detection in Remote Sensing Images.",,
"8th Pacific Rim Symposium on Image and Video Technology, PSIVT 2017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048550265&partnerID=40&md5=925d172c3598a3b989316081f13cd31c","The proceedings contain 36 papers. The special focus in this conference is on Image and Video Technology. The topics include: Parallel Education Systems Under Perspective of System Construction for New IT Era; foot Modeling Based on Machine Vision and Social Manufacturing Research; computerized Adaptive English Ability Assessment Based on Deep Learning; china-Finland EduCloud Platform Towards Innovative Education; SPSE: A Smart Phone-Based Student Evaluation; research on the Construction of Corpus for Automatic Solution of Elementary Mathematics Statistics Applications; constructing a Learning Map with Lattice for a Programming Course; data Mining as a Cloud Service for Learning Artificial Intelligence; crowd Counting from a Still Image Using Multi-scale Fully Convolutional Network with Adaptive Human-Shaped Kernel; biometric System Based on Registration of Dorsal Hand Vein Configurations; On Road Vehicle Detection Using an Improved Faster RCNN Framework with Small-Size Region Up-Scaling Strategy; Fast Haze Removal of UAV Images Based on Dark Channel Prior; watercolour Rendering of Portraits; blind Image Deblurring via Salient Structure Detection and Sparse Representation; blur Estimation for Natural Edge Appearance in Computational Photography; structure-Preserving Texture Smoothing via Adaptive Patches; robust Blind Deconvolution Using Relative Total Variation as a Regularization Penalty; an Aircraft Tracking Method in Simulated Infrared Image Sequences Based on Regional Distribution; DESIS - DLR Earth Sensing Imaging Spectrometer; FireBIRD Mission Data for Gas Flaring Analysis; a Multi-scale Triplet Deep Convolutional Neural Network for Person Re-identification; Automatic Ship Detection on Multispectral and Thermal Infrared Aerial Images Using MACS-Mar Remote Sensing Platform; context-Awareness Based Adaptive Gaussian Mixture Background Modeling.",,
"19th Pacific-Rim Conference on Multimedia, PCM 2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057246470&partnerID=40&md5=64e6c86a5145909e8190f3f5dcb1b542","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.",,
"19th Pacific-Rim Conference on Multimedia, PCM 2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057222002&partnerID=40&md5=b43bc9db0a6c6dd8ce8e69bb3979fe1a","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.",,
"19th Pacific-Rim Conference on Multimedia, PCM 2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054544658&partnerID=40&md5=78422fb46096c60049b42c102cf40f2e","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.",,
"From Google Maps to a fine-grained catalog of street trees","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034650490&doi=10.1016%2fj.isprsjprs.2017.11.008&partnerID=40&md5=684612dea51a1ddb8d9fd9a7b0f1e953","Up-to-date catalogs of the urban tree population are of importance for municipalities to monitor and improve quality of life in cities. Despite much research on automation of tree mapping, mainly relying on dedicated airborne LiDAR or hyperspectral campaigns, tree detection and species recognition is still mostly done manually in practice. We present a fully automated tree detection and species recognition pipeline that can process thousands of trees within a few hours using publicly available aerial and street view images of Google MapsTM. These data provide rich information from different viewpoints and at different scales from global tree shapes to bark textures. Our work-flow is built around a supervised classification that automatically learns the most discriminative features from thousands of trees and corresponding, publicly available tree inventory data. In addition, we introduce a change tracker that recognizes changes of individual trees at city-scale, which is essential to keep an urban tree inventory up-to-date. The system takes street-level images of the same tree location at two different times and classifies the type of change (e.g., tree has been removed). Drawing on recent advances in computer vision and machine learning, we apply convolutional neural networks (CNN) for all classification tasks. We propose the following pipeline: download all available panoramas and overhead images of an area of interest, detect trees per image and combine multi-view detections in a probabilistic framework, adding prior knowledge; recognize fine-grained species of detected trees. In a later, separate module, track trees over time, detect significant changes and classify the type of change. We believe this is the first work to exploit publicly available image data for city-scale street tree detection, species recognition and change tracking, exhaustively over several square kilometers, respectively many thousands of trees. Experiments in the city of Pasadena, California, USA show that we can detect &gt;70% of the street trees, assign correct species to &gt;80% for 40 different species, and correctly detect and classify changes in &gt;90% of the cases. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Image interpretation; Street trees; Urban areas; Very high resolution","Deep learning; Forestry; Learning systems; Neural networks; Pipelines; Supervised learning; Convolutional neural network; Discriminative features; Image interpretation; Probabilistic framework; Street trees; Supervised classification; Urban areas; Very high resolution; Trees (mathematics); algorithm; classification; computer vision; image analysis; image classification; image resolution; lidar; machine learning; quality of life; software; texture; tree; urban area; California; Pasadena; United States"
"Computational Optical Sensing and Imaging, COSI 2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051247040&partnerID=40&md5=592bc145e4770885fa672c5161d89849","This proceedings contains 53 papers. COSI encompasses the latest advances in computational imaging research. Representative topics include compressive sensing, tomographic imaging, light-field sensing, digital holography, SAR, phase retrieval, computational spectroscopy, blind deconvolution and phase diversity, pointspread function engineering and digital/optical super resolution. The conference topics include: Non-Line-of-Sight Imaging using Superheterodyne Interferometry; Resolving Non Line-of-Sight (NLoS) motion using Speckle; Indirect Imaging Using Correlography; Micro Resolution Time-of-Flight Imaging; Passive Non-line-of-sight Source Classification from Coherence Measurements; Diffuse Time-of-flight Imaging with a Single-Photon Camera; Imaging with Phasor Fields for Non-Line-of Sight Applications; Indirect Imaging Using Virtualized Pattern Projection; Aparna Viswanath, Muralidhar M. Balaji, Prasanna Rangarajan, Duncan MacFarlane, and Marc P Christensen; Multi-layered Born scattering model for 3D phase imaging with multiple scattering objects; Depth-resolved Lensless Imaging; 3D Fluorescence Microscopy with DiffuserCam; Double-Cubic Point Spread Function for 3D Extended-Depth Localization Microscopy; Depth Sensitivity Improvement of Region-of-Interest Diffuse Optical Tomography from Superficial Signal Regression; Manob Jyoti Saikia, Rakesh Manjappa, Kunal Mankodiya, and Rajan Kanhirodan; On Block-Reference Coherent Diffraction Imaging; Deep Learning Enhances Mobile Microscopy; A Novel Optical Structure to Implement One-dimensional Fourier Transform with Spherical Lenses; Seeing through Multimode Fibers with Deep Learning; Plenoptic imaging from intensity correlations; A new method for designing highly efficient metasurface devices: Local Phase Method; Novel Optimizations for Phase Retrieval; Phase Retrieval Based on Wave Modulation; Enhanced Phase Retrieval using Quantum Illumination; Temporal Super-resolution Full Waveform LiDAR; Super-Resolution Imaging Based on Spectral Dimensional Information; Remote Sensing of Photoplethysmogram using Multi Spot Illumination; Enlarged Field of View Scattering Imaging Using Speckle Autocorrelation; Mitigating metalens aberrations via computational imaging; Shane Colburn and Arka Majumdar; Binarization threshold optimization of ghost imaging; Ghost Imaging With Gram-Schmidt Orthogonalization; Comparison between ghost imaging and traditional active optical imaging; Demonstration of computational temporal ghost imaging: detecting fast signals beyond bandwidth of detectors; Imaging the Joint Probability Distribution of Spatially Entangled Photon Pairs with a Camera; Optimization of light field fluctuation patterns in ghost imaging by mutual coherence minimization based on dictionary learning; Characterizing the optical memory effect using quantum illumination; Compressive Ultrafast Single Pixel Camera; Encrypted Single Pixel Imaging with Basis Illumination Patterns; Correlation Matrix Estimation from Compressed Measurements in a Pattern Recognition System; Exploiting Inter Voxel Correlation in Compressed Computational Imaging; Naren Viswanathan, Suresh Venkatesh, and David Schurig; Double-threshold Denoising for Single-pixel Camera; Multi-object Recognition in Turbid Water Using Compressive Sensing; Covariance Matrix Estimation from Multiple Subsets in Compressive Spectral Imaging; Compressive Spectral Polarization Imaging Using a Single Pixel Detector; Subsampling Schemes for the 2D Nuclear Magnetic Resonance Spectroscopy; Compressive coded LED and coded aperture spectral video system; Spatial Super-resolution reconstruction via SSCSI Compressive Spectral Imagers; Snapshot Compressive Spectral+Depth Imaging with Color-Coded Apertures; Spectral zooming in SSCSI Compressive Spectral Imagers; Compressive Photon-Sieve Spectral Imaging; Field-varying aberration recovery in EUV microscopy using mask roughness; Computational Cannula Microscopy: Utilizing a Simple Glass Needle for Imaging; Integral Refractive Index Imaging of Flowing Cell Nuclei; Compressive hyperspectral imaging for snapshot multi-channel fluorescence microscopy; Cell imaging by phase extraction neural network (PhENN); Quantitative Phase Maps of Live Cells Classified By Transfer Learning and Generative Adversarial Network (GAN); A Neuro-Inspired Model for Image Motion Processing; Optical Sensing and Control Based on Machine Learning; Deep Learned Phase Mask for Single Image Depth Estimation and 3D scanning; Neural Network classification for intensity imaging through multimode optical fibres; Phase Unwrapping Using Residual Neural Networks; Bending-Independent Imaging through Glass-Air Disordered Fiber Based on Deep Learning; Speckle suppression using the convolutional neural network with an exponential linear unit. The key terms of this proceedings include Compressive Sensing, Computational microscopy, Depth-resolved and turbid imaging, Imaging through aberrations, Structured illumination and super resolution, Indirect and non-line-of-sight imaging, Machine Learning in Computational Sensing and Imaging, Phase Retrieval, Postdeadline Papers - COSI, Quantum Computational Imaging.",,
"Satellite super-resolution images depending on deep learning methods: A comparative study","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049241923&doi=10.1109%2fICSPCC.2017.8242625&partnerID=40&md5=f6a21c3541e6b462c7e876a5f4b2354e","The deep learning neural network is a recent development that has become the subject of research in the computer vision and remote sensing disciplines. Super resolution (SR) images can be obtained using deep neural network methods that achieve a higher performance than all previous traditional methods. Here, in this study, the objective is to describe existing deep learning methods for SR satellite images. Different satellite data are used to predict the performance of each deep learning model. This article presents a brief overview of most deep learning techniques and compares them to obtain a more effective and efficient model. The deep network cascade model outperforms other deep learning algorithms; this algorithm is dependable in the reconstruction process for obtaining SR images and overcomes some drawbacks found in traditional reconstruction algorithms. The sparse coding network method remains valuable, and with some enhancements, further improvement in results can be achieved. © 2017 IEEE.","Deep learning; Deep Network Cascade; Remote Sensing; Satellite Images; Sparse Coding Network; Super-resolution","Codes (symbols); Deep learning; Image coding; Learning algorithms; Network coding; Optical resolving power; Remote sensing; Satellites; Deep networks; Learning neural networks; Neural network method; Reconstruction algorithms; Reconstruction process; Satellite images; Sparse coding; Super resolution; Deep neural networks"
"Hyperspectral image classification based on convolutional neural network and dimension reduction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050360175&doi=10.1109%2fCAC.2017.8243039&partnerID=40&md5=1847f7936d973f861c020e7f51028480","One of the most important ways to explore the information in hyperspectral images (HSIs) is accurate classification of targets. Deep learning algorithm has made a great breakthrough in many areas due to its strong ability of data mining. Typical deep learning models such as convolutional neural network (CNN), deep belief network (DBN) and so on, not only combines the advantages of unsupervised and supervised learning but also have a good performance in large data classification. In this paper, a hybrid classification method combined CNN with dimension reduction (DR) operated by principal component analysis (PCA) is proposed, which fully takes the spatial information and the spectral characteristics of HSI into account. Furthermore, in order to solve the problem of sample imbalance, virtual samples are introduced to the experiments. Numerical results show that the proposed DR-CNN method, especially with virtual samples (named as DR-CNN-vs method in this paper) has promising prospect in the field of HSI classification. © 2017 IEEE.","classification; convolutional neural network (CNN); dimension reduction (DR)principal component analysis (PCA); remote sensing image; virtual samples","Convolution; Data mining; Deep learning; Image classification; Independent component analysis; Learning algorithms; Neural networks; Numerical methods; Principal component analysis; Remote sensing; Spectroscopy; Convolutional neural network; Convolutional Neural Networks (CNN); Deep belief network (DBN); Dimension reduction; Hybrid classification; Remote sensing images; Spectral characteristics; Virtual sample; Classification (of information)"
"3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041173338&doi=10.1109%2fICCV.2017.605&partnerID=40&md5=119726a6bccd2d32810c3f1e9e1c31ba","Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN)for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object's class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods. © 2017 IEEE.",,"Computer vision; Formal languages; Neural networks; Recurrent neural networks; Reinforcement learning; Remote sensing; Semantics; Classification results; Convolutional neural network; Discriminative features; High level semantics; High quality images; Learning methods; Object localization; Recurrent neural network (RNN); Deep learning"
"Combining Unmixing and Deep Feature Learning for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048304181&doi=10.1109%2fDICTA.2017.8227419&partnerID=40&md5=0556855b365a167fb63963b09e4614bc","Image classification is one of the critical tasks in hyperspectral remote sensing. In recent years, significant improvement have been achieved by various classification methods. However, mixed spectral responses from different ground materials still create confusions in complex scenes. In this regard, unmixing approaches are being successfully carried out to decompose mixed pixels into a collection of spectral signatures. Considering the usefulness of these techniques, we propose to utilize the unmixing results as an input to classifiers for better classification accuracy. We propose a novel band group based structure preserving nonnegative matrix factorization (NMF) method to estimate the individual spectral responses from different materials within different ranges of wavelengths. Then we train a convolutional neural network (CNN) with the unmixing results to generate powerful features and eventually classify the data. This method is evaluated on a new dataset and compared with several state-of-the-art models, which shows the promising potential of our method. © 2017 IEEE.","Convolutional Neural Network; Image Classification; Nonnegative Matrix Factorization; Unmixing","Classification (of information); Convolution; Deep learning; Factorization; Image classification; Neural networks; Remote sensing; Spectroscopy; Classification accuracy; Classification methods; Convolutional neural network; Convolutional Neural Networks (CNN); Hyperspectral remote sensing; Nonnegative matrix factorization; Nonnegative matrix factorization method (NMF); Unmixing; Matrix algebra"
"Disaster detection from aerial imagery with convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046417494&doi=10.1109%2fKCIC.2017.8228593&partnerID=40&md5=528e7a1adc914706aabbd9f6257d8ad0","In recent years, analysis of remote sensing imagery is imperatives in the domain of environmental and climate monitoring primarily for the application of detecting and managing a natural disaster. Satellite imagery or aerial imagery is beneficial because it can widely capture the condition of the surface ground and provides a massive amount of information in a piece of satellite imagery. Since obtaining satellite imagery or aerial imagery is getting more ease in recent years, landslide detection and flood detection is highly in demand. In this paper, we propose automatic natural disaster detection particularly for landslide and flood detection by implementing convolutional neural network (CNN) in extracting the feature of disaster more effectively. CNN is robust to shadow, able to obtain the characteristic of disaster adequately and most importantly able to overcome misdetection or misjudgment by operators, which will affect the effectiveness of disaster relief. The neural network consists of 2 phases: Training phase and testing phase. We created training data patches of pre-disaster and post-disaster by clipping and resizing aerial imagery obtained from Google Earth Aerial Imagery. We are currently focusing on two countries which are Japan and Thailand. Training dataset for both landslide and flood consist of 50000 patches. All patches are trained in CNN to extract region where changes occurred or known as disaster region occurred without delay. We obtained accuracy of our system in around 80%-90% of both disaster detections. Based on the promising results, the proposed method may assist in our understanding of the role of deep learning in disaster detection. © 2017 IEEE.","aerial imagery; change detection; disaster detection; flood convolutional neural network; landslide","Aerial photography; Antennas; Convolution; Deep learning; Disaster prevention; Disasters; Floods; Intelligent computing; Landslides; Neural networks; Remote sensing; Aerial imagery; Amount of information; Change detection; Climate monitoring; Convolutional neural network; Convolutional Neural Networks (CNN); Landslide detection; Remote sensing imagery; Satellite imagery"
"Estimating Ground-Level PM2.5 by Fusing Satellite and Station Observations: A Geo-Intelligent Deep Learning Approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037526810&doi=10.1002%2f2017GL075710&partnerID=40&md5=6797f187e1fad2e177de2db601eac8b1","Fusing satellite observations and station measurements to estimate ground-level PM2.5 is promising for monitoring PM2.5 pollution. A geo-intelligent approach, which incorporates geographical correlation into an intelligent deep learning architecture, is developed to estimate PM2.5. Specifically, it considers geographical distance and spatiotemporally correlated PM2.5 in a deep belief network (denoted as Geoi-DBN). Geoi-DBN can capture the essential features associated with PM2.5 from latent factors. It was trained and tested with data from China in 2015. The results show that Geoi-DBN performs significantly better than the traditional neural network. The out-of-sample cross-validation R2 increases from 0.42 to 0.88, and RMSE decreases from 29.96 to 13.03 μg/m3. On the basis of the derived PM2.5 distribution, it is predicted that over 80% of the Chinese population live in areas with an annual mean PM2.5 of greater than 35 μg/m3. This study provides a new perspective for air pollution monitoring in large geographic regions. ©2017. American Geophysical Union. All Rights Reserved.","deep learning; geo-intelligent; PM2.5; satellite remote sensing","Pollution; Pollution detection; Remote sensing; Satellites; Air pollution monitoring; Deep belief networks; Geo-intelligent; Geographical correlation; Learning architectures; PM2.5; Satellite observations; Satellite remote sensing; Deep learning; artificial intelligence; artificial neural network; atmospheric pollution; model validation; particulate matter; pollution monitoring; remote sensing; satellite data; spatiotemporal analysis; China"
"A hybrid deep learning approach for texture analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046166863&doi=10.1109%2fICMIP.2017.5&partnerID=40&md5=5ebb3b70324576a798cf5cd15f094402","Texture classification is a problem that has variousapplications such as remote sensing and forest speciesrecogni- tion. Solutions tend to be custom fit to the datasetused but fails to generalize. The Convolutional NeuralNetwork (CNN) in combination with Support Vector Machine(SVM) form a robust selection between powerful invariantfeature extractor and accurate classifier. The fusion ofexperts provides stability in classification rates amongdifferent datasets. © 2017 IEEE.","Classifications; CNN; Confusion matrix; Deep learning; Fusion; SVM; Texture","Classification (of information); Fusion reactions; Image processing; Remote sensing; Support vector machines; Textures; Classification rates; Confusion matrices; Custom fits; Learning approach; Texture analysis; Texture classification; Deep learning"
"An Unsupervised Convolutional Feature Fusion Network for Deep Representation of Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038854510&doi=10.1109%2fLGRS.2017.2767626&partnerID=40&md5=2987a96e808f650f85843e8aafe3aad9","Unsupervised learning of a convolutional neural network (CNN) is a feasible method to represent and classify remote sensing images, where labeling the observed data to prepare training samples is a highly expensive and time-consuming task. In this letter, we propose an unsupervised convolutional feature fusion network to formulate an easy-to-train but effective CNN representation of remote sensing images. The efficiency and effectiveness are derived from the following two aspects. First, the proposed method trains a deep CNN through unsupervised learning of each CNN layer in a greedy layer-wise manner, which makes the training relatively easy and efficient. Second, the feature fusion strategy in the proposed network can effectively use both the information from individual layers and the important interactions between different layers. As a result, the proposed network requires only several layers to obtain comparable or even better results than very deep networks. The experiments on unsupervised deep representations and the classification of remote sensing images demonstrate the efficiency and effectiveness of the proposed method. IEEE","Convolution; Convolutional neural network (CNN); feature fusion network; Fuses; Remote sensing; Sensors; sparsity; Training; unsupervised deep learning; Unsupervised learning","Convolution; Deep learning; Efficiency; Electric fuses; Image classification; Image fusion; Neural networks; Personnel training; Sensors; Unsupervised learning; Classification of remote sensing image; Convolutional Neural Networks (CNN); Different layers; Feature fusion; Remote sensing images; sparsity; Time-consuming tasks; Training sample; Remote sensing"
"FompNet: Compressive sensing reconstruction with deep learning over wireless fading channels","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046468950&doi=10.1109%2fWCSP.2017.8171076&partnerID=40&md5=eae4920c36f3d0e75f6652926ace5422","With the ability to reconstruct signals from a highly incomplete number of samples, Compressive Sensing (CS) has been proposed in bandwidth-constrained scenarios like remote sensing, where signals exist some degree of redundancy. In CS, reconstruction approaches are of great importance. However, current reconstruction approaches are of highly computational complexity because they use greedy or convex optimization algorithms with many iterations. On the other hand, the effect of wireless fading channel on the distortion performance is not well studied, which is difficult to be analyzed and formulated in CS. To tackle these challenges, we proposed a novel deep learning based reconstruction approach called FompNet. The most distinctive characteristic of FompNet is that it involves Fast Orthogonal Matching Pursuit (FastOMP) with low computational complexity before a deep learning network, i.e., Convolutional Neural Network (CNN). With FastOMP, FompNet can dispose of real size images, while existing deep learning based reconstruction approaches are specified to small image patches. Besides, FastOMP makes the deep learning network adaptive to different sampling ratios, instead of another newly training process if the sampling ratio is changed. Furthermore, CNN can alleviate the effect of channel fading and significantly improve the reconstruction performance under different channel environments. Experiment results show that FompNet outperforms existing reconstruction approaches in terms of distortion and computational complexity under various channel conditions. © 2017 IEEE.","Compressive Sensing; Convolutional Neural Network; deep learning; image reconstruction; wireless fading channel","Complex networks; Compressed sensing; Computational complexity; Convex optimization; Convolution; Fading channels; Image reconstruction; Neural networks; Remote sensing; Wireless telecommunication systems; Compressive sensing; Convex optimization algorithms; Convolutional neural network; Convolutional Neural Networks (CNN); Distortion performance; Low computational complexity; Orthogonal matching pursuit; Wireless fading channels; Deep learning"
"Feasibility study of multi-pixel retrieval of optical thickness and droplet effective radius of inhomogeneous clouds using deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037708405&doi=10.5194%2famt-10-4747-2017&partnerID=40&md5=8dc7f9ba08bb2ba36b9504db6fa06607","Three-dimensional (3-D) radiative-transfer effects are a major source of retrieval errors in satellite-based optical remote sensing of clouds. The challenge is that 3-D effects manifest themselves across multiple satellite pixels, which traditional single-pixel approaches cannot capture. In this study, we present two multi-pixel retrieval approaches based on deep learning, a technique that is becoming increasingly successful for complex problems in engineering and other areas. Specifically, we use deep neural networks (DNNs) to obtain multi-pixel estimates of cloud optical thickness and column-mean cloud droplet effective radius from multispectral, multi-pixel radiances. The first DNN method corrects traditional bispectral retrievals based on the plane-parallel homogeneous cloud assumption using the reflectances at the same two wavelengths. The other DNN method uses so-called convolutional layers and retrieves cloud properties directly from the reflectances at four wavelengths. The DNN methods are trained and tested on cloud fields from large-eddy simulations used as input to a 3-D radiative-transfer model to simulate upward radiances. The second DNN-based retrieval, sidestepping the bispectral retrieval step through convolutional layers, is shown to be more accurate. It reduces 3-D radiative-transfer effects that would otherwise affect the radiance values and estimates cloud properties robustly even for optically thick clouds.",,"artificial neural network; cloud; cloud droplet; feasibility study; large eddy simulation; learning; optical depth; pixel; radiative transfer; reflectance; remote sensing; satellite data; wavelength"
"A central-point-enhanced convolutional neural network for high-resolution remote-sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048958895&doi=10.1080%2f01431161.2017.1362131&partnerID=40&md5=172248e26fc6ba56ff4ed5220c8b37bf","As one of the most important algorithms in the field of deep learning technology, the convolutional neural network (CNN) has been successfully applied in many fields. CNNs can recognize objects in an image by considering morphology and structure rather than simply individual pixels. One advantage of CNNs is that they exhibit translational invariance; when an image contains a certain degree of distortion or shift, a CNN can still recognize the object in the image. However, this advantage becomes a disadvantage when CNNs are applied to pixel-based classification of remote-sensing images, because their translational invariance characteristics causes distortions in land-cover boundaries and outlines in the classification result image. This problem severely limits the application of CNNs in remote-sensing classification. To solve this problem, we propose a central-point-enhanced convolutional neural network (CE-CNN) to classify high-resolution remote-sensing images. By introducing the central-point-enhanced layer when classifying a sample, the CE-CNN increases the weight of the central point in feather maps while preserving the original textures and characteristics. In our experiment, we selected four representative positions on a high-resolution remote-sensing image to test the classification ability of the proposed method and compared the CE-CNN with the traditional multi-layer percep-tron (MLP) and a traditional CNN. The results show that the proposed method can not only achieves a higher classification accuracy but also less distortion and fewer incorrect results at the boundaries of land covers. We further compared the CE-CNN with six state-of-the-art methods: k-NN, maximum likelihood, classification and regression tree (CART), MLP, support vector machine, and CNN. The results show that the CE-CNN’s classification accuracy is better than the other methods. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Convolution; Deep learning; Image enhancement; Maximum likelihood; Nearest neighbor search; Neural networks; Pixels; Remote sensing; Classification and regression tree; Convolutional neural network; Convolutional Neural Networks (CNN); High resolution remote sensing images; Morphology and structures; Pixel based classifications; Remote sensing classification; State-of-the-art methods; Image classification; accuracy assessment; algorithm; artificial neural network; image classification; image resolution; maximum likelihood analysis; remote sensing; support vector machine"
"A new difference image creation method based on deep neural networks for change detection in remote-sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054266723&doi=10.1080%2f01431161.2017.1371861&partnerID=40&md5=f008660f988d09db89f59198f65b841e","In this article, we propose a novel difference image (DI) creation method for unsupervised change detection in multi-temporal multispectral remote-sensing images based on deep learning theory. First, we apply deep belief network to learn local and high-level features from the local neighbour of a given pixel in an unsupervised manner. Second, a back propagation algorithm is improved to build a DI based on selected training samples, which can highlight the difference on changed regions and suppress the false changes on unchanged regions. Finally, we get the change trajectory map using simple clustering analysis. The proposed scheme is tested on three remote-sensing data sets. Qualitative and quantitative evaluations show its superior performance compared to the traditional pixel-level and texture-level-based approaches. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Backpropagation algorithms; Pixels; Remote sensing; Clustering analysis; Deep belief networks; High-level features; Multispectral remote sensing image; Quantitative evaluation; Remote sensing data; Remote sensing images; Unsupervised change detection; Deep neural networks; artificial neural network; back propagation; data set; detection method; image processing; performance assessment; pixel; remote sensing; spectral resolution"
"Aeroplane detection in very-high-resolution images using deep feature representation and rotation-invariant hough forests","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054226619&doi=10.1080%2f2150704X.2017.1363430&partnerID=40&md5=e8de3b9154c8f47929f91a0d6de2c76d","This letter proposes a processing chain for detecting aeroplanes from very high-resolution (VHR) remotely sensed images with the fusion of deep feature representation and rotation-invariant Hough forests. First, superpixel segmentation is used to generate meaningful and non-redundant patches. Second, deep learning techniques are exploited to construct a multi-layer feature encoder for representing high-order features of patches. Third, a set of multi-scale rotation-invariant Hough forests are trained to detect aeroplanes of varying orientations and sizes. Experiments show that the proposed method is a promising solution for detecting aeroplanes from VHR remotely sensed images, with a completeness, correctness, and F-measure of 0.956, 0.970, and 0.963, respectively. Comparative studies with four existing methods also demonstrate that the proposed method outperforms the other existing methods in accurately detecting aeroplanes of varying appearances, orientations, and sizes. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Aircraft; Deep learning; Feature extraction; Forestry; Remote sensing; Comparative studies; Feature representation; Learning techniques; Remotely sensed images; Rotation invariant; Superpixel segmentations; Very high resolution; Very high resolution (VHR) image; Aircraft detection; accuracy assessment; comparative study; detection method; forest; image resolution; imaging method; pixel"
"Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040367775&doi=10.1109%2fMGRS.2017.2762307&partnerID=40&md5=4257bc32a75d94610ee4151c89ae206d","Central to the looming paradigm shift toward data-intensive science, machine-learning techniques are becoming increasingly important. In particular, deep learning has proven to be both a major breakthrough and an extremely powerful tool in many fields. Shall we embrace deep learning as the key to everything? Or should we resist a black-box solution? These are controversial issues within the remote-sensing community. In this article, we analyze the challenges of using deep learning for remote-sensing data analysis, review recent advances, and provide resources we hope will make deep learning in remote sensing seem ridiculously simple. More importantly, we encourage remote-sensing scientists to bring their expertise into deep learning and use it as an implicit general model to tackle unprecedented, large-scale, influential challenges, such as climate change and urbanization. © 2013 IEEE.",,"Climate change; Climate models; Remote sensing; Black Box solution; Data intensive science; General model; Machine learning techniques; Paradigm shifts; Remote sensing data; Deep learning; climate change; data interpretation; machine learning; paradigm shift; remote sensing; science and technology; urbanization"
"Deep learning for semantic segmentation of remote sensing images with rich spectral content","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041831317&doi=10.1109%2fIGARSS.2017.8127520&partnerID=40&md5=b5a00aadad0a498ac00b6427e61272f3","With the rapid development of Remote Sensing acquisition techniques, there is a need to scale and improve processing tools to cope with the observed increase of both data volume and richness. Among popular techniques in remote sensing, Deep Learning gains increasing interest but depends on the quality of the training data. Therefore, this paper presents recent Deep Learning approaches for fine or coarse land cover semantic segmentation estimation. Various 2D architectures are tested and a new 3D model is introduced in order to jointly process the spatial and spectral dimensions of the data. Such a set of networks enables the comparison of the different spectral fusion schemes. Besides, we also assess the use of a 'noisy ground truth' (i.e. outdated and low spatial resolution labels) for training and testing the networks. © 2017 IEEE.","Deep Learning; Multispectral; Noisy Training; Remote Sensing; Semantic Segmentation",
"Building extraction from remote sensing images with deep learning in a supervised manner","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041840580&doi=10.1109%2fIGARSS.2017.8127295&partnerID=40&md5=5bdb74217fac5cd12a9d92284b3f36bf","Building extraction from remote sensing images is a longstanding topic in land use analysis and applications of remote sensing. Variations in shape and appearance of buildings, occlusions and other unpredictable factors increase the hardness of automatic building extraction. Numerous methods have been proposed during the last several decays, but most of these works are task oriented and lack of generalization. This paper applys deep learning to building extraction in a supervised manner. A deep deconvolution neural network with 27 Convolution/Deconvolution weight layers is designed to realize building extraction in pixel level. As such a deep network is prone to overfitting, a data augment method that suits pixel-wise prediction tasks in remote sensing is suggested. Moreover, an overall training and inferencing architecture is proposed. Our methods are finally applied to building extraction tasks and get competitive results with other methods published. © 2017 IEEE.","Building Extraction; Convolution Neural Networks; Deconvolution; Deep learning; Remote Sensing",
"A Deep-Learning-Based Forecasting Ensemble to Predict Missing Data for Remote Sensing Analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039446572&doi=10.1109%2fJSTARS.2017.2760202&partnerID=40&md5=8fe5d528459684d10cb1a2187e3eef63","The problem of missing data in remote sensing analysis is manifold. The situation becomes more serious during multitemporal analysis when data at various a-periodic timestamps are missing. In this work, we have proposed a deep-learning-based framework (Deep-STEP-FE) for reconstructing the missing data to facilitate analysis with remote sensing time series. The idea is to utilize the available data from both earlier and subsequent timestamps, while maintaining the causality constraint in spatiotemporal analysis. The framework is based on an ensemble of multiple forecasting modules, built upon the observed data in the time-series sequence. The coupling between the forecasting modules is accomplished with the help of dummy data, initially predicted using the earlier part of the sequence. Then, the dummy data are progressively improved in an iterative manner so that it can best conform to the next part of the sequence. Each of the forecasting modules in the ensemble is based on Deep-STEP, a variant of the deep stacking network learning approach. The work has been validated using a case study on predicting the missing images in normalized difference vegetation index time series, derived from Landsat-7 TM-5 satellite imagery over two spatial zones in India. Comparative performance analysis demonstrates the effectiveness of the proposed forecasting ensemble. © 2008-2012 IEEE.","Causality constraint; deep learning; ensemble; missing data; prediction; remote sensing","Deep learning; Forecasting; Satellite imagery; Time series; Time series analysis; Causality constraint; Comparative performance analysis; ensemble; Missing data; Multi-temporal analysis; Normalized difference vegetation index time series; Remote sensing analysis; Spatiotemporal analysis; Remote sensing; ensemble forecasting; Landsat thematic mapper; learning; NDVI; prediction; remote sensing; satellite imagery; spatiotemporal analysis; India"
"Natural language description of remote sensing images based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041835974&doi=10.1109%2fIGARSS.2017.8128075&partnerID=40&md5=f851dba3520ebe95021f699e3d0b6b93","The semantic description of remote sensing image is a useful and meaningful task, which can help us to get a better understanding of the scene depicted in the remote sensing images and make better use of the remote sensing images. Nature language provides good solution for describing the semantic information of remote sensing images. Nature language description of a remote sensing image is to generate a meaningful sentence given a remote sensing image. This paper presents a novel method based on deep learning. First, a convolutional neural network is utilized to detect the main objects of the remote sensing images. Then a recurrent neural network language model is utilized to generate the natural language descriptions of the objects which are detected in the first step. Experimental results on a set of remote sensing images demonstrate that the proposed method is able to generate desirable description of the scene. © 2017 IEEE.","Convolutional neural network; Natural language description; Recurrent neural network; Remote sensing image",
"SVM or deep learning? A comparative study on remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978054052&doi=10.1007%2fs00500-016-2247-2&partnerID=40&md5=1753c5e766218705c4396734b348ad6f","With constant advancements in remote sensing technologies resulting in higher image resolution, there is a corresponding need to be able to mine useful data and information from remote sensing images. In this paper, we study auto-encoder (SAE) and support vector machine (SVM), and to examine their sensitivity, we include additional umber of training samples using the active learning frame. We then conduct a comparative evaluation. When classifying remote sensing images, SVM can also perform better than SAE in some circumstances, and active learning schemes can be used to achieve high classification accuracy in both methods. © 2016, Springer-Verlag Berlin Heidelberg.","Active learning; Remote sensing; Sparse auto-encoder; Spatial big data; Support vector machine","Artificial intelligence; Big data; Image reconstruction; Image resolution; Learning systems; Remote sensing; Support vector machines; Active Learning; Active learning scheme; Auto encoders; Classification accuracy; Comparative evaluations; Remote sensing image classification; Remote sensing images; Remote sensing technology; Image classification"
"Balanced data driven sparsity for unsupervised deep feature learning in remote sensing images classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041828446&doi=10.1109%2fIGARSS.2017.8127041&partnerID=40&md5=139787e1a0b5ec8511602207b6d469d3","There are many attempts that utilize deep learning methods to solve the problem of classification in remote sensing images. Convolutional Neural Networks (CNN) have made very good performance for various visual tasks, and marked their important place in all deep learning models. However, for some classification tasks of remote sensing images, CNN could not demonstrate their full potential because of lacking large amounts of labeled training data. Some efforts have been made to combine CNN with unlabeled data to tackle the problem by performing unsupervised learning. In this work we propose the balanced data driven sparsity to help train CNN in an unsupervised way. The experiments over the real world remote sensing images demonstrate that the proposed method improves the performance of the recent methods. © 2017 IEEE.","convolutional neural network; Remote sensing images classification; sparsity; unsupervised learning",
"Deep-learning Versus OBIA for scattered shrub detection with Google Earth Imagery: Ziziphus lotus as case study","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038210007&doi=10.3390%2frs9121220&partnerID=40&md5=3fd539e1f4720629c9fc5103786e2486","There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been traditionally performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image often cannot be extrapolated to a different image. Recently, deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in computer vision and are offering promising results in land cover mapping. This paper analyzes the potential of CNN-based methods for detection of plant species of conservation concern using free high-resolution Google EarthTM images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. Compared to the best performing OBIA-method, the best CNN-detector achieved up to 12% better precision, up to 30% better recall and up to 20% better balance between precision and recall. Besides, the knowledge that CNNs acquired in the first image can be re-utilized in other regions, which makes the detection process very fast. A natural conclusion of this work is that including CNN-models as classifiers, e.g., ResNet-classifier, could further improve OBIA methods. The provided methodology can be systematically reproduced for other species detection using our codes available through (https://github.com/EGuirado/CNN-remotesensing). © 2017 by the author.","Convolutional Neural Networks (CNNs); Land cover mapping; Object-Based Image Analysis (OBIA); Plant species detection; Remote sensing; Ziziphus lotus","Biodiversity; Conservation; Convolution; Deep learning; Deep neural networks; Ecosystems; Image analysis; Land use; Neural networks; Object recognition; Plants (botany); Remote sensing; Convolutional neural network; Land cover mapping; Object based image analysis (OBIA); Plant species; Ziziphus lotus; Mapping"
"Fast multiclass object detection in optical remote sensing images using region based convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041828889&doi=10.1109%2fIGARSS.2017.8127088&partnerID=40&md5=b49129e3f7f1cc48496723b2fa5eceda","Fast multiclass object detection for remote sensing images plays an important role for a wide range of applications. Traditional methods based on a sliding window search lead to heavy computational costs and are unsuitable for multiclass detection. Recently, deep learning algorithms, especially faster region based convolutional neural networks (Faster R-CNN), which adopt a region proposal paradigm to avoid exhaustive search, has achieved state-of-the-art multiclass detection performance in computer vision. This paper investigates the use of Faster R-CNN in the earth observation community. We have three contributions: 1) It's the first time to successfully use Faster R-CNN for object detection in remote sensing images. It achieved faster speed (22 ×faster) and better performance (a mAP of 78% vs. 72%) than traditional methods; 2) we adopt data augmentation to train Faster R-CNN with limited samples; 3) we successfully tested our method on large-scale google earth images, which shows robustness of our method. © 2017 IEEE.","convolutional neural networks; object detection; region proposal network; Remote sensing",
"Ship classification with deep learning using COSMO-SkyMed SAR data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041821981&doi=10.1109%2fIGARSS.2017.8127014&partnerID=40&md5=f98265854eb3a47f86d0ca827f15d499","Ship classification with spaceborne high resolution synthetic aperture radar (SAR) has wide applications in maritime traffic monitoring, fishing law-enforcement operation, marine security, etc. Deep learning, which has the ability of learning features itself, is successfully used in computer vision and artificial intelligence, and introduced into remote sensing field in recent years. In this study, the Italian COSMO-SkyMed SAR images acquired on Jul. 12-15, 2010 were used for ship classification with convolution neural networks in the Google's TensorFlow environment. The results show that cargo ships could be discriminated from non-cargo ships. Due to variations of radar illumination directions and ship poses, more data are necessary for sub-category classification. © 2017 IEEE.","convolutional neural networks (CNN); ship classification; Synthetic aperture radar (SAR); Tensorflow",
"Urban land cover classification with missing data using deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041812683&doi=10.1109%2fIGARSS.2017.8128164&partnerID=40&md5=6934ad164d6d0796bc120a97b18c0341","Fusing different sensors with different data modalities is a common technique to improve land cover classification performance in remote sensing. However, all modalities are rarely available for all test data, and this missing data problem poses severe challenges for multi-modal learning. Inspired by recent successes in deep learning, we propose as a remedy a convolutional neural network architecture for urban remote sensing image segmentation trained on data modalities which are not all available at test time. We train our architecture with a cost function particularly suited for imbalanced classes, as this is a frequent problem in remote sensing. We demonstrate the method using a benchmark dataset containing RGB and DSM images. Assuming that the DSM images are missing during testing, our method outperforms both a CNN trained on RGB images as well as an ensemble of two CNNs trained on the RGB images, by exploiting the training time information of the missing modality. © 2017 IEEE.","Convolutional neural networks; Deep learning; Missing data; Remote sensing",
"Soft Computing in Remote Sensing Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039035409&doi=10.1007%2fs40010-017-0431-0&partnerID=40&md5=58beed6d2c81338d0b8b7544caa374e2","This review paper discusses recent developments in soft computing techniques and applications specific to remote sensing, especially in the last two decades. Even though, the applications have been spread across in many areas of remote sensing, utilization of these techniques has been found to be strongly biased towards remote sensing data classification because of their ability to handle mixed pixels and also varied spatial (ranging 1–250 m) and spectral resolutions (from 1 to 100 of spectral bands). Success with conventional methods for these problems has been only moderate. This has attracted more research in the soft computing data classification. In particular, the soft computing methods outperform conventional methods when they are applied to complex data handling especially for multi-source and very high dimensional data classification with one or more combination of classifiers. Fairly recent soft computing concepts like deep learning, analytical hierarchical processes and feature extraction from hyperspectral imagery and applications of optimization techniques like particle swarm optimization, genetic algorithms in handling remote sensing problems have been covered to encourage research community for further exploration in such important topics. © 2017, The National Academy of Sciences, India.","Artificial neural networks; Ensemble classifier; Fuzzy C-means classifier; Genetic algorithms; Support vector machines",
"Hyperspectral Images Classification with Gabor Filtering and Convolutional Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033675724&doi=10.1109%2fLGRS.2017.2764915&partnerID=40&md5=079c2b03877272a38acf921cc319f199","Recently, the capability of deep learning-based approaches, especially deep convolutional neural networks (CNNs), has been investigated for hyperspectral remote sensing feature extraction (FE) and classification. Due to the large number of learnable parameters in convolutional filters, lots of training samples are needed in deep CNNs to avoid the overfitting problem. On the other hand, Gabor filtering can effectively extract spatial information including edges and textures, which may reduce the FE burden of the CNNs. In this letter, in order to make the most of deep CNN and Gabor filtering, a new strategy, which combines Gabor filters with convolutional filters, is proposed for hyperspectral image classification to mitigate the problem of overfitting. The obtained results reveal that the proposed model provides competitive results in terms of classification accuracy, especially when only a limited number of training samples are available. © 2004-2012 IEEE.","Convolutional neural network (CNN); deep learning; feature extraction (FE); Gabor filtering; hyperspectral images (HSIs)","Convolution; Deep learning; Deep neural networks; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Independent component analysis; Information filtering; Iron; Neural networks; Personnel training; Principal component analysis; Remote sensing; Sampling; Spectroscopy; Classification accuracy; Convolutional neural network; Gabor filtering; Hyperspectral remote sensing; Images classification; Learning-based approach; Over fitting problem; Spatial informations; Gabor filters"
"On the possibility of conditional adversarial networks for multi-sensor image matching","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041859053&doi=10.1109%2fIGARSS.2017.8127535&partnerID=40&md5=75e032c043b90fe0c774269f9cd25bb5","A major research area in remote sensing is the problem of multi-sensor data fusion. Especially the combination of images acquired by different sensor types, e.g. active and passive, is a difficult task. Over the last years deep learning methods have proven their high potential for remote sensing applications. In this paper we will show how a deep learning method can be valuable for the problem of optical and SAR image matching. We investigate the possible of conditional generative adversarial networks (cGANs) for the generation of artificial templates. Contrary to common template generation approaches for image matching, the generation of templates using cGANs does not require the extraction of features. Our results show the possibility of realistic SAR-like template generation from optical images through cGANs and the potential of these templates for enhancing the matching of optical and SAR images by means of reliability and accuracy. © 2017 IEEE.","artificial template generation; conditional GANs; deep learning; image matching; multi-sensor",
"Deep convolutional neural network based large-scale oil palm tree detection for high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041840309&doi=10.1109%2fIGARSS.2017.8127085&partnerID=40&md5=fb468c25f08e070d28643e03475aabfa","This paper proposed a deep convolutional neural network (DCNN) based framework for large-scale oil palm tree detection using high-resolution remote sensing images in Malaysia. Different from the previous palm tree or tree crown detection studies, the palm trees in our study area are very crowded and their crowns often overlap. Moreover, there are various land cover types in our study area, e.g. impervious, bare land, and other vegetation, etc. The main steps of our proposed method include large-scale and multi-class sample collection, AlexNet-based DCNN training and optimization, sliding window-based label prediction, and post-processing. Compared with the manually interpreted ground truth, our proposed method achieves detection accuracies of 92%-97% in our study area, which are greatly higher than the accuracies obtained from another two detection methods used in this paper. © 2017 IEEE.","Deep convolutional neural network; Deep learning; Large-scale object detection; Oil palm trees",
"Discriminative Feature Learning for Unsupervised Change Detection in Heterogeneous Images Based on a Coupled Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028988667&doi=10.1109%2fTGRS.2017.2739800&partnerID=40&md5=48e803d5259f76c4662ccad2bb8b578e","With the application requirement, the technique for change detection based on heterogeneous remote sensing images is paid more attention. However, detecting changes between two heterogeneous images is challenging as they cannot be compared in low-dimensional space. In this paper, we construct an approximately symmetric deep neural network with two sides containing the same number of coupled layers to transform the two images into the same feature space. The two images are connected with the two sides and transformed into the same feature space, in which their features are more discriminative and the difference image can be generated by comparing paired features pixel by pixel. The network is first built by stacked restricted Boltzmann machines, and then, the parameters are updated in a special way based on clustering. The special way, motivated by that two heterogeneous images share the same reality in unchanged areas and retain respective properties in changed areas, shrinks the distance between paired features transformed from unchanged positions, and enlarges the distance between paired features extracted from changed positions. It is achieved through introducing two types of labels and updating parameters by adaptively changed learning rate. This is different from the existing methods based on deep learning that just do operations on positions predicted to be unchanged and extract only one type of labels. The whole process is completely unsupervised without any priori knowledge. Besides, the method can also be applied to homogeneous images. We test our method on heterogeneous images and homogeneous images. The proposed method achieves quite high accuracy. © 2017 IEEE.","Change detection; deep neural network; features extracting; heterogeneous images","Deep learning; Deep neural networks; Feature extraction; Image reconstruction; Image sensors; Neural networks; Optical sensors; Pixels; Remote sensing; Space optics; Space-based radar; Synthetic aperture radar; Application requirements; Change detection; Coupled neural networks; Discriminative features; features extracting; heterogeneous images; Restricted boltzmann machine; Unsupervised change detection; Image processing; artificial neural network; detection method; image analysis; learning; parameter estimation; pixel; remote sensing; transform; unsupervised classification"
"Stacked Sparse Autoencoder Modeling Using the Synergy of Airborne LiDAR and Satellite Optical and SAR Data to Map Forest Above-Ground Biomass","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030755958&doi=10.1109%2fJSTARS.2017.2748341&partnerID=40&md5=d01df9ad133728a85aaaf0f0f461c5f2","Timely, spatially complete, and reliable forest above-ground biomass (AGB) data are a prerequisite to support forest management and policy formulation. Traditionally, forest AGB is spatially estimated by integrating satellite images, in particular, optical data, with field plots from forest inventory programs. However, field data are limited in remote and unmanaged areas. In addition, optical reflectance usually saturates at high-density biomass level and is subject to cloud contaminations. Thus, this study aimed to develop a deep learning based workflow for mapping forest AGB by integrating Landsat 8 and Sentinel-1A images with airborne light detection and ranging (LiDAR) data. A reference AGB map was derived from the wall-to-wall LiDAR data and field measurements. The LiDAR plots - stratified random samples of forest biomass extracted from the LiDAR simulated strips in the reference map - were adopted as a surrogate for traditional field plots. In addition to the deep learning model, i.e., stacked sparse autoencoder network (SSAE), five different prediction techniques including multiple stepwise linear regressions, K-nearest neighbor, support vector machine, back propagation neural networks, and random forest were individually used to establish the relationship between LiDAR-derived forest biomass and the satellite predictors. Optical variables (Landsat 8 OLI), SAR variables (Sentinel-1A), and their combined variables were individually input to the six prediction models. Results showed that the SSAE model had the best performance for the forest biomass estimation. The combined optical and microwave dataset as explanatory variables improved the modeling performance compared to either the optical-only or microwave-only data, regardless of prediction algorithms. The best mapping accuracy was obtained by the SSAE model with inputs of optical and microwave integrated metrics that yielded R2 of 0.812, root mean squared error (RMSE) of 21.753 Mg/ha, and relative RMSE (RMSEr) of 14.457%. Overall, the SSAE model with inputs of combined Landsat 8 OLI and Sentinel-1A information could result in accurate estimation of forest biomass by using the stratification-sampled and LiDAR-derived AGB as ground reference data. The modeling workflow has the potential to promote future forest growth monitoring and carbon stock assessment across large areas. © 2008-2012 IEEE.","Biomass; deep learning (DL); Landsat 8; light detection and ranging (LiDAR); Sentinel-1A; stacked sparse autoencoder network (SSAE)","Backpropagation; Biomass; Carbon; Data integration; Decision trees; Deep learning; Forecasting; Forestry; Imaging techniques; Learning systems; Mapping; Mean square error; Nearest neighbor search; Neural networks; Optical image storage; Radar imaging; Remote sensing; Satellites; Space-based radar; Synthetic aperture radar; Tracking radar; Auto encoders; Biomedical optical imaging; LANDSAT; Light detection and ranging; Optical imaging; Optical saturation; Sentinel-1; Optical radar; aboveground biomass; forest inventory; lidar; machine learning; mapping; modeling; satellite imagery; Sentinel; synthetic aperture radar"
"Multilayer Projective Dictionary Pair Learning and Sparse Autoencoder for PolSAR Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032220488&doi=10.1109%2fTGRS.2017.2727067&partnerID=40&md5=0da323cc8cbeea47616a0b76b2485e4a","Polarimetric synthetic aperture radar (PolSAR) image classification is a vital application in remote sensing image processing. In general, PolSAR image classification is actually a high-dimensional nonlinear mapping problem. The methods based on sparse representation and deep learning have shown a great potential for PolSAR image classification. Therefore, a novel PolSAR image classification method based on multilayer projective dictionary pair learning (MDPL) and sparse autoencoder (SAE) is proposed in this paper. First, MDPL is used to extract features, and the abstract degree of the extracted features is high. Second, in order to get the nonlinear relationship between elements of feature vectors in an adaptive way, SAE is also used in this paper. Three PolSAR images are used to test the effectiveness of our method. Compared with several state-of-the-art methods, our method achieves very competitive results in PolSAR image classification. © 2012 IEEE.","Multilayer projective dictionary pair learning (MDPL); polarimetric synthetic aperture radar (PolSAR); sparse autoencoder (SAE); sparse representation","Encoding (symbols); Feature extraction; Glossaries; Image processing; Learning systems; Multilayers; Polarimeters; Radar; Remote sensing; Scattering; Synthetic aperture radar; Auto encoders; Classification methods; Non-linear relationships; Nonhomogeneous media; Polarimetric synthetic aperture radars; Remote sensing image processing; Sparse representation; State-of-the-art methods; Image classification; algorithm; image analysis; image classification; image processing; numerical method; remote sensing; synthetic aperture radar"
"Integrating edge/boundary priors with classification scores for building detection in very high resolution data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041849309&doi=10.1109%2fIGARSS.2017.8127705&partnerID=40&md5=20effcd9ba3a20568da7e9fcae130afb","Automatic and accurate detection of man-made objects, such as buildings, is one of the main problems that the remote sensing community has been focusing on for the last decades. In this paper, we propose a Conditional Random Field (CRF) formulation which is using edge/boundary localization priors towards accurate building detection. These edge priors have been integrated/fused with the classification scores from a deep learning Convolutional Neural Network (CNN) architecture under a single energy formulation. The validation of the developed methodology had been performed on the recently published SpaceNet dataset. Experimental results and quantitative evaluation, based on different accuracy statistics, indicate the great potential of the proposed approach. © 2017 IEEE.","CRFs; Deep Learning; Man made objects",
"Multi-scale-and-depth convolutional neural network for remote sensed imagery pan-sharpening","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041863646&doi=10.1109%2fIGARSS.2017.8127731&partnerID=40&md5=da2c19b95fda6ac1a2a46f80f405133d","Pan-sharpening is a fundamental and significant task in the field of remote sensed imagery fusion, which demands fusion of panchromatic and multi-spectral images with the rich information accurately preserved in both spatial and spectral domains. In this paper, to overcome the drawbacks of traditional pan-sharpening methodologies, we employed the advanced concept of deep learning to propose a Multi-Scale-and-Depth Convolutional Neural Network (MSDCNN) as an end-to-end pan-sharpening model. By the results of a large number of quantitative and visual assessments, the qualities of images fused by the proposed network have been confirmed superior to compared state-of-the-art methods. © 2017 IEEE.","Convolutional neural network; Deep learning; Pan-sharpening; Remote Sensing; Residual Learning",
"Evaluation the performance of fully convolutional networks for building extraction compared with shallow models","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041808473&doi=10.1109%2fIGARSS.2017.8127086&partnerID=40&md5=e09ccfc9634229f6bffe09756619234d","With the development of machine learning, many researchers have used machine learning models for building extraction in high resolution remote sensing images. Especially for the recently proposed deep learning models, it has been widely used for building detection in the urban monitoring. In this study, the performances of images segmentation for building extraction based on Fully Convolutional Networks (FCN) model and shallow models are qualitatively and quantitatively compared. Firstly, the public aerial dataset of Massachusetts building dataset[1] are preprocessed to extract features. Then, we trained shallow models and deep model on Massachusetts building dataset. Moreover, the trained FCN model and shallow models are used to extract the building on the same image. Finally, we compared performances between shallow models and deep model. It is found that FCN gives the highest recall 0.63, precision 0.62, and F-measure rate 0.63. The qualitative and quantitative analysis of the building extraction results fully demonstrates that FCN gives the best performance compared with traditional shallow models. © 2017 IEEE.","aerial image; Building extraction; deep learning; Fully Convolutional Networks (FCN); shallow models",
"Convolutional neural networks for multispectral image cloud masking","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041842432&doi=10.1109%2fIGARSS.2017.8127438&partnerID=40&md5=f96bfae6583c3a225d194aedfb69ca1b","Convolutional neural networks (CNN) have proven to be state of the art methods for many image classification tasks and their use is rapidly increasing in remote sensing problems. One of their major strengths is that, when enough data is available, CNN perform an end-to-end learning without the need of custom feature extraction methods. In this work, we study the use of different CNN architectures for cloud masking of Proba-V multispectral images. We compare such methods with the more classical machine learning approach based on feature extraction plus supervised classification. Experimental results suggest that CNN are a promising alternative for solving cloud masking problems. © 2017 IEEE.","cloud detection; cloud masking; Convolutional neural networks; deep learning; Proba-V",
"Hierarchical feature exttratction for object recogition in complex SAR image using modified convolutional auto-encoder","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041831362&doi=10.1109%2fIGARSS.2017.8127087&partnerID=40&md5=ab7a96ed35509f917ed85fa68a64c637","Automatic target recognition is a crucial task for SAR remote sensing. Unlike other methods, the unsupervised representation learning based on deep architecture can obtain robust high-level features directly from raw data. A drawback of most unsupervised representation learning methods in SAR ATR is that they only deal with amplitude images. In addition, many methods utlize a single layer architecture to extract pixel-level/mid-level features which are probably sensitive to condition variation. In this paper, a feature extraction method based on modified stacked convolutional denoising auto-encoder (MSCDAE) for complex SAR images is proposed, where convolutional kernels of MSCDAE are learned by 1-D modified denoising auto-encoders. By stacking the convolutional layers and pooling layers, high-level respresntation of objects are learned. The features are subsequently sent to a trained SVM for object classification. Experimetnal results demonstrate that the proposed method can provide a significant improvement in the ATR performance. © 2017 IEEE.","automatic target recognition(ATR); Deep learning; Modified convolutional denoising autoencoder (MCSAE); synthetic aperture radar(SAR)",
"Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038620511&doi=10.1109%2fIGARSS.2017.8127684&partnerID=40&md5=da0cdd0aba454a93bb78573715e3d57c","New challenges in remote sensing impose the necessity of designing pixel classification methods that, once trained on a certain dataset, generalize to other areas of the earth. This may include regions where the appearance of the same type of objects is significantly different. In the literature it is common to use a single image and split it into training and test sets to train a classifier and assess its performance, respectively. However, this does not prove the generalization capabilities to other inputs. In this paper, we propose an aerial image labeling dataset that covers a wide range of urban settlement appearances, from different geographic locations. Moreover, the cities included in the test set are different from those of the training set. We also experiment with convolutional neural networks on our dataset. © 2017 IEEE.","classification benchmark; convolutional neural networks; deep learning; High-resolution images",
"Toward country scale building detection with convolutional neural network using aerial images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034400093&doi=10.1109%2fIGARSS.2017.8127091&partnerID=40&md5=46c98647ef262705ce62797e8440702e","Establishing up-to-date nationwide building maps is essential to understand urban dynamics, such as estimating population and urban planning and many other applications. However, an efficient and effective solution is yet to be developed. In this paper, for the first time we evaluate three state-of-the-art CNNs for detecting buildings across entire United States using aerial images. The three CNN architectures, fully convolutional neural network, conditional random field as recurrent neural network, and SegNet, support semantic pixel-wise labeling and focus on capturing textural information at multi-scale. We use 1-meter resolution NAIP images as the test data set, and compare the detection results across the three methods. In addition, we propose to combine signed distance function labels with SegNet, which is the preferred CNN architecture identified by our extensive evaluations. The results are further improved in terms of precision, recall rate and the number of building detected. On average, model inference on test images is less than one minute for an area of size ∼ 56 km2. With these promising results and the time required to process images, the framework offers great potential toward country scale building mapping with remote sensing imagery. © 2017 IEEE.","building extractions; convolutional; deep learning; NAIP; semantic segmentation",
"SAR-images categorization and applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041513365&doi=10.23919%2fELMAR.2017.8124423&partnerID=40&md5=30ce4d8ea22333294473fc93386ecc42","Radars, especially Synthetic Aperture Radars (SAR) are key sensors for satellite and terrestrial remote sensing applications. The main advantage of SAR-technology over optical sensor systems (LIDAR, visible, infrared and hyperspectral cameras), is its ability to capture SAR-images in all weather conditions, day and night. The high resolution (of one meter) TerraSAR-X mission and Tandem TerraSAR-X mission, launched by the German Aerospace Agency in 2007 and 2010, respectively, and free accessible images from Sentinel missions of the European Space Agency (ESA), started in 2016, enable an enormous amount of remote sensing data for different Earth observation applications, such as disaster and pollution monitoring and prevention, for military and civil applications. Tandem sensors provide precise interferometric measurements with resolution of a few millimeters. For all applications, appropriate SAR-images' analysis is crucial. In this paper, we will present some of our own developed algorithms for SARimages' analysis, classification and characterization, parametric and nonparametric, based mainly on wavelet multiresolution analysis, Bayesian statistics and Deep learning classification. Some applications will be reviewed. © 2017 Croatian Society Electronics in Marine - ELMAR.","Categorization; Classifcation; Multiresolution Analysis; Remote Sensing; SAR-imagery; Stacked Autoencoder","Deep learning; Disaster prevention; Earth (planet); Image analysis; Military applications; Military photography; Multiresolution analysis; Optical radar; Radar; Remote sensing; Satellites; Space applications; Space optics; Space-based radar; Synthetic aperture radar; Synthetic apertures; Auto encoders; Categorization; Classifcation; Hyper-spectral cameras; Interferometric measurement; Remote sensing applications; SAR imagery; Wavelet multi-resolution analysis; Radar imaging"
"Deep highway unit network for land cover type classification with GF-3 SAR imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043453877&doi=10.1109%2fBIGSARDATA.2017.8124926&partnerID=40&md5=6abbdec09c90a1f8611339a9b1e4b07f","The fully polarized synthetic aperture radar (SAR) is an advanced earth observation system with day and night imaging capability, which can obtain rich information of terrain and has a wide range of applications in environmental protection, urban planning and resource investigation. As the first selfdeveloped C-band multi-polarized SAR image, the acquisition of massive data and operational operation of Chinese SAR remote sensing has entered the era of big data. Under the era of remote sensing large data, however, SAR image interpretation is a great challenge for scientific applications. At present, big data-based intelligent methods such as computer vision technology have achieved great success. Deep learning such as deep highway unit networks has revolutionized the computer vision area. However, due to the characteristics of SAR microwave band imaging and phase coherence processing, SAR images are very different from ordinary optical images in terms of band, projection direction, data composition and so on. Therefore, deep learning can not be directly used for quad-pol SAR image classification. In this paper, deep learning is applied to land cover type classification with GF-3 quad-pol SAR imagery. A deep highway unit network is employed to automatically extract a hierarchic feature representation from the data, based on which the land cover type classification can be conducted. Our classification model is trained on limited training data from forest resource inventory and planning data, and tested on a Radarsat-2 quad-pol images, which is the image of the same area acquired at different times. We also employ the machine learning such as SVM, Random Forest on the same samples for comparison. The deep highway unit network trained by the GF-3 images, which can reduce speckle, fully excavate the regularity of SAR images in time and space. © 2017 IEEE.","Deep highway unit networks; Deep learning; GaoFen-3; Land cover type classification","Big data; Classification (of information); Computer vision; Decision trees; Deep learning; Geometrical optics; Image classification; Learning systems; Remote sensing; Space optics; Synthetic aperture radar; Computer vision technology; Earth observation systems; Feature representation; Forest resource inventory; GaoFen-3; Land-cover types; SAR image classifications; Scientific applications; Radar imaging"
"A study on leaf area index and SAR image of oil palm with entropy decomposition and deep learning classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045318902&doi=10.1109%2fPIERS-FALL.2017.8293148&partnerID=40&md5=9013c331cdb3c3e7ce71c80956bbab58","In recent years, the increase in demand for food resources leads to the rapid expansion of oil palm plantations which is one of the more prominent commercial crops for countries located near the Equator. To cope with the expansion, a better plantation monitoring system is required which can be done via satellite remote sensing. Leaf area index (LAI) is one of the prominent biophysical properties of oil palms used by many to determine the overall health of oil palms and LAI is also monitored and maintained at a certain level to optimize fruit productivity. Therefore, it is important for plantation manager to monitor the LAI of plantation plots over time to perform optimization measures where microwave remote sensing can be used to aid them in monitoring their plantations. This paper attempts to explore the relationship between LAI of oil palm with microwave backscatter and later uses Deep Learning (DL) to classify the oil palm plots based on their LAI. Ground truth measurements were conducted in Lekir Estate, Perak state, Malaysia in October 2013 to obtain the vegetative parameters of oil palms together with C band microwave image from Radarsat-2 taken during the same time period. First, the relationship between LAI of oil palms and satellite data processed through entropy decomposition was studied in order to obtain a clearer picture on how LAI of oil palms affects microwave backscattering mechanism. Unsupervised Entropy Decomposition with Wishart Classifier was then used to cluster the data in order to observe the separability of the data in terms of LAI. Due to the constraint of C band data with fast attenuation of signal within the palm canopy, only 3 clusters can be obtained from the Lekir Estate microwave image which are ground, young palms (lesser than 3 years old) and mature palms (older than 3 years). Based on this information, ground truth information is then prepared into the three particular classes as training data for supervised classification. Deep Neural Network (DNN) via Caffe framework was used to perform the classification. In the first experiment, classification was done with entropy decomposition data where H and Alpha were used as input data for the DNN. DNN managed to obtain testing accuracy of 72.1%. To improve accuracy, another experiment using calibrated intensity data was used instead. DNN managed to obtain 83.33% testing accuracy for this experiment. This shows that modern machine learning classifier works better with raw data instead of processed data such as entropy decomposition as processed data have loss of potential information. Other than that, this work also found that better data separability in the context of LAI can be achieved if microwave with higher penetration power such as L band or P band to be used. © 2018 Electromagnetics Academy. All rights reserved.",,"Backscattering; Deep neural networks; Entropy; Image classification; Imaging systems; Microwaves; Palm oil; Personnel training; Radar imaging; Remote sensing; Space-based radar; Supervised learning; Synthetic aperture radar; Biophysical properties; Microwave backscattering; Microwave remote sensing; Monitoring system; Oil palm plantations; Optimization measures; Satellite remote sensing; Supervised classification; Classification (of information)"
"Prolongation of SMAP to Spatiotemporally Seamless Coverage of Continental U.S. Using a Deep Learning Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033488135&doi=10.1002%2f2017GL075619&partnerID=40&md5=5be011f09563821673ca09b39628ecf2","The Soil Moisture Active Passive (SMAP) mission has delivered valuable sensing of surface soil moisture since 2015. However, it has a short time span and irregular revisit schedules. Utilizing a state-of-the-art time series deep learning neural network, Long Short-Term Memory (LSTM), we created a system that predicts SMAP level-3 moisture product with atmospheric forcings, model-simulated moisture, and static physiographic attributes as inputs. The system removes most of the bias with model simulations and improves predicted moisture climatology, achieving small test root-mean-square errors (<0.035) and high-correlation coefficients >0.87 for over 75% of Continental United States, including the forested southeast. As the first application of LSTM in hydrology, we show the proposed network avoids overfitting and is robust for both temporal and spatial extrapolation tests. LSTM generalizes well across regions with distinct climates and environmental settings. With high fidelity to SMAP, LSTM shows great potential for hindcasting, data assimilation, and weather forecasting. ©2017. American Geophysical Union. All Rights Reserved.","deep learning; hindcasting; LSTM; remote sensing; SMAP; soil moisture","Deep learning; Long short-term memory; Mean square error; Moisture; Remote sensing; Soil moisture; Soils; Weather forecasting; Correlation coefficient; Hindcasting; Learning neural networks; LSTM; Root mean square errors; SMAP; Soil moisture active passive (SMAP); Surface soil moisture; Deep neural networks; artificial neural network; data assimilation; hindcasting; learning; numerical model; remote sensing; satellite mission; soil moisture; spatial analysis; spatiotemporal analysis; weather forecasting; United States"
"Hyperspectral Data Haze Monitoring Based on Deep Residual Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038895516&doi=10.3788%2fAOS201737.1128001&partnerID=40&md5=a2fa0184c3410b9f9e54902d2652914f","Haze monitoring is one of the key technologies for environmental governance. At present, the cost of the ground haze monitoring is very high and the accuracy of the multispectral remote sensing haze monitoring is low. The hyperspectral sensing data haze monitoring is studied by deep learning. A hyperspectral haze monitoring algorithm based on deep residual network is presented. The features of haze hyperspectral curves are obtained with the deep network. The difficulty of the network training is decreased with the residual leaning method, and a haze monitoring model is achieved. The experimental results of the Suzhou Hyperion hyperspectral data sets show that, compared with other methods of remote haze monitoring, the proposed method has higher recognition accuracy in haze monitoring. © 2017, Chinese Lasers Press. All right reserved.","Air pollution monitoring; Deep learning; Deep residual network; Haze monitoring; Hyperspectral remote sensing; Machine learning; Remote sensing","Deep learning; Environmental technology; Learning systems; Remote sensing; Air pollution monitoring; Environmental governances; Hyperspectral Data; Hyperspectral remote sensing; Hyperspectral sensing; Monitoring algorithms; Multispectral remote sensing; Recognition accuracy; Monitoring"
"Recognizing terrain features on terrestrial surface using a deep learning model - An example with crater detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040218412&doi=10.1145%2f3149808.3149814&partnerID=40&md5=ad71ee15cce34b18c8c015398e5150bf","This paper exploits the use of a popular deep learning model - the faster-RCNN - to support automatic terrain feature detection and classification using a mixed set of optimal remote sensing and natural images. Crater detection is used as the case study in this research since this geomorphological feature provides important information about surface aging. Craters, such as impact craters, also effect global changes in many aspects, such as geography, topography, mineral and hydrocarbon production, etc. The collected data were labeled and the network was trained through a GPU server. Experimental results show that the faster-RCNN model coupled with a widely used convolutional network ZF-net performs well in detecting craters on the terrestrial surface. © 2017 Association for Computing Machinery.","Crater; Deep learning; Region proposal network; Terrain feature recognition","Deep learning; Landforms; Remote sensing; Convolutional networks; Crater; Crater detections; Hydrocarbon production; Impact craters; Learning models; Terrain features; Terrestrial surface; Feature extraction"
"Deep learning for multisensor image resolution enhancement","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040242092&doi=10.1145%2f3149808.3149815&partnerID=40&md5=2ad3445b25152414e30abe77796641fb","We describe a deep learning convolutional neural network (CNN) for enhancing low resolution multispectral satellite imagery without the use of a panchromatic image. For training, low resolution images are used as input and corresponding high resolution images are used as the target output (label). The CNN learns to automatically extract hierarchical features that can be used to enhance low resolution imagery. The trained network can then be effectively used for super-resolution enhancement of low resolution multispectral images where no corresponding high resolution image is available. The CNN enhances all four spectral bands of the low resolution image simultaneously and adjusts pixel values of the low resolution to match the dynamic range of the high resolution image. The CNN yields higher quality images than standard image resampling methods. © 2017 Association for Computing Machinery.","Convolutional neural networks; Deep learning; Image resolution; Multispectral; Remote sensing; Super resolution","Convolution; Deep learning; Deep neural networks; Image resolution; Neural networks; Optical resolving power; Remote sensing; Satellite imagery; Convolutional neural network; High resolution image; Low resolution images; Low resolution multispectral images; Low-resolution imagery; Multi-spectral; Multispectral satellite imagery; Super resolution; Image enhancement"
"Super-resolution land cover mapping based on deep learning and level set method","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039907576&doi=10.1109%2fECTICon.2017.8096298&partnerID=40&md5=b4df69cd37d630f0cee327e3d03d6178","In this paper, we proposed an approach for super-resolution land cover mapping on remote sensing images based on the deep learning technique, namely Convolutional Neural Network (CNN) by combining with the level set method (LSM). Here, the CNN is used to find the probabilities that a subpixel belonging to a land cover class, and the LSM is employed to fine tune the boundaries among land cover classes. The QUICKBIBD satellite image data cover a part of Kasetsart University was used for evaluation. Experimental result showed that the proposed method has achieved superior accuracy than both Hopfield and Pixel-Swapping methods. © 2017 IEEE.","Deep Learning; Level set method; Supper-resolution mapping","Deep learning; Drop breakup; Level measurement; Neural networks; Numerical methods; Optical resolving power; Pixels; Remote sensing; Convolutional neural network; Land cover mapping; Learning techniques; Level Set method; Remote sensing images; Satellite image datas; Super resolution; Supper resolutions; Mapping"
"A Comparative Analysis of Deep Learning Techniques for Sub-Tropical Crop Types Recognition from Multitemporal Optical/SAR Image Sequences","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040603108&doi=10.1109%2fSIBGRAPI.2017.57&partnerID=40&md5=ff4bb0d0d3dcd484c4a7d52036f59216","Remote Sensing (RS) data have been increasingly applied to assess agricultural yield, production and crop condition. In tropical areas, crop dynamics are complex due to multiple agricultural practices such as irrigation, non-tillage, crop rotation and multiple harvest per year. Spatial and temporal information can improve the performance in land-cover and crop type classification tasks. In this context Deep Learning (DL) have emerged as a powerful state-of-the-art technique in the RS community. This work presents a comparative analysis of traditional and DL (supervised and unsupervised) approaches for crop classification on sequences of multitemporal optical and SAR images. Three different approaches are compared: the image stacking approach, which is used as baseline, and two DL based approaches using Autoencoders (AEs) and Convolutional Neural Networks (CNNs). Experiments were carried out in two datasets from two different municipalities in Brazil, Ipuã in São Paulo state and Campo Verde in Mato Grosso state. It is shown that CNN and AE outperformed the traditional approach based on image stacking in terms of Overall Accuracy and Class Accuracy. © 2017 IEEE.","Autoencoders; Convolutional Neural Networks; Crop Recognition; Multitemporal Images","Agriculture; Classification (of information); Convolution; Crops; Deep learning; Learning systems; Neural networks; Remote sensing; Synthetic aperture radar; Agricultural practices; Autoencoders; Convolutional neural network; Crop recognition; Crop type classification; Multi-temporal image; State-of-the-art techniques; Traditional approaches; Image analysis"
"Learning Deep Features on Multiple Scales for Coffee Crop Recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040625729&doi=10.1109%2fSIBGRAPI.2017.41&partnerID=40&md5=e64a7132f810053513eaa0970500f05d","Geographic mapping of coffee crops by using remote sensing images and supervised classification has been a challenging research subject. Besides the intrinsic problems caused by the nature of multi-spectral information, coffee crops are non-seasonal and usually planted in mountains, which requires encoding and learning a huge diversity of patterns during the classifier training. In this paper, we propose a new approach for automatic mapping coffee crops by combining two recent trends on pattern recognition for remote sensing applications: deep learning and fusion/selection of features from multiple scales. The proposed approach is a pixel-wise strategy that consists in the training and combination of convolutional neural networks designed to receive as input different context windows around labeled pixels. Final maps are created by combining the output of those networks for a non-labeled set of pixels. Experimental results show that multiple scales produces better coffee crop maps than using single scales. Experiments also show the proposed approach is effective in comparison with baselines. © 2017 IEEE.","Agriculture; Coffee Crops; Deep Learning; High-resolution Images; Remote Sensing","Agriculture; Classification (of information); Crops; Deep learning; Mapping; Neural networks; Pattern recognition; Pixels; Automatic mapping; Classifier training; Convolutional neural network; High resolution image; Remote sensing applications; Remote sensing images; Research subjects; Supervised classification; Remote sensing"
"Surface water mapping by deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028462983&doi=10.1109%2fJSTARS.2017.2735443&partnerID=40&md5=9283603714e218ae3e8c4ca89938aa0a","Mapping of surface water is useful in a variety of remote sensing applications, such as estimating the availability of water, measuring its change in time, and predicting droughts and floods. Using the imagery acquired by currently active Landsat missions, a surface water map can be generated from any selected region as often as every 8 days. Traditional Landsat water indices require carefully selected threshold values that vary depending on the region being imaged and on the atmospheric conditions. They also suffer from many false positives, arising mainly from snow and ice, and from terrain and cloud shadows being mistaken for water. Systems that produce high-quality water maps usually rely on ancillary data and complex rule-based expert systems to overcome these problems. Here, we instead adopt a data-driven, deep-learning-based approach to surface water mapping. We propose a fully convolutional neural network that is trained to segment water on Landsat imagery. Our proposed model, named DeepWaterMap, learns the characteristics of water bodies from data drawn from across the globe. The trained model separates water from land, snow, ice, clouds, and shadows using only Landsat bands as input. Our code and trained models are publicly available at http://live.ece.utexas.edu/research/deepwatermap/. © 2008-2012 IEEE.","Computer vision; convolutional neural networks; landsat; machine learning; remote sensing","Computer vision; Convolution; Deep learning; Expert systems; Learning algorithms; Learning systems; Mapping; Neural networks; Remote sensing; Snow; Atmospheric conditions; Convolutional neural network; High quality water; LANDSAT; Landsat imagery; Learning-based approach; Remote sensing applications; Rule based expert systems; Surface waters"
"Image restoration in diffractive optical systems using deep learning and deconvolution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041107177&doi=10.18287%2f2412-6179-2017-41-6-875-887&partnerID=40&md5=666463f6886f6d8bc22f25ff137d9592","In recent years, several pioneering works were dedicated to imaging systems based on simple diffractive structures like Fresnel lenses or phase zone plates. Such systems are much lighter and cheaper than classical refractive optical systems. However, the quality of images obtained by diffractive optics suffers from stronger distortions of various types. In this paper, we show that a combination of the high-precision lens design with post-capture computational reconstruction allows one to attain a much higher image quality. The proposed reconstruction procedure uses a sequence of color correction, deconvolution, and a feedforward deep learning neural network. An improvement both in lens manufacturing and in image processing may contribute to the emergence of ultra-lightweight imaging systems varying from cameras for nano- and picosatellites to surveillance systems. © 2018, Institution of Russian Academy of Sciences. All rights reserved.","Color correction; Deconvolution; Deep learning; Harmonic lens; PSF estimation; Remote sensing","Deconvolution; Feedforward neural networks; Image enhancement; Image reconstruction; Imaging systems; Optical data processing; Optical instrument lenses; Remote sensing; Security systems; Color correction; Computational reconstruction; Diffractive optical systems; Diffractive structures; Learning neural networks; PSF estimation; Reconstruction procedure; Surveillance systems; Deep learning"
"A patch-based convolutional neural network for remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027998504&doi=10.1016%2fj.neunet.2017.07.017&partnerID=40&md5=493ffcffbd9bbe7c3b62f991a8838644","Availability of accurate land cover information over large areas is essential to the global environment sustainability; digital classification using medium-resolution remote sensing data would provide an effective method to generate the required land cover information. However, low accuracy of existing per-pixel based classification methods for medium-resolution data is a fundamental limiting factor. While convolutional neural networks (CNNs) with deep layers have achieved unprecedented improvements in object recognition applications that rely on fine image structures, they cannot be applied directly to medium-resolution data due to lack of such fine structures. In this paper, considering the spatial relation of a pixel to its neighborhood, we propose a new deep patch-based CNN system tailored for medium-resolution remote sensing data. The system is designed by incorporating distinctive characteristics of medium-resolution data; in particular, the system computes patch-based samples from multidimensional top of atmosphere reflectance data. With a test site from the Florida Everglades area (with a size of 771 square kilometers), the proposed new system has outperformed pixel-based neural network, pixel-based CNN and patch-based neural network by 24.36%, 24.23% and 11.52%, respectively, in overall classification accuracy. By combining the proposed deep CNN and the huge collection of medium-resolution remote sensing data, we believe that much more accurate land cover datasets can be produced over large areas. © 2017 Elsevier Ltd","CNN; Deep learning; Medium-resolution; Patch-based; Remote sensing imagery; Spatial context","Convolution; Deep learning; Image classification; Image reconstruction; Neural networks; Object recognition; Pixels; Remote sensing; Sustainable development; Classification accuracy; Convolutional neural network; Land cover informations; Patch based; Pixel based classifications; Remote sensing image classification; Remote sensing imagery; Spatial context; Classification (of information); Article; artificial neural network; atmosphere; classification; comparative study; convolutional neural network; data base; data processing; experimental study; Florida; image analysis; image display; image quality; mathematical computing; measurement accuracy; priority journal; remote sensing; statistical parameters; image processing; procedures; remote sensing; Image Processing, Computer-Assisted; Neural Networks (Computer); Remote Sensing Technology"
"MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031806729&doi=10.1109%2fLGRS.2017.2752750&partnerID=40&md5=801f4f67b235492d76f4bc0c15a32605","With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks. However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model G and a discriminative model D. We treat D as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. G can produce numerous images that are similar to the training data; therefore, D can learn better representations of remotely sensed images using the training data provided by G. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-theart methods. © 2004-2012 IEEE.","Generative adversarial networks (GANs); scene classification; unsupervised representation learning","Feature extraction; Gallium nitride; Gas generators; Image classification; Image enhancement; Personnel training; Remote sensing; Supervised learning; Adversarial networks; Computational model; Scene classification; Training data; unsupervised representation learning; Classification (of information)"
"From Photogrammetry to Computer Vision","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040624954&doi=10.13203%2fj.whugis20170283&partnerID=40&md5=cfc54e7f7ad3de52ab00129c1830e3de","We outline the history of photogrammetry from the aspects of, perspective geometry, camera, platform, measure methods and measure instruments, and summarize previous contributions to photogrammetry. A brief review of computer vision history is given. The tight connections between computer vision and photogrammetry are discussed in terms of geometric principles, and some differences in applications are also considered. From the aspect of semantics, we analyze the development of remote sensing and its relations to machine learning and computer vision, including their common approaches and different applications. The prevailing deep learning raised from connectionism is also reviewed and its successful applications in photogrammetry are analyzed. At last, we expect that the future development of photogrammetry will be more tightly cross-integrated with computer vision, machine learning and artificial intelligence. © 2017, Research and Development Office of Wuhan University. All right reserved.","Computer vision; Machine learning; Photogrammetry; Remote sensing","Artificial intelligence; Learning systems; Photogrammetry; Remote sensing; Semantics; Connectionism; Perspective geometry; Computer vision; artificial intelligence; computer simulation; future prospect; machine learning; photogrammetry; remote sensing"
"Overview of Passive Optical Multispectral and Hyperspectral Image Simulation Techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038393607&doi=10.1109%2fJSTARS.2017.2759240&partnerID=40&md5=05e8a4a14ff5c23d385b3bdf6bd96bff","The simulation of optical images can play key roles in the development of new instruments, the quantitative evaluation of algorithms and in the training of both image analysis software and human analysts. Methods for image simulation include surrogate data collections, operations on empirical imagery, statistical generation techniques, and full physical modeling approaches. Each method offers advantages or disadvantages in terms of time, cost, and realism. Current state of the art suggests three-dimensional radiative transfer models capture most of the significant characteristics of real imagery and find valuable use in system development and evaluation programs. Emerging computational power available from multithreading, graphical processing units, and techniques from deep learning will continue to enable even more realistic simulations in the near future. © 2008-2012 IEEE.","Hyperspectral imaging; image simulation; multispectral imaging; optical remote sensing","Computer software; Geometrical optics; Graphics processing unit; Radiative transfer; Remote sensing; Spectroscopy; Graphical processing unit (GPUs); Hyperspectral image simulation; Image analysis software; Image simulations; Multispectral imaging; Optical remote sensing; Quantitative evaluation; Three-dimensional radiative transfer; Hyperspectral imaging; computer simulation; image analysis; imaging method; multispectral image; remote sensing"
"A transfer learning approach to parking lot classification in aerial imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040198908&doi=10.1109%2fNYSDS.2017.8085049&partnerID=40&md5=f466853c0165a504a31eefbb28c2274a","The importance of satellite imagery analysis has increased dramatically over the last several years, keeping pace with the rapid improvements seen in both remote sensing platforms and sensors. As this field expands, so too does the interest in using machine learning methods to automate parts of the imagery analyst's workflow. In this paper we address one aspect of this challenge: the development of a method for the automatic extraction of parking lots from aerial imagery. To the best of our knowledge, there has been no prior work conducted on the development of an end-to-end pipeline for this particular task. Due to the limited size of our dataset and to accommodate the potentially limited size of future datasets, we propose a deep learning approach using transfer learning. This process hinges upon the use of state of the art Convolutional Neural Networks (CNNs), trained on general image classification datasets. These networks were then fine-tuned on our custom dataset, to establish a comprehensive benchmark for this task. Our method exhibits promising results for automatic parking lot extraction, and is generalizable enough to work with different input types, including high resolution aerial orthoimagery, satellite imagery, full motion video (FMV), and UAV imagery. © 2017 IEEE.","Automation; Deep Learning; Geospatial; Neural Network; Satellite Imagery","Aerial photography; Automation; Classification (of information); Deep learning; Extraction; Image analysis; Image classification; Image enhancement; Learning systems; Neural networks; Remote sensing; Satellites; Automatic extraction; Classification datasets; Convolutional neural network; Full motion video; Geo-spatial; Machine learning methods; Remote sensing platforms; Transfer learning; Satellite imagery"
"2017 New York Scientific Data Summit, NYSDS 2017 - Proceedings","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040163332&partnerID=40&md5=c7684f3ebbd3c225080160ede87a5705","The proceedings contain 22 papers. The topics discussed include: statistical data reduction for streaming data; progressive clustering of big data with GPU acceleration and visualization; making a case for high-bandwidth monitoring - a use case for analysis on the wire; implementing a distributed volumetric data analytics toolkit on apache spark; building near-real-time processing pipelines with the spark-MPI platform; comparative study of deep learning framework in HPC environments; a scientific data provenance harvester for distributed applications; parallelizing x-ray photon correlation spectroscopy software tools using python multiprocessing; capturing provenance as a diagnostic tool for work- flow performance evaluation and optimization; robust and scalable deep learning for x-ray synchrotron image analysis; machine learning aided prediction of family history of depression; sensor network-based wind field estimation using deep learning; remote sensing data integration for mapping glacial extents; a transfer learning approach to parking lot classification in aerial imagery; meeting the challenges of data analysis on the wire; visualization of Higgs potentials and decays from sources beyond the standard model including dark matter and extra dimensions; automated x-ray diffraction of irradiated materials; and visualization of higher genus carbon nanomaterials: free energy, persistent current and entanglement entropy.",,
"Hyperspectral classification using stacked autoencoders with deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038571985&doi=10.1109%2fWHISPERS.2014.8077532&partnerID=40&md5=68044c701849eaec0d5b72753e8bc784","In this study, stacked autoencoders which are widely utilized in deep learning research are applied to remote sensing domain for hyperspectral classification. High dimensional hyperspectral data is an excellent candidate for deep learning methods. However, there are no works in literature that focuses on such deep learning approaches for hyperspectral imagery. This study aims to fill this gap by utilizing stacked autoencoders. Experiments are conducted on the Pavia University scene. Using stacked autoencoders, intrinsic representations of the data are learned in an unsupervised way. Using labeled data, these representations are fine tuned. Then, using a soft-max activation function, hyperspectral classification is done. Parameter optimization of Stacked Autoencoders (SAE) is done with extensive experiments. Results are competitive with the state-of-the-art techniques. © 2014 IEEE.","Deep Learning; Hyperspectral Classification; Stacked Autoencoders","Classification (of information); Deep learning; Image processing; Learning systems; Signal processing; Spectroscopy; Activation functions; Autoencoders; Hyper-spectral classification; Hyper-spectral imageries; Hyperspectral Data; Learning approach; Parameter optimization; State-of-the-art techniques; Remote sensing"
"Classification of hyperspectral image based on principal component analysis and deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035796196&doi=10.1109%2fICEIEC.2017.8076581&partnerID=40&md5=c1d564539b8832dc0ef516a67c8bb453","Classification is an important way to explore the information in hyperspectral images (HSIs). Deep learning algorithm has been introduced into HSI processing and achieved some preliminary results. Deep belief network (DBN) is a typical deep learning model, which combines the advantages of unsupervised and supervised learning and has a good performance in big data classification. In this paper, taken into account both the spatial information and the spectral characteristics of HSI, a hybrid classification method combined DBN with principal component analysis (PCA) is proposed. Experimental results show that the PCA-DBN method has promising prospect in the HSI classification. © 2017 IEEE.","back propagation (BP) neural network; classification; deep belief network (DBN); F-norm; Remote sensing image; restricted boltzmann machine (RBM)","Backpropagation; Big data; Classification (of information); Deep learning; Image classification; Independent component analysis; Learning algorithms; Remote sensing; Spectroscopy; Back propagation neural networks; Deep belief network (DBN); F norms; Remote sensing images; Restricted boltzmann machine; Principal component analysis"
"Special Section Guest Editorial: Feature and Deep Learning in Remote Sensing Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040313862&doi=10.1117%2f1.JRS.11.042601&partnerID=40&md5=29de82d0097d810e54f65deed42047b1","This PDF file contains the editorial ""Special Section Guest Editorial: Feature and Deep Learning in Remote Sensing Application"" for JARS Vol. 11 Issue 4. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","computer vision; convolutional neural networks; deep learning; remote sensing","Computer vision; Deep learning; Deep neural networks; Neural networks; Convolutional neural network; PDF files; Remote sensing applications; Special sections; Remote sensing"
"Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032865390&doi=10.1117%2f1.JRS.11.042609&partnerID=40&md5=51c93993ddf0601eab19bf381ddf9762","In recent years, deep learning (DL), a rebranding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, and natural language processing. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV, e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should not only be aware of advancements such as DL, but also be leading researchers in this area. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools, and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as they relate to (i) inadequate data sets, (ii) human-understandable solutions for modeling physical phenomena, (iii) big data, (iv) nontraditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial, and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL. © 2017 The Authors.","big data; computer vision; deep learning; hyperspectral; multispectral; remote sensing","Big data; Computer vision; Learning algorithms; Natural language processing systems; Remote sensing; Speech recognition; Surveys; Heterogeneous data sources; HyperSpectral; Multi-spectral; NAtural language processing; Neural networks (NNS); Physical phenomena; State of the art; Transfer learning; Deep learning"
"Multiview Canonical Correlation Analysis Networks for Remote Sensing Image Recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029176261&doi=10.1109%2fLGRS.2017.2738671&partnerID=40&md5=37582ccc144b3870169a4d67801e61ab","In the past decade, deep learning (DL) algorithms have been widely used for remote sensing (RS) image recognition tasks. As the most typical DL model, convolutional neural networks (CNNs) achieves outstand performance for big RS data classification. Recently, a variant of CNN, dubbed canonical correlation analysis network (CCANet), was proposed to abstract the two-view image features. Extensive experiments conducted on several benchmark databases validate the effectiveness of CCANet. However, the CCANet structure is powerless when the observations arrive from more than two sources. To serve the multiview purpose, in this letter, we propose multiview CCANets (MCCANets). Particularly, the MCCANet model learns the stacked multiperspective filter banks by the MCCA method and builds a deep convolutional structure. In the output stage, the binarization and the blockwise histogram are employed as nonlinear processing and feature pooling, respectively. To access the effectiveness of the MCCANet, we conduct a host of experiments on the RSSCN7 RS database. Extensive experimental results demonstrate that the MCCANet outperforms the two-view CCANet. © 2004-2012 IEEE.","Deep learning (DL); image recognition; multiview canonical correlation analysis (MCCA); remote sensing (RS)","Convolution; Correlation methods; Deep learning; Eigenvalues and eigenfunctions; Graphic methods; Image analysis; Image recognition; Image reconstruction; Learning algorithms; Learning systems; Neural networks; Algorithm design and analysis; Benchmark database; Canonical correlation analysis; Convolutional neural network; Data classification; Histograms; Nonlinear processing; Remote sensing images; Remote sensing"
"Remote Sensing Image Scene Classification: Benchmark and State of the Art","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017152027&doi=10.1109%2fJPROC.2017.2675998&partnerID=40&md5=4465016135c1113851e3e2cf656e9c03","Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various data sets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning data sets and methods for scene classification is still lacking. In addition, almost all existing data sets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale data set, termed 'NWPU-RESISC45,' which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This data set contains 31 500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 1) is large-scale on the scene classes and the total image number; 2) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion; and 3) has high within-class diversity and between-class similarity. The creation of this data set will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed data set, and the results are reported as a useful baseline for future research. © 1963-2012 IEEE.","Benchmark data set; deep learning; handcrafted features; remote sensing image; scene classification; unsupervised feature learning","Deep learning; Image classification; Remote sensing; Benchmark data; handcrafted features; Remote sensing images; Scene classification; Unsupervised feature learning; Classification (of information)"
"Spectral-spatial feature learning for hyperspectral imagery classification using deep stacked sparse autoencoder","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028544452&doi=10.1117%2f1.JRS.11.042604&partnerID=40&md5=86b41fbd528da7ce21fce3ee13cf17a6","Classification of hyperspectral remote sensing imagery is one of the most popular topics because of its intrinsic potential to gather spectral signatures of materials and provides distinct abilities to object detection and recognition. In the last decade, an enormous number of methods were suggested to classify hyperspectral remote sensing data using spectral features, though some are not using all information and lead to poor classification accuracy; on the other hand, the exploration of deep features is recently considered a lot and has turned into a research hot spot in the geoscience and remote sensing research community to enhance classification accuracy. A deep learning architecture is proposed to classify hyperspectral remote sensing imagery by joint utilization of spectral-spatial information. A stacked sparse autoencoder provides unsupervised feature learning to extract high-level feature representations of joint spectral-spatial information; then, a soft classifier is employed to train high-level features and to fine-tune the deep learning architecture. Comparative experiments are performed on two widely used hyperspectral remote sensing data (Salinas and PaviaU) and a coarse resolution hyperspectral data in the long-wave infrared range. The obtained results indicate the superiority of the proposed spectral-spatial deep learning architecture against the conventional classification methods. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","deep features; deep learning; hyperspectral imagery classification; softmax regression; spectral-spatial unsupervised feature learning; stacked sparse autoencoder","Architecture; Deep learning; Image classification; Infrared radiation; Machine learning; Object detection; Object recognition; Remote sensing; Spectroscopy; Auto encoders; deep features; Hyperspectral imagery classifications; Softmax regressions; Unsupervised feature learning; Classification (of information)"
"Social Media: New Perspectives to Improve Remote Sensing for Emergency Response","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018653090&doi=10.1109%2fJPROC.2017.2684460&partnerID=40&md5=86732fe314687ff0ace1eea442c0e43e","Remote sensing is a powerful technology for Earth observation (EO), and it plays an essential role in many applications, including environmental monitoring, precision agriculture, resource managing, urban characterization, disaster and emergency response, etc. However, due to limitations in the spectral, spatial, and temporal resolution of EO sensors, there are many situations in which remote sensing data cannot be fully exploited, particularly in the context of emergency response (i.e., applications in which real/near-real-time response is needed). Recently, with the rapid development and availability of social media data, new opportunities have become available to complement and fill the gaps in remote sensing data for emergency response. In this paper, we provide an overview on the integration of social media and remote sensing in time-critical applications. First, we revisit the most recent advances in the integration of social media and remote sensing data. Then, we describe several practical case studies and examples addressing the use of social media data to improve remote sensing data and/or techniques for emergency response. © 1963-2012 IEEE.","Deep learning; emergency response; remote sensing; social media","Deep learning; Emergency services; Environmental technology; Social networking (online); Earth observations; Emergency response; Environmental Monitoring; Remote sensing data; Social media; Social media datum; Temporal resolution; Time-critical applications; Remote sensing"
"A Novel Methodology to Label Urban Remote Sensing Images Based on Location-Based Social Media Photos","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029173213&doi=10.1109%2fJPROC.2017.2730585&partnerID=40&md5=1b88a3a6cbae343445f602e4077a9bc4","With the rapid development of the internet and popularization of intelligent mobile devices, social media is evolving fast and contains rich spatial information, such as geolocated posts, tweets, photos, video, and audio. Those location-based social media data have offered new opportunities for hazards and disaster identification or tracking, recommendations for locations, friends or tags, pay-per-click advertising, etc. Meanwhile, a massive amount of remote sensing (RS) data can be easily acquired in both high temporal and spatial resolution with a multiple satellite system, if RS maps can be provided, to possibly enable the monitoring of our location-based living environments with some devices like charge-coupled device (CCD) cameras but on a much larger scale. To generate the classification maps, usually, labeled RS image pixels should be provided by RS experts to train a classification system. Traditionally, labeled samples are obtained according to ground surveys, image photo interpretation or a combination of the aforementioned strategies. All the strategies should be taken care of by domain experts, in a means which is costly, time consuming, and sometimes of a low quality due to reasons such as photo interpretation based on RS images only. These practices and constraints make it more challenging to classify land-cover RS images using big RS data. In this paper, a new methodology is proposed to classify urban RS images by exploiting the semantics of location-based social media photos (SMPs). To validate the effectiveness of this methodology, an automatic classification system is developed based on RS images as well as SMPs via big data analysis techniques including active learning, crowdsourcing, shallow machine learning, and deep learning. As the labels of RS training data are given by ordinary people with a crowdsourcing technique, the developed system is named Crowd4RS. The quantitative and qualitative experiments confirm the effectiveness of the proposed Crowd4RS system as well as the proposed methodology for automatically generating RS image maps in terms of classification results based on big RS data made up of multispectral RS images in a high spatial resolution and a large amount of photos from social media sites, such as Flickr and Panoramio. © 1963-2012 IEEE.","Big data; crowdsourcing; deep learning; remote sensing; social media","Artificial intelligence; Big data; Charge coupled devices; Crowdsourcing; Deep learning; Image resolution; Location; Photointerpretation; Remote sensing; Semantics; Social networking (online); Automatic classification systems; Classification results; Data analysis techniques; High spatial resolution; Multiple satellite systems; Qualitative experiments; Social media; Urban remote sensing images; Image classification"
"Knowledge-guided golf course detection using a convolutional neural network fine-Tuned on temporally augmented data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038431766&doi=10.1117%2f1.JRS.11.042619&partnerID=40&md5=80cdc45d64f260f014ca821bf33d914f","The tremendous success of deep learning models such as convolutional neural networks (CNNs) in computer vision provides a method for similar problems in the field of remote sensing. Although research on repurposing pretrained CNN to remote sensing tasks is emerging, the scarcity of labeled samples and the complexity of remote sensing imagery still pose challenges. We developed a knowledge-guided golf course detection approach using a CNN fine-Tuned on temporally augmented data. The proposed approach is a combination of knowledge-driven region proposal, data-driven detection based on CNN, and knowledge-driven postprocessing. To confront data complexity, knowledge-derived cooccurrence, composition, and area-based rules are applied sequentially to propose candidate golf regions. To confront sample scarcity, we employed data augmentation in the temporal domain, which extracts samples from multitemporal images. The augmented samples were then used to fine-Tune a pretrained CNN for golf detection. Finally, commission error was further suppressed by postprocessing. Experiments conducted on GF-1 imagery prove the effectiveness of the proposed approach. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","convolutional neural network; data augmentation; golf course; object detection; remote sensing.","Complex networks; Convolution; Deep learning; Neural networks; Object detection; Recreational facilities; Sports; Convolutional neural network; Data augmentation; Data complexity; Detection approach; Golf course; Learning models; Multi-temporal image; Remote sensing imagery; Remote sensing"
"Land Cover Classification via Multitemporal Spatial Data by Deep Recurrent Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029152660&doi=10.1109%2fLGRS.2017.2728698&partnerID=40&md5=532543fbe0945f3278b11f16f6534880","Nowadays, modern earth observation programs produce huge volumes of satellite images time series that can be useful to monitor geographical areas through time. How to efficiently analyze such a kind of information is still an open question in the remote sensing field. Recently, deep learning methods proved suitable to deal with remote sensing data mainly for scene classification(i.e., convolutional neural networks on single images) while only very few studies exist involving temporal deep learning approaches [i.e., recurrent neural networks (RNNs)] to deal with remote sensing time series. In this letter, we evaluate the ability of RNNs, in particular, the long short-term memory (LSTM) model, to perform land cover classification considering multitemporal spatial data derived from a time series of satellite images. We carried out experiments on two different data sets considering both pixel-based and object-based classifications. The obtained results show that RNNs are competitive compared with the state-of-the-art classifiers, and may outperform classical approaches in the presence of low represented and/or highly mixed classes. We also show that the alternative feature representation generated by LSTM can improve the performances of standard classifiers. © 2004-2012 IEEE.","Deep learning; land cover classification; recurrent neural networks (RNNs); satellite image time series","Deep learning; Deep neural networks; Long short-term memory; Neural networks; Recurrent neural networks; Remote sensing; Satellites; Time series; Convolutional neural network; Earth observation programs; Feature representation; Land cover classification; Object-based classifications; Recurrent neural network (RNNs); Satellite images; Scene classification; Classification (of information)"
"Deep kernel learning method for SAR image target recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032729397&doi=10.1063%2f1.4993064&partnerID=40&md5=b098278681f053b2c8c63c7758be1146","With the development of deep learning, research on image target recognition has made great progress in recent years. Remote sensing detection urgently requires target recognition for military, geographic, and other scientific research. This paper aims to solve the synthetic aperture radar image target recognition problem by combining deep and kernel learning. The model, which has a multilayer multiple kernel structure, is optimized layer by layer with the parameters of Support Vector Machine and a gradient descent algorithm. This new deep kernel learning method improves accuracy and achieves competitive recognition results compared with other learning methods. © 2017 Author(s).",,"Learning systems; Radar imaging; Remote sensing; Synthetic aperture radar; Gradient descent algorithms; Image target recognition; Kernel learning methods; Learning methods; Multiple kernels; Scientific researches; Synthetic aperture radar (SAR) images; Target recognition; Radar target recognition; learning; support vector machine; telecommunication; article"
"Application of deep convolutional neural networks for ocean front recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029782035&doi=10.1117%2f1.JRS.11.042610&partnerID=40&md5=c22b71fdf7f6551a88b63901e2aabf47","Ocean fronts have been a subject of study for many years, a variety of methods and algorithms have been proposed to address the problem of ocean fronts. However, all these existing ocean front recognition methods are built upon human expertise in defining the front based on subjective thresholds of relevant physical variables. This paper proposes a deep learning approach for ocean front recognition that is able to automatically recognize the front. We first investigated four existing deep architectures, i.e., AlexNet, CaffeNet, GoogLeNet, and VGGNet, for the ocean front recognition task using remote sensing (RS) data. We then propose a deep network with fewer layers compared to existing architecture for the front recognition task. This network has a total of five learnable layers. In addition, we extended the proposed network to recognize and classify the front into strong and weak ones. We evaluated and analyzed the proposed network with two strategies of exploiting the deep model: full-training and fine-tuning. Experiments are conducted on three different RS image datasets, which have different properties. Experimental results show that our model can produce accurate recognition results. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","convolutional neural networks; deep learning; fine-tuning; full-training; ocean front recognition; remote sensing","Convolution; Deep learning; Network architecture; Network layers; Neural networks; Oceanography; Remote sensing; Convolutional neural network; Deep architectures; Existing architectures; Fine tuning; Ocean front; Physical variables; Recognition methods; Remote sensing data; Deep neural networks"
"Semantic labeling of high-resolution aerial images using an ensemble of fully convolutional networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034968011&doi=10.1117%2f1.JRS.11.042617&partnerID=40&md5=03821d017d2a05a041a9fb8b83b8a98c","High-resolution remote sensing data classification has been a challenging and promising research topic in the community of remote sensing. In recent years, with the rapid advances of deep learning, remarkable progress has been made in this field, which facilitates a transition from hand-crafted features designing to an automatic end-to-end learning. A deep fully convolutional networks (FCNs) based ensemble learning method is proposed to label the high-resolution aerial images. To fully tap the potentials of FCNs, both the Visual Geometry Group network and a deeper residual network, ResNet, are employed. Furthermore, to enlarge training samples with diversity and gain better generalization, in addition to the commonly used data augmentation methods (e.g., rotation, multiscale, and aspect ratio) in the literature, aerial images from other datasets are also collected for cross-scene learning. Finally, we combine these learned models to form an effective FCN ensemble and refine the results using a fully connected conditional random field graph model. Experiments on the ISPRS 2-D Semantic Labeling Contest dataset show that our proposed end-to-end classification method achieves an overall accuracy of 90.7%, a state-of-the-art in the field. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","aerial images; convolutional neural network; ensemble learning; fully convolutional network; semantic labeling","Antennas; Aspect ratio; Classification (of information); Convolution; Image segmentation; Neural networks; Remote sensing; Semantic Web; Semantics; Aerial images; Convolutional networks; Convolutional neural network; Ensemble learning; Semantic labeling; Deep learning"
"Deep convolutional neural networks for building extraction from orthoimages and dense image matching point clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038568173&doi=10.1117%2f1.JRS.11.042620&partnerID=40&md5=f9b439e031fbaa38c5ec58ac7bb40cb7","Automatic extraction of buildings from remote sensing data is an attractive research topic, useful for several applications, such as cadastre and urban planning. This is mainly due to the inherent artifacts of the used data and the differences in viewpoint, surrounding environment, and complex shape and size of the buildings. This paper introduces an efficient deep learning framework based on convolutional neural networks (CNNs) toward building extraction from orthoimages. In contrast to conventional deep approaches in which the raw image data are fed as input to the deep neural network, in this paper the height information is exploited as an additional feature being derived from the application of a dense image matching algorithm. As test sites, several complex urban regions of various types of buildings, pixel resolutions and types of data are used, located in Vaihingen in Germany and in Perissa in Greece. Our method is evaluated using the rates of completeness, correctness, and quality and compared with conventional and other ""shallow"" learning paradigms such as support vector machines. Experimental results indicate that a combination of raw image data with height information, feeding as input to a deep CNN model, provides potentials in building detection in terms of robustness, flexibility, and efficiency. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","building extraction; convolutional neural networks; deep learning; dense image matching; orthoimage; point cloud","Buildings; Complex networks; Convolution; Deep learning; Extraction; Image matching; Image processing; Neural networks; Remote sensing; Support vector machines; Urban growth; Automatic extraction; Building extraction; Convolutional neural network; Image matching algorithm; Orthoimages; Point cloud; Remote sensing data; Surrounding environment; Deep neural networks"
"Boosting the Accuracy of Multispectral Image Pansharpening by Learning a Deep Residual Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028507222&doi=10.1109%2fLGRS.2017.2736020&partnerID=40&md5=3510ad6b22b2319ddce6baca3eff6565","In the field of multispectral (MS) and panchromatic image fusion (pansharpening), the impressive effectiveness of deep neural networks has recently been employed to overcome the drawbacks of the traditional linear models and boost the fusion accuracy. However, the existing methods are mainly based on simple and flat networks with relatively shallow architectures, which severely limits their performance. In this letter, the concept of residual learning is introduced to form a very deep convolutional neural network to make the full use of the high nonlinearity of the deep learning models. Through both quantitative and visual assessments on a large number of high-quality MS images from various sources, it is confirmed that the proposed model is superior to all the mainstream algorithms included in the comparison, and achieves the highest spatial-spectral unified accuracy. © 2004-2012 IEEE.","Convolutional neural network; data fusion; pansharpening; remote sensing; residual learning","Convolution; Data fusion; Deep learning; Feature extraction; Image fusion; Learning systems; Neural networks; Personnel training; Remote sensing; Spectrum analysis; Convolutional neural network; High nonlinearity; Multispectral images; Pan-sharpening; Panchromatic images; residual learning; Spatial resolution; Visual assessments; Deep neural networks"
"Distinguishing Cloud and Snow in Satellite Images via Deep Convolutional Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028506011&doi=10.1109%2fLGRS.2017.2735801&partnerID=40&md5=7317b6c7547f2fc95f4c5a423d2828ab","Cloud and snow detection has significant remote sensing applications, while they share similar low-level features due to their consistent color distributions and similar local texture patterns. Thus, accurately distinguishing cloud from snow in pixel level from satellite images is always a challenging task with traditional approaches. To solve this shortcoming, in this letter, we proposed a deep learning system to classify cloud and snow with fully convolutional neural networks in pixel level. Specifically, a specially designed fully convolutional network was introduced to learn deep patterns for cloud and snow detection from the multispectrum satellite images. Then, a multiscale prediction strategy was introduced to integrate the low-level spatial information and high-level semantic information simultaneously. Finally, a new and challenging cloud and snow data set was labeled manually to train and further evaluate the proposed method. Extensive experiments demonstrate that the proposed deep model outperforms the state-of-the-art methods greatly both in quantitative and qualitative performances. © 2017 IEEE.","Cloud and snow detection; fully convolutional network; multiscale prediction","Convolution; Feature extraction; Neural networks; Pixels; Remote sensing; Satellites; Semantics; Convolutional networks; Convolutional neural network; Image color analysis; Multiscale predictions; Remote sensing applications; Spatial informations; State-of-the-art methods; Traditional approaches; Snow"
"Do deep convolutional neural networks really need to be deep when applied for remote scene classification?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029792797&doi=10.1117%2f1.JRS.11.042613&partnerID=40&md5=c81782d24338f2fca65f9baefcff6bfc","Deep convolutional neural networks (CNNs) have been widely used to obtain highlevel representation in various computer vision tasks. However, for remote scene classification, there are not sufficient images to train a very deep CNN from scratch. From two viewpoints of generalization power, we propose two promising kinds of deep CNNs for remote scenes and try to find whether deep CNNs need to be deep for remote scene classification. First, we transfer successful pretrained deep CNNs to remote scenes based on the theory that depth of CNNs brings the generalization power by learning available hypothesis for finite data samples. Second, according to the opposite viewpoint that generalization power of deep CNNs comes from massive memorization and shallow CNNs with enough neural nodes have perfect finite sample expressivity, we design a lightweight deep CNN (LDCNN) for remote scene classification. With five well-known pretrained deep CNNs, experimental results on two independent remote-sensing datasets demonstrate that transferred deep CNNs can achieve state-of-the-art results in an unsupervised setting. However, because of its shallow architecture, LDCNN cannot obtain satisfactory performance, regardless of whether in an unsupervised, semisupervised, or supervised setting. CNNs really need depth to obtain general features for remote scenes. This paper also provides baseline for applying deep CNNs to other remote sensing tasks. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","convolutional neural network; deep learning; generalization power; remote sensing; scene classification","Convolution; Deep learning; Neural networks; Remote sensing; Sampling; Convolutional neural network; Data sample; Finite samples; generalization power; Scene classification; Semi-supervised; State of the art; Deep neural networks"
"Translation-Aware semantic segmentation via conditional least-square generative adversarial networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040311479&doi=10.1117%2f1.JRS.11.042622&partnerID=40&md5=dc2dfe2f51297a51d76b480e2f57de18","Semantic segmentation has recently made rapid progress in the field of remote sensing and computer vision. However, many leading approaches cannot simultaneously translate label maps to possible source images with a limited number of training images. The core issue is insufficient adversarial information to interpret the inverse process and proper objective loss function to overcome the vanishing gradient problem. We propose the use of conditional least squares generative adversarial networks (CLS-GAN) to delineate visual objects and solve these problems. We trained the CLS-GAN network for semantic segmentation to discriminate dense prediction information either from training images or generative networks. We show that the optimal objective function of CLS-GAN is a special class of f-divergence and yields a generator that lies on the decision boundary of discriminator that reduces possible vanished gradient.We also demonstrate the effectiveness of the proposed architecture at translating images from label maps in the learning process. Experiments on a limited number of high resolution images, including close-range and remote sensing datasets, indicate that the proposed method leads to the improved semantic segmentation accuracy and can simultaneously generate high quality images from label maps. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep Learning; Divergence.; Generative Adversarial Network; Semantic Segmentation","Deep learning; Image segmentation; Inverse problems; Remote sensing; Semantic Web; Semantics; Adversarial networks; Divergence; High quality images; High resolution image; Objective functions; Prediction informations; Proposed architectures; Semantic segmentation; Image enhancement"
"Contextually guided very-high-resolution imagery classification with semantic segments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028705162&doi=10.1016%2fj.isprsjprs.2017.08.011&partnerID=40&md5=5a709832851fea41a1d3f6187b896199","Contextual information, revealing relationships and dependencies between image objects, is one of the most important information for the successful interpretation of very-high-resolution (VHR) remote sensing imagery. Over the last decade, geographic object-based image analysis (GEOBIA) technique has been widely used to first divide images into homogeneous parts, and then to assign semantic labels according to the properties of image segments. However, due to the complexity and heterogeneity of VHR images, segments without semantic labels (i.e., semantic-free segments) generated with low-level features often fail to represent geographic entities (such as building roofs usually be partitioned into chimney/antenna/shadow parts). As a result, it is hard to capture contextual information across geographic entities when using semantic-free segments. In contrast to low-level features, “deep” features can be used to build robust segments with accurate labels (i.e., semantic segments) in order to represent geographic entities at higher levels. Based on these semantic segments, semantic graphs can be constructed to capture contextual information in VHR images. In this paper, semantic segments were first explored with convolutional neural networks (CNN) and a conditional random field (CRF) model was then applied to model the contextual information between semantic segments. Experimental results on two challenging VHR datasets (i.e., the Vaihingen and Beijing scenes) indicate that the proposed method is an improvement over existing image classification techniques in classification performance (overall accuracy ranges from 82% to 96%). © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Contextual information; CRF model; Deep learning; Semantic segmentation; VHR images","Classification (of information); Deep learning; Image segmentation; Neural networks; Random processes; Remote sensing; Semantics; Classification performance; Classification technique; Contextual information; Convolutional neural network; Crf models; Geographic object-based image analysis; Semantic segmentation; VHR images; Image classification; artificial neural network; complexity; image analysis; image classification; image resolution; machine learning; numerical model; remote sensing; segmentation; Beijing [China]; China"
"Generative adversarial networks-based semi-supervised learning for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032860153&doi=10.3390%2frs9101042&partnerID=40&md5=e77ea2ac30af37958bdbbb61d3e6894e","Classification of hyperspectral image (HSI) is an important research topic in the remote sensing community. Significant efforts (e.g., deep learning) have been concentrated on this task. However, it is still an open issue to classify the high-dimensional HSI with a limited number of training samples. In this paper, we propose a semi-supervised HSI classification method inspired by the generative adversarial networks (GANs). Unlike the supervised methods, the proposed HSI classification method is semi-supervised, which can make full use of the limited labeled samples as well as the sufficient unlabeled samples. Core ideas of the proposed method are twofold. First, the three-dimensional bilateral filter (3DBF) is adopted to extract the spectral-spatial features by naturally treating the HSI as a volumetric dataset. The spatial information is integrated into the extracted features by 3DBF, which is propitious to the subsequent classification step. Second, GANs are trained on the spectral-spatial features for semi-supervised learning. A GAN contains two neural networks (i.e., generator and discriminator) trained in opposition to one another. The semi-supervised learning is achieved by adding samples from the generator to the features and increasing the dimension of the classifier output. Experimental results obtained on three benchmark HSI datasets have confirmed the effectiveness of the proposed method, especially with a limited number of labeled samples. © 2017 by the authors.","Generative adversarial networks (GANs); Hyperspectral image (HSI); Semi-supervised classification; Three-dimensional bilateral filter (3DBF)","Image classification; Nonlinear filtering; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Bilateral filters; Classification methods; Semi- supervised learning; Semi-supervised classification; Spatial informations; Supervised methods; Volumetric dataset; Classification (of information)"
"Application of deep networks to oil spill detection using polarimetric synthetic aperture radar images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029809914&doi=10.3390%2fapp7100968&partnerID=40&md5=2191640ba3a9bb4789fa4299d7a24eb3","Polarimetric synthetic aperture radar (SAR) remote sensing provides an outstanding tool in oil spill detection and classification, for its advantages in distinguishing mineral oil and biogenic lookalikes. Various features can be extracted from polarimetric SAR data. The large number and correlated nature of polarimetric SAR features make the selection and optimization of these features impact on the performance of oil spill classification algorithms. In this paper, deep learning algorithms such as the stacked autoencoder (SAE) and deep belief network (DBN) are applied to optimize the polarimetric feature sets and reduce the feature dimension through layer-wise unsupervised pre-training. An experiment was conducted on RADARSAT-2 quad-polarimetric SAR image acquired during the Norwegian oil-on-water exercise of 2011, in which verified mineral, emulsions, and biogenic slicks were analyzed. The results show that oil spill classification achieved by deep networks outperformed both support vector machine (SVM) and traditional artificial neural networks (ANN) with similar parameter settings, especially when the number of training data samples is limited. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Autoencoder; Deep belief network; Oil spill; Polarimetric synthetic aperture radar (SAR); Remote sensing",
"Application of convolutional neural network in classification of high resolution agricultural remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030991050&doi=10.5194%2fisprs-archives-XLII-2-W7-989-2017&partnerID=40&md5=90678176ab9e554fbfe2d237b0dc74c6","With the rapid development of Precision Agriculture (PA) promoted by high-resolution remote sensing, it makes significant sense in management and estimation of agriculture through crop classification of high-resolution remote sensing image. Due to the complex and fragmentation of the features and the surroundings in the circumstance of high-resolution, the accuracy of the traditional classification methods has not been able to meet the standard of agricultural problems. In this case, this paper proposed a classification method for high-resolution agricultural remote sensing images based on convolution neural networks(CNN). For training, a large number of training samples were produced by panchromatic images of GF-1 high-resolution satellite of China. In the experiment, through training and testing on the CNN under the toolbox of deep learning by MATLAB, the crop classification finally got the correct rate of 99.66% after the gradual optimization of adjusting parameter during training. Through improving the accuracy of image classification and image recognition, the applications of CNN provide a reference value for the field of remote sensing in PA. © Authors 2017. CC BY 4.0 License.","Agricultural remote sensing; Convolutional neural network; Crop classification; Deep learning; High resolution image","Agriculture; Convolution; Crops; Deep learning; Image classification; Image enhancement; Image recognition; Neural networks; Agricultural remote sensing; Convolution neural network; Convolutional neural network; Crop classification; High resolution image; High resolution remote sensing; High resolution remote sensing images; High resolution satellites; Remote sensing"
"Sensing urban land-use patterns by integrating Google Tensorflow and scene-classification models","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030982378&doi=10.5194%2fisprs-archives-XLII-2-W7-981-2017&partnerID=40&md5=b6415c1c8183f2586f59e1abffab21a7","With the rapid progress of China's urbanization, research on the automatic detection of land-use patterns in Chinese cities is of substantial importance. Deep learning is an effective method to extract image features. To take advantage of the deep-learning method in detecting urban land-use patterns, we applied a transfer-learning-based remote-sensing image approach to extract and classify features. Using the Google Tensorflow framework, a powerful convolution neural network (CNN) library was created. First, the transferred model was previously trained on ImageNet, one of the largest object-image data sets, to fully develop the model's ability to generate feature vectors of standard remote-sensing land-cover data sets (UC Merced and WHU-SIRI). Then, a random-forest-based classifier was constructed and trained on these generated vectors to classify the actual urban land-use pattern on the scale of traffic analysis zones (TAZs). To avoid the multi-scale effect of remote-sensing imagery, a large random patch (LRP) method was used. The proposed method could efficiently obtain acceptable accuracy (OA = 0.794, Kappa = 0.737) for the study area. In addition, the results show that the proposed method can effectively overcome the multi-scale effect that occurs in urban land-use classification at the irregular land-parcel level. The proposed method can help planners monitor dynamic urban land use and evaluate the impact of urban-planning schemes. © Authors 2017. CC BY 4.0 License.","Deep learning; Land parcels; Land use; Scene classification; Tensorflow","Decision trees; Deep learning; Image processing; Remote sensing; Automatic Detection; Convolution neural network; Land parcels; Remote sensing imagery; Remote sensing images; Scene classification; Tensorflow; Traffic analysis zones; Land use"
"Deep learning in remote sensing scene classification: a data augmentation enhanced convolutional neural network framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018360536&doi=10.1080%2f15481603.2017.1323377&partnerID=40&md5=f748a6d86cb2378a553536e8baae6776","The recent emergence of deep learning for characterizing complex patterns in remote sensing imagery reveals its high potential to address some classic challenges in this domain, e.g. scene classification. Typical deep learning models require extremely large datasets with rich contents to train a multilayer structure in order to capture the essential features of scenes. Compared with the benchmark datasets used in popular deep learning frameworks, however, the volumes of available remote sensing datasets are particularly limited, which have restricted deep learning methods from achieving full performance gains. In order to address this fundamental problem, this article introduces a methodology to not only enhance the volume and completeness of training data for any remote sensing datasets, but also exploit the enhanced datasets to train a deep convolutional neural network that achieves state-of-the-art scene classification performance. Specifically, we propose to enhance any original dataset by applying three operations–flip, translation, and rotation to generate augmented data–and use the augmented dataset to train and obtain a more descriptive deep model. The proposed methodology is validated in three recently released remote sensing datasets, and confirmed as an effective technique that significantly contributes to potentially revolutionary changes in remote sensing scene classification, empowered by deep learning. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","big data; convolutional neural network (CNN); data augmentation; deep learning; remote sensing scene classification","artificial neural network; data set; image classification; remote sensing; satellite data; satellite imagery"
"Deep Learning-Based Large-Scale Automatic Satellite Crosswalk Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028730117&doi=10.1109%2fLGRS.2017.2719863&partnerID=40&md5=8824bd8502f7f16e743d97b647ac0828","High-resolution satellite imagery has been increasingly used on remote sensing classification problems. One of the main factors is the availability of this kind of data. Despite the high availability, very little effort has been placed on the zebra crossing classification problem. In this letter, crowdsourcing systems are exploited in order to enable the automatic acquisition and annotation of a large-scale satellite imagery database for crosswalks related tasks. Then, this data set is used to train deep-learning-based models in order to accurately classify satellite images that contain or not contain zebra crossings. A novel data set with more than 240000 images from 3 continents, 9 countries, and more than 20 cities was used in the experiments. The experimental results showed that freely available crowdsourcing data can be used to accurately (97.11%) train robust models to perform crosswalk classification on a global scale. © 2004-2012 IEEE.","Crosswalk classification; deep learning; large-scale satellite imagery; zebra crossing classification","Classification (of information); Crosswalks; Crowdsourcing; Data structures; Deep learning; Personnel training; Remote sensing; Satellites; Continents; Google; Large-scale satellites; Roads; Urban areas; Satellite imagery"
"Domain Adaptation Using Representation Learning for the Classification of Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021801307&doi=10.1109%2fJSTARS.2017.2711360&partnerID=40&md5=046ea8f8eee99a1fc371def1b86489b5","Traditional machine learning (ML) techniques are often employed to perform complex pattern recognition tasks for remote sensing images, such as land-use classification. In order to obtain acceptable classification results, these techniques require there to be sufficient training data available for every particular image. Obtaining training samples is challenging, particularly for near real-Time applications. Therefore, past knowledge must be utilized to overcome the lack of training data in the current regime. This challenge is known as domain adaptation (DA), and one of the common approaches to this problem is based on finding invariant representations for both the training and test data, which are often assumed to come from different 'domains.' In this study, we consider two deep learning techniques for learning domain-invariant representations: Denoising autoencoders (DAE) and domain-Adversarial neural networks (DANN). While the DAE is a typical two-stage DA technique (unsupervised invariant representation learning followed by supervised classification), DANN is an end-To-end approach where invariant representation learning and classification are considered jointly during training. The proposed techniques are applied to both hyperspectral and multispectral images under different DA scenarios. Results obtained show that the proposed techniques outperform traditional approaches, such as principal component analysis (PCA) and kernel PCA, and can also compete with a fully supervised model in the multispatial scenario. © 2008-2012 IEEE.","Adversarial neural network; autoencoders (AEs); deep learning; domain adaptation (DA); land-use classification; representation learning","Agriculture; Artificial intelligence; Classification (of information); Complex networks; Deep neural networks; Earth (planet); Education; Image classification; Image reconstruction; Land use; Learning systems; Neural networks; Pattern recognition; Personnel training; Principal component analysis; Remote sensing; Supervised learning; Autoencoders; Classification of remote sensing image; Domain adaptation; Invariant representation; Landuse classifications; representation learning; Supervised classification; Traditional approaches; Deep learning; artificial neural network; image classification; imaging method; land use; machine learning; principal component analysis; remote sensing; spectral analysis"
"GCN: GPU-Based Cube CNN Framework for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030640993&doi=10.1109%2fICPP.2017.13&partnerID=40&md5=a9eb47e845d94e546e4752893c7a6cf4","Hyperspectral image classification has been proved significant in remote sensing field. Traditional classification methods have meet bottlenecks due to the lack of remote sensing background knowledge or high dimensionality. Deep learning based methods, such as deep convolutional neural network (CNN), can effectively extract high level features from raw data. But the training of deep CNN is rather time-consuming. The general purpose graphic processing units (GPUs) have been considered as one of the most common co-processors that can help accelerate deep learning applications. In this paper we propose a GPU-based Cube CNN (GCN) framework for hyperspectral image classification. First, a Parallel Neighbor Pixels Extraction (PNPE) algorithm is designed to enable the framework directly loading raw hyperspectral image into GPU's global memory, and extracting samples into data cube. Then, based on the peculiarity of hyperspectral image and cube convolution, we propose a novel Cube CNN-to-GPU mapping mechanism that transfers the training of Cube CNN to GPU effectively. Finally, the mini-batch gradient descent(MBGD) algorithm is improved with Computing United Device Architecture(CUDA) multi-streaming technique, which further speeds up network training in GCN framework. Experiments on KSC dataset, PU dataset and SA dataset show that, compared with state-of-art framework Caffe, we achieve up to 83% and 67% reduction in network training time without losing accuracy, when using SGD (Stochastic Gradient Descent) and MBGD algorithm respectively. Experiments across different GPUs show the same performance trend, which demonstrates the good extendibility of GCN framework. © 2017 IEEE.","Deep CNN; GPU parallel computing; Hyperspectral image classification","Arts computing; Convolution; Deep learning; Deep neural networks; Geometry; Hyperspectral imaging; Image classification; Network function virtualization; Neural networks; Program processors; Remote sensing; Spectroscopy; Stochastic systems; Back-ground knowledge; Classification methods; Convolutional neural network; Deep CNN; General purpose graphic processing units; GPU parallel computing; Learning-based methods; Stochastic gradient descent; Graphics processing unit"
"Forest Change Detection in Incomplete Satellite Images with Deep Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021757762&doi=10.1109%2fTGRS.2017.2707528&partnerID=40&md5=9ae56f74aa06b44b2094674d09a20ec4","Land cover change monitoring is an important task from the perspective of regional resource monitoring, disaster management, land development, and environmental planning. In this paper, we analyze imagery data from remote sensing satellites to detect forest cover changes over a period of 29 years (1987-2015). Since the original data are severely incomplete and contaminated with artifacts, we first devise a spatiotemporal inpainting mechanism to recover the missing surface reflectance information. The spatial filling process makes use of the available data of the nearby temporal instances followed by a sparse encoding-based reconstruction. We formulate the change detection task as a region classification problem. We build a multiresolution profile (MRP) of the target area and generate a candidate set of bounding-box proposals that enclose potential change regions. In contrast to existing methods that use handcrafted features, we automatically learn region representations using a deep neural network in a data-driven fashion. Based on these highly discriminative representations, we determine forest changes and predict their onset and offset timings by labeling the candidate set of proposals. Our approach achieves the state-of-the-art average patch classification rate of 91.6% (an improvement of ∼16%) and the mean onset/offset prediction error of 4.9 months (an error reduction of five months) compared with a strong baseline. We also qualitatively analyze the detected changes in the unlabeled image regions, which demonstrate that the proposed forest change detection approach is scalable to new regions. © 2017 IEEE.","Change detection; deep learning; image inpainting; multitemporal spectral data; remote sensing","Deep learning; Disaster prevention; Disasters; Forestry; Remote sensing; Signal detection; Change detection; Classification rates; Environmental planning; Image Inpainting; Region classifications; Remote sensing satellites; Spectral data; Surface reflectance; Deep neural networks; algorithm; artificial neural network; detection method; forest cover; image analysis; image classification; land cover; reconstruction; remote sensing; satellite data; satellite imagery; surface reflectance"
"Vegetation recognition based on deep learning with feature fusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033578402&doi=10.1145%2f3133264.3133276&partnerID=40&md5=1a09dd5eb73852292fe74a18e2745514","Vegetation recognition is an important task for optical remote sensing image parsing, providing information on vegetation cover for disaster monitoring, natural environment change monitoring, etc. In recent years, deep learning methods have driven a great advance in object recognition. Convolutional neural network (CNN) is a typical deep learning method which can recognize vegetation objects from optical remote sensing images. However, CNN methods exploit only high-level features for vegetation recognition and do not consider multi-scale vegetation objects, and thus cannot achieve high-accuracy pixel-wise semantic segmentation of vegetation objects from complex optical remote sensing images at various resolutions. To deal with these problems, a vegetation recognition method based on a deep fully convolutional network with feature fusion (FCN-FF) is proposed in this paper. The FCN-FF extracts hierarchical features from optical remote sensing images, and then fuses high-level and low-level features from a deep layer and a shallow layer for vegetation recognition. The fusion of different levels of feature improves recognition accuracy of vegetation from complex images. Furthermore, the FCN-FF selects vegetation objects with various spatial resolutions as training samples and thus it is suitable to multi-scale vegetation objects. In this study, a validation experiment and a comparative experiment are performed. The experimental results show that our method achieves high-accuracy pixel-wise vegetation recognition from complex optical remote sensing images. © 2017 Association for Computing Machinery.","Feature fusion; Fully convolutional network; Multi-scale objects; Vegetation recognition","Complex networks; Convolution; Deep learning; Image enhancement; Image segmentation; Learning systems; Neural networks; Object recognition; Optical data processing; Pixels; Remote sensing; Semantics; Vegetation; Comparative experiments; Convolutional networks; Convolutional neural network; Feature fusion; Hierarchical features; Multi-scale objects; Optical remote sensing; Semantic segmentation; Image processing"
"Joint Learning from Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030216163&doi=10.1109%2fCVPRW.2017.199&partnerID=40&md5=4cf69d595cc04ab85cf44dc970c55298","In this work, we investigate the use of OpenStreetMap data for semantic labeling of Earth Observation images. Deep neural networks have been used in the past for remote sensing data classification from various sensors, including multispectral, hyperspectral, SAR and LiDAR data. While OpenStreetMap has already been used as ground truth data for training such networks, this abundant data source remains rarely exploited as an input information layer. In this paper, we study different use cases and deep network architectures to leverage OpenStreetMap data for semantic labeling of aerial and satellite images. Especially, we look into fusion based architectures and coarseto- fine segmentation to include the OpenStreetMap layer into multispectral-based deep fully convolutional networks. We illustrate how these methods can be successfully used on two public datasets: ISPRS Potsdam and DFC2017. We show that OpenStreetMap data can efficiently be integrated into the vision-based deep learning models and that it significantly improves both the accuracy performance and the convergence speed of the networks. © 2017 IEEE.",,"Computer vision; Deep learning; Deep neural networks; Image processing; Observatories; Pattern recognition; Remote sensing; Semantics; Convergence speed; Convolutional networks; Earth observation images; Earth observations; Ground truth data; Information layers; Remote-sensing data classification; Semantic labeling; Network architecture"
"Transfer learning for high resolution aerial image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028769475&doi=10.1109%2fAIPR.2016.8010600&partnerID=40&md5=f2d0a648fd777ba8012c1b845ce78998","With rapid developments in satellite and sensor technologies, increasing amount of high spatial resolution aerial images have become available. Classiïcation of these images are important for many remote sensing image understanding tasks, such as image retrieval and object detection. Meanwhile, image classiïcation in the computer vision ïeld is revolutionized with recent popularity of the convolutional neural networks (CNN), based on which the state-of-the-art classiïcation results are achieved. Therefore, the idea of applying the CNN for high resolution aerial image classiïcation is straightforward. However, it is not trivial mainly because the amount of labeled images in remote sensing for training a deep neural network is limited. As a result, transfer learning techniques were adopted for this problem, where the CNN used for the classiïcation problem is pre-trained on a larger dataset beforehand. In this paper, we propose a speciïc ïne-tuning strategy that results in better CNN models for aerial image classiïcation. Extensive experiments were carried out using the proposed approach with different CNN architectures. Our proposed method shows competitive results compared to the existing approaches, indicating the superiority of the proposed ïne-tuning algorithm. © 2016 IEEE.",,"Deep learning; Deep neural networks; Image classification; Image processing; Image reconstruction; Neural networks; Object detection; Pattern recognition; Positive ions; Remote sensing; Convolutional neural network; High spatial resolution; High-resolution aerial images; Remote sensing images; Sensor technologies; State of the art; Transfer learning; Tuning algorithm; Image retrieval"
"Multilevel cloud detection in remote sensing images based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018512559&doi=10.1109%2fJSTARS.2017.2686488&partnerID=40&md5=0005957cc7563c8b3fd1a335c008035d","Cloud detection is one of the important tasks for remote sensing image processing. In this paper, a novel multilevel cloud detection method based on deep learning is proposed for remote sensing images. First, the simple linear iterative clustering (SLIC) method is improved to segment the image into good quality superpixels. Then, a deep convolutional neural network (CNN) with two branches is designed to extract the multiscale features from each superpixel and predict the superpixel as one of three classes including thick cloud, thin cloud, and noncloud. Finally, the predictions of all the superpixels in the image yield the cloud detection result. In the proposed cloud detection framework, the improved SLIC method can obtain accurate cloud boundaries by optimizing initial cluster centers, designing dynamic distance measure, and expanding search space. Moreover, different from traditional cloud detection methods that cannot achieve multilevel detection of cloud, the designed deep CNN model can not only detect cloud but also distinguish thin cloud from thick cloud. Experimental results indicate that the proposed method can detect cloud with higher accuracy and robustness than compared methods. © 2008-2012 IEEE.","Cloud detection; convolutional neural network (CNN); deep learning; remote sensing images; superpixel","Deep learning; Deep neural networks; Image processing; Image reconstruction; Image segmentation; Iterative methods; Neural networks; Pixels; Space optics; Cloud detection method; Convolutional neural network; Initial cluster centers; Iterative clustering; Multi-level detection; Multi-scale features; Remote sensing image processing; Remote sensing images; Remote sensing; artificial neural network; cloud cover; detection method; image analysis; machine learning; pixel; remote sensing"
"Change Detection Method for High Resolution Remote Sensing Images Using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030645839&doi=10.11947%2fj.AGCS.2017.20170036&partnerID=40&md5=5b45590bd9b6f22dae833097817e8e0c","A novel change detection method is proposed based on deep learning to improve the accuracy of change detection in very high spatial resolution remote sensing images. On the base of image pre-processing, spectral and texture changes are extracted by modified change vector analysis and grey level co-occurrence matrix respectively, both concerning spatial-contextual information. Most likely changed and unchanged pixel-pairs are obtained by an adaptive threshold for selecting the labeled samples. The proposed model based on Gaussian-Bernoulli deep Boltzmann machines with a label layer is built to learn high-level features and is trained for determining the change areas. Experimental results on WorldView-3 and Pléiades-1 show that the proposed method out performs the compared methods in the accuracy of change detection. © 2017, Surveying and Mapping Press. All right reserved.","Change detection; Deep learning; High resolution remote sensing","Deep learning; Image enhancement; Change detection; Change vector analysis; Contextual information; Deep boltzmann machines; Grey level co-occurrence matrixes; High resolution remote sensing; High resolution remote sensing images; Very high spatial resolutions; Remote sensing; detection method; image analysis; machine learning; pixel; Pleiades; remote sensing; spatial resolution; threshold; WorldView"
"Unsupervised deep feature learning for urban village detection from high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027247537&doi=10.14358%2fPERS.83.8.567&partnerID=40&md5=87604b0c95a3281f4ac277d441181e06","Urban villages (UVs) are a typical informal settlement in China resulting from the rapid urbanization in recent decades. Their formation and demolition are attracting increasing interest. In the remote sensing community, UVs have been detected based on hand-crafted features. However, the hand-crafted features just consider one or several characteristics of UVs, and ignore many effective cues hiding in the image. Recently, deep learning has been used to automatically learn suitable feature representations from a huge amount of data, without much expertise or effort in designing features. Motivated by its great success, this paper aims to use deep learning for detecting UVs. Because of the scarce labeled samples, this paper presents a novel unsupervised deep learning method to learn a data-driven feature. Experiments show the datadriven feature obtained with the proposed method outperform the existing unsupervised deep neural networks, and achieve results comparable to that obtained using the best hand-crafted features. © 2017 American Society for Photogrammetry and Remote Sensing.",,"Deep learning; Deep neural networks; Image reconstruction; Remote sensing; Rural areas; Data driven; Deep feature learning; Feature representation; High resolution remote sensing images; Informal settlements; Learning methods; Rapid urbanizations; Feature extraction; algorithm; automation; informal settlement; machine learning; remote sensing; unsupervised classification; urban area; urbanization; village; China"
"Deep fusion of remote sensing data for accurate classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020375761&doi=10.1109%2fLGRS.2017.2704625&partnerID=40&md5=c35483cc2ddb6e12f97956f0d01dcfc8","The multisensory fusion of remote sensing data has obtained a great attention in recent years. In this letter, we propose a new feature fusion framework based on deep neural networks (DNNs). The proposed framework employs deep convolutional neural networks (CNNs) to effectively extract features of multi-/hyperspectral and light detection and ranging data. Then, a fully connected DNN is designed to fuse the heterogeneous features obtained by the previous CNNs. Through the aforementioned deep networks, one can extract the discriminant and invariant features of remote sensing data, which are useful for further processing. At last, logistic regression is used to produce the final classification results. Dropout and batch normalization strategies are adopted in the deep fusion framework to further improve classification accuracy. The obtained results reveal that the proposed deep fusion model provides competitive results in terms of classification accuracy. Furthermore, the proposed deep learning idea opens a new window for future remote sensing data fusion. © 2017 IEEE.","Convolutional neural network (CNN); Data fusion; Deep neural network (DNN); Feature extraction (FE); Hyperspectral image (HSI); Light detection and ranging (LiDAR); Multispectral image (MSI)","Data fusion; Deep neural networks; Neural networks; Optical radar; Classification accuracy; Classification results; Convolutional neural network; Heterogeneous features; Light detection and ranging; Multi-sensory fusion; Normalization strategies; Remote sensing data fusion; Remote sensing"
"Multitemporal Very High Resolution from Space: Outcome of the 2016 IEEE GRSS Data Fusion Contest","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021836557&doi=10.1109%2fJSTARS.2017.2696823&partnerID=40&md5=3a257e184c1e9cf303eef7d8c65b6495","In this paper, the scientific outcomes of the 2016 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society are discussed. The 2016 Contest was an open topic competition based on a multitemporal and multimodal dataset, which included a temporal pair of very high resolution panchromatic and multispectral Deimos-2 images and a video captured by the Iris camera on-board the International Space Station. The problems addressed and the techniques proposed by the participants to the Contest spanned across a rather broad range of topics, and mixed ideas and methodologies from the remote sensing, video processing, and computer vision. In particular, the winning team developed a deep learning method to jointly address spatial scene labeling and temporal activity modeling using the available image and video data. The second place team proposed a random field model to simultaneously perform coregistration of multitemporal data, semantic segmentation, and change detection. The methodological key ideas of both these approaches and the main results of the corresponding experimental validation are discussed in this paper. © 2008-2012 IEEE.","Change detection; convolutional neural networks (CNN); deep learning; image analysis and data fusion; multimodal; multiresolution; multisource; random fields; tracking; video from space","Cameras; Data fusion; Deep learning; Deep neural networks; Earth (planet); Education; Image analysis; Image fusion; Image resolution; Neural networks; Remote sensing; Semantics; Sensors; Signal detection; Space optics; Space stations; Surface discharges; Video signal processing; Change detection; Convolutional neural network; Iris; Multi-modal; Multiresolution; Multisources; Random fields; video from space; Data integration; artificial neural network; data processing; experimental study; image analysis; model validation; multispectral image; panchromatic image; remote sensing; tracking"
"Object-oriented and multi-scale target classification and recognition based on hierarchical ensemble learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009971764&doi=10.1016%2fj.compeleceng.2016.12.026&partnerID=40&md5=647be42342b75c1a751d7b6b1633713a","Target classification and recognition (TCR) of high resolution remote-sensing image is the important ability for earth observation system and unmanned autonomous system. It is difficult to improve the precision of TCR because of different imaging mechanism. In this paper, we propose a brain-inspired computing model for TCR using cognitive computing and deep learning. Accordingly, we have built an ensemble learning algorithm based on deep spiking convolutional neural network and hierarchical latent Dirichlet allocation. The hierarchical features were extracted from remote-sensing image. Then a TCR algorithm for small sample sizes and complex target was designed, which uses the incremental and reinforcement learning based on object-oriented and multi-scale data argumentation. Experimental results demonstrate that our algorithm has state-of-the-art performance on public data sets of optical remote-sensing image and synthetic aperture image. The model proposed can provide reference to explore an essential significance in brain-inspired intelligence, and has significant value in military and civil affairs. © 2017 Elsevier Ltd","Brain inspired computing; Deep spiking convolutional neural network; Hierarchical latent Dirichlet allocation; High resolution remote-sensing image; Multimedia neural cognitive computing; Target classification and recognition","Brain; Complex networks; Convolution; Image reconstruction; Neural networks; Reinforcement learning; Remote sensing; Statistics; Synthetic apertures; Brain-inspired computing; Cognitive Computing; Convolutional neural network; High resolution remote sensing images; Latent Dirichlet allocation; Target Classification; Learning algorithms"
"A review of supervised object-based land-cover image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021219961&doi=10.1016%2fj.isprsjprs.2017.06.001&partnerID=40&md5=4f2c9b47e10220570b65c016d57ade0f","Object-based image classification for land-cover mapping purposes using remote-sensing imagery has attracted significant attention in recent years. Numerous studies conducted over the past decade have investigated a broad array of sensors, feature selection, classifiers, and other factors of interest. However, these research results have not yet been synthesized to provide coherent guidance on the effect of different supervised object-based land-cover classification processes. In this study, we first construct a database with 28 fields using qualitative and quantitative information extracted from 254 experimental cases described in 173 scientific papers. Second, the results of the meta-analysis are reported, including general characteristics of the studies (e.g., the geographic range of relevant institutes, preferred journals) and the relationships between factors of interest (e.g., spatial resolution and study area or optimal segmentation scale, accuracy and number of targeted classes), especially with respect to the classification accuracy of different sensors, segmentation scale, training set size, supervised classifiers, and land-cover types. Third, useful data on supervised object-based image classification are determined from the meta-analysis. For example, we find that supervised object-based classification is currently experiencing rapid advances, while development of the fuzzy technique is limited in the object-based framework. Furthermore, spatial resolution correlates with the optimal segmentation scale and study area, and Random Forest (RF) shows the best performance in object-based classification. The area-based accuracy assessment method can obtain stable classification performance, and indicates a strong correlation between accuracy and training set size, while the accuracy of the point-based method is likely to be unstable due to mixed objects. In addition, the overall accuracy benefits from higher spatial resolution images (e.g., unmanned aerial vehicle) or agricultural sites where it also correlates with the number of targeted classes. More than 95.6% of studies involve an area less than 300 ha, and the spatial resolution of images is predominantly between 0 and 2 m. Furthermore, we identify some methods that may advance supervised object-based image classification. For example, deep learning and type-2 fuzzy techniques may further improve classification accuracy. Lastly, scientists are strongly encouraged to report results of uncertainty studies to further explore the effects of varied factors on supervised object-based image classification. © 2017 The Authors","GEOBIA; Land-cover mapping; Meta-analysis; OBIA; Review; Supervised object-based classification","Classification (of information); Decision trees; Deep learning; Image resolution; Mapping; Remote sensing; Reviews; GEOBIA; Land cover mapping; Meta analysis; OBIA; Object-based classifications; Image classification; accuracy assessment; array; correlation; database; fuzzy mathematics; geomorphological mapping; image classification; land cover; meta-analysis; performance assessment; random walk method; spatial resolution; supervised learning; uncertainty analysis; unmanned vehicle"
"DeepEddy: A simple deep architecture for mesoscale oceanic eddy detection in SAR images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028508387&doi=10.1109%2fICNSC.2017.8000171&partnerID=40&md5=b984a1757e902f1d7198748dabcf7520","Automatic detection of mesoscale oceanic eddies is in great demand to monitor their dynamics which play a significant role in ocean current circulation and marine climate change. Traditional methods of eddies detection using remotely sensed data are usually based on physical parameters, geometrics, handcrafted features or expert knowledge, they face a great challenge in accuracy and efficiency due to the high variability of oceanic eddies and our limited understanding of their physical process, especially for rich and large remotely sensed data. In this paper, we propose a simple deep architecture DeepEddy to detect oceanic eddies automatically and be free of expert knowledge. DeepEddy can learn high-level and invariant features of oceanic eddies hierarchically. It is designed with two principal component analysis (PCA) convolutional layers for eddies feature learning, a binary hashing layer for non-linear transformation, a feature pooling layer using block-wise histograms and spatial pyramid pooling to resolve the complicated structures and poses of oceanic eddies, and a classifier for the final eddies identification. We verify the accuracy of the architecture with comprehensive experiments on high spatial resolution Synthetic Aperture Radar (SAR) images. We achieve the state-of-the-art accuracy of 96.68%. © 2017 IEEE.","Automatic detection; Deep learning; Feature learning; Mesoscale oceanic eddies","Climate change; Deep learning; Linear transformations; Mathematical transformations; Oceanography; Principal component analysis; Radar imaging; Remote sensing; Synthetic aperture radar; Automatic Detection; Complicated structures; Feature learning; High spatial resolution; Non-linear transformations; Oceanic eddies; Remotely sensed data; Synthetic aperture radar (SAR) images; Feature extraction"
"Zoom out CNNs features for optical remote sensing change detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029359679&doi=10.1109%2fICIVC.2017.7984667&partnerID=40&md5=6f331166ef9e64d4bdcd714137f699bd","In this paper, we propose a novel unsupervised optical remote sensing change detection (CD) based on pre-trained convolutional neural network (CNN) on ImageNet dataset and superpixel (SLIC) segmentation technique. The proposed approach can be divided into three steps. First, bi-temporal images are stacked, and Principal Component Analysis (PCA) is applied to extract three higher uncorrelated channels, which will be later segmented into superpixels. Second, we zoom out each region into three levels and fit them separately into a pre-trained CNN. Third, we extract features of different zooming levels that represent the same region (superpixel) and concatenate them. We compare the concatenated features to get the final change map. The experimental results demonstrate the efficacy of the proposed approach. © 2017 IEEE.","Change detection; Convolutional neural network; Deep learning; Remote sensing","Convolution; Deep learning; Neural networks; Pixels; Principal component analysis; Change detection; Convolutional neural network; Optical remote sensing; Segmentation techniques; Super pixels; Temporal images; Zoom-out; Remote sensing"
"Cell classification using convolutional neural networks in medical hyperspectral imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029379996&doi=10.1109%2fICIVC.2017.7984606&partnerID=40&md5=6d57d646b798c7ece8513ffc0b366d25","Hyperspectral imaging is a rising imaging modality in the field of medical applications, and the combination of both spectral and spatial information provides wealth information for cell classification. In this paper, deep convolutional neural network (CNN) is employed to achieve blood cell discrimination in medical hyperspectral images (MHSI). As a deep learning architecture, CNNs are expected to get more discriminative and semantic features, which effect classification accuracy to a certain extent. Experimental results based on two real medical hyperspectral image data sets demonstrate that cell classification using CNNs is effective. In addition, compared to traditional support vector machine (SVM), the proposed method, which jointly exploits spatial and spectral features, can achieve better classification performance, showcasing the CNN-based methods' tremendous potential for accurate medical hyperspectral data classification. © 2017 IEEE.","Blood cell classification; Convolutional neural network; Deep learning; Medical hyperspectral imagery","Blood; Cells; Convolution; Cytology; Deep learning; Deep neural networks; Hyperspectral imaging; Image classification; Independent component analysis; Medical applications; Medical imaging; Neural networks; Remote sensing; Semantics; Spectroscopy; Support vector machines; Blood cell classifications; Cell classification; Classification accuracy; Classification performance; Convolutional neural network; Learning architectures; Medical hyperspectral; Spatial informations; Classification (of information)"
"2017 2nd International Conference on Image, Vision and Computing, ICIVC 2017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029347959&partnerID=40&md5=fa1f0ddac98f515d15281a090272fa36","The proceedings contain 229 papers. The topics discussed include: saliency detection with relative location measure in light field image; ship detection in harbor area in SAR images based on constructing an accurate sea-clutter model; a method of lane detection and tracking for expressway based on RANSAC; a novel saliency computation model for traffic sign detection; detection of point sources in x-ray astronomical images using elliptical Gaussian filters; defect inspection of medicine vials using LBP features and SVM classifier; infrared dim small target detection method based on background prediction and high-order statistics; integrating saliency and ResNet for airport detection in large-size remote sensing images; a real-time object recognition for forward looking sonar; efficient HD video and image salient object detection with hierarchical Boolean map approach; unconstrained face detection based on cascaded convolutional neural networks in surveillance video; an improved canny algorithm based on adaptive 2D-Otsu and Newton iterative; natural scene text detection based on SWT, MSER and candidate classification; yBRIEF: a study of non-Gaussian binary elementary features; fine-grained object detection based on self-adaptive anchors; and detecting Chinese calligraphy style consistency by deep learning and one-class SVM.",,
"Construction and application of new intelligent MOOC teaching system based on deep learning neural network in remote sensing course","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027727240&doi=10.1109%2fITME.2016.0109&partnerID=40&md5=3c723beb6411d5790c457766deec4cdb","This paper introduced new intelligent personalized MOOC teaching system in remote sensing course. This paper provides theoretical support and case support based on deep learning for new MOOC teaching system and its application in remote sensing course. The results are compared with evaluation of learning effects before using the system, and achieved better results. © 2016 IEEE.","MOOC; Remote sensing Course; Teaching System","Deep learning; Deep neural networks; Education; Remote sensing; ITS applications; Learning effects; Learning neural networks; MOOC; Teaching systems; Teaching"
"Multi-modal remote sensing data fusion framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027231134&doi=10.5194%2fisprs-archives-XLII-4-W2-85-2017&partnerID=40&md5=ac0b085180f3f98a095b31de679cb845","The inconsistency between the freely available remote sensing datasets and crowd-sourced data from the resolution perspective forms a big challenge in the context of data fusion. In classical classification problems, crowd-sourced data are represented as points that may or not be located within the same pixel. This discrepancy can result in having mixed pixels that could be unjustly classified. Moreover, it leads to failure in retaining sufficient level of details from data inferences. In this paper we propose a method that can preserve detailed inferences from remote sensing datasets accompanied with crowd-sourced data. We show that advanced machine learning techniques can be utilized towards this objective. The proposed method relies on two steps, firstly we enhance the spatial resolution of the satellite image using Convolutional Neural Networks and secondly we fuse the crowd-sourced data with the upscaled version of the satellite image. However, the covered scope in this paper is concerning the first step. Results show that CNN can enhance Landsat 8 scenes resolution visually and quantitatively. © Authors 2017.","Convolutional neural networks; Crowd-sourced data; Data fusion; Deep learning; Super resolution","Classification (of information); Convolution; Data fusion; Deep learning; Learning systems; Neural networks; Open source software; Open systems; Pixels; Software engineering; Convolutional neural network; Crowd-sourced data; Level of detail; Machine learning techniques; Remote sensing data fusion; Satellite images; Spatial resolution; Super resolution; Remote sensing"
"Learning deep features for classification of typical ecological environmental elements in high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047093014&doi=10.1109%2fISCID.2017.200&partnerID=40&md5=51e417e51b009481188495bd03974e72","Ecological environmental elements are greatly related to both humans and nature. The rapid development of remote sensing technology has provided us more and more high-resolution remote sensing images for monitoring these elements timely and objectively. However, it has been a great challenge to recognize these elements from such images due to their diversity and complexity. In this paper, a classification approach of ecological environmental elements based on deep learning features of objects is proposed. At first, a deep convolutional neural network (DCNN) is trained for discriminating different ecological environmental elements. To extract deep features of irregular-shaped regions, sub-images are clipped from each region and used to represent the corresponding region. Then, deep features of these sub-images are extracted by the trained DCNN. After that, the softmax classifier is used to predict class probabilities of all sub-images. The class of one region is determined by considering the class probabilities of its sub-images according to the 'winner-Takes-All' strategy. Finally, the thematic maps of ecological environmental elements are achieved. The proposed approach is evaluated by the classification experiments on a test set of typical ecological environmental elements in high-resolution remote sensing images, and the classification accuracy reaches to 98.44%. Moreover, the classification accuracy on irregular-shaped regions also reaches to 96.77%. These results have testified the effectiveness of the proposed approach. © 2017 IEEE.","Deep convolutional neural network (dcnn); Deep learning; Ecological environmental elements; High-resolution remote sensing images","Classification (of information); Convolution; Deep learning; Deep neural networks; Ecology; Image classification; Maps; Neural networks; Class probabilities; Classification accuracy; Classification approach; Convolutional neural network; High resolution remote sensing images; Remote sensing technology; Test sets; Thematic maps; Remote sensing"
"Deep learning for effective detection of excavated soil related to illegal tunnel activities","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028541465&doi=10.1109%2fUEMCON.2017.8249062&partnerID=40&md5=e9237a023b93a33934da149e7f3f3978","This paper presents a new deep learning based approach for soil detection using high resolution multispectral satellite images with a resolution of 0.31 m. In particular, a deep convolutional neural network (CNN) is proposed for soil detection to identify potential tunnel digging activities. Spatial and spectral information in the multispectral image cube has been incorporated into the CNN. We also propose a novel method to handle imbalance learning in the context of deep CNN model training. Experimental results on Worldview-2 (WV-2) multispectral satellite images captured at the border between USA and Mexico showed that the proposed CNN model can effectively detect soil in the remote sensed images, and the proposed imbalance learning technique improved the detection performance significantly. © 2017 IEEE.","convolutional neural network; deep learning; multispectral satellite images; pansharpening; soil detection; sparsity based model; tunnel activity","Convolution; Deep learning; Image enhancement; Mobile telecommunication systems; Neural networks; Remote sensing; Satellites; Soils; Ubiquitous computing; Convolutional neural network; Detection performance; Learning techniques; Learning-based approach; Multispectral images; Multispectral satellite image; Pan-sharpening; Spectral information; Deep neural networks"
"AID: A benchmark data set for performance evaluation of aerial scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018642692&doi=10.1109%2fTGRS.2017.2685945&partnerID=40&md5=6a6d206c079d5cade515b765cd9ec95a","Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become an active task in the remote sensing area, and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing data sets for aerial scene classification, such as UC-Merced data set and WHU-RS19, contain relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms. This paper describes the Aerial Image data set (AID): a large-scale data set for aerial scene classification. The goal of AID is to advance the state of the arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than 10000 aerial scene images. In addition, a comprehensive review of the existing aerial scene classification techniques as well as recent widely used deep learning methods is given. Finally, we provide a performance analysis of typical aerial scene classification and deep learning approaches on AID, which can be served as the baseline results on this benchmark. © 1980-2012 IEEE.","Aerial images; Benchmark; Scene classification","Benchmarking; Deep learning; Image reconstruction; Learning systems; Remote sensing; Semantics; Aerial image data; Data-driven approach; High resolution remote sensing imagery; Large scale data sets; Learning approach; Performance analysis; Scene classification; Specific semantics; Classification (of information); aerial survey; algorithm; automation; benchmarking; data set; image classification; machine learning; performance assessment; remote sensing"
"Object-Based Convolutional Neural Network for High-Resolution Imagery Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017136781&doi=10.1109%2fJSTARS.2017.2680324&partnerID=40&md5=b39d84c303c7b4ef6e6a4863a6d4a83d","Timely and accurate classification and interpretation of high-resolution images are very important for urban planning and disaster rescue. However, as spatial resolution gets finer, it is increasingly difficult to recognize complex patterns in high-resolution remote sensing images. Deep learning offers an efficient strategy to fill the gap between complex image patterns and their semantic labels. However, due to the hierarchical abstract nature of deep learning methods, it is difficult to capture the precise outline of different objects at the pixel level. To further reduce this problem, we propose an object-based deep learning method to accurately classify the high-resolution imagery without intensive human involvement. In this study, high-resolution images were used to accurately classify three different urban scenes: Beijing (China), Pavia (Italy), and Vaihingen (Germany). The proposed method is built on a combination of a deep feature learning strategy and an object-based classification for the interpretation of high-resolution images. Specifically, high-level feature representations extracted through the convolutional neural networks framework have been systematically investigated over five different layer configurations. Furthermore, to improve the classification accuracy, an object-based classification method also has been integrated with the deep learning strategy for more efficient image classification. Experimental results indicate that with the combination of deep learning and object-based classification, it is possible to discriminate different building types in Beijing Scene, such as commercial buildings and residential buildings with classification accuracies above 90%. © 2017 IEEE.","Convolutional neural network (CNN); deep learning; high-resolution image; image classification","Buildings; Complex networks; Convolution; Deep learning; Image reconstruction; Learning systems; Neural networks; Office buildings; Remote sensing; Semantics; Classification accuracy; Classification and interpretation; Complex image patterns; Convolutional neural network; High resolution image; High resolution imagery; High resolution remote sensing images; Object-based classifications; Image classification; accuracy assessment; artificial neural network; disaster management; image classification; image resolution; pixel; remote sensing; urban planning; Beijing [Beijing (ADS)]; Beijing [China]; China; Germany; Italy; Lombardy; Pavia; Vaihingen"
"Object-Based Land-Cover Supervised Classification for Very-High-Resolution UAV Images Using Stacked Denoising Autoencoders","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015631926&doi=10.1109%2fJSTARS.2017.2672736&partnerID=40&md5=7e2a02a4649b4f51fcecbc6d8045647e","Over the last decade, object-based image classification (OBIC) has become a mainstream method in remote sensing land-use/land-cover applications. Many supervised classification methods have been proposed in the OBIC framework. However, most did not use deep learning methods. In this paper, a new deep-learning-based OBIC framework is introduced. First, we segment the original image into objects by graph-based minimal-spanning-tree segmentation algorithm. Second, we extract the spectral, spatial, and texture features for each object. Then we put all features into stacked autoencoders (SAE) or stacked denoising autoencoders (SDAE) network, and trained the parameters of the network using training samples. Finally, all objects were classified by the network. Based on our SAE/SDAE OBIC framework, we achieved 97% overall accuracy when classifying an UAV image into five categories. In addition, our experiment shows that our framework increases overall accuracy by approximately 6% when compared to the linear support vector machine (linear SVM) and radial basis function kernel support vector machine (RBF SVM) algorithms when sufficient training samples are lacking. © 2017 IEEE.","Deep learning (DL); object-based image classification (OBIC); stacked autoencoders (SAE); stacked denoising autoencoders (SDAE)","Deep learning; Graphic methods; Image classification; Land use; Learning systems; Network function virtualization; Radial basis function networks; Remote sensing; Sampling; Supervised learning; Support vector machines; Trees (mathematics); Unmanned aerial vehicles (UAV); Linear Support Vector Machines; Minimal spanning tree; Object-based image classification; Overall accuracies; Radial basis function kernels; Supervised classification; Texture features; Very high resolution; Image segmentation; algorithm; artificial neural network; image classification; land cover; land use change; remote sensing; support vector machine; unmanned vehicle"
"Feature learning and change feature classification based on deep learning for ternary change detection in SAR images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019562216&doi=10.1016%2fj.isprsjprs.2017.05.001&partnerID=40&md5=0b146e4a003113e3459940a57564aeb4","Ternary change detection aims to detect changes and group the changes into positive change and negative change. It is of great significance in the joint interpretation of spatial-temporal synthetic aperture radar images. In this study, sparse autoencoder, convolutional neural networks (CNN) and unsupervised clustering are combined to solve ternary change detection problem without any supervison. Firstly, sparse autoencoder is used to transform log-ratio difference image into a suitable feature space for extracting key changes and suppressing outliers and noise. And then the learned features are clustered into three classes, which are taken as the pseudo labels for training a CNN model as change feature classifier. The reliable training samples for CNN are selected from the feature maps learned by sparse autoencoder with certain selection rules. Having training samples and the corresponding pseudo labels, the CNN model can be trained by using back propagation with stochastic gradient descent. During its training procedure, CNN is driven to learn the concept of change, and more powerful model is established to distinguish different types of changes. Unlike the traditional methods, the proposed framework integrates the merits of sparse autoencoder and CNN to learn more robust difference representations and the concept of change for ternary change detection. Experimental results on real datasets validate the effectiveness and superiority of the proposed framework. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Convolutional neural networks; Deep learning; Representation learning; Sparse autoencoder; Synthetic aperture radar; Ternary change detection","Backpropagation; Classification (of information); Convolution; Deep learning; Image classification; Learning systems; Neural networks; Radar; Radar imaging; Sampling; Stochastic models; Stochastic systems; Synthetic aperture radar; Tracking radar; Auto encoders; Change detection; Convolutional neural network; Feature classification; Joint interpretation; Representation learning; Stochastic gradient descent; Unsupervised clustering; Feature extraction; artificial neural network; back propagation; detection method; image processing; machine learning; synthetic aperture radar; training; transform; unsupervised classification"
"Polarimetric SAR oil spill detection based on deep networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049399578&doi=10.1109%2fIST.2017.8261559&partnerID=40&md5=5ac676111d609376c539e51a03762fc9","Polarimetric SAR remote sensing provides an outstanding capability of oil spill detection and classification for its advantages in distinguishing mineral oil and biogenic look-Alikes. In this paper, deep learning algorithms including Stacked Auto-Encoder (SAE) and Deep Believe Network (DBN) are applied to optimize the polarimetric feature sets and reduce the feature dimension through the processes of layer-wise unsupervised pre-Training. An experiment was conducted on RADARSAT-2 quad-polarimetric SAR image acquired during Norwegian oil-on-water exercise, in which verified mineral, emulsions, and biogenic slicks were provided. The results show that oil spill classification achieved by deep networks outperformed support vector machine (SVM) and traditional artificial neural network (ANN) with similar parameter settings, especially when the number of training data samples is limited. © 2017 IEEE.","Auto-Encoder; Deep Believe Network; oil spill; polarimetric SAR; remote sensing","Classification (of information); Imaging systems; Neural networks; Oil spills; Polarimeters; Remote sensing; Signal encoding; Support vector machines; Synthetic aperture radar; Auto encoders; Feature dimensions; Oil spill detection; Parameter setting; Polarimetric features; Polarimetric SAR; Pre-training; Training data; Deep learning"
"Deep recurrent neural networks for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019010234&doi=10.1109%2fTGRS.2016.2636241&partnerID=40&md5=728dbdd06a948cf8923d18455c950246","In recent years, vector-based machine learning algorithms, such as random forests, support vector machines, and 1-D convolutional neural networks, have shown promising results in hyperspectral image classification. Such methodologies, nevertheless, can lead to information loss in representing hyperspectral pixels, which intrinsically have a sequence-based data structure. A recurrent neural network (RNN), an important branch of the deep learning family, is mainly designed to handle sequential data. Can sequence-based RNN be an effective method of hyperspectral image classification? In this paper, we propose a novel RNN model that can effectively analyze hyperspectral pixels as sequential data and then determine information categories via network reasoning. As far as we know, this is the first time that an RNN framework has been proposed for hyperspectral image classification. Specifically, our RNN makes use of a newly proposed activation function, parametric rectified tanh (PRetanh), for hyperspectral sequential data analysis instead of the popular tanh or rectified linear unit. The proposed activation function makes it possible to use fairly high learning rates without the risk of divergence during the training procedure. Moreover, a modified gated recurrent unit, which uses PRetanh for hidden representation, is adopted to construct the recurrent layer in our network to efficiently process hyperspectral data and reduce the total number of parameters. Experimental results on three airborne hyperspectral images suggest competitive performance in the proposed mode. In addition, the proposed network architecture opens a new window for future research, showcasing the huge potential of deep recurrent networks for hyperspectral data analysis. © 1980-2012 IEEE.","Convolutional neural network (CNN); Deep learning; Gated recurrent unit (GRU); Hyperspectral image classification; Long short-term memory (LSTM); Recurrent neural network (RNN)","Activation analysis; Chemical activation; Convolution; Data handling; Decision trees; Deep learning; Deep neural networks; Hyperspectral imaging; Image classification; Independent component analysis; Information analysis; Learning algorithms; Network architecture; Network layers; Pixels; Spectroscopy; Activation functions; Competitive performance; Convolutional neural network; Convolutional Neural Networks (CNN); Gated recurrent unit (GRU); Hyperspectral data analysis; Recurrent neural network (RNN); Sequential data analysis; Long short-term memory; artificial neural network; image classification; machine learning; pixel; remote sensing; spectral analysis; vector"
"Semantic segmentation of objects from airborne imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049587655&doi=10.1109%2fACDTJ.2017.8259608&partnerID=40&md5=83f77f50b6d74fc940613879d24b5d10","Extraction of objects from images acquired by airborne sensors is the one of the most important topics in Aerial Photograph Interpretation (API). The task is challenging due to the very heterogeneous appearance of man-made and natural objects on the ground. Meanwhile images acquired by airborne sensors are very high-resolution, which requires high computational costs. This paper presents an efficient approach for automated extraction of objects at pixel level. We propose to combine a powerful classifier and an efficient contextual model for semantic segmentation of objects in images. Multiple image features are used to train the classifier, other features are used to learn the contextual model. We employ Random forest (RF) as classifier which allows one to learn very fast on big data. The outputs given by RF are then combined with a fully connected conditional random field (CRF) model for improving classification performance. Experiments have been conducted on a challenging aerial image dataset from a recent ISPRS Semantic Labeling Contest. We obtained state-of-the-art performance with a reasonable computational demand. © 2017 IEEE.","Aerial image; Deep learning; Image segmentation; Machine learning; Object detection; Random forest; Remote sensing; Semantic labeling","Antennas; Classification (of information); Decision trees; Deep learning; Extraction; Image acquisition; Learning systems; Object detection; Random processes; Remote sensing; Semantics; Aerial images; Classification performance; Conditional random field; Multiple image features; Random forests; Semantic labeling; Semantic segmentation; State-of-the-art performance; Image segmentation"
"Peanut planting area change monitoring from remote sensing images based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046693905&doi=10.1109%2fICSAI.2017.8248497&partnerID=40&md5=27a509c1d3fd1d9b144b9a35eec3f089","As a powerful image processing technology, deep learning can extract representative and distinguishing features from remote sensing images in a hierarchical way. Considering the spectral information as the network input, the output spectral features from the network can be directly fed into a subsequent classifier to realize the classification based on pixel level. This paper presents a method based on convolutional neural networks (CNN) for peanut planting area extracting from Landsat-8 multispectral remote sensing images and applied in Zhenyang county in Henan province. The experimental results show that the architecture of CNN achieve good performance, the overall accuracy is 96.42%, kappa efficient is 0.944. Statistical calculations show that the peanut planting area of Zhengyang county increased by 12.1% in 2017 compared with 2015 which consistent with the current agricultural subsidy policy. The exploration of using CNN for crop recognition from multispectral images has excellent performance and great potential in agricultural remote sensing classification. © 2017 IEEE.","change monitoring; CNN; deep learning; peanuts; remote sensing","Classification (of information); Image processing; Neural networks; Oilseeds; Remote sensing; Agricultural remote sensing; Agricultural subsidy policies; Convolutional neural network; Image processing technology; Multispectral remote sensing image; peanuts; Remote sensing images; Statistical calculations; Deep learning"
"Hyperspectral image classification with convolutional networks trained with self-dual attribute profiles [Öz-ikili Öznitelik profilleri ile eǧitilmiş evrişimsel sinir aǧlari ile hiperspektral goruntu siniflandirma]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026286364&doi=10.1109%2fSIU.2017.7960208&partnerID=40&md5=b1c298a166cee0b1fbbe64fc217a5556","Attribute profiles are widely regarded among the most prominent spectral-spatial pixel description methods, providing high performance at a low computational cost. Following their success with computer vision applications, deep learning methods on the other hand are also being rapidly deployed and adapted into the remote sensing image analysis domain, where they already provide competitive description performances. The combination of attribute profiles with convolutional neural networks has recently taken place, showing that these powerful approaches can collaborate. In this paper we explore that direction one step further, by first feeding a convolutional neural network self-dual attribute profiles stacked as a tensor, and then by harvesting the ultimate layer's features for a supervised classification. Our preliminary experiments indicate that this approach leads to a performance improvement. © 2017 IEEE.","attribute profiles; convolutional neural networks; hyperspectral images; pixel classification; tree of shapes","Codes (symbols); Convolution; Deep learning; Hyperspectral imaging; Independent component analysis; Neural networks; Pixels; Remote sensing; Spectroscopy; attribute profiles; Computer vision applications; Convolutional networks; Convolutional neural network; Performance improvements; Pixel classification; Supervised classification; Tree of shapes; Image classification"
"The development of deep learning in synthetic aperture radar imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025649406&doi=10.1109%2fRSIP.2017.7958802&partnerID=40&md5=cfa5b10b64fb6acdaa9b270245c20366","The usage of remote sensing to observe environments necessitates interdisciplinary approaches to derive effective, impactful research. One remote sensing technique, Synthetic Aperture Radar, has shown significant benefits over traditional remote sensing techniques but comes at the price of additional complexities. To adequately cope with these, researchers have begun to employ advanced machine learning techniques known as deep learning to Synthetic Aperture Radar data. Deep learning represents the next stage in the evolution of machine intelligence which places the onus of identifying salient features on the network rather than researcher. This paper will outline machine learning techniques as it has been used previously on SAR; what is deep learning and where it fits in compared to traditional machine learning; what benefits can be derived by applying it to Synthetic Aperture Radar imagery; and finally describe some obstacles that still need to be overcome in order to provide constient and long term results from deep learning in SAR. © 2017 IEEE.","Machine learning; Marine technology; Monitoring; Synthetic aperture radar","Artificial intelligence; Education; Learning algorithms; Learning systems; Marine engineering; Marine radar; Monitoring; Radar; Radar imaging; Remote sensing; Synthetic aperture radar; Tracking radar; Long-term results; Machine intelligence; Machine learning techniques; Marine technology; Remote sensing techniques; Salient features; Synthetic Aperture Radar Imagery; Deep learning"
"RSIP 2017 - International Workshop on Remote Sensing with Intelligent Processing, Proceedings","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025614270&partnerID=40&md5=4bec51198775a5472ae68893d6a8bc01","The proceedings contain 28 papers. The topics discussed include: airborne ka-band digital beamforming SAR system and flight test; the development of deep learning in synthetic aperture radar imagery; a novel multi-target track initiation method based on convolution neural network; a modified faster R-CNN based on CFAR algorithm for SAR ship detection; low-rank matrix decomposition with a spectral-spatial regularization for change detection in hyperspectral imagery; superpixel-based multiple change detection in very-high-resolution remote sensing images; multi-temporal PolSAR crops classification using polarimetric-feature-driven deep convolutional neural network; change detection of SAR images based on supervised contractive autoencoders and fuzzy clustering; sea ice type classification based on random forest machine learning with Cryosat-2 SAR altimeter data; feature enhancement for multi-polarimetric SAR images: a novel approach based on PDE and regularization; deep residual learning for remote sensed imagery pansharpening; a weakly supervised road extraction approach via deep convolutional nets based image segmentation; integrating H-A-α with fully convolutional networks for fully PolSAR classification; an enhanced deep convolutional neural network for densely packed objects detection in remote sensing images; algorithm of remote sensing image matching based on corner-point; SAR ship detection using sea-land segmentation-based convolutional neural network; hyperspectral image classification based on spectral-spatial feature extraction; and classification of very high resolution SAR image based on convolutional neural network.",,
"Multi-temporal PolSAR crops classification using polarimetric-feature-driven deep convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025646275&doi=10.1109%2fRSIP.2017.7958818&partnerID=40&md5=2a6149436922cbcc4cf1315cbbdee8fc","Multi-temporal PolSAR data is suitable for crops classification and growth monitoring. It is still difficult to establish a classifier with good robustness and high generation over a long temporal acquisition duration. This work aims to provide a solution to this task by exploring benefits from both the target scattering mechanism interpretation and the advanced deep learning. A polarimetric-feature-driven deep convolutional neural network classification scheme is established. Comparison studies with multi-temporal UAVSAR datasets validate the efficiency and superiority of the proposal. © 2017 IEEE.","Classification; CNN; Deep learning; Multi-temporal; Polarimetric feature; PolSAR; Rotion domain","Convolution; Crops; Deep learning; Deep neural networks; Education; Neural networks; Polarimeters; Remote sensing; Comparison study; Convolutional neural network; Growth monitoring; Multi-temporal; Polarimetric features; PolSAR; Rotion domain; Scattering mechanisms; Classification (of information)"
"Deep residual learning for remote sensed imagery pansharpening","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025589420&doi=10.1109%2fRSIP.2017.7958794&partnerID=40&md5=1340498db43e058500987d21a71d568b","We proposed a deep convolutional network for multi-spectral image pan-sharpening to overcome the drawbacks of traditional methods and improve the fusion accuracy. To break the performance limitation of deep networks, residual learning with specific adaption to image fusion tasks is applied to optimize the architecture of proposed network. Results of adequate experiments support that our model can yield high resolution multi-spectral images with state-of-the-art qualities, as the information in both spatial and spectral domains has been accurately preserved. © 2017 IEEE.","convolutional neural network; image fusion; Pansharpening; residual learning","Convolution; Education; Image fusion; Neural networks; Remote sensing; Spectroscopy; Convolutional networks; Convolutional neural network; Multispectral images; Pan-sharpening; Performance limitations; Remote sensed imagery; residual learning; Spectral domains; Deep learning"
"A modified faster R-CNN based on CFAR algorithm for SAR ship detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025641265&doi=10.1109%2fRSIP.2017.7958815&partnerID=40&md5=21f6771823658e41cbbd7e2d9e0b09f1","SAR ship detection is essential to marine monitoring. Recently, with the development of the deep neural network and the spring of the SAR images, SAR ship detection based on deep neural network has been a trend. However, the multi-scale ships in SAR images cause the undesirable differences of features, which decrease the accuracy of ship detection based on deep learning methods. Aiming at this problem, this paper modifies the Faster R-CNN, a state-of-the-art object detection networks, by the traditional constant false alarm rate (CFAR). Taking the objects proposals generated by Faster R-CNN for the guard windows of CFAR algorithm, this method picks up the small-sized targets. By reevaluating the bounding boxes which have relative low classification scores in detection network, this method gain better performance of detection. © 2017 IEEE.","CFAR; Faster R-CNN; SAR; Ship Detection","Deep learning; Deep neural networks; Object detection; Remote sensing; Ships; Synthetic aperture radar; CFAR; Constant false alarm rate; Detection networks; Faster R-CNN; Learning methods; Marine monitoring; Ship detection; State of the art; Radar imaging"
"Classification of very high resolution SAR image based on convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025689165&doi=10.1109%2fRSIP.2017.7958811&partnerID=40&md5=7316c3a90da1003ea4138e01514db8e9","The new advanced very high resolution (VHR) synthetic aperture radar (SAR) sensor is a kind of high-tech imaging radar developed rapidly in recent years, and it can get even less than 1 m high resolution SAR image. The feature of the VHR SAR image is different from the low or medium resolution SAR image and it contains more abundant information, so the traditional SAR image classification methods can't be directly applied in VHR SAR image classification. In order to achieve high precision classification performance of the VHR SAR image, convolutional neural network (CNN), a kind of representative deep learning method, is applied in this paper. Compared with the traditional supervised classification methods, such as minimum distance and maximum likelihood, the CNN method obtained better classification result with 97.0% average accuracy. The experiments demonstrate that the CNN is an effective and favorable classification method for VHR SAR image classification. © 2017 IEEE.","classification; convolutional neural network; deep learning; very high resolution SAR image","Classification (of information); Convolution; Deep learning; Deep neural networks; Education; Image classification; Maximum likelihood; Neural networks; Radar; Remote sensing; Synthetic aperture radar; Classification methods; Classification performance; Classification results; Convolutional neural network; High-resolution SAR; SAR image classifications; Supervised classification; Very high resolution; Radar imaging"
"SAR ship detection using sea-land segmentation-based convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025647154&doi=10.1109%2fRSIP.2017.7958806&partnerID=40&md5=e6ef437d02b7d08e23caac618c7aa332","Reliable automatic ship detection in Synthetic Aperture Radar (SAR) imagery plays an important role in the surveillance of maritime activity. Apart from the well-known Spectral Residual (SR) and CFAR detector, there has emerged a novel method for SAR ship detection, based on the deep learning features. Within this paper, we present a framework of Sea-Land Segmentation-based Convolutional Neural Network (SLS-CNN) for ship detection that attempts to combine the SLS-CNN detector, saliency computation and corner features. For this, sea-land segmentation based on the heat map of SR saliency and probability distribution of the corner is applied, which is followed by SLS-CNN detector, and a final merged minimum bounding rectangles. The framework has been tested and assessed on ALOS PALSAR and TerraSAR-X imagery. Experimental results on representative SAR images of different kinds of ships demonstrate the efficiency and robustness of our proposed SLS-CNN detector. © 2017 IEEE.","deep learning; SAR; ship detector; SLS-CNN; target detect","Convolution; Deep learning; Edge detection; Education; Feature extraction; Neural networks; Probability distributions; Remote sensing; Ships; Synthetic aperture radar; Tracking radar; Convolutional neural network; Corner feature; Maritime activities; Minimum bounding rectangle; Sea-land segmentations; SLS-CNN; Synthetic Aperture Radar Imagery; Target detect; Radar imaging"
"Fast vehicle detection in UAV images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025581951&doi=10.1109%2fRSIP.2017.7958795&partnerID=40&md5=e30f4a8a6d5ef54f91582048c6dadd8a","Fast and accurate vehicle detection in unmanned aerial vehicle (UAV) images remains a challenge, due to its very high spatial resolution and very few annotations. Although numerous vehicle detection methods exist, most of them cannot achieve real-time detection for different scenes. Recently, deep learning algorithms has achieved fantastic detection performance in computer vision, especially regression based convolutional neural networks YOLOv2. It's good both at accuracy and speed, outperforming other state-of-the-art detection methods. This paper for the first time aims to investigate the use of YOLOv2 for vehicle detection in UAV images, as well as to explore the new method for data annotation. Our method starts with image annotation and data augmentation. CSK tracking method is used to help annotate vehicles in images captured from simple scenes. Subsequently, a regression based single convolutional neural network YOLOv2 is used to detect vehicles in UAV images. To evaluate our method, UAV video images were taken over several urban areas, and experiments were conducted on this dataset and Stanford Drone dataset. The experimental results have proven that our data preparation strategy is useful, and YOLOv2 is effective for real-time vehicle detection of UAV video images. © 2017 IEEE.","CNN; CSK; UAV; vehicle detection; YOLOv2","Convolution; Deep learning; Deep neural networks; Neural networks; Remote sensing; Vehicles; Convolutional neural network; Data augmentation; Detection methods; Detection performance; Real-time detection; Vehicle detection; Very high spatial resolutions; YOLOv2; Unmanned aerial vehicles (UAV)"
"Can a Machine Generate Humanlike Language Descriptions for a Remote Sensing Image?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017140410&doi=10.1109%2fTGRS.2017.2677464&partnerID=40&md5=6c3a2319d95a81ecfabb40259bcfa905","This paper investigates an intriguing question in the remote sensing field: 'can a machine generate humanlike language descriptions for a remote sensing image?' The automatic description of a remote sensing image (namely, remote sensing image captioning) is an important but rarely studied task for artificial intelligence. It is more challenging as the description must not only capture the ground elements of different scales, but also express their attributes as well as how these elements interact with each other. Despite the difficulties, we have proposed a remote sensing image captioning framework by leveraging the techniques of the recent fast development of deep learning and fully convolutional networks. The experimental results on a set of high-resolution optical images including Google Earth images and GaoFen-2 satellite images demonstrate that the proposed method is able to generate robust and comprehensive sentence description with desirable speed performance. © 2016 IEEE.","Fully convolutional networks (FCNs); high-resolution optical remote sensing image; image understanding; remote sensing image captioning","Geometrical optics; Image reconstruction; Convolutional networks; Google earths; High-resolution optical images; Intriguing questions; Language description; Remote sensing images; Satellite images; Speed performance; Remote sensing; artificial intelligence; experimental study; image analysis; image processing; language; network analysis; optical method; remote sensing; satellite imagery; software; spectral resolution"
"Efficient deep auto-encoder learning for the classification of hyperspectral images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025457832&doi=10.1109%2fICVRV.2016.16&partnerID=40&md5=2c94e5ee4d8129d1dafb4cd42b515031","Hyperspectral Image (HSI) classification is one of the most pervasive issue in hyperspectral remote sensing field. Deep learning is an efficient learning algorithm that has been recently applied to HSI classification. This paper proposes a new spectralspatial HSI classification method based on the deep features extraction using stacked-Auto-encoders (SAE) and unsupervised HIS segmentation. Specifically, first the SAE model is exploited as a classical spectral information-based classifier to extract the deep features. Second, spatial dominated information is extracted by using effective boundary adjustment based segmentation technique. Finally, maximum voting criteria is used to merge the extracted spectral and spatial features, which results into the accurate spectral-spatial HSI classification. Experimental results with widely-used hyperspectral data confirms that the new spectral and spatial classification approach is able to improve results significantly in terms of classification accuracies. © 2016 IEEE.","Deep learning; Hyperspectral Image Classification; Segmentation; Stacked auto-encoder (SAE); Support vector machine (SVM)","Deep learning; Education; Hyperspectral imaging; Image classification; Image segmentation; Independent component analysis; Learning algorithms; Learning systems; Spectroscopy; Support vector machines; Virtual reality; Visualization; Auto encoders; Classification accuracy; Classification methods; Features extraction; Hyperspectral remote sensing; Segmentation techniques; Spatial classification; Spectral information; Classification (of information)"
"Learning to Diversify Deep Belief Networks for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016393818&doi=10.1109%2fTGRS.2017.2675902&partnerID=40&md5=3f6231f58a0adb7e24517f649336b546","In the literature of remote sensing, deep models with multiple layers have demonstrated their potentials in learning the abstract and invariant features for better representation and classification of hyperspectral images. The usual supervised deep models, such as convolutional neural networks, need a large number of labeled training samples to learn their model parameters. However, the real-world hyperspectral image classification task provides only a limited number of training samples. This paper adopts another popular deep model, i.e., deep belief networks (DBNs), to deal with this problem. The DBNs allow unsupervised pretraining over unlabeled samples at first and then a supervised fine-tuning over labeled samples. But the usual pretraining and fine-tuning method would make many hidden units in the learned DBNs tend to behave very similarly or perform as 'dead' (never responding) or 'potential over-tolerant' (always responding) latent factors. These results could negatively affect description ability and thus classification performance of DBNs. To further improve DBN's performance, this paper develops a new diversified DBN through regularizing pretraining and fine-tuning procedures by a diversity promoting prior over latent factors. Moreover, the regularized pretraining and fine-tuning can be efficiently implemented through usual recursive greedy and back-propagation learning framework. The experiments over real-world hyperspectral images demonstrated that the diversity promoting prior in both pretraining and fine-tuning procedure lead to the learned DBNs with more diverse latent factors, which directly make the diversified DBNs obtain much better results than original DBNs and comparable or even better performances compared with other recent hyperspectral image classification methods. © 1980-2012 IEEE.","Deep belief network (DBN); diversity; hyperspectral image; image classification","Backpropagation; Bayesian networks; Deep learning; Hyperspectral imaging; Independent component analysis; Neural networks; Remote sensing; Sampling; Spectroscopy; Backpropagation learning; Classification performance; Convolutional neural network; Deep belief network (DBN); Deep belief networks; Description abilities; diversity; Fine-tuning methods; Image classification; artificial neural network; image classification; numerical model; parameter estimation; remote sensing"
"Exploiting deep matching and SAR data for the geo-localization accuracy improvement of optical satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021174875&doi=10.3390%2frs9060586&partnerID=40&md5=3b9635e16e34909710708c5d7d4cb19b","Improving the geo-localization of optical satellite images is an important pre-processing step for many remote sensing tasks like monitoring by image time series or scene analysis after sudden events. These tasks require geo-referenced and precisely co-registered multi-sensor data. Images captured by the high resolution synthetic aperture radar (SAR) satellite TerraSAR-X exhibit an absolute geo-location accuracy within a few decimeters. These images represent therefore a reliable source to improve the geo-location accuracy of optical images, which is in the order of tens of meters. In this paper, a deep learning-based approach for the geo-localization accuracy improvement of optical satellite images through SAR reference data is investigated. Image registration between SAR and optical images requires few, but accurate and reliable matching points. These are derived from a Siamese neural network. The network is trained using TerraSAR-X and PRISM image pairs covering greater urban areas spread over Europe, in order to learn the two-dimensional spatial shifts between optical and SAR image patches. Results confirm that accurate and reliable matching points can be generated with higher matching accuracy and precision with respect to state-of-the-art approaches. © 2017 by the authors.","Geo-referencing; Multi-sensor image matching; Satellite images; Siamese neural network; Synthetic aperture radar","Deep learning; Geometrical optics; Radar; Satellites; Space-based radar; Synthetic aperture radar; Time series analysis; Tracking (position); Accuracy and precision; Georeferencing; High resolution synthetic aperture radar; Learning-based approach; Multi sensor images; Optical satellite images; Satellite images; State-of-the-art approach; Radar imaging"
"One-dimensional convolutional neural network land-cover classification of multi-seasonal hyperspectral imagery in the San Francisco Bay Area, California","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021141369&doi=10.3390%2frs9060629&partnerID=40&md5=b5d4b55184f94de54ef2d4dd8c88c6aa","In this study, a 1-D Convolutional Neural Network (CNN) architecture was developed, trained and utilized to classify single (summer) and three seasons (spring, summer, fall) of hyperspectral imagery over the San Francisco Bay Area, California for the year 2015. For comparison, the Random Forests (RF) and Support Vector Machine (SVM) classifiers were trained and tested with the same data. In order to support space-based hyperspectral applications, all analyses were performed with simulated Hyperspectral Infrared Imager (HyspIRI) imagery. Three-season data improved classifier overall accuracy by 2.0% (SVM), 1.9% (CNN) to 3.5% (RF) over single-season data. The three-season CNN provided an overall classification accuracy of 89.9%, which was comparable to overall accuracy of 89.5% for SVM. Both three-season CNN and SVM outperformed RF by over 7% overall accuracy. Analysis and visualization of the inner products for the CNN provided insight to distinctive features within the spectral-temporal domain. A method for CNN kernel tuning was presented to assess the importance of learned features. We concluded that CNN is a promising candidate for hyperspectral remote sensing applications because of the high classification accuracy and interpretability of its inner products. © 2017 by the authors.","1-dimensional (1-D); Convolutional Neural Network (CNN); Deep learning; Hyperspectral imagery; Machine learning; Multi-seasonal; Random Forests (RF); Regional land cover; Support Vector Machine (SVM); TensorFlow","Convolution; Decision trees; Deep learning; Education; Image classification; Learning systems; Neural networks; Remote sensing; Spectroscopy; Support vector machines; 1-dimensional (1-D); Convolutional neural network; Hyper-spectral imageries; Land cover; Multi-seasonal; Random forests; TensorFlow; Deep neural networks"
"Hourglass-shape network based semantic segmentation for high resolution aerial imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021108235&doi=10.3390%2frs9060522&partnerID=40&md5=ee3e40b607e860d09426442dd9e4089b","A new convolution neural network (CNN) architecture for semantic segmentation of high resolution aerial imagery is proposed in this paper. The proposed architecture follows an hourglass-shaped network (HSN) design being structured into encoding and decoding stages. By taking advantage of recent advances in CNN designs, we use the composed inception module to replace common convolutional layers, providing the network with multi-scale receptive areas with rich context. Additionally, in order to reduce spatial ambiguities in the up-sampling stage, skip connections with residual units are also employed to feed forward encoding-stage information directly to the decoder. Moreover, overlap inference is employed to alleviate boundary effects occurring when high resolution images are inferred from small-sized patches. Finally, we also propose a post-processing method based on weighted belief propagation to visually enhance the classification results. Extensive experiments based on the Vaihingen and Potsdam datasets demonstrate that the proposed architectures outperform three reference state-of-the-art network designs both numerically and visually. © 2017 by the authors.","Aerial images; Convolutional neural networks; Deep learning; Remote sensing; Semantic labeling","Aerial photography; Convolution; Decoding; Deep learning; Deep neural networks; Encoding (symbols); Image segmentation; Neural networks; Remote sensing; Semantic Web; Semantics; Aerial images; Classification results; Convolution neural network; Convolutional neural network; High resolution aerial imagery; Postprocessing methods; Proposed architectures; Semantic labeling; Network architecture"
"Learning oriented region-based convolutional neural networks for building detection in satellite remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021104333&doi=10.5194%2fisprs-archives-XLII-1-W1-461-2017&partnerID=40&md5=2fc9d81a4686f5e6e99fe16fe36dca3b","The automated building detection in aerial images is a fundamental problem encountered in aerial and satellite images analysis. Recently, thanks to the advances in feature descriptions, Region-based CNN model (R-CNN) for object detection is receiving an increasing attention. Despite the excellent performance in object detection, it is problematic to directly leverage the features of R-CNN model for building detection in single aerial image. As we know, the single aerial image is in vertical view and the buildings possess significant directional feature. However, in R-CNN model, direction of the building is ignored and the detection results are represented by horizontal rectangles. For this reason, the detection results with horizontal rectangle cannot describe the building precisely. To address this problem, in this paper, we proposed a novel model with a key feature related to orientation, namely, Oriented R-CNN (OR-CNN). Our contributions are mainly in the following two aspects: 1) Introducing a new oriented layer network for detecting the rotation angle of building on the basis of the successful VGG-net R-CNN model; 2) the oriented rectangle is proposed to leverage the powerful R-CNN for remote-sensing building detection. In experiments, we establish a complete and bran-new data set for training our oriented R-CNN model and comprehensively evaluate the proposed method on a publicly available building detection data set. We demonstrate State-of-the-art results compared with the previous baseline methods.","Building detection; Deep learning; OR-CNN; Satellite remote sensing images; VGG","Buildings; Deep learning; Education; Geometry; Image analysis; Image processing; Image reconstruction; Network layers; Neural networks; Object detection; Object recognition; Remote sensing; Satellites; Automated buildings; Building detection; Convolutional neural network; Directional feature; Feature description; OR-CNN; Satellite remote sensing; State of the art; Feature extraction"
"Roof type selection based on patch-based classification using deep learning for high resolution satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021082463&doi=10.5194%2fisprs-archives-XLII-1-W1-653-2017&partnerID=40&md5=89483e1635f94cdf20a0841433e483e4","3D building reconstruction from remote sensing image data from satellites is still an active research topic and very valuable for 3D city modelling. The roof model is the most important component to reconstruct the Level of Details 2 (LoD2) for a building in 3D modelling. While the general solution for roof modelling relies on the detailed cues (such as lines, corners and planes) extracted from a Digital Surface Model (DSM), the correct detection of the roof type and its modelling can fail due to low quality of the DSM generated by dense stereo matching. To reduce dependencies of roof modelling on DSMs, the pansharpened satellite images as a rich resource of information are used in addition. In this paper, two strategies are employed for roof type classification. In the first one, building roof types are classified in a state-of-the-art supervised pre-trained convolutional neural network (CNN) framework. In the second strategy, deep features from deep layers of different pre-trained CNN model are extracted and then an RBF kernel using SVM is employed to classify the building roof type. Based on roof complexity of the scene, a roof library including seven types of roofs is defined. A new semi-automatic method is proposed to generate training and test patches of each roof type in the library. Using the pre-trained CNN model does not only decrease the computation time for training significantly but also increases the classification accuracy.","Convolutional neural networks; Deep learning method; High resolution satellite imagery; Roof reconstruction","Complex networks; Convolution; Deep learning; Deep neural networks; Edge detection; Education; Image classification; Image reconstruction; Neural networks; Roofs; Satellite imagery; Satellites; Stereo image processing; 3-d building reconstruction; Classification accuracy; Convolutional neural network; Dense stereo matching; Digital surface models; High resolution satellite imagery; Learning methods; Semiautomatic methods; Three dimensional computer graphics"
"Building extraction from remote sensing data using fully convolutional networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021131595&doi=10.5194%2fisprs-archives-XLII-1-W1-481-2017&partnerID=40&md5=167f49ab6d588c1b6ccf5909d85fa676","Building detection and footprint extraction are highly demanded for many remote sensing applications. Though most previous works have shown promising results, the automatic extraction of building footprints still remains a nontrivial topic, especially in complex urban areas. Recently developed extensions of the CNN framework made it possible to perform dense pixel-wise classification of input images. Based on these abilities we propose a methodology, which automatically generates a full resolution binary building mask out of a Digital Surface Model (DSM) using a Fully Convolution Network (FCN) architecture. The advantage of using the depth information is that it provides geometrical silhouettes and allows a better separation of buildings from background as well as through its invariance to illumination and color variations. The proposed framework has mainly two steps. Firstly, the FCN is trained on a large set of patches consisting of normalized DSM (nDSM) as inputs and available ground truth building mask as target outputs. Secondly, the generated predictions from FCN are viewed as unary terms for a Fully connected Conditional Random Fields (FCRF), which enables us to create a final binary building mask. A series of experiments demonstrate that our methodology is able to extract accurate building footprints which are close to the buildings original shapes to a high degree. The quantitative and qualitative analysis show the significant improvements of the results in contrast to the multy-layer fully connected network from our previous work.","Binary classification; Building footprint; Deep learning; DSM; Fully connected CRF; Fully convolutional networks","Bins; Complex networks; Convolution; Data mining; Deep learning; Extraction; Network layers; Random processes; Remote sensing; Binary classification; Building footprint; Conditional random field; Convolutional networks; Fully connected CRF; Fully connected networks; Quantitative and qualitative analysis; Remote sensing applications; Buildings"
"Retrieval of remote sensing images based on semisupervised deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027269893&doi=10.11834%2fjrs.20176105&partnerID=40&md5=431a191ab6760a0192b6250640f77be2","Massive data and diversity characteristics exert higher demands on the retrieval of remote sensing images. The feature extraction algorithm is the most outstanding factor that influences the performance of retrieval methods. Traditional feature extraction methods cause the problem of semantic gap, which occurs when the low-level feature does not perfectly reflect or match the purpose of retrieval. BOVW and multilayer cluster analysis have been proposed to solve semantic gap. However, the usability of these methods is limited given their dependence on classification or cluster algorithms and artificiality. A semi supervised deep-learning method was proposed in this paper. This method combines Sparse Auto encoder (SA) and the principle of Convolutional Neural Networks (CNNs). The method involves four steps: first, the remote sensing images are pretreated with the ZCA whitening method. Second, the feature dictionary is extracted using SA, an algorithm that deals with nonannotated data. Subsequently, the feature dictionary is utilized in image convolution following the principle of CNNs, which imitates the neural net of organisms and decreases the number of features. Average pooling is conducted after image convolution. Both convolution and pooling are implemented to reduce model complexity, thus calculating distance faster. Third, the soft max classifier categorizes remote sensing images into five classes. Lastly, the remote sensing image retrieval is sorted based on the Euclidean distance between the query image and database in the same category as the query image. Experimental results based on high-resolution remote sensing images demonstrate that the proposed method is effective and is more accurate than the methods that are based on color and texture features. Image classification before sorting speeds up retrieval by 27.6%. In addition, the semi-supervised deep learning algorithm is stable when the number of returned images increases. Given that the number of neurons in the hidden SA layer and the region size of pooling primarily affect retrieval results, this study conducted several optimization experiments on these two parameters. Moreover, the algorithm in this research performed well when the number of images in the data set and the retrieval accuracy on a larger data set increased, which are meaningful for the retrieval of massive remote sensing images. The semisupervised deep learning algorithm decreases the time of image annotation, which is an exhausting job. Unlike traditional methods that extracts the features of color, texture, and shape, the method in this study directly extracts the feature dictionary from image elements with good accuracy. Moreover, the efficiency of the proposed method is guaranteed by convolution and pooling, which reduce the feature dimension. Furthermore, our experiment proves that this algorithm performs well in retrieving high-resolution remote sensing images. © 2017, Science Press. All right reserved.","Convolutional neural networks; Deep learning; Remote sensing image retrieval; Softmax classifier; Sparse auto encoder","Cluster analysis; Clustering algorithms; Convolution; Data mining; Deep learning; Extraction; Feature extraction; Image classification; Image reconstruction; Image retrieval; Image texture; Learning algorithms; Learning systems; Neural networks; Optimization; Query processing; Semantics; Signal encoding; Auto encoders; Color and texture features; Convolutional neural network; Diversity characteristics; Feature extraction algorithms; Feature extraction methods; High resolution remote sensing images; Remote sensing image retrieval; Remote sensing"
"Deep learning for urban remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020230588&doi=10.1109%2fJURSE.2017.7924536&partnerID=40&md5=40435f3512b5218df6753f2d2d2085db","This work shows how deep learning techniques can benefit to remote sensing. We focus on tasks which are recurrent in Earth Observation data analysis. For classification and semantic mapping of aerial images, we present various deep network architectures and show that context information and dense labeling allow to reach better performances. For estimation of normals in point clouds, combining Hough transform with convolutional networks also improves the accuracy of previous frameworks by detecting hard configurations like corners. It shows that deep learning allows to revisit remote sensing and offers promising paths for urban modeling and monitoring. © 2017 IEEE.",,"Classification (of information); Deep learning; Edge detection; Hough transforms; Network architecture; Semantics; Aerial images; Context information; Convolutional networks; Earth observation data; Learning techniques; Point cloud; Semantic mapping; Urban remote sensing; Remote sensing"
"Per city-block, density estimation at build-up areas from aerial RGB imagery with deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020180182&doi=10.1109%2fJURSE.2017.7924605&partnerID=40&md5=e9b316b0fd72c7f56bcc3e38e8bf1115","Estimating the density of the 'urban fabric' land cover classes is of major importance for various urban and regional planning activities. However, the generation of such maps is still challenging requiring significant time and labor costs for the per city-block analysis of very high resolution remote sensing data. In this paper, we propose a supervised classification approach based on deep learning towards the accurate density estimation of build-up areas. In particular, for the training procedure we exploit information both from maps (open street, google, etc) and from very high resolution RGB google image mosaics. A patch-based, deep learning model was trained against five land cover classes. During the prediction phase the per city-block classification procedure delivered the locations and percentages of impervious, soil and green regions. Experimental results and validation at two European cities i.e., Athens and Bilbao, indicated overall accuracy rates of 95%. Results, also, highly match with the corresponding layers from the Copernicus Urban Atlas product. © 2017 IEEE.",,"Compensation (personnel); Deep learning; Regional planning; Wages; Density estimation; European cities; Learning models; Overall accuracies; Supervised classification; Training procedures; Urban and regional planning; Very high resolution; Remote sensing"
"Patch-based deep learning architectures for sparse annotated very high resolution datasets","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020225988&doi=10.1109%2fJURSE.2017.7924538&partnerID=40&md5=ee68410ed530c5ac242effbb21041315","In this paper, we compare the performance of different deep-learning architectures under a patch-based framework for the semantic labeling of sparse annotated urban scenes from very high resolution images. In particular, the simple convolutional network ConvNet, the AlexNet and the VGG models have been trained and tested on the publicly available, multispectral, very high resolution Summer Zurich v1.0 dataset. Experiments with patches of different dimensions have been performed and compared, indicating the optimal size for the semantic segmentation of very high resolution satellite data. The overall validation and assessment indicated the robustness of the high level features that are computed with the employed deep architectures for the semantic labeling of urban scenes. © 2017 IEEE.",,"Deep learning; Image segmentation; Network architecture; Semantics; Convolutional networks; Deep architectures; High-level features; Learning architectures; Semantic labeling; Semantic segmentation; Very high resolution; Very high resolution (VHR) image; Remote sensing"
"2017 Joint Urban Remote Sensing Event, JURSE 2017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020180239&partnerID=40&md5=cccbd42d55f9ff35e94eace465fd4ef8","The proceedings contain 99 papers. The topics discussed include: modeling the urban thermal environment distributions in Taipei basin using local climate zone (LCZ); mapping the diurnal thermal response of the urban heat island with landsat TIRS; CNN-based pansharpening of multi-resolution remote-sensing images; training convolutional neural networks for semantic classification of remote sensing imagery; deep learning for urban remote sensing; learning class- and location-specific priors for urban semantic labeling with CNNs; and patch-based deep learning architectures for sparse annotated very high resolution datasets.",,
"A comparison on multiple level features for fusion of hyperspectral and LiDAR data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020223833&doi=10.1109%2fJURSE.2017.7924601&partnerID=40&md5=66e3615623a35944bfcfdc7e19f5ad16","Remote sensed images contain a wealth of information. Next to diverse sensor technologies that allow us to measure different aspects of objects on the Earth (spectral characteristics in hyperspectral (HS) images, height in Light Detection And Ranging (LiDAR) data), we also have advanced image processing algorithms that have been developed to mine relevant information from multisensor remote sensing data for Earth observation. However, automatic interpretation of remote sensed images is still very difficult. In this paper, we compare multiple level features for fusion of HS and LiDAR data for urban area classification. Experimental results on fusion of HS and LiDAR data from the 2013 IEEE GRSS Data Fusion Contest demonstrate that middle-level morphological attribute features outperform high-level deep learning features. Compared to the methods using raw data fusion and deep learning fusion, with the graph-based fusion method [4], overall classification accuracies were improved by 8%. © 2017 IEEE.","Deep learning; Graph fusion; Hyperspectral; LiDAR; Urban remote sensing","Data fusion; Deep learning; Graphic methods; Image processing; Optical radar; HyperSpectral; Image processing algorithm; Light detection and ranging; Multiple level features; Multisensor remote sensing; Spectral characteristics; Urban area classification; Urban remote sensing; Remote sensing"
"Classification of Quickbird imagery over urban area using convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020191797&doi=10.1109%2fJURSE.2017.7924631&partnerID=40&md5=7bac458225ee0e2ca7a80a513d864560","During the past decades significant efforts have been made in developing various methods for Very high spatial resolution (VHSR) remotely sensed image classification; most of them are based on handcrafted learning-based features. Recently deep learning-based techniques have demonstrated excellent performance in remote sensing applications. In this paper we address the problem of urban imagery classification by developing a convolutional neural network (CNN) approach, which are the most popular deep learning approach for image classification. We design a custom CNN that operates on local patches in order to produce pixel-level classification map. The performance of the proposed model is validated on an exhaustive experimental comparison on a set of 20 QuickBird pansharpened multi-spectral images in urban zones. The obtained results outperform those obtained by different classification approaches on the same dataset. © 2017 IEEE.","Classification; Convolutional neural networks; Deep learning; Urban area; VHSR imagery","Classification (of information); Convolution; Deep learning; Neural networks; Remote sensing; Spectroscopy; Classification approach; Convolutional neural network; Experimental comparison; Remote sensing applications; Remotely sensed images; Urban areas; Very high spatial resolutions; VHSR imagery; Image classification"
"A spectral clustering based method for hyperspectral urban image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020235874&doi=10.1109%2fJURSE.2017.7924602&partnerID=40&md5=64eecbf3abdb9649ad54b85e13662741","Recently, a growing number of advanced hyperspectral remote sensing image classification techniques have been proposed and reported superiority in accuracy on the public available urban datasets, e.g., the Washington DC, and the Pavia Centre and University. Since the task of hyperspectral image classification is basically a special case of pattern recognition, many of these dominate techniques are machine learning based methods, such as the manifold learning, the sparse representation, and the newly raised deep learning. However, according to the literature review, most of them are the supervised learning methods, which require a certain number of high quality training samples to obtain an optimal model, and there is a huge imbalance between the researches of the unsupervised approaches compare to the supervised ones. In fact, the high performance of hyperspectral image unsupervised classification (i.e., clustering) without any prior information still leaves as a huge challenge. In this work, we present a method based on spectral clustering algorithm for hyperspectral urban image unsupervised classification, which tries to overcome the following two issues of the kmeans clustering in practice: (1) the course of dimensionality of the hyperspectral data, and (2) the unstable results caused by the initialization. In detail, the principal component analysis (PCA) is employed to reduce the feature dimensionality of the spectral domain, and then a spectral clustering based algorithm with adaptive neighbors is introduced for data clustering in the projected feature space. Experimental results on the standard hyperspectral urban image show that the proposed method not only outperforms the kmeans clustering in accuracy, but also provides reproducible performance. © 2017 IEEE.",,"Classification (of information); Cluster analysis; Deep learning; Image classification; Principal component analysis; Remote sensing; Space optics; Spectroscopy; Hyperspectral Data; Hyperspectral Remote Sensing Image; Sparse representation; Spectral clustering; Spectral clustering algorithms; Supervised learning methods; Unsupervised approaches; Unsupervised classification; Clustering algorithms"
"Empowering geo spatial analysis with big data platform: Natural resource management","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020019640&doi=10.1109%2fICCIC.2016.7919693&partnerID=40&md5=ccf154c9299d11169fd2adf4c9ac99bf","The continuous monitoring of the natural resources generates huge volume of geo spatial data. The structured, semi structured, unstructured data arrive from multiple resources like remote sensing satellite imagery, GPS, field deployed buoys, collected samples-investigated lab reports by biological-chemical scientists and physical oceanographers. The images, geo tagging together constitute the geo spatial data paradigm with the integrated layers and the associated technological components. The Conventional data analysis platforms are not enough to handle the voluminous multi-dimensional geo spatial data. The imagery analysis on the geo spatial data can be carried out with the image mining and deep learning BIGDATA algorithms. This paper briefs the BIG DATA platform for geospatial data analysis-a day to day affair of the natural resource management. This paper showcases how by BIGDATA platform empowerment the voluminous, continuous flow of unclassified geospatial data can be easily stored, extracted, analyzed and visualized. © 2016 IEEE.","big data; data analysis; Geospatial data; remote sensing; visualization","Artificial intelligence; Chemical analysis; Data handling; Data reduction; Data visualization; Flow visualization; Information analysis; Information management; Natural resources; Natural resources management; Remote sensing; Resource allocation; Satellite imagery; Chemical scientists; Continuous monitoring; Geo-spatial analysis; Geo-spatial data; Multiple resources; Natural resource management; Remote sensing satellites; Technological components; Big data"
"Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017192157&doi=10.1109%2fLGRS.2017.2681128&partnerID=40&md5=6c296283c35b5bb62ef0e1641ea33be8","Deep learning (DL) is a powerful state-of-the-art technique for image processing including remote sensing (RS) images. This letter describes a multilevel DL architecture that targets land cover and crop type classification from multitemporal multisource satellite imagery. The pillars of the architecture are unsupervised neural network (NN) that is used for optical imagery segmentation and missing data restoration due to clouds and shadows, and an ensemble of supervised NNs. As basic supervised NN architecture, we use a traditional fully connected multilayer perceptron (MLP) and the most commonly used approach in RS community random forest, and compare them with convolutional NNs (CNNs). Experiments are carried out for the joint experiment of crop assessment and monitoring test site in Ukraine for classification of crops in a heterogeneous environment using nineteen multitemporal scenes acquired by Landsat-8 and Sentinel-1A RS satellites. The architecture with an ensemble of CNNs outperforms the one with MLPs allowing us to better discriminate certain summer crop types, in particular maize and soybeans, and yielding the target accuracies more than 85% for all major crops (wheat, maize, sunflower, soybeans, and sugar beet). © 2017 IEEE.","Agriculture; convolutional neural networks (CNNs); crop classification; deep learning (DL); joint experiment of crop assessment and monitoring (JECAM); Landsat-8; remote sensing (RS); Sentinel-1; TensorFlow; Ukraine","Crops; Decision trees; Deep learning; Image processing; Network architecture; Optical data processing; Satellite imagery; Sugar beets; Assessment and monitoring; Crop type classification; Heterogeneous environments; Multi layer perceptron; Multi-source satellite imagery; Remote sensing images; State-of-the-art techniques; Unsupervised neural networks; Remote sensing"
"Upscaling of surface soil moisture using a deep learning model with VIIRS RDR","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019546584&doi=10.3390%2fijgi6050130&partnerID=40&md5=d4d76b7700cc674c09cd210a5b85a513","In current upscaling of in situ surface soil moisture practices, commonly used novel statistical or machine learning-based regression models combined with remote sensing data show some advantages in accurately capturing the satellite footprint scale of specific local or regional surface soil moisture. However, the performance of most models is largely determined by the size of the training data and the limited generalization ability to accomplish correlation extraction in regression models, which are unsuitable for larger scale practices. In this paper, a deep learning model was proposed to estimate soil moisture on a national scale. The deep learning model has the advantage of representing nonlinearities and modeling complex relationships from large-scale data. To illustrate the deep learning model for soil moisture estimation, the croplands of China were selected as the study area, and four years of Visible Infrared Imaging Radiometer Suite (VIIRS) raw data records (RDR) were used as input parameters, then the models were trained and soil moisture estimates were obtained. Results demonstrate that the estimated models captured the complex relationship between the remote sensing variables and in situ surface soil moisture with an adjusted coefficient of determination of R2 = 0.9875 and a root mean square error (RMSE) of 0.0084 in China. These results were more accurate than the Soil Moisture Active Passive (SMAP) active radar soil moisture products and the Global Land data assimilation system (GLDAS) 0-10 cm depth soil moisture data. Our study suggests that deep learning model have potential for operational applications of upscaling in situ surface soil moisture data at the national scale. © 2017 by the authors.","Deep learning; Surface soil moisture; VIIRS",
"Classification for high resolution remote sensing imagery using a fully convolutional network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019898857&doi=10.3390%2frs9050498&partnerID=40&md5=033d8c053f196ffe0a9c4f67806cf48b","As a variant of Convolutional Neural Networks (CNNs) in Deep Learning, the Fully Convolutional Network (FCN) model achieved state-of-the-art performance for natural image semantic segmentation. In this paper, an accurate classification approach for high resolution remote sensing imagery based on the improved FCN model is proposed. Firstly, we improve the density of output class maps by introducing Atrous convolution, and secondly, we design a multi-scale network architecture by adding a skip-layer structure to make it capable for multi-resolution image classification. Finally, we further refine the output class map using Conditional Random Fields (CRFs) post-processing. Our classification model is trained on 70 GF-2 true color images, and tested on the other 4 GF-2 images and 3 IKONOS true color images. We also employ object-oriented classification, patch-based CNN classification, and the FCN-8s approach on the same images for comparison. The experiments show that compared with the existing approaches, our approach has an obvious improvement in accuracy. The average precision, recall, and Kappa coefficient of our approach are 0.81, 0.78, and 0.83, respectively. The experiments also prove that our approach has strong applicability for multi-resolution image classification. © 2017 by the authors.","Classification; Convolutional neural network (CNN); Deep learning; Fully convolutional network (FCN); High resolution; Remote sensing","Classification (of information); Convolution; Deep learning; Image segmentation; Network architecture; Neural networks; Remote sensing; Semantics; Classification approach; Conditional Random Fields(CRFs); Convolutional networks; Convolutional neural network; High resolution; High resolution remote sensing imagery; Object oriented classification; State-of-the-art performance; Image classification"
"Precipitation identification with bispectral satellite information using deep learning approaches","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019030320&doi=10.1175%2fJHM-D-16-0176.1&partnerID=40&md5=558e83622cfe32be2e28350847eda415","In the development of a satellite-based precipitation product, two important aspects are sufficient precipitation information in the satellite-input data and proper methodologies, which are used to extract such information and connect it to precipitation estimates. In this study, the effectiveness of the state-of-the-art deep learning (DL) approaches to extract useful features from bispectral satellite information, infrared (IR), and water vapor (WV) channels, and to produce rain/no-rain (R/NR) detection is explored. To verify the methodologies, two models are designed and evaluated: the first model, referred to as the DL-IR only method, applies deep learning approaches to the IR data only; the second model, referred to as the DL-IR+WV method, incorporates WV data to further improve the precipitation identification performance. The radar stage IV data are the reference data used as ground observation. The operational product, Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks-Cloud Classification System (PERSIANN-CCS), serves as a baseline model with which to compare the performances. The experiments show significant improvement for both models in R/NR detection. The overall performance gains in the critical success index (CSI) are 21.60% and 43.66% over the verification periods for the DL-IR only model and the DL-IR+WV model compared to PERSIANN-CCS, respectively. In particular, the performance gains in CSI are as high as 46.51% and 94.57% for the models for the winter season. Moreover, specific case studies show that the deep learning techniques and the WV channel information effectively help recover a large number of missing precipitation pixels under warm clouds while reducing false alarms under cold clouds. © 2017 American Meteorological Society.","Classification; Hydrometeorology; Neural networks; Remote sensing; Satellite observations","algorithm; artificial neural network; climate classification; hydrometeorology; machine learning; precipitation (climatology); radar; remote sensing; satellite data"
"Land-Use Classification via Extreme Learning Classifier Based on Deep Convolutional Features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016418363&doi=10.1109%2fLGRS.2017.2672643&partnerID=40&md5=c7198d1a9c0045e0add39522d93c074e","One of the challenging issues in high-resolution remote sensing images is classifying land-use scenes with high quality and accuracy. An effective feature extractor and classifier can boost classification accuracy in scene classification. This letter proposes a deep-learning-based classification method, which combines convolutional neural networks (CNNs) and extreme learning machine (ELM) to improve classification performance. A pretrained CNN is initially used to learn deep and robust features. However, the generalization ability is finite and suboptimal, because the traditional CNN adopts fully connected layers as classifier. We use an ELM classifier with the CNN-learned features instead of the fully connected layers of CNN to obtain excellent results. The effectiveness of the proposed method is tested on the UC-Merced data set that has 2100 remotely sensed land-use-scene images with 21 categories. Experimental results show that the proposed CNN-ELM classification method achieves satisfactory results. © 2017 IEEE.","Convolutional neural network (CNN); extreme learning machine (ELM); land-use classification; scene understanding","Convolution; Image reconstruction; Land use; Learning systems; Neural networks; Remote sensing; Classification accuracy; Classification methods; Classification performance; Convolutional neural network; Extreme learning machine; Generalization ability; High resolution remote sensing images; Landuse classifications; Classification (of information)"
"Change detection in SAR images based on deep semi-NMF and SVD networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019916979&doi=10.3390%2frs9050435&partnerID=40&md5=9f3421d50a9f58bd85213bfd57dd2952","With the development of Earth observation programs, more and more multi-temporal synthetic aperture radar (SAR) data are available from remote sensing platforms. Therefore, it is demanding to develop unsupervised methods for SAR image change detection. Recently, deep learning-based methods have displayed promising performance for remote sensing image analysis. However, these methods can only provide excellent performance when the number of training samples is sufficiently large. In this paper, a novel simple method for SAR image change detection is proposed. The proposed method uses two singular value decomposition (SVD) analyses to learn the non-linear relations between multi-temporal images. By this means, the proposed method can generate more representative feature expressions with fewer samples. Therefore, it provides a simple yet effective way to be designed and trained easily. Firstly, deep semi-nonnegative matrix factorization (Deep Semi-NMF) is utilized to select pixels that have a high probability of being changed or unchanged as samples. Next, image patches centered at these sample pixels are generated from the input multi-temporal SAR images. Then, we build SVD networks, which are comprised of two SVD convolutional layers and one histogram feature generation layer. Finally, pixels in both multi-temporal SAR images are classified by the SVD networks, and then the final change map can be obtained. The experimental results of three SAR datasets have demonstrated the effectiveness and robustness of the proposed method. © 2017 by the authors.","Change detection; Deep Semi-NMF; Fuzzy c-means; Nonnegative matrix factorization; SVD networks; Synthetic aperture radar","Factorization; Image analysis; Image reconstruction; Matrix algebra; Pixels; Radar; Remote sensing; Singular value decomposition; Synthetic aperture radar; Change detection; Deep Semi-NMF; Earth observation programs; Fuzzy C mean; Multi-temporal SAR images; Nonnegative matrix factorization; Remote sensing platforms; Semi-nonnegative matrix factorizations; Radar imaging"
"Hyperspectral Image Superresolution by Transfer Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012981972&doi=10.1109%2fJSTARS.2017.2655112&partnerID=40&md5=5361ab25a83106398d41540f82a6012a","Hyperspectral image superresolution is a highly attractive topic in computer vision and has attracted many researchers' attention. However, nearly all the existing methods assume that multiple observations of the same scene are required with the observed low-resolution hyperspectral image. This limits the application of superresolution. In this paper, we propose a new framework to enhance the resolution of hyperspectral images by exploiting the knowledge from natural images: The relationship between low/high-resolution images is the same as that between low/high-resolution hyperspectral images. In the proposed framework, the mapping between low- A nd high-resolution images can be learned by deep convolutional neural network and be transferred to hyperspectral image by borrowing the idea of transfer learning. In addition, to study the spectral characteristic between low- A nd high-resolution hyperspectral image, collaborative nonnegative matrix factorization (CNMF) is proposed to enforce collaborations between the low- A nd high-resolution hyperspectral images, which encourages the estimated solution to extract the same endmembers with low-resolution hyperspectral image. The experimental results on ground based and remote sensing data suggest that the proposed method achieves comparable performance without requiring any auxiliary images of the same scene. © 2016 IEEE.","Collaborative nonnegative matrix factorization (CNMF); convolutional neural network (CNN); hyperspectral image (HSI) superresolution","Deep learning; Deep neural networks; Factorization; Independent component analysis; Matrix algebra; Neural networks; Optical resolving power; Remote sensing; Convolutional neural network; High resolution image; Image super-resolution; Nonnegative matrix factorization; Remote sensing data; Spectral characteristics; Super resolution; Transfer learning; Spectroscopy; artificial neural network; computer vision; image resolution; imaging method; mapping; remote sensing"
"Gated convolutional neural network for semantic segmentation in high-resolution images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019963914&doi=10.3390%2frs9050446&partnerID=40&md5=e432a607714b9d9b0d9b3c0f969765aa","Semantic segmentation is a fundamental task in remote sensing image processing. The large appearance variations of ground objects make this task quite challenging. Recently, deep convolutional neural networks (DCNNs) have shown outstanding performance in this task. A common strategy of these methods (e.g., SegNet) for performance improvement is to combine the feature maps learned at different DCNN layers. However, such a combination is usually implemented via feature map summation or concatenation, indicating that the features are considered indiscriminately. In fact, features at different positions contribute differently to the final performance. It is advantageous to automatically select adaptive features when merging different-layer feature maps. To achieve this goal, we propose a gated convolutional neural network to fulfill this task. Specifically, we explore the relationship between the information entropy of the feature maps and the label-error map, and then a gate mechanism is embedded to integrate the feature maps more effectively. The gate is implemented by the entropy maps, which are generated to assign adaptive weights to different feature maps as their relative importance. Generally, the entropy maps, i.e., the gates, guide the network to focus on the highly-uncertain pixels, where detailed information from lower layers is required to improve the separability of these pixels. The selected features are finally combined to feed into the classifier layer, which predicts the semantic label of each pixel. The proposed method achieves competitive segmentation accuracy on the public ISPRS 2D Semantic Labeling benchmark, which is challenging for segmentation by only using the RGB images. © 2017 by the authors.","CNN; Deep learning; Gate; ISPRS; Remote sensing; Semantic segmentation","Convolution; Deep learning; Deep neural networks; Image processing; Image reconstruction; Neural networks; Pixels; Remote sensing; Semantic Web; Semantics; Convolutional neural network; Gate; High resolution image; Information entropy; ISPRS; Remote sensing image processing; Segmentation accuracy; Semantic segmentation; Image segmentation"
"Learning dual multi-scale manifold ranking for semantic segmentation of high-resolution images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019931510&doi=10.3390%2frs9050500&partnerID=40&md5=ded09a1880e9fe749d096501ded7bece","Semantic image segmentation has recently witnessed considerable progress by training deep convolutional neural networks (CNNs). The core issue of this technique is the limited capacity of CNNs to depict visual objects. Existing approaches tend to utilize approximate inference in a discrete domain or additional aides and do not have a global optimum guarantee. We propose the use of the multi-label manifold ranking (MR) method in solving the linear objective energy function in a continuous domain to delineate visual objects and solve these problems. We present a novel embedded single stream optimization method based on the MR model to avoid approximations without sacrificing expressive power. In addition, we propose a novel network, which we refer to as dual multi-scale manifold ranking (DMSMR) network, that combines the dilated, multi-scale strategies with the single stream MR optimization method in the deep learning architecture to further improve the performance. Experiments on high resolution images, including close-range and remote sensing datasets, demonstrate that the proposed approach can achieve competitive accuracy without additional aides in an end-to-end manner. © 2017 by the authors.","Deep convolutional neural networks; High resolution image; Manifold ranking; Semantic segmentation; Single stream optimization","Convolution; Deep neural networks; Neural networks; Remote sensing; Semantics; Video streaming; Convolutional neural network; High resolution image; Manifold ranking; Semantic segmentation; Stream optimization; Image segmentation"
"Sensing spatial distribution of urban land use by integrating points-of-interest and Google Word2Vec model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992200230&doi=10.1080%2f13658816.2016.1244608&partnerID=40&md5=c7178d40cd467bc1d52412b440f97e75","Urban land use information plays an essential role in a wide variety of urban planning and environmental monitoring processes. During the past few decades, with the rapid technological development of remote sensing (RS), geographic information systems (GIS) and geospatial big data, numerous methods have been developed to identify urban land use at a fine scale. Points-of-interest (POIs) have been widely used to extract information pertaining to urban land use types and functional zones. However, it is difficult to quantify the relationship between spatial distributions of POIs and regional land use types due to a lack of reliable models. Previous methods may ignore abundant spatial features that can be extracted from POIs. In this study, we establish an innovative framework that detects urban land use distributions at the scale of traffic analysis zones (TAZs) by integrating Baidu POIs and a Word2Vec model. This framework was implemented using a Google open-source model of a deep-learning language in 2013. First, data for the Pearl River Delta (PRD) are transformed into a TAZ-POI corpus using a greedy algorithm by considering the spatial distributions of TAZs and inner POIs. Then, high-dimensional characteristic vectors of POIs and TAZs are extracted using the Word2Vec model. Finally, to validate the reliability of the POI/TAZ vectors, we implement a K-Means-based clustering model to analyze correlations between the POI/TAZ vectors and deploy TAZ vectors to identify urban land use types using a random forest algorithm (RFA) model. Compared with some state-of-the-art probabilistic topic models (PTMs), the proposed method can efficiently obtain the highest accuracy (OA = 0.8728, kappa = 0.8399). Moreover, the results can be used to help urban planners to monitor dynamic urban land use and evaluate the impact of urban planning schemes. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","deep learning; Land use; point-of-interest; topic model; Word2Vec","algorithm; GIS; land use; modeling; remote sensing; spatial distribution; urban area; urban planning; China; Guangdong; Zhujiang Delta"
"Segment-before-detect: Vehicle detection and classification through semantic segmentation of aerial images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017577194&doi=10.3390%2frs9040368&partnerID=40&md5=f593a17d5957731d923e52164436cd1d","Like computer vision before, remote sensing has been radically changed by the introduction of deep learning and, more notably, Convolution Neural Networks. Land cover classification, object detection and scene understanding in aerial images rely more and more on deep networks to achieve new state-of-the-art results. Recent architectures such as Fully Convolutional Networks can even produce pixel level annotations for semantic mapping. In this work, we present a deep-learning based segment-before-detect method for segmentation and subsequent detection and classification of several varieties of wheeled vehicles in high resolution remote sensing images. This allows us to investigate object detection and classification on a complex dataset made up of visually similar classes, and to demonstrate the relevance of such a subclass modeling approach. Especially, we want to show that deep learning is also suitable for object-oriented analysis of Earth Observation data as effective object detection can be obtained as a byproduct of accurate semantic segmentation. First, we train a deep fully convolutional network on the ISPRS Potsdam and the NZAM/ONERA Christchurch datasets and show how the learnt semantic maps can be used to extract precise segmentation of vehicles. Then, we show that those maps are accurate enough to perform vehicle detection by simple connected component extraction. This allows us to study the repartition of vehicles in the city. Finally, we train a Convolutional Neural Network to perform vehicle classification on the VEDAI dataset, and transfer its knowledge to classify the individual vehicle instances that we detected. © 2017 by the authors.","Deep learning; Object classification; Semantic segmentation; Vehicle detection","Classification (of information); Convolution; Deep learning; Image classification; Image processing; Image reconstruction; Image segmentation; Neural networks; Object recognition; Remote sensing; Semantics; Vehicles; Connected component extraction; Convolution neural network; Convolutional neural network; High resolution remote sensing images; Land cover classification; Object classification; Semantic segmentation; Vehicle detection; Object detection"
"Training Deep Convolutional Neural Networks for Land-Cover Classification of High-Resolution Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013301566&doi=10.1109%2fLGRS.2017.2657778&partnerID=40&md5=79afc402f535c270a3ebc9140b2f3c42","Deep convolutional neural networks (DCNNs) have recently emerged as a dominant paradigm for machine learning in a variety of domains. However, acquiring a suitably large data set for training DCNN is often a significant challenge. This is a major issue in the remote sensing domain, where we have extremely large collections of satellite and aerial imagery, but lack the rich label information that is often readily available for other image modalities. In this letter, we investigate the use of DCNN for land-cover classification in high-resolution remote sensing imagery. To overcome the lack of massive labeled remote-sensing image data sets, we employ two techniques in conjunction with DCNN: transfer learning (TL) with fine-tuning and data augmentation tailored specifically for remote sensing imagery. TL allows one to bootstrap a DCNN while preserving the deep visual feature extraction learned over an image corpus from a different image domain. Data augmentation exploits various aspects of remote sensing imagery to dramatically expand small training image data sets and improve DCNN robustness for remote sensing image data. Here, we apply these techniques to the well-known UC Merced data set to achieve the land-cover classification accuracies of 97.8 ± 2.3%, 97.6 ± 2.6%, and 98.5 ± 1.4% with CaffeNet, GoogLeNet, and ResNet, respectively. © 2017 IEEE.","Deep convolutional neural network (DCNN); deep learning; high-resolution remote sensing imagery; land-cover classification; Transfer learning (TL)","Aerial photography; Classification (of information); Convolution; Deep learning; Deep neural networks; Feature extraction; Image classification; Image reconstruction; Learning systems; Neural networks; Satellite imagery; Convolutional neural network; High resolution imagery; High resolution remote sensing imagery; Land cover classification; Remote sensing imagery; Remote sensing images; Transfer learning; Visual feature extraction; Remote sensing"
"Multiple regularizations deep learning for paddy growth stages classification from LANDSAT-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016940505&doi=10.1109%2fICACSIS.2016.7872790&partnerID=40&md5=458e4f5f04fc973966dc7e6c006fb829","This study uses remote sensing technology that can provide information about the condition of the earth's surface area, fast, and spatially. The study area was in Karawang District, lying in the Northern part of West Java-Indonesia. We address a paddy growth stages classification using LANDSAT 8 image data obtained from multi-sensor remote sensing image taken in October 2015 to August 2016. This study pursues a fast and accurate classification of paddy growth stages by employing multiple regularizations learning on some deep learning methods such as DNN (Deep Neural Networks) and 1-D CNN (1-D Convolutional Neural Networks). The used regularizations are Fast Dropout, Dropout, and Batch Normalization. To evaluate the effectiveness, we also compared our method with other machine learning methods such as (Logistic Regression, SVM, Random Forest, and XGBoost). The data used are seven bands of LANDSAT-8 spectral data samples that correspond to paddy growth stages data obtained from i-Sky (eye in the sky) Innovation system. The growth stages are determined based on paddy crop phenology profile from time series of LANDSAT-8 images. The classification results show that MLP using multiple regularization Dropout and Batch Normalization achieves the highest accuracy for this dataset. © 2016 IEEE.","Classification; Deep Learning; Fast Dropout Training; LANDSAT 8; Machine Learning; Paddy Growth Stage; Remote Sensing","Artificial intelligence; Classification (of information); Decision trees; Deep neural networks; Image classification; Image reconstruction; Information systems; Learning systems; Neural networks; Remote sensing; Classification results; Convolutional neural network; Growth stages; LANDSAT; Logistic regressions; Machine learning methods; Remote sensing images; Remote sensing technology; Deep learning"
"Transferring pre-trained deep CNNs for remote scene classification with general features learned from linear PCA network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019378344&doi=10.3390%2frs9030225&partnerID=40&md5=33f09f8ca4c345420166a06980fe1c9c","Deep convolutional neural networks (CNNs) have been widely used to obtain high-level representation in various computer vision tasks. However, in the field of remote sensing, there are not sufficient images to train a useful deep CNN. Instead, we tend to transfer successful pre-trained deep CNNs to remote sensing tasks. In the transferring process, generalization power of features in pre-trained deep CNNs plays the key role. In this paper, we propose two promising architectures to extract general features from pre-trained deep CNNs for remote scene classification. These two architectures suggest two directions for improvement. First, before the pre-trained deep CNNs, we design a linear PCA network (LPCANet) to synthesize spatial information of remote sensing images in each spectral channel. This design shortens the spatial ""distance"" of target and source datasets for pre-trained deep CNNs. Second, we introduce quaternion algebra to LPCANet, which further shortens the spectral ""distance"" between remote sensing images and images used to pre-train deep CNNs. With five well-known pre-trained deep CNNs, experimental results on three independent remote sensing datasets demonstrate that our proposed framework obtains state-of-the-art results without fine-tuning and feature fusing. This paper also provides baseline for transferring fresh pre-trained deep CNNs to other remote sensing tasks. © 2017 by the authors.","Convolutional neural network; Deep learning; General feature; Principle component analysis; Remote scene classification","Convolution; Deep learning; Deep neural networks; Image reconstruction; Linear networks; Network architecture; Neural networks; Principal component analysis; Convolutional neural network; General feature; Principle component analysis; Remote sensing images; Scene classification; Spatial informations; Transferring process; Without fine-tuning; Remote sensing"
"Learning and Transferring Convolutional Neural Network Knowledge to Ocean Front Recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010695311&doi=10.1109%2fLGRS.2016.2643000&partnerID=40&md5=e369784931607301e7e4d80332947b28","In this letter, we investigated how to apply a deep learning method, in particular convolutional neural networks (CNNs), to an ocean front recognition task. Exploring deep CNNs knowledge to ocean front recognition is a challenging task, because the training data is very scarce. This letter overcomes this challenge using a sequence of transfer learning steps via fine-Tuning. The core idea is to extract deep knowledge of the CNN model from a large data set and then transfer the knowledge to our ocean front recognition task on limited remote sensing (RS) images. We conducted experiments on two different RS image data sets, with different visual properties, i.e., colorful and gray-level data, which were both downloaded from the National Oceanic and Atmospheric Administration (NOAA). The proposed method was compared with the conventional handcraft descriptor with bag-of-visual-words, original CNN model, and last-layer fine-Tuned CNN model. Our method showed a significantly higher accuracy than other methods in both datasets. © 2004-2012 IEEE.","Convolutional neural networks (CNNs); fine-Tuning; ocean front recognition; transfer learning","Convolution; Neural networks; Remote sensing; Bag-of-visual-words; Convolutional neural network; Deep knowledge; Large datasets; National Oceanic and Atmospheric Administration; Remote sensing images; Transfer learning; Visual properties; Oceanography"
"Deep learning for ocean remote sensing: An application of convolutional neural networks for super-resolution on satellite-derived SST data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016967048&doi=10.1109%2fPRRS.2016.7867019&partnerID=40&md5=aa5bb17305fe1ca49863e291ddb84a59","In this paper, we propose to address the downscaling of ocean remote sensing data using image super-resolution models based on deep learning, and more particularly Convolutional Neural Networks (CNNs). The goal of this study, for which we focus on satellite-derived Sea Surface Temperature (SST) data, is to evaluate the efficiency and the relevance of deep learning architectures applied to oceanographic remote sensing data. By using a CNN architecture, namely SRCNN (Super Resolution CNN), on a large-scale dataset of SST fields, we show that it allows a considerable gain in terms of PSNR compared to classical downscaling techniques. These results point out the relevance of deep learning models specifically trained for ocean remote sensing data and advocate for other applications to the reconstruction of high-resolution sea surface geophysical fields from multi-sensor satellite observations. © 2016 IEEE.","Convolutional Neural Networks; Deep Learning; Ocean Remote Sensing Data; Super-Resolution","Atmospheric temperature; Convolution; Deep learning; Network architecture; Neural networks; Oceanography; Optical resolving power; Pattern recognition; Satellites; Surface waters; Convolutional neural network; Image super resolutions; Large-scale dataset; Learning architectures; Ocean remote sensing; Remote sensing data; Sea surface temperature (SST); Super resolution; Remote sensing"
"2016 9th IAPR Workshop on Pattern Recognition in Remote Sensing, PRRS 2016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017011087&partnerID=40&md5=510c5451499a6db52db070ce560d8e32","The proceedings contain 14 papers. The topics discussed include: improving semantic orthophotos by a fast method based on harmonic inpainting; segmentation of 3D outdoor scenes using hierarchical clustering structure and perceptual grouping laws; river sediment yield classification using remote sensing imagery; reflectance-based 3D shape refinement of surfaces with spatially varying BRDF properties; 4d change detection based on persistent scatterer interferometry; robust alignment for UAV images based on adaptive adjustment; a comprehensive study on object proposals methods for vehicle detection in aerial images; deep learning for ocean remote sensing: an application of convolutional neural networks for super-resolution on satellite-derived SST data; evaluating imaging quality of the Offner hyperspectrometer; fast extraction of dominant planes in MLS-data of urban areas; discriminative archetypal self-taught learning for multispectral landcover classification; semisupervised classification of hyperspectral remote sensing images with spatial majority voting; towards vegetation species discrimination by using data-driven descriptors; and point cloud registration by combining shape and intensity contexts.",,
"Towards vegetation species discrimination by using data-driven descriptors","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016993939&doi=10.1109%2fPRRS.2016.7867024&partnerID=40&md5=a500cf37b6c0bf7e4f1d68c6ba625e18","In this paper, we analyse the use of Convolutional Neural Networks (CNNs or ConvNets) to discriminate vegetation species with few labelled samples. To the best of our knowledge, this is the first work dedicated to the investigation of the use of deep features in such task. The experimental evaluation demonstrate that deep features significantly outperform wellknown feature extraction techniques. The achieved results also show that it is possible to learn and classify vegetation patterns even with few samples. This makes the use of our approach feasible for real-world mapping applications, where it is often difficult to obtain large training sets. © 2016 IEEE.","Deep Learning; Feature Learning; High-resolution Images; Image Classification; Machine Learning; Remote Sensing","Deep learning; Feature extraction; Image classification; Learning systems; Neural networks; Pattern recognition; Vegetation; Convolutional neural network; Experimental evaluation; Feature extraction techniques; Feature learning; High resolution image; Mapping applications; Vegetation pattern; Vegetation species; Remote sensing"
"Spectral characteristics and species identification of rhododendrons using a discriminative restricted Boltzmann machine","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016956939&doi=10.1080%2f00387010.2017.1278709&partnerID=40&md5=664fbd998aec8cd05ae79343fe0f4481","Rhododendrons are an important genus of alpine flowering plant used ornamentally worldwide. The purpose of this study is to improve the application of remote-sensing technology for investigating and monitoring mountain rhododendron germplasm. Research area is the Baili Rhododendron National Forest Park located in the karst region of Guizhou Province, China. Field spectrometry was used to acquire spectral data for 20 samples extracted from eight rhododendron species. A deep-learning algorithm from a discriminative restricted Boltzmann machine was used with the original spectral data from the different rhododendron species to obtain the optimal parameters for the model. Simultaneously, the data processing methodology from the discriminative restricted Boltzmann machine was used to recognize the original spectra, the noise smoothed spectra, and the first- and second-order spectral derivatives with accuracies of 88.54%, 88.54%, 93.75%, and 90.62%, respectively. The results show that the discriminative restricted Boltzmann machine is effective in recognizing spectral information for different rhododendron species. Changes in the first-order derivative gave the most accurate classification, but changes in the second-order derivative significantly reduced the sample training time. Changes in both derivatives therefore proved useful in recognizing and extracting particular features of the plant species. This research may therefore further support the use of hyperspectral remote-sensing imagery for investigating and monitoring germplasm, species classification, and physiological parameter inversions for rhododendrons from various mountain regions of China. © 2017 Taylor & Francis.","Discriminative restricted Boltzmann machine; rhododendron; spectral recognition; vegetation spectrum",
"Active Deep Learning for Classification of Hyperspectral Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988373662&doi=10.1109%2fJSTARS.2016.2598859&partnerID=40&md5=549c38d6ca2cb2cb29bd5d1f7a78da9e","Active deep learning classification of hyperspectral images is considered in this paper. Deep learning has achieved success in many applications, but good-quality labeled samples are needed to construct a deep learning network. It is expensive getting good labeled samples in hyperspectral images for remote sensing applications. An active learning algorithm based on a weighted incremental dictionary learning is proposed for such applications. The proposed algorithm selects training samples that maximize two selection criteria, namely representative and uncertainty. This algorithm trains a deep network efficiently by actively selecting training samples at each iteration. The proposed algorithm is applied for the classification of hyperspectral images, and compared with other classification algorithms employing active learning. It is shown that the proposed algorithm is efficient and effective in classifying hyperspectral images. © 2008-2012 IEEE.","Active learning; deep learning; remote sensing classification; sparse representation","Artificial intelligence; Image classification; Iterative methods; Remote sensing; Sampling; Spectroscopy; Active Learning; Active-learning algorithm; Classification algorithm; Dictionary learning; Hyper-spectral images; Remote sensing applications; Selection criteria; Training sample; Learning algorithms; algorithm; image classification; multispectral image; remote sensing"
"Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992121956&doi=10.1109%2fTGRS.2016.2612821&partnerID=40&md5=6d05c27e787d8081e511fd69f49e6dbf","We propose an end-to-end framework for the dense, pixelwise classification of satellite imagery with convolutional neural networks (CNNs). In our framework, CNNs are directly trained to produce classification maps out of the input images. We first devise a fully convolutional architecture and demonstrate its relevance to the dense classification problem. We then address the issue of imperfect training data through a two-step training approach: CNNs are first initialized by using a large amount of possibly inaccurate reference data, and then refined on a small amount of accurately labeled data. To complete our framework, we design a multiscale neuron module that alleviates the common tradeoff between recognition and precise localization. A series of experiments show that our networks consider a large amount of context to provide fine-grained classification maps. © 1980-2012 IEEE.","Classification; convolutional neural networks (CNNs); deep learning; satellite images","Convolution; Image reconstruction; Neural networks; Remote sensing; Satellite imagery; Classification maps; Convolutional neural network; Large amounts; Pixelwise classification; Reference data; Remote sensing image classification; Training data; Two-step training; Image classification; accuracy assessment; artificial neural network; deconvolution; design method; image analysis; image classification; pixel; remote sensing; satellite imagery"
"SatCNN: satellite image dataset classification using agile convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994662729&doi=10.1080%2f2150704X.2016.1235299&partnerID=40&md5=ec3a7a77d69cf91b1b0176a76d9d166a","With the launch of various remote-sensing satellites, more and more high-spatial resolution remote-sensing (HSR-RS) images are becoming available. Scene classification of such a huge volume of HSR-RS images is a big challenge for the efficiency of the feature learning and model training. The deep convolutional neural network (CNN), a typical deep learning model, is an efficient end-to-end deep hierarchical feature learning model that can capture the intrinsic features of input HSR-RS images. However, most published CNN architectures are borrowed from natural scene classification with thousands of training samples, and they are not designed for HSR-RS images. In this paper, we propose an agile CNN architecture, named as SatCNN, for HSR-RS image scene classification. Based on recent improvements to modern CNN architectures, we use more efficient convolutional layers with smaller kernels to build an effective CNN architecture. Experiments on SAT data sets confirmed that SatCNN can quickly and effectively learn robust features to handle the intra-class diversity even with small convolutional kernels, and the deeper convolutional layers allow spontaneous modelling of the relative spatial relationships. With the help of fast graphics processing unit acceleration, SatCNN can be trained within about 40 min, achieving overall accuracies of 99.65% and 99.54%, which is the state-of-the-art for SAT data sets. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",,"Classification (of information); Computer graphics; Convolution; Deep neural networks; Graphics processing unit; Network architecture; Neural networks; Program processors; Remote sensing; Satellite imagery; Convolutional kernel; Convolutional neural network; Deep convolutional neural networks; Hierarchical features; High spatial resolution; Remote sensing satellites; Scene classification; Spatial relationships; Image classification; artificial neural network; data set; image classification; image processing; image resolution; numerical model; remote sensing; satellite imagery"
"Land-Use Classification with Remote Sensing Image Based on Stacked Autoencoder","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015170363&doi=10.1109%2fICIICII.2016.0044&partnerID=40&md5=200777d4392cb6227fbbc960be9a3677","Focused on the issue that conventional land-use classification methods can't reach better performance, a new remote sensing image classification method based on Stacked Autoencoder inspired by deep learning was proposed. Firstly, the deep network model was built through the stacked layers of Autoencoder, then the unsupervised Greedy layer-wise training algorithm was used to train each layer in turn for more robust expressing, characteristics were learnt supervised by Back Propagation neural network and the whole net was optimized by using error back propagation. Finally, GF-1 remote sensing data were used for evaluation and the total accuracy and kappa accuracy which were higher than that of Support Vector Machine and Back Propagation neural network reached 95.5% and 95.3% respectively. The experiment results show that the proposed method can effectively improve the accuracy of land cover classification. © 2016 IEEE.","Back Propagation (BP) neural network; deep learning; Land cover classification; remote sensing image; stacked autoencoder (SAE)","Backpropagation algorithms; Deep learning; Deep neural networks; Image classification; Image reconstruction; Information analysis; Information retrieval; Intelligent computing; Land use; Learning systems; Neural networks; Remote sensing; Torsional stress; Auto encoders; Back propagation neural networks; Error back propagation; Land cover classification; Landuse classifications; Remote sensing image classification; Remote sensing images; Training algorithms; Backpropagation"
"Proceedings - 2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration, ICIICII 2016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015217596&partnerID=40&md5=617893c935d898fa86419c022d9cccb4","The proceedings contain 89 papers. The topics discussed include: regional optimal control strategy of electric vehicle based on the spatial load forecasting technology; research on the estimation of crowd density based on video image processing; a dynamic weighing method for portal crane in bulk port: based on clustering and BP neural network; extended dissipativity of discrete-time Markov jump neural networks with mixed delays; the optimizing design of wheeled robot tracking system by PID control algorithm based on BP neural network; exploring Chinese language evolution via computer corpus analysis: a case study of Liru's lexicalization and grammaticalization; study on ontology model of sports products after-sale O2O service; tunnel construction ventilation monitoring system based on Fieldbus technology; a method of human action recognition based on spatio-temporal interest points and PLSA; a novel framework for container code-character recognition based on deep learning and template matching; moving object tracking method based on improved Camshift Algorithm; sub-pixel location of motion blurred weak celestial objects in optical sensor image based on elliptical 2D Gaussian surface fitting; big data mining and intercultural business discourse studies: a case study of Li Ning's corporate social responsibility reports; research on multi-sensor information fusion algorithm with sensor fault diagnosis; and land-use classification with remote sensing image based on stacked autoencoder.",,
"Scene classification based on a hierarchical convolutional sparse auto-encoder for high spatial resolution imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007022366&doi=10.1080%2f01431161.2016.1266059&partnerID=40&md5=7b9d1b2fcd00d2362291b9a6941bb913","Efficiently representing and recognizing the semantic classes of the subregions of large-scale high spatial resolution (HSR) remote-sensing images are challenging and critical problems. Most of the existing scene classification methods concentrate on the feature coding approach with handcrafted low-level features or the low-level unsupervised feature learning approaches, which essentially prevent them from better recognizing the semantic categories of the scene due to their limited mid-level feature representation ability. In this article, to overcome the inadequate mid-level representation, a patch-based spatial-spectral hierarchical convolutional sparse auto-encoder (HCSAE) algorithm, based on deep learning, is proposed for HSR remote-sensing imagery scene classification. The HCSAE framework uses an unsupervised hierarchical network based on a sparse auto-encoder (SAE) model. In contrast to the single-level SAE, the HCSAE framework utilizes the significant features from the single-level algorithm in a feedforward and full connection approach to the maximum extent, which adequately represents the scene semantics in the high level of the HCSAE. To ensure robust feature learning and extraction during the SAE feature extraction procedure, a ‘dropout’ strategy is also introduced. The experimental results using the UC Merced data set with 21 classes and a Google Earth data set with 12 classes demonstrate that the proposed HCSAE framework can provide better accuracy than the traditional scene classification methods and the single-level convolutional sparse auto-encoder (CSAE) algorithm. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",,"Classification (of information); Convolution; Deep learning; Extraction; Image classification; Image resolution; Semantics; Signal encoding; Uranium compounds; Extraction procedure; Hierarchical network; High spatial resolution; High spatial resolution imagery; Mid-level representation; Remote sensing imagery; Remote sensing images; Unsupervised feature learning; Remote sensing; algorithm; data set; image classification; image resolution; imagery; remote sensing; spatial resolution"
"Convolutional neural networks for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994519156&doi=10.1016%2fj.neucom.2016.09.010&partnerID=40&md5=4db3fadf382d1db026b1e657fcc3cbb9","As a powerful visual model, convolutional neural networks (CNNs) have demonstrated remarkable performance in various visual recognition problems, and attracted considerable attention in recent years. However, due to the highly correlated bands and insufficient training samples of hyperspectral image data, it still remains a challenging problem to effectively apply the CNN models on hyperspectral images. In this paper, an efficient CNN architecture has been proposed to boost its discriminative capability for hyperspectral image classification, in which the original data is used as the input and the final CNN outputs are the predicted class-related results. The proposed CNN infrastructure has several distinct advantages. Firstly, different from traditional classification methods those need hand-crafted features, the CNN model used here is designed to deal with the problem of hyperspectral image analysis in an end-to-end way. Secondly, the parameters of the CNN model are optimized from a small training set, while the over-fitting problem of the neural network has been alleviated to some extent. Finally, in order to better deal with the hyperspectral image information, 1 × 1 convolutional layers have been adopted, and an average pooling layer and larger dropout rates have also been employed in the whole CNN procedure. The experiments on three benchmark data sets have demonstrated that the proposed CNN architecture considerably outperforms other state-of-the-art methods. © 2016 Elsevier B.V.","Convolutional neural networks; Deep learning; Hyperspectral image classification","Classification (of information); Convolution; Independent component analysis; Network architecture; Neural networks; Spectroscopy; Classification methods; Convolutional neural network; Deep learning; Hyper-spectral images; Hyperspectral image analysis; Hyperspectral image classification; Hyperspectral image datas; State-of-the-art methods; Image classification; Article; artificial neural network; controlled study; convolutional neural network; hyperspectral image classification; image analysis; image display; image processing; imaging and display; optical resolution; plots and curves; priority journal; probability; remote sensing; support vector machine"
"Spectral–spatial multi-feature-based deep learning for hyperspectral remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976870858&doi=10.1007%2fs00500-016-2246-3&partnerID=40&md5=4d28a30bc73bc89095d21629a816d73c","Hyperspectral remote sensing has a strong ability in information expression, so it provides better support for classification. The methods proposed to deal the hyperspectral data classification problems were build one by one. However, most of them committed to spectral feature extraction that means wasting some valuable information and poor classification results. Thus, we should pay more attention to multi-features. And on the other hand, due to extreme requirements for classification accuracy, we should hierarchically explore more deep features. The first thought is machine learning, but the traditional machine learning classifiers, like the support vector machine, are not friendly to larger inputs and features. This paper introduces a hybrid of principle component analysis (PCA), guided filtering, deep learning architecture into hyperspectral data classification. In detail, as a mature dimension reduction architecture, PCA is capable of reducing the redundancy of hyperspectral information. In addition, guided filtering provides a passage to spatial-dominated information concisely and effectively. According to the stacked autoencoders which is a efficient deep learning architecture, deep-level multi-features are not in mystery. Two public data set PaviaU and Salinas are used to test the proposed algorithm. Experimental results demonstrate that the proposed spectral–spatial hyperspectral image classification method can show competitive performance. Multi-feature learning based on deep learning exhibits a great potential on the classification of hyperspectral images. When the number of samples is 30 % and the iteration number is over 1000, the accuracy rates for both of the two data set are over 99 %. © 2016, Springer-Verlag Berlin Heidelberg.","Deep learning; Hyperspectral image classification; Multi-feature learning; Remote sensing","Artificial intelligence; Classification (of information); Feature extraction; Image reconstruction; Independent component analysis; Information filtering; Iterative methods; Learning systems; Principal component analysis; Remote sensing; Spectroscopy; Statistical tests; Deep learning; Hyperspectral data classification; Hyperspectral image classification; Hyperspectral remote sensing; Hyperspectral Remote Sensing Image; Multi features; Principle component analysis; Spectral feature extraction; Image classification"
"Deep learning based oil palm tree detection and counting for high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010660533&doi=10.3390%2frs9010022&partnerID=40&md5=926ddd781b57ded476ecbaec1fd5a361","Oil palm trees are important economic crops in Malaysia and other tropical areas. The number of oil palm trees in a plantation area is important information for predicting the yield of palm oil, monitoring the growing situation of palm trees and maximizing their productivity, etc. In this paper, we propose a deep learning based framework for oil palm tree detection and counting using high-resolution remote sensing images for Malaysia. Unlike previous palm tree detection studies, the trees in our study area are more crowded and their crowns often overlap. We use a number of manually interpreted samples to train and optimize the convolutional neural network (CNN), and predict labels for all the samples in an image dataset collected through the sliding window technique. Then, we merge the predicted palm coordinates corresponding to the same palm tree into one palm coordinate and obtain the final palm tree detection results. Based on our proposed method, more than 96% of the oil palm trees in our study area can be detected correctly when compared with the manually interpreted ground truth, and this is higher than the accuracies of the other three tree detection methods used in this study. © 2016 by the authors; licensee MDPI, Basel, Switzerland.","Convolutional neural network (CNN); Deep learning; Object detection; Oil palm trees","Convolution; Forestry; Image reconstruction; Neural networks; Object detection; Object recognition; Palm oil; Remote sensing; Convolutional neural network; Deep learning; Ground truth; High resolution remote sensing images; Image datasets; Oil palm tree; Sliding window techniques; Tree detections; Palmprint recognition"
"Bidirectional adaptive feature fusion for remote sensing scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038005752&doi=10.1007%2f978-981-10-7302-1_40&partnerID=40&md5=8ac17b0db41efed96916c398d7de39ee","Convolutional neural networks (CNN) have been excellent for scene classification in nature scene. However, directly using the pre-trained deep models on the aerial image is not proper, because of the spatial scale variability and rotation variability of the HSR remote sensing images. In this paper, a bidirectional adaptive feature fusion strategy is investigated to deal with the remote sensing scene classification. The deep learning feature and the SIFT feature are fused together to get a discriminative image presentation. The fused feature can not only describe the scenes effectively by employing deep learning feature but also overcome the scale and rotation variability with the usage of the SIFT feature. By fusing both SIFT feature and global CNN feature, our method achieves state-of-the-art scene classification performance on the UCM and the AID datasets. © Springer Nature Singapore Pte Ltd. 2017.","Feature fusion; Remote sensing scene classification","Computer vision; Deep learning; Image processing; Neural networks; Remote sensing; Adaptive features; Convolutional neural network; Feature fusion; Image presentations; Remote sensing images; Scale and rotation; Scene classification; Spatial-scale variability; Classification (of information)"
"Single-Image Super-Resolution for Remote Sensing Data Using Deep Residual-Learning Neural Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035090889&doi=10.1007%2f978-3-319-70096-0_64&partnerID=40&md5=105c9596f036ca649b9abab26886c1a5","Single image super-resolution (SISR) plays an important role in remote sensing image processing. In recent years, deep convolutional neural networks have achieved state-of-the-art performance in the SISR field of common camera images. Although the SISR method based on deep learning is effective on general camera images, it is not necessarily effective on remote sensing images because of the significant difference between remote sensing images and common camera images. In this paper, the VDSR network (proposed by Kim et al. in 2016) was found to be invalid for Sentinel-2A remote sensing images; we then proposed our own neural network, which is called the remote sensing deep residual-learning (RS-DRL) network. Our network achieved better performance than VDSR on Sentinel-2A remote sensing images. © 2017, Springer International Publishing AG.","Deep convolution neural network; Residual-Learning; Sentinel-2A; Single-Image Super-Resolution","Cameras; Convolution; Deep neural networks; Image processing; Neural networks; Optical resolving power; Convolution neural network; Convolutional neural network; Learning neural networks; Remote sensing image processing; Residual-Learning; Sentinel-2A; Single images; State-of-the-art performance; Remote sensing"
"Moving object detection in video satellite image based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041184183&doi=10.1117%2f12.2296714&partnerID=40&md5=49868353b54769831f685142960d1b43","Moving object detection in video satellite image is studied. A detection algorithm based on deep learning is proposed. The small scale characteristics of remote sensing video objects are analyzed. Firstly, background subtraction algorithm of adaptive Gauss mixture model is used to generate region proposals. Then the objects in region proposals are classified via the deep convolutional neural network. Thus moving objects of interest are detected combined with prior information of sub-satellite point. The deep convolution neural network employs a 21-layer residual convolutional neural network, and trains the network parameters by transfer learning. Experimental results about video from Tiantuo-2 satellite demonstrate the effectiveness of the algorithm. © 2017 SPIE.","Convolutional neural network; Deep learning; Moving object detection; Object detection; Transfer learning; Video satellite","Convolution; Deep learning; Deep neural networks; Image segmentation; Motion compensation; Neural networks; Object recognition; Optical radar; Remote sensing; Satellites; Background subtraction algorithms; Convolution neural network; Convolutional neural network; Deep convolutional neural networks; Detection algorithm; Gauss mixture models; Moving-object detection; Transfer learning; Object detection"
"High efficient optical remote sensing images acquisition for Nano-satellite: Reconstruction algorithms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041017885&doi=10.1117%2f12.2278180&partnerID=40&md5=848acf99070ab1d1eb7e067d48638ad4","Large amount of data is one of the most obvious features in satellite based remote sensing systems, which is also a burden for data processing and transmission. The theory of compressive sensing(CS) has been proposed for almost a decade, and massive experiments show that CS has favorable performance in data compression and recovery, so we apply CS theory to remote sensing images acquisition. In CS, the construction of classical sensing matrix for all sparse signals has to satisfy the Restricted Isometry Property (RIP) strictly, which limits applying CS in practical in image compression. While for remote sensing images, we know some inherent characteristics such as non-negative, smoothness and etc.. Therefore, the goal of this paper is to present a novel measurement matrix that breaks RIP. The new sensing matrix consists of two parts: The standard Nyquist sampling matrix for thumbnails and the conventional CS sampling matrix. Since most of sun-synchronous based satellites y around the earth 90 minutes and the revisit cycle is also short, lots of previously captured remote sensing images of the same place are available in advance. This drives us to reconstruct remote sensing images through a deep learning approach with those measurements from the new framework. Therefore, we propose a novel deep convolutional neural network (CNN) architecture which takes in undersampsing measurements as input and outputs an intermediate reconstruction image. It is well known that the training procedure to the network costs long time, luckily, the training step can be done only once, which makes the approach attractive for a host of sparse recovery problems. © 2017 SPIE.","Compressive sensing; Image acquisition; Image compression; Image restoration","Compressed sensing; Computer system recovery; Data handling; Deep learning; Deep neural networks; Digital storage; Image acquisition; Image processing; Image reconstruction; Matrix algebra; Neural networks; Remote sensing; Satellites; Signal processing; Compressive sensing; Convolutional neural network; Data processing and transmission; Inherent characteristics; Optical remote sensing; Reconstruction algorithms; Remote sensing system; Restricted isometry properties (RIP); Image compression"
"Semantic segmentation of liss-4 satellite imagery for mapping built-up areas using deep learning convolutional neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047009973&partnerID=40&md5=e884964b8bc7b8773af9ce31e4db041d","Mapping built-up areas is important for city growth monitoring and calibrating models for city growth forecasting. Aerial and satellite ortho-imagery is popularly used for this purpose. Knowledge based, semi-automated approach for this task is challenging and requires significant human intervention and application of subjective expertise for iterative refinement of rule sets used for semantic segmentation. This is due to high heterogeneity in shape, density and composition of built‐up areas. Further, methods that rely on calculating tonal and textural indices fail to generalize well and require manual tuning of thresholds for different regions for accurate segmentation. This paper investigates a fully automated deep learning approach based on a convolutional neural network for this task. This eliminates the need of human expertise for defining complex rulesets for semantic segmentation and does not require subjective retuning of deterministic thresholds for different regions. Specifically, this paper discusses design, training and performance evaluation of a deep convolutional network for identifying built-up land areas in LISS-4 satellite ortho-imagery. The network was trained and evaluated on multi‐spectral LISS‐4 satellite ortho-imagery covering the central core and peripheral developing areas of a city. © 2017 Asian Association on Remote Sensing. All rights reserved.","Artificial intelligence; Deep learning; Ortho-imagery; Urban growth modelling","Antennas; Artificial intelligence; Automation; Convolution; Deep learning; Deep neural networks; Iterative methods; Knowledge based systems; Mapping; Neural networks; Remote sensing; Semantic Web; Semantics; Space applications; Space optics; Urban growth; Convolutional networks; Convolutional neural network; Growth modelling; Human intervention; Iterative refinement; Ortho-imagery; Performance evaluations; Semantic segmentation; Satellite imagery"
"Efficient generation of image chips for training deep learning algorithms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023204544&doi=10.1117%2f12.2261702&partnerID=40&md5=ef2f2054a1b517d8f9eb526b350a36af","Training deep convolutional networks for satellite or aerial image analysis often requires a large amount of training data. For a more robust algorithm, training data need to have variations not only in the background and target, but also radiometric variations in the image such as shadowing, illumination changes, atmospheric conditions, and imaging platforms with different collection geometry. Data augmentation is a commonly used approach to generating additional training data. However, this approach is often insufficient in accounting for real world changes in lighting, location or viewpoint outside of the collection geometry. Alternatively, image simulation can be an efficient way to augment training data that incorporates all these variations, such as changing backgrounds, that may be encountered in real data. The Digital Imaging and Remote Sensing Image Image Generation (DIRSIG) model is a tool that produces synthetic imagery using a suite of physics-based radiation propagation modules. DIRSIG can simulate images taken from different sensors with variation in collection geometry, spectral response, solar elevation and angle, atmospheric models, target, and background. Simulation of Urban Mobility (SUMO) is a multi-modal traffic simulation tool that explicitly models vehicles that move through a given road network. The output of the SUMO model was incorporated into DIRSIG to generate scenes with moving vehicles. The same approach was used when using helicopters as targets, but with slight modifications. Using the combination of DIRSIG and SUMO, we quickly generated many small images, with the target at the center with different backgrounds. The simulations generated images with vehicles and helicopters as targets, and corresponding images without targets. Using parallel computing, 120,000 training images were generated in about an hour. Some preliminary results show an improvement in the deep learning algorithm when real image training data are augmented with the simulated images, especially when obtaining sufficient real data was particularly challenging. © 2017 SPIE.","Convolutional Neural Network; Data Augmentation; Deep Learning; Remote Sensing; Simulation; Synthetic Imagery; Target Detection","Automatic target recognition; Convolution; Deep learning; Deep neural networks; Education; Geometry; Helicopters; Image processing; Image reconstruction; Neural networks; Remote sensing; Target tracking; Traffic control; Vehicles; Aerial image analysis; Atmospheric conditions; Convolutional networks; Convolutional neural network; Data augmentation; Radiometric variations; Simulation; Synthetic imagery; Learning algorithms"
"Unsupervised representation learning with deep convolutional neural network for remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041808871&doi=10.1007%2f978-3-319-71589-6_9&partnerID=40&md5=e74370ec37423e25777b8ee5ec051296","With the rapid growth in quantity and quality of remote sensing images, extracting the useful information in them effectively and efficiently becomes feasible but also challenging. Convolutional neural network (CNN) is a suitable method to deal with such challenge since it can effectively represent and extract the information. However, the CNN can release their potentials only when enough labelled data provided for the learning procedure. This is a very time-consuming task and even infeasible for the applications with non-cooperative objects or scenes. Unsupervised CNN learning methods, which relieve the need for the labels in the training data, is a feasible solution for the problem. In this work, we investigate a real-world motivated sparsity based unsupervised deep CNN learning method. At first, the method formulates a balanced data driven population and lifetime sparsity prior and thus construct the unsupervised learning method through a layerwise mean. Then we further perform the method on the deep model with multiple CNN layers. Finally, the method is used for the remote sensing image representation and scenes classification. The experimental results over the public UC-Merced Land-use dataset demonstrate that the developed algorithm obtained satisfactory results compared with the recent methods. © Springer International Publishing AG 2017.","Convolutional neural network; Remote sensing images; Scene classification; Sparsity; Unsupervised representation learning","Convolution; Deep learning; Deep neural networks; Image classification; Land use; Learning systems; Neural networks; Population statistics; Unsupervised learning; Uranium compounds; Convolutional neural network; Remote sensing images; Scene classification; Sparsity; Unsupervised representation learning; Remote sensing"
"Application of deep learning in colorization of lidar derived intensity images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047437718&partnerID=40&md5=e98529c2f9c9af1cd2608f54d2a315a3","Most aerial LiDAR systems have accompanying aerial cameras in order to capture not only the terrain of the surveyed area, but also its true color appearance. However, there are surveys wherein only LiDAR information is available. Usual causes of absence of aerial photographs are presence of atmospheric clouds during survey, poor lighting conditions are aerial camera problems. These leave areas having terrain information but lacking aerial photographs. Intensity images can be derived from LiDAR data but they are only grayscale images. A deep-learning model can be developed to create a complex function in a form of deep neural networks from the pixel values of LiDAR-derived intensity images and true-color images. This complex function can then be used to predict the true-color images of a certain area using intensity images from LiDAR. The predicted true-color images do not necessarily need to be accurate compared to the real world. They are only intended to look realistic so that they can be used as base maps. © Corrosion and Prevention 2017. All rights reserved.","Aerial LiDAR; Artificial intelligence; Colorization; Deep-learning; Intensity images","Aerial photography; Artificial intelligence; Cameras; Color; Complex networks; Deep learning; Deep neural networks; Human computer interaction; Optical radar; Remote sensing; Space applications; Space optics; Surveys; Aerial lidars; Aerial Photographs; Colorization; Complex functions; Gray-scale images; Intensity images; Learning models; Lighting conditions; Antennas"
"Level-of-detail assessment of structural surface damage using spatially sequential stereo images and deep learning methods","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032333626&doi=10.12783%2fshm2017%2f14232&partnerID=40&md5=cb702f4c8002677740c2f0f0d70f20e0","In this paper, we report an innovative framework for automating structural surface damage assessment in engineering practice. Assessment of structural surface damage has been heavily relied on human-based inspection, which incurs significant cost to stakeholders of civil structures and infrastructure and often severe risk to the inspectors. Recognizing the promise of aerial robotics that can access dangerous locations and envisaging a future of structural inspection that ought to be fully autonomous, we have developed a framework, termed level-of-detail assessment of structural surface damage, that is geared towards real-time implementation for use in practice. The level-of-detail assessment is enabled by a remote sensing approach based on a small Unmanned Aerial Vehicle (UAV or drone) platform with an integrated payload of a low-cost stereo camera and a compact embedded computer. To achieve real-time detection, we propose the use of the faster region-based Convolution Neural Network (faster RCNN) as an artificial intelligence (AI) utility at different imaging depths. The stereo-camera based geometric reconstruction provides the basis of achieving level-of-detail quantitative damage assessment. In this paper, we further propose a novel data preparation method to accommodate the RCNN training. In the end, we will showcase some of these results based on our current implementation and experimental evaluation.",,"Antennas; Cameras; Costs; Damage detection; Deep learning; Real time control; Remote sensing; Risk assessment; Safety engineering; Stereo image processing; Unmanned aerial vehicles (UAV); Convolution neural network; Engineering practices; Experimental evaluation; Real-time implementations; Remote sensing approaches; Small unmanned aerial vehicles; Structural inspections; Structural surfaces; Structural health monitoring"
"Airplane detection in remote sensing images using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045582429&doi=10.1117%2f12.2285776&partnerID=40&md5=a87478633fec356cb24f5a9768f2e39a","Airplane detection in remote sensing images remains a challenging problem and has also been taking a great interest to researchers. In this paper we propose an effective method to detect airplanes in remote sensing images using convolutional neural networks. Deep learning methods show greater advantages than the traditional methods with the rise of deep neural networks in target detection, and we give an explanation why this happens. To improve the performance on detection of airplane, we combine a region proposal algorithm with convolutional neural networks. And in the training phase, we divide the background into multi classes rather than one class, which can reduce false alarms. Our experimental results show that the proposed method is effective and robust in detecting airplane. © 2018 SPIE.","airplane detection; convolutional neutral network; region proposal","Aircraft; Convolution; Deep neural networks; Image processing; Neural networks; Pattern recognition; Remote sensing; Airplane detections; Convolutional neural network; False alarms; Learning methods; Neutral network; Region proposals; Remote sensing images; Training phase; Aircraft detection"
"Cascade Convolutional Neural Network Based on Transfer-Learning for Aircraft Detection on High-Resolution Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027272999&doi=10.1155%2f2017%2f1796728&partnerID=40&md5=5c620c28eb362ce13ceb93ab2c1ecac9","Aircraft detection from high-resolution remote sensing images is important for civil and military applications. Recently, detection methods based on deep learning have rapidly advanced. However, they require numerous samples to train the detection model and cannot be directly used to efficiently handle large-area remote sensing images. A weakly supervised learning method (WSLM) can detect a target with few samples. However, it cannot extract an adequate number of features, and the detection accuracy requires improvement. We propose a cascade convolutional neural network (CCNN) framework based on transfer-learning and geometric feature constraints (GFC) for aircraft detection. It achieves high accuracy and efficient detection with relatively few samples. A high-accuracy detection model is first obtained using transfer-learning to fine-tune pretrained models with few samples. Then, a GFC region proposal filtering method improves detection efficiency. The CCNN framework completes the aircraft detection for large-area remote sensing images. The framework first-level network is an image classifier, which filters the entire image, excluding most areas with no aircraft. The second-level network is an object detector, which rapidly detects aircraft from the first-level network output. Compared with WSLM, detection accuracy increased by 3.66%, false detection decreased by 64%, and missed detection decreased by 23.1%. © 2017 Bin Pan et al.",,"Convolution; Feature extraction; Image reconstruction; Military applications; Military photography; Neural networks; Object detection; Remote sensing; Convolutional neural network; Detection accuracy; Detection efficiency; Efficient detection; Geometric feature constraint; High resolution remote sensing images; Remote sensing images; Weakly supervised learning; Aircraft detection"
"The potential of deep features for small object class identification in very high resolution remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022204927&doi=10.1007%2f978-3-319-59876-5_63&partnerID=40&md5=c443d940f2e3ff14925b633725be5f41","Various generative and discriminative methods have been transferred from the computer vision field to remote sensing applications using different low and high semantic level descriptors. However, as classical approaches have shown their limits in representation learning and are not intended to deal with the great variability of the data. With the emergence of large-scale annotated datasets in vision, the convolutional deep approaches represent the most winning solutions by supporting this variability with spatial context integration through different semantic abstraction levels. In the lack of annotated remote sensing data, in this paper, we are comparing the performances of deep features produced by six different CNNs that have been trained on well established computer vision datasets with respect to the detection of small objects (cars) in very high resolution Pleiades imagery. Our findings show good generalization performance and are very encouraging for future applications. © Springer International Publishing AG 2017.","Deep learning; Object detection; Very high resolution","Computer vision; Education; Image analysis; Object detection; Object recognition; Remote sensing; Semantics; Annotated datasets; Classical approach; Discriminative methods; Future applications; Generalization performance; Remote sensing applications; Remote sensing data; Very high resolution; Deep learning"
"Towards better exploiting convolutional neural networks for remote sensing scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979775123&doi=10.1016%2fj.patcog.2016.07.001&partnerID=40&md5=852df9e23d17f93517e03ab8577da6db","We present an analysis of three possible strategies for exploiting the power of existing convolutional neural networks (ConvNets or CNNs) in different scenarios from the ones they were trained: full training, fine tuning, and using ConvNets as feature extractors. In many applications, especially including remote sensing, it is not feasible to fully design and train a new ConvNet, as this usually requires a considerable amount of labeled data and demands high computational costs. Therefore, it is important to understand how to better use existing ConvNets. We perform experiments with six popular ConvNets using three remote sensing datasets. We also compare ConvNets in each strategy with existing descriptors and with state-of-the-art baselines. Results point that fine tuning tends to be the best performing strategy. In fact, using the features from the fine-tuned ConvNet with linear SVM obtains the best results. We also achieved state-of-the-art results for the three datasets used. © 2016 Elsevier Ltd","Aerial scenes; Convolutional neural networks; Deep learning; Feature extraction; Fine-tune; Hyperspectral images; Remote sensing","Convolution; Feature extraction; Neural networks; Spectroscopy; Computational costs; Convolutional neural network; Deep learning; Feature extractor; Hyper-spectral images; Labeled data; Scene classification; State of the art; Remote sensing"
"Efficient deep belief network based hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040217541&doi=10.1007%2f978-3-319-71598-8_31&partnerID=40&md5=46a13b489a6561c50b11c4ba571e57a7","Hyperspectral Image (HSI) classification plays a key role remote sensing field. Recently, deep learning has demonstrated its effectiveness in HSI Classification field. This paper presents a spectral-spatial HSI classification technique established on the deep learning based deep belief network (DBN) for deep and abstract feature extraction and adaptive boundary adjustment based segmentation. Proposed approach focuses on integrating the deep learning based spectral features and segmentation based spatial features into a framework for improved performance. Specifically, first the deep DBN model is exploited as a spectral feature extraction based classifier to extract the deep spectral features. Second, spatial contextual features are obtained by utilizing effective adaptive boundary adjustment based segmentation technique. Finally, maximum voting based criteria is operated to integrate the results of extracted spectral and spatial information for improved HSI classification. In general, exploiting spectral features from DBN process and spatial features from segmentation and integration of spectral and spatial information by maximum voting based criteria, has a substantial effect on the performance of HSI classification. Experimental performance on real and widely used hyperspectral data sets with different contexts and resolutions demonstrates the accuracy of the proposed technique and performance is comparable to several recently proposed HSI classification techniques. © Springer International Publishing AG 2017.","Deep belief network; Hyperspectral image classification; Segmentation","Deep learning; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Image segmentation; Remote sensing; Spectroscopy; Classification technique; Contextual feature; Deep belief network (DBN); Deep belief networks; Hyperspectral Data; Segmentation techniques; Spatial informations; Spectral feature extraction; Classification (of information)"
"JM-Net and cluster-SVM for aerial scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031944828&doi=10.24963%2fijcai.2017%2f332&partnerID=40&md5=e4fc0af752f22cd15b4bfd8bffb6f849","Aerial scene classification, which is a fundamental problem for remote sensing imagery, can automatically label an aerial image with a specific semantic category. Although deep learning has achieved competitive performance for aerial scene classification, training the conventional neural networks with aerial datasets will easily stick in overfitting. Because the aerial datasets only contain a few hundreds or thousands images, meanwhile the conventional networks usually contain millions of parameters to be trained. To address the problem, a novel convolutional neural network named Justify Mentioned Net (JM-Net) is proposed in this paper, which has different size of convolution kernels in same layer and ignores the fully convolution layer, so it has fewer parameters and can be trained well on aerial datasets. Additionally, Cluster-SVM, a strategy to improve the accuracy and speed up the classification is used in the specific task. Finally, our method surpass the state-of-art result on the challenging AID dataset while cost shorter time and used smaller storage space.",,"Arts computing; Classification (of information); Convolution; Deep learning; Multilayer neural networks; Remote sensing; Semantics; Support vector machines; Competitive performance; Convolution kernel; Convolutional neural network; Different sizes; Remote sensing imagery; Scene classification; Specific semantics; Specific tasks; Antennas"
"Inference and discovery in remote sensing data with features extracted using deep networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037741960&doi=10.1007%2f978-3-319-71078-5_10&partnerID=40&md5=5384617b203916e3983c31d49e8ef034","We aim to develop a process by which we can extract generic features from aerial image data that can both be used to infer the presence of objects and characteristics and to discover new ways of representing the landscape. We investigate the fine-tuning of a 50-layer ResNet deep convolutional neural network that was pre-trained with ImageNet data and extracted features at several layers throughout these pre-trained and the fine-tuned networks. These features were applied to several supervised classification problems, obtaining a significant correlation between the classification accuracy and layer number. Visualising the activation of the networks’ nodes found that fine-tuning had not achieved coherent representations at later layers. We conclude that we need to train with considerably more varied data but that, even without fine tuning, features derived from a deep network can produce better classification results than with image data alone. © Ordnance Survey Limited 2017.","Deep learning; Feature extraction; Remote sensing","Artificial intelligence; Deep learning; Deep neural networks; Feature extraction; Neural networks; Remote sensing; Aerial image data; Classification accuracy; Classification results; Coherent representations; Convolutional neural network; Remote sensing data; Supervised classification; Without fine-tuning; Classification (of information)"
"Convolutional neural network using generated data for SAR ATR with limited samples","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045564736&doi=10.1117%2f12.2292997&partnerID=40&md5=80d91e0352962f205a3df8ebf323f32a","Being able to adapt all weather at all times, it has been a hot research topic that using Synthetic Aperture Radar(SAR) for remote sensing. Despite all the well-known advantages of SAR, it is hard to extract features because of its unique imaging methodology, and this challenge attracts the research interest of traditional Automatic Target Recognition(ATR) methods. With the development of deep learning technologies, convolutional neural networks(CNNs) give us another way out to detect and recognize targets, when a huge number of samples are available, but this premise is often not hold, when it comes to monitoring a specific type of ships. In this paper, we propose a method to enhance the performance of Faster R-CNN with limited samples to detect and recognize ships in SAR images. © 2018 SPIE.","ATR; Convolutional Neural Networks; Generative Adversarial Networks; SAR","Automatic target recognition; Convolution; Deep learning; Image enhancement; Neural networks; Radar imaging; Remote sensing; Ships; Synthetic aperture radar; Adversarial networks; Convolutional neural network; Hot research topics; Learning technology; Number of samples; Research interests; SAR ATR; SAR Images; Radar target recognition"
"DRLnet: Deep difference representation learning network and an unsupervised optimization framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031928610&doi=10.24963%2fijcai.2017%2f477&partnerID=40&md5=2da27084d42e476be6fb212e246a2c6e","Change detection and analysis (CDA) is an important research topic in the joint interpretation of spatial-temporal remote sensing images. The core of CDA is to effectively represent the difference and measure the difference degree between bi-temporal images. In this paper, we propose a novel difference representation learning network (DRLnet) and an effective optimization framework without any supervision. Difference measurement, difference representation learning and unsupervised clustering are combined as a single model, i.e., DRLnet, which is driven to learn clustering-friendly and discriminative difference representations (DRs) for different types of changes. Further, DRLnet is extended into a recurrent learning framework to update and reuse limited training samples and prevent the semantic gaps caused by the saltation in the number of change types from over-clustering stage to the desired one. Experimental results identify the effectiveness of the proposed framework.",,"Artificial intelligence; Remote sensing; Semantics; Difference degrees; Difference measurements; Joint interpretation; Learning frameworks; Optimization framework; Remote sensing images; Spatial temporals; Unsupervised clustering; Deep learning"
"Spectral unmixing and sub-pixel classification: Analysis of learning strategies","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047018747&partnerID=40&md5=16985fa83482a86a8df788df7226e318","Sub-pixel details in the hyperspectral images are generally ignored by the conventional classifiers. However, some recent approaches use this information to generate fine resolution land cover maps from images having coarse spatial resolution. Two main aspects in this regard are: 1) estimation of fractional abundances of the reference signatures at each pixel (spectral un-mixing); and 2) prediction of class distributions at sub-pixel scale (sub-pixel classification). This study proposes some spectral unmixing as well as sub-pixel mapping techniques that take in to account certain constraints which are usually ignored by the conventional approaches. In the context of spectral unmixing methods, our main contribution is the analysis of auto-encoders when compared with ELM, STM and SVM. In case of sub-pixel mapping methods, our study may be summarized as the modelling deep auto-encoders for predicting the spatial distributions at target scale. Also, we have compared the effectiveness of Auto-Encoders and their convolutional counterparts in learning the coarse image features. Among the proposed unmixing approaches, autoencoder approach has given better results when compared to that of SVM and STM. The deep learning based sub-pixel mapping approaches have also produced good results, even for complex scenes. The sensitivities of all these techniques towards various tunable parameters are also analyzed. © 38th Asian Conference on Remote Sensing - Space Applications: Touching Human Lives, ACRS 2017. Alll rights reserved.","Autoencoder; Convolutional neural network; Spectral unmixing; Sub-pixel mapping","Convolution; Deep learning; Neural networks; Photomapping; Remote sensing; Signal encoding; Space applications; Space optics; Spectroscopy; Auto encoders; Class distributions; Conventional approach; Conventional classifier; Convolutional neural network; Spectral unmixing; Sub-pixel classification; Sub-pixel mapping; Pixels"
"Dimensionality-varied convolutional neural network for spectral-spatial classification of hyperspectral data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045711918&doi=10.1117%2f12.2295865&partnerID=40&md5=2e08715fcfa16cd0013f96397a58a2b6","Hyperspectral image (HSI) classification is one of the most popular topics in remote sensing community. Traditional and deep learning-based classification methods were proposed constantly in recent years. In order to improve the classification accuracy and robustness, a dimensionality-varied convolutional neural network (DVCNN) was proposed in this paper. DVCNN was a novel deep architecture based on convolutional neural network (CNN). The input of DVCNN was a set of 3D patches selected from HSI which contained spectral-spatial joint information. In the following feature extraction process, each patch was transformed into some different 1D vectors by 3D convolution kernels, which were able to extract features from spectral-spatial data. The rest of DVCNN was about the same as general CNN and processed 2D matrix which was constituted by by all 1D data. So that the DVCNN could not only extract more accurate and rich features than CNN, but also fused spectral-spatial information to improve classification accuracy. Moreover, the robustness of network on water-absorption bands was enhanced in the process of spectral-spatial fusion by 3D convolution, and the calculation was simplified by dimensionality varied convolution. Experiments were performed on both Indian Pines and Pavia University scene datasets, and the results showed that the classification accuracy of DVCNN improved by 32.87% on Indian Pines and 19.63% on Pavia University scene than spectral-only CNN. The maximum accuracy improvement of DVCNN achievement was 13.72% compared with other state-of-the-art HSI classification methods, and the robustness of DVCNN on water-absorption bands noise was demonstrated. © 2017 SPIE.","Convolutional neural network; Deep learning; Dimensionality varied; Hyperspectral image classification; Spectralspatial information","Convolution; Deep learning; Hyperspectral imaging; Image classification; Neural networks; Optical radar; Remote sensing; Spectroscopy; Water absorption; Classification accuracy; Classification methods; Convolutional neural network; Convolutional Neural Networks (CNN); Dimensionality varied; Spatial informations; Spectral-spatial classification; Spectralspatial information; Classification (of information)"
"Building change detection via a combination of CNNs using only RGB aerial imageries","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040035308&doi=10.1117%2f12.2277912&partnerID=40&md5=287c1b6aa242893c1f1103e88d1e2f29","Building change information extracted from remote sensing imageries is important for various applications such as urban management and marketing planning. The goal of this work is to develop a methodology for automatically capturing building changes from remote sensing imageries. Recent studies have addressed this goal by exploiting 3-D information as a proxy for building height. In contrast, because in practice it is expensive or impossible to prepare 3-D information, we do not rely on 3-D data but focus on using only RGB aerial imageries. Instead, we employ deep convolutional neural networks (CNNs) to extract effective features, and improve change detection accuracy in RGB remote sensing imageries. We consider two aspects of building change detection, building detection and subsequent change detection. Our proposed methodology was tested on several areas, which has some differences such as dominant building characteristics and varying brightness values. On all over the tested areas, the proposed method provides good results for changed objects, with recall values over 75 % with a strict overlap requirement of over 50% in intersection-over-union (IoU). When the IoU threshold was relaxed to over 10%, resulting recall values were over 81%. We conclude that use of CNNs enables accurate detection of building changes without employing 3-D information. © 2017 SPIE.","aerial imagery; building change detection; convolutional neural network (CNN); deep learning; dilated convolution; remote sensing imagery; siamese architecture; urban management","Aerial photography; Buildings; Convolution; Deep learning; Deep neural networks; Image enhancement; Marketing; Neural networks; Urban planning; Aerial imagery; Building change detection; Convolutional neural network; Remote sensing imagery; Urban management; Remote sensing"
"Spectral-Spatial Classification of Hyperspectral Imagery Based on Stacked Sparse Autoencoder and Random Forest","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020312254&doi=10.1080%2f22797254.2017.1274566&partnerID=40&md5=269970d07c55acceaeb6f55eebe5685b","It is of great interest in exploiting spectral-spatial information for hyperspectral image (HSI) classification at different spatial resolutions. This paper proposes a new spectral-spatial deep learning-based classification paradigm. First, pixel-based scale transformation and class separability criteria are employed to measure appropriate spatial resolution HSI, and then we integrate the spectral and spatial information (i.e., both implicit and explicit features) together to construct a joint spectral-spatial feature set. Second, as a deep learning architecture, stacked sparse autoencoder provides strong learning performance and is expected to exploit even more abstract and high-level feature representations from both spectral and spatial domains. Specifically, random forest (RF) classifier is first introduced into stacked sparse autoencoder for HSI classification, based on the fact that it provides better tradeoff among generalization performance, prediction accuracy and operation speed compared to other traditional procedures. Experiments on two real HSIs demonstrate that the proposed framework generates competitive performance. © 2017 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","class separability; Classification; hyperspectral imagery; random forest (RF); stacked sparse autoencoder (SSA)","Classification (of information); Decision trees; Deep learning; Remote sensing; Spectroscopy; Auto encoders; Class separability; Competitive performance; Generalization performance; Hyper-spectral imageries; Implicit and explicit features; Random forests; Spectral-spatial classification; Image classification"
"Ship detection leveraging deep neural networks in WorldView-2 Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041082280&doi=10.1117%2f12.2277533&partnerID=40&md5=a78f07c33aba1ff2a0cf79c8fc80e418","Interpretation of high-resolution satellite images has been so difficult that skilled interpreters must have checked the satellite images manually because of the following issues. One is the requirement of the high detection accuracy rate. The other is the variety of the target, taking ships for example, there are many kinds of ships, such as boat, cruise ship, cargo ship, aircraft carrier, and so on. Furthermore, there are similar appearance objects throughout the image; therefore, it is often difficult even for the skilled interpreters to distinguish what object the pixels really compose. In this paper, we explore the feasibility of object extraction leveraging deep learning with high-resolution satellite images, especially focusing on ship detection. We calculated the detection accuracy using the WorldView-2 images. First, we collected the training images labelled as ""ship"" and ""not ship"". After preparing the training data, we defined the deep neural network model to judge whether ships are existing or not, and trained them with about 50,000 training images for each label. Subsequently, we scanned the evaluation image with different resolution windows and extracted the ""ship"" images. Experimental result shows the effectiveness of the deep learning based object detection. © 2017 SPIE.","CNN; Deep learning; Object detection; World view-2","Aircraft carriers; Deep neural networks; Feature extraction; Fighter aircraft; Image processing; Object detection; Object recognition; Personnel training; Remote sensing; Satellites; Ships; Signal processing; Training aircraft; Detection accuracy; Different resolutions; High resolution satellite images; Neural network model; Object extraction; Satellite images; Training image; World views; Deep learning"
"Recent trends in ELM and MLELM: A review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048267044&doi=10.25046%2faj020108&partnerID=40&md5=f22b9b38e3d596e8564adec11b7c4a77","Extreme Learning Machine (ELM) is a high effective learning algorithm for the single hidden layer feed forward neural networks. Compared with the existing neural network learning algorithm it solves the slow training speed and over-fitting problems. It has been used in different fields and applications such as biomedical engineering, computer vision, remote sensing, chemical process and control and robotics. It has better generalization stability, sparsity, accuracy, robustness, optimal control and fast learning rate This paper introduces a brief review about ELM and MLELM, describing the principles and latest research progress about the algorithms, theories and applications. Next, Multilayer Extreme Learning Machine (MLELM) and other state-of-the-art classifiers are trained on this suitable training feature vector for classification of data. Deep learning has the advantage of approximating the complicated function and mitigating the optimization difficulty associated with deep models. Multilayer extreme learning machine is a learning algorithm of an Artificial Neural Network (ANN) which takes to be good for deep learning and extreme learning machine. This review presents a comprehensive view of these advances in ELM and MLELM which may be worthy of exploring in the future. © 2017 ASTES Publishers. All rights reserved.","Artificial Neural Network (ANN); Deep Learning; Extreme learning machine (ELM); MLELM",
"Underwater inherent optical properties estimation using a depth aided deep neural network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042230445&doi=10.1155%2f2017%2f8351232&partnerID=40&md5=f20793f14a4a46da6e0bb652c700b1ac","Underwater inherent optical properties (IOPs) are the fundamental clues to many research fields such as marine optics, marine biology, and underwater vision. Currently, beam transmissometers and optical sensors are considered as the ideal IOPs measuring methods. But these methods are inflexible and expensive to be deployed. To overcome this problem, we aim to develop a novel measuring method using only a single underwater image with the help of deep artificial neural network. The power of artificial neural network has been proved in image processing and computer vision fields with deep learning technology. However, image-based IOPs estimation is a quite different and challenging task. Unlike the traditional applications such as image classification or localization, IOP estimation looks at the transparency of the water between the camera and the target objects to estimate multiple optical properties simultaneously. In this paper, we propose a novel Depth Aided (DA) deep neural network structure for IOPs estimation based on a single RGB image that is even noisy. The imaging depth information is considered as an aided input to help our model make better decision. © 2017 Zhibin Yu et al.",,"Deep learning; Image processing; Marine biology; Meteorological instruments; Neural networks; Optical data processing; Optical properties; Image processing and computer vision; Inherent optical properties; Inherent optical properties (IOPs); Learning technology; Measuring method; Neural network structures; Novel measuring method; Underwater vision; Deep neural networks; water; algorithm; artificial neural network; computer network; human; light; optics; remote sensing; signal processing; Algorithms; Computer Communication Networks; Humans; Light; Neural Networks (Computer); Optics and Photonics; Remote Sensing Technology; Signal Processing, Computer-Assisted; Water"
"Large-scale machine learning in the earth sciences","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051764600&doi=10.4324%2f9781315371740&partnerID=40&md5=a987e134a90024ad980eb96652a6f7e1","From the Foreword: “While large-scale machine learning and data mining have greatly impacted a range of commercial applications, their use in the field of Earth sciences is still in the early stages. This book, edited by Ashok Srivastava, Ramakrishna Nemani, and Karsten Steinhaeuser, serves as an outstanding resource for anyone interested in the opportunities and challenges for the machine learning community in analyzing these data sets to answer questions of urgent societal interest…I hope that this book will inspire more computer scientists to focus on environmental applications, and Earth scientists to seek collaborations with researchers in machine learning and data mining to advance the frontiers in Earth sciences.” --Vipin Kumar, University of Minnesota Large-Scale Machine Learning in the Earth Sciences provides researchers and practitioners with a broad overview of some of the key challenges in the intersection of Earth science, computer science, statistics, and related fields. It explores a wide range of topics and provides a compilation of recent research in the application of machine learning in the field of Earth Science. Making predictions based on observational data is a theme of the book, and the book includes chapters on the use of network science to understand and discover teleconnections in extreme climate and weather events, as well as using structured estimation in high dimensions. The use of ensemble machine learning models to combine predictions of global climate models using information from spatial and temporal patterns is also explored. The second part of the book features a discussion on statistical downscaling in climate with state-of-the-art scalable machine learning, as well as an overview of methods to understand and predict the proliferation of biological species due to changes in environmental conditions. The problem of using large-scale machine learning to study the formation of tornadoes is also explored in depth. The last part of the book covers the use of deep learning algorithms to classify images that have very high resolution, as well as the unmixing of spectral signals in remote sensing images of land cover. The authors also apply long-tail distributions to geoscience resources, in the final chapter of the book © 2017 by Taylor & Francis Group, LLC.",,"Artificial intelligence; Climate models; Data mining; Deep learning; Earth sciences; Forecasting; Remote sensing; Commercial applications; Environmental applications; Environmental conditions; Large-scale machine learning; Machine learning communities; Scalable machine learning; Spatial and temporal patterns; University of Minnesota; Learning algorithms"
"A theoretical construct for progressive construction site safety implementing situational awareness in unmanned aircraft systems to improve decision-making and safety","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031745080&partnerID=40&md5=2a9d775653133a12e88a9537cb73abe3","As unmanned aerial systems (UAS) become more prevalent tools for providing construction professionals with data used to manage the construction process, they may also provide great promise to positively impact jobsite safety. With their organic capability to move quickly and access difficult areas of the jobsite, their real-time remote sensing capabilities can provide greater degrees of jobsite Level 2 Situation Awareness (SA). SA improves the decision-making and performance of the human jobsite safety manager and construction personnel in complex, dynamic environments by enhancing ambient awareness, an essential attribute for jobsite safety. Unfortunately, this complex environment may also make filtering and organizing information in a timely and accurate manner difficult for the jobsite safety manager. This results in less than optimal decisions. This paper discusses the embodiment of deep learning neural network imagery processing for Level 2 SA enabled by the UAS to enhance decision-making in construction site safety management.",,"Complex networks; Construction; Deep neural networks; Human resource management; Image enhancement; Information filtering; Information management; Managers; Remote sensing; Unmanned aerial vehicles (UAV); Complex environments; Construction personnel; Construction professionals; Construction site safety; Learning neural networks; Situational awareness; Unmanned aerial systems; Unmanned aircraft system; Decision making"
"A hybrid DBN and CRF model for spectral-spatial classification of hyperspectral images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020094033&doi=10.19139%2fsoic.v5i2.309&partnerID=40&md5=50c21726fa037cf0180cb0f19c654e4e","Hyperspectral image classification plays an important role in remote sensing image analysis. Recent techniques have attempted to investigate the capabilities of deep learning approaches to tackle the hyperspectral image classification. This work shows how to further improve the hyperspectral image classification through using both a deep representation and contextual information. To implement this objective, this work proposes a new Conditional Random Field (CRF) model (named DBN-CRF) with the potentials defined over the deep features produced by a Deep Belief Network (DBN). The newly formulated DBN-CRF model takes advantage of the strength of DBNs in learning a good representation and the ability of CRFs to model contextual (spatial) information in both the observations and labels. Within a piecewise training framework, an efficient training method is proposed to train the whole DBN-CRF model end-to-end. This means that the parameters in DBN and CRF can be jointly trained and thus the proposed method can fully use the strength of both DBN and CRF. Moreover, in the proposed training method, the end-to-end training can be implemented with a standard back-propagation algorithm, avoiding the repeated inference usually involved in CRF training and thus is computationally efficient. Experiments on real-world hyperspectral data show that our method outperforms the most recent approaches in hyperspectral image classification. © 2017 International Academic Press.","Conditional random field; Deep belief network; Deep learning; Hyperspectral image; Spectral-spatial classification",
"Multimodal learning and inference from visual and remotely sensed data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011599461&doi=10.1177%2f0278364916679892&partnerID=40&md5=1f6a7dc8074a3eb14bd8a51113a6a18c","Autonomous vehicles are often tasked to explore unseen environments, aiming to acquire and understand large amounts of visual image data and other sensory information. In such scenarios, remote sensing data may be available a priori, and can help to build a semantic model of the environment and plan future autonomous missions. In this paper, we introduce two multimodal learning algorithms to model the relationship between visual images taken by an autonomous underwater vehicle during a survey and remotely sensed acoustic bathymetry (ocean depth) data that is available prior to the survey. We present a multi-layer architecture to capture the joint distribution between the bathymetry and visual modalities. We then propose an extension based on gated feature learning models, which allows the model to cluster the input data in an unsupervised fashion and predict visual image features using just the ocean depth information. Our experiments demonstrate that multimodal learning improves semantic classification accuracy regardless of which modalities are available at classification time, allows for unsupervised clustering of either or both modalities, and can facilitate mission planning by enabling class-based or image-based queries. © The Author(s) 2016.","autonomous exploration; classification; deep learning; marine robotics; Multimodal learning; semantic mapping","Autonomous underwater vehicles; Bathymetry; Classification (of information); Image segmentation; Learning algorithms; Remote sensing; Semantics; Surveys; Underwater acoustics; Autonomous exploration; Marine robotics; Multi-layer architectures; Multi-modal learning; Remotely sensed data; Semantic classification; Semantic mapping; Unsupervised clustering; Deep learning"
"Gait recognition based on convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060626125&doi=10.5194%2fisprs-archives-XLII-2-W4-207-2017&partnerID=40&md5=02e373b2e1338fed86abae294248765d","In this work we investigate the problem of people recognition by their gait. For this task, we implement deep learning approach using the optical flow as the main source of motion information and combine neural feature extraction with the additional embedding of descriptors for representation improvement. In order to find the best heuristics, we compare several deep neural network architectures, learning and classification strategies. The experiments were made on two popular datasets for gait recognition, so we investigate their advantages and disadvantages and the transferability of considered methods. © 2017 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Biometrics; Convolutional Neural Networks; Gait Recognition; Optical Flow","Biometrics; Convolution; Deep neural networks; Gait analysis; Network architecture; Neural networks; Optical flows; Convolutional neural network; Descriptors; Gait recognition; Learning approach; Motion information; People recognition; Pattern recognition"
"Advanced feature extraction for earth observation data processing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061440643&doi=10.1016%2fB978-0-12-409548-9.10341-0&partnerID=40&md5=503591b629751c1c3db13697bdb3079e","Extracting rich and semantically discriminative features from remote-sensing data is of paramount relevance to advance in the understanding, visualization, classification, and representation of the data. In this article we taxonomically categorize recent feature extraction methods that have been applied to remote-sensing problems, focusing on kernel-based methods, convolutional neural networks, and principal curves. We review the formulation and relevant aspects of these methods. Advantages and shortcomings are illustrated in real experiments dealing with multi- and hyperspectral image processing of varying spatial resolutions. © 2018 Elsevier Inc. All rights reserved.","Biophysical parameter retrieval; Deep learning; Dimensionality reduction; Entropy; Feature extraction; Image classification; Kernel methods; Multivariate analysis; Principal curves",
"Proceedings of SPIE - The International Society for Optical Engineering","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021793858&partnerID=40&md5=209404b2d51bfad97617fe0a0207727d","The proceedings contain 20 papers. The topics discussed include: use of UAV for support of intensive agricultural management decisions: from science to commercial applications; a case study of comparing radiometrically calibrated reflectance of an image mosaic from unmanned aerial system with that of a single image from manned aircraft over a same area; UAV remote sensing for phenotyping drought tolerance in peanuts; UAS imaging for automated crop lodging detection: a case study over an experimental maize field; distinguishing plant population and variety with UAV-derived vegetation indices; a predictive model for turfgrass color and quality evaluation using deep learning and UAV imageries; and 3-D reconstruction optimization using imagery captured by unmanned aerial vehicles.",,
"Fine-grained visual marine vessel classification for coastal surveillance and defense applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041449835&doi=10.1117%2f12.2278864&partnerID=40&md5=0d1b045f760f1255e09b1be8641d283e","The need for capabilities of automated visual content analysis has substantially increased due to presence of large number of images captured by surveillance cameras. With a focus on development of practical methods for extracting effective visual data representations, deep neural network based representations have received great attention due to their success in visual categorization of generic images. For fine-grained image categorization, a closely related yet a more challenging research problem compared to generic image categorization due to high visual similarities within subgroups, diverse applications were developed such as classifying images of vehicles, birds, food and plants. Here, we propose the use of deep neural network based representations for categorizing and identifying marine vessels for defense and security applications. First, we gather a large number of marine vessel images via online sources grouping them into four coarse categories; naval, civil, commercial and service vessels. Next, we subgroup naval vessels into fine categories such as corvettes, frigates and submarines. For distinguishing images, we extract state-of-the-art deep visual representations and train support-vector-machines. Furthermore, we fine tune deep representations for marine vessel images. Experiments address two scenarios, classification and verification of naval marine vessels. Classification experiment aims coarse categorization, as well as learning models of fine categories. Verification experiment embroils identification of specific naval vessels by revealing if a pair of images belongs to identical marine vessels by the help of learnt deep representations. Obtaining promising performance, we believe these presented capabilities would be essential components of future coastal and on-board surveillance systems. © 2017 SPIE.","coastal surveillance; convolutional neural networks; deep learning; fine-grained visual categorization; image categorization; image verification; naval marine vessels","Deep learning; Deep neural networks; Image processing; Imaging systems; Marine applications; Monitoring; Network security; Neural networks; Remote sensing; Security systems; Service vessels; Coastal surveillance; Convolutional neural network; Image Categorization; Image verification; Marine vessels; Visual categorization; Naval vessels"
"Generating high-accuracy urban distribution map for short-term change monitoring based on convolutional neural network by utilizing SAR imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040167452&doi=10.1117%2f12.2277901&partnerID=40&md5=182aa61bdcb437e65084f9b748ef83ca","In the developing countries, urban areas are expanding rapidly. With the rapid developments, a short term monitoring of urban changes is important. A constant observation and creation of urban distribution map of high accuracy and without noise pollution are the key issues for the short term monitoring. SAR satellites are highly suitable for day or night and regardless of atmospheric weather condition observations for this type of study. The current study highlights the methodology of generating high-accuracy urban distribution maps derived from the SAR satellite imagery based on Convolutional Neural Network (CNN), which showed the outstanding results for image classification. Several improvements on SAR polarization combinations and dataset construction were performed for increasing the accuracy. As an additional data, Digital Surface Model (DSM), which are useful to classify land cover, were added to improve the accuracy. From the obtained result, high-accuracy urban distribution map satisfying the quality for short-term monitoring was generated. For the evaluation, urban changes were extracted by taking the difference of urban distribution maps. The change analysis with time series of imageries revealed the locations of urban change areas for short-term. Comparisons with optical satellites were performed for validating the results. Finally, analysis of the urban changes combining X-band, L-band and C-band SAR satellites was attempted to increase the opportunity of acquiring satellite imageries. Further analysis will be conducted as future work of the present study. © 2017 SPIE.","change monitoring; convolutional neural network; deep learning; land cover; Synthetic Aperture Rader; urban distribution map","Convolution; Deep learning; Developing countries; Geographical distribution; Image analysis; Neural networks; Noise pollution; Remote sensing; Satellite imagery; Satellites; Space-based radar; Synthetic aperture radar; Synthetic apertures; Time series analysis; Weather satellites; Additional datum; Change analysis; Convolutional neural network; Digital surface models; Land cover; Optical satellites; Short-term monitoring; Urban distribution; Radar imaging"
"Large-scale mapping of small roads in lidar images using deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020455811&doi=10.1007%2f978-3-319-59129-2_17&partnerID=40&md5=5311329fc79c2c5ba6b87c579557118e","Detailed and complete mapping of forest roads is important for the forest industry since they are used for timber transport by trucks with long trailers. This paper proposes a new automatic method for large-scale mapping forest roads from airborne laser scanning data. The method is based on a fully convolutional neural network that performs end-to-end segmentation. To train the network, a large set of image patches with corresponding road label information are applied. The final network is then applied to detect and map forest roads from lidar data covering the Etnedal municipality in Norway. The results show that we are able to map the forest roads with an overall accuracy of 97.2%. We conclude that the method has a strong potential for large-scale operational mapping of forest roads. © Springer International Publishing AG 2017.","Convolutional neural networks; Deep learning; Lidar; Remote sensing","Convolution; Deep learning; Deep neural networks; Forestry; Image analysis; Neural networks; Optical radar; Remote sensing; Transportation; Airborne Laser scanning; Automatic method; Complete mappings; Convolutional neural network; Forest industry; Label information; Overall accuracies; Timber transport; Mapping"
"International Conference on Advances in Information and Communication Technology, ICTA 2016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009471220&partnerID=40&md5=4c8066bcd1d56c70a60d5836a171c9b6","The proceedings contain 69 papers. The special focus in this conference is on Advances in Information and Communication Technology. The topics include: Multimodal based clouds computing systems for healthcare and risk forecasting based on subjective analysis; telematics and advanced transportation services; toward affective speech-to-speech translation; a computer vision based machine for walnuts sorting using robot operating system; a fpga based two level optimized local filter designfor high speed image processing applications; a frequency dependent investigation of complex shear modulus estimation; a method to enhance the remote sensing images based on the local approach using kmeans algorithm; a method for clustering and identifying http automated software communication; a new neuro-fuzzy inference system for insurance forecasting; a new schema to identify s-farnesyl cysteine prenylation sites with substrate motifs; a novel framework based on deep learning and unmanned aerial vehicles to assess the quality of rice fields; a semi-supervised learning method for hybrid filtering; a study on fitness representation in genetic programming; an evaluation of hand pyramid structure for hand representation based on kernels; an exploratory study on students’ performance classification using hybrid of decision tree and naïve bayes approaches; an iterative method to solve boundary value problems with irregular boundary conditions; classifying human body postures by a support vector machine with two simple features; comparing modified pso algorithms for mrs in unknown environment exploration and estimation localization in wireless sensor network based on multi-objective grey wolf optimizer.",,
"18th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036644650&partnerID=40&md5=473310af948151c9ab7e6ff25c314f1e","The proceedings contain 63 papers. The special focus in this conference is on Advanced Concepts for Intelligent Vision Systems. The topics include: Sensing forest for pattern recognition; analysis of skeletal shape trajectories for person re-identification; deep learning on underwater marine object detection: A survey; a novel and accurate local 3D representation for face recognition; human face detection improvement using incremental learning based on low variance directions; multi-view pose estimation with flexible mixtures-of-parts; shearlet-based region map guidance for improving hyperspectral image classification; evaluation of dimensionality reduction methods for remote sensing images using classification and 3D visualization; towards condition analysis for machine vision based traffic sign inventory; body related occupancy maps for human action recognition; learning siamese features for finger spelling recognition; AMD classification in choroidal OCT using hierarchical texton mining; prostate size inference from abdominal ultrasound images with patch based prior information; Omnidirectional localization in vSLAM with uncertainty propagation and bayesian regression; Visual localization based on place recognition using multi-feature combination (D- λ LBP++HOG); homography-based navigation system for unmanned aerial vehicles; robust tracking in weakly dynamic scenes; texturizing and refinement of 3D city models with mobile devices; image classification for ground traversability estimation in robotics; InSAR coherence-dependent fuzzy C-means flood mapping using particle swarm optimization; Facial expression recognition using local region specific dense optical flow and LBP features; monitoring and evaluation of flooded areas based on fused texture descriptors; anomaly detection in crowded scenarios using local and global gaussian mixture models; JND-guided perceptual pre-filtering for HEVC compression of UHDTV video contents.",,
"Shape-based object extraction in high-resolution remote-sensing images using deep Boltzmann machine","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997282964&doi=10.1080%2f01431161.2016.1253897&partnerID=40&md5=12aad6781a0f6adf4f0541d12729d058","In this article, we proposed a novel method based on deep learning shape priors for object extraction in high-resolution (HR) remote-sensing images. Specifically, the deep Boltzmann machines (DBMs) are applied to model the shape priors via the unsupervised training process, which qualify for the advantages of deep learning method, especially the powerful feature learning and modelling ability. The deep shape model is integrated into a new energy function to eliminate the influence of disturbing background. The energy function combines image appearance information and region information. A new region term in the function is proposed to eliminate the influence of object shadow. The process of object extraction is achieved by minimizing the energy function with an iterative optimization algorithm and the Split Bregman method is applied to derive a global solution during the minimization process. Quantitative and qualitative experiments are conducted on the aircraft data set acquired by QuickBird with 60 cm resolution and the results demonstrate the effectiveness of the proposed method. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",,"Extraction; Feature extraction; Iterative methods; Object recognition; Remote sensing; Training aircraft; Deep boltzmann machines; High resolution remote sensing images; Iterative optimization algorithms; Qualitative experiments; Region information; Remote sensing images; Split bregman methods; Unsupervised training; Deep learning; algorithm; image analysis; QuickBird; remote sensing; shape analysis"
"Deep-STEP: A Deep Learning Approach for Spatiotemporal Prediction of Remote Sensing Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997638197&doi=10.1109%2fLGRS.2016.2619984&partnerID=40&md5=fefc27ad4255056731d88b26d71d0caf","With the advent of advanced remote sensing technologies in past few decades, acquiring higher resolution satellite images has become easier and cheaper in recent days. However, on the other hand, it has offered a big challenge to the remote sensing community in smart image interpretation from such huge volume of data. Deep learning, which offers efficient algorithms for extracting multiple levels of feature abstractions, may be suitable to serve the purpose. This letter presents a deep learning approach (Deep-STEP) for spatiotemporal prediction of satellite remote sensing data. The proposed learning architecture is derived from a deep stacking network, consisting of a stack of multilayer perceptron, each of which models the spatial feature of the associated region at a particular time instant. The proposed method has been demonstrated on normalized difference vegetation index (NDVI) data sets, derived from satellite remote sensing imagery, containing several thousands to millions of pixels/records. The experimental results (related to NDVI prediction) reveal that the proposed architecture exhibits fairly satisfactory performance with promising learning capabilities. © 2004-2012 IEEE.","Deep learning; deep stacking network (DSN); satellite remote sensing imagery; spatiotemporal prediction","Forecasting; Network architecture; Satellite imagery; Satellites; Image interpretation; Learning architectures; Normalized difference vegetation index datum; Proposed architectures; Remote sensing technology; Satellite remote sensing; Satellite remote sensing data; Spatio-temporal prediction; Remote sensing"
"Stacked Autoencoder-based deep learning for remote-sensing image classification: a case study of African land-cover mapping","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993997577&doi=10.1080%2f01431161.2016.1246775&partnerID=40&md5=9c4cf77fd3f04440f38fffb146d746a3","Land-cover mapping is an important research topic with broad applicability in the remote-sensing domain. Machine learning algorithms such as Maximum Likelihood Classifier (MLC), Support Vector Machine (SVM), Artificial Neural Network (ANN), and Random Forest (RF) have been playing an important role in this field for many years, although deep neural networks are experiencing a resurgence of interest. In this article, we demonstrate early efforts to apply deep learning-based classification methods to large-scale land-cover mapping. Based on the Stacked Autoencoder (SAE), one of the deep learning models, we built a classification framework for large-scale remote-sensing image processing. We adjusted and optimized the model parameters based on our test samples. We compared the performance of the SAE-based approach with traditional classification algorithms including RF, SVM, and ANN with multiple performance analytics. Results show that the SAE classifier trained with an entire set of African training samples achieves an overall classification accuracy of 78.99% when assessed by test samples collected independently of training samples, which is higher than the accuracies achieved by the other three classifiers (76.03%, 77.74%, and 77.86% of RF, SVM, and ANN, respectively) based on the same set of test samples. We also demonstrated the advantages of SAE in prediction time and land-cover mapping results in this study. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",,"Decision trees; Deep neural networks; Image classification; Learning algorithms; Maximum likelihood; Neural networks; Remote sensing; Sampling; Support vector machines; Classification accuracy; Classification algorithm; Classification framework; Classification methods; Maximum likelihood classifiers; Remote sensing image classification; Remote sensing image processing; Stacked autoencoder; Mapping; algorithm; artificial neural network; image classification; land cover; mapping; optimization; remote sensing"
"Deep Learning with Attribute Profiles for Hyperspectral Image Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995532079&doi=10.1109%2fLGRS.2016.2619354&partnerID=40&md5=6b8886f823a96b3066e90bb39ad8ee1a","Effective spatial-spectral pixel description is of crucial significance for the classification of hyperspectral remote sensing images. Attribute profiles are considered as one of the most prominent approaches in this regard, since they can capture efficiently arbitrary geometric and spectral properties. Lately though, the advent of deep learning in its various forms has also led to remarkable classification performances by operating directly on hyperspectral input. In this letter, we explore the collaboration potential of these two powerful feature extraction approaches. Specifically, we propose a new strategy for hyperspectral image classification, where attribute filtered images are stacked and provided as input to convolutional neural networks. Our experiments with two real hyperspectral remote sensing data sets show that the proposed strategy leads to a performance improvement, as opposed to using each of the involved approaches individually. © 2004-2012 IEEE.","Attribute profiles (APs); deep learning; hyperspectral images; mathematical morphology; pixel classification","Hyperspectral imaging; Image classification; Independent component analysis; Mathematical morphology; Neural networks; Pixels; Remote sensing; Spectroscopy; Attribute profiles (APs); Classification performance; Convolutional neural network; Hyperspectral remote sensing data; Hyperspectral Remote Sensing Image; Performance improvements; Pixel classification; Spectral properties; Deep learning"
"Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027047340&doi=10.1109%2fTGRS.2016.2601622&partnerID=40&md5=49cf775b38f8074e002425dd83fb8e94","Object detection in very high resolution optical remote sensing images is a fundamental problem faced for remote sensing image analysis. Due to the advances of powerful feature representations, machine-learning-based object detection is receiving increasing attention. Although numerous feature representations exist, most of them are handcrafted or shallow-learning-based features. As the object detection task becomes more challenging, their description capability becomes limited or even impoverished. More recently, deep learning algorithms, especially convolutional neural networks (CNNs), have shown their much stronger feature representation power in computer vision. Despite the progress made in nature scene images, it is problematic to directly use the CNN feature for object detection in optical remote sensing images because it is difficult to effectively deal with the problem of object rotation variations. To address this problem, this paper proposes a novel and effective approach to learn a rotation-invariant CNN (RICNN) model for advancing the performance of object detection, which is achieved by introducing and learning a new rotation-invariant layer on the basis of the existing CNN architectures. However, different from the training of traditional CNN models that only optimizes the multinomial logistic regression objective, our RICNN model is trained by optimizing a new objective function via imposing a regularization constraint, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, hence achieving rotation invariance. To facilitate training, we first train the rotation-invariant layer and then domain-specifically fine-tune the whole RICNN network to further boost the performance. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method. © 2016 IEEE.","Convolutional neural networks (CNNs); machine learning; object detection; remote sensing images; rotation-invariant CNN (RICNN)","Artificial intelligence; Computer vision; Convolution; Feature extraction; Image reconstruction; Learning algorithms; Learning systems; Neural networks; Object recognition; Remote sensing; Rotation; Comprehensive evaluation; Convolutional neural network; Feature representation; Multinomial logistic regression; Optical remote sensing; Remote sensing images; Rotation invariant; Very high resolution; Object detection; algorithm; artificial neural network; computer vision; data set; deconvolution; detection method; image analysis; image resolution; machine learning; regression analysis; remote sensing"
"Equitable development through deep learning: The case of sub-national population density estimation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013149587&doi=10.1145%2f3001913.3001921&partnerID=40&md5=35b20c640c255b7f24c52b818fd1567e","High-resolution population density maps are a critical component for global development efforts, including service delivery, resource allocation, and disaster response. Traditional population density efforts are predominantly survey driven, which are laborious, prohibitively expensive, infrequently updated, and inaccurate - especially in remote areas. Furthermore, these maps are developed on a regionalbasis where the methods used vary region to region, hence introducing notable spatio-temporal heterogeneity and bias. The advent of global-scale satellite imagery provides us with an unprecedented opportunity to create inexpensive, accurate, homogeneous, and rapidly updated population maps. To fulfill this vision, we must overcome both infrastructure and methodological obstacles. We propose a convolutional neural network approach that addresses some of the methodological challenges, while employing a publicly available, albeit low resolution, remote sensed product. The method converts satellite images into population density estimates. To explore both the accuracy and generalizability of our approach, we train our neural network on Tanzanian imagery and test the model on Kenyan data. We show that our method is able to generalize to unseen data and we improve upon the current state of the art by 177 percent. © 2016 ACM.",,"Deep learning; Neural networks; Population distribution; Population dynamics; Population statistics; Remote sensing; Convolutional neural network; Critical component; Disaster response; Global development; Population densities; Population density estimation; Satellite images; State of the art; Satellite imagery"
"How useful is region-based classification of remote sensing images in a deep learning framework?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007500060&doi=10.1109%2fIGARSS.2016.7730327&partnerID=40&md5=447a965150bec7cc9ea8b10ef97c263e","In this paper, we investigate the impact of segmentation algorithms as a preprocessing step for classification of remote sensing images in a deep learning framework. Especially, we address the issue of segmenting the image into regions to be classified using pre-trained deep neural networks as feature extractors for an SVM-based classifier. An efficient segmentation as a preprocessing step helps learning by adding a spatially-coherent structure to the data. Therefore, we compare algorithms producing superpixels with more traditional remote sensing segmentation algorithms and measure the variation in terms of classification accuracy. We establish that superpixel algorithms allow for a better classification accuracy as a homogenous and compact segmentation favors better generalization of the training samples. © 2016 IEEE.","Deep learning; Image classification; Remote sensing; Segmentation algorithms; Superpixels",
"Deep learning approach for large scale land cover mapping based on remote sensing data fusion","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007439465&doi=10.1109%2fIGARSS.2016.7729043&partnerID=40&md5=9676385ad01b356aa38dc655f942456d","In the paper we propose the methodology for solving the large scale classification and area estimation problems in the remote sensing domain on the basis of deep learning paradigm. It is based on a hierarchical model that includes self-organizing maps (SOM) for data preprocessing and segmentation (clustering), ensemble of multi-layer perceptrons (MLP) for data classification and heterogeneous data fusion and geospatial analysis for post-processing. The proposed methodology is applied for generation of high resolution land cover and land use maps for the territory of Ukraine from 1990 to 2010 and 2015. © 2016 IEEE.","big data; Deep learning; geospatial analysis; Landsat; neural network; remote sensing data",
"Cloud detection of remote sensing images by deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007504703&doi=10.1109%2fIGARSS.2016.7729176&partnerID=40&md5=8e70b129b39c67c8604a1e0df53082d1","Cloud detection plays a major role for remote sensing image processing. Most of the existed cloud detection methods use the low-level feature of the cloud, which often cause error result especially for thin cloud and complex scene. In this paper, a novel cloud detection method based on deep learning framework is proposed. The designed deep Convolutional Neural Networks (CNNs) consists of four convolutional layers and two fully-connected layers, which can mine the deep features of cloud. The image is firstly clustered into superpixels as sub-region through simple linear iterative cluster (SLIC) method. Through the designed network model, the probability of each superpixel that belongs to cloud region is predicted, so that the cloud probability map of the image is generated. Lastly, the cloud region is obtained according to the gradient of the cloud map. Through the proposed method, both thin cloud and thick cloud can be detected well, and the result is insensitive to complex scene. Experimental results indicate that the proposed method is more robust and effective than compared methods. © 2016 IEEE.","Cloud Detection; Convolutional Neural Networks; Deep Learning; Superpixel",
"A universal remote sensing image quality improvement method with deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007417752&doi=10.1109%2fIGARSS.2016.7730813&partnerID=40&md5=56066dec204dce6ae3696e6019d7b8f4","In this paper, we introduced a deep learning model: Convolutional neural network(CNN) from the field of natural image classification and restoration, to solve general quality improving tasks for remote sensing images, including super-resolution, denoising and haze removal. To take advantage of the content similarity among aerial images and the learning ability of deep learning models, we proposed the idea of training CNN on datasets collected from aerial images with specific degenerating factors, then apply the model to matched tasks. Experiments showed that our network achieved superior performance in quantified results, and visually reconstructed a satisfying majority of missing details from low-quality observations. © 2016 IEEE.","blind denoising; Convolutional neural network; image restoration; non-uniform haze removal; single image super-resolution",
"Scene classification of high resolution remote sensing images using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007486834&doi=10.1109%2fIGARSS.2016.7729193&partnerID=40&md5=3141d35cebc9116ff7fedff4c575eaff","Scene classification of high resolution remote sensing images plays an important role for a wide range of applications. While significant efforts have been made in developing various methods for scene classification, most of them are based on handcrafted or shallow learning-based features. In this paper, we investigate the use of deep convolutional neural network (CNN) for scene classification. To this end, we first adopt two simple and effective strategies to extract CNN features: (1) using pre-trained CNN models as universal feature extractors, and (2) domain-specifically fine-tuning pre-trained CNN models on our scene classification dataset. Then, scene classification is carried out by using simple classifiers such as linear support vector machine (SVM). In our work, three off-the-shelf CNN models including AlexNet [1], VGGNet [2], and GoogleNet [3] are investigated. Comprehensive evaluations on a publicly available 21 classes land use dataset and comparisons with several state-of-the-art approaches demonstrate that deep CNN features are effective for scene classification of high resolution remote sensing images. © 2016 IEEE.","convolutional neural network (CNN); deep learning; feature extraction; remote sensing images; Scene classification",
"Fully convolutional neural networks for remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007415739&doi=10.1109%2fIGARSS.2016.7730322&partnerID=40&md5=c34f36f1b299b0d5f020a2a4c3b60d45","We propose a convolutional neural network (CNN) model for remote sensing image classification. Using CNNs provides us with a means of learning contextual features for large-scale image labeling. Our network consists of four stacked convolutional layers that downsample the image and extract relevant features. On top of these, a deconvolutional layer upsamples the data back to the initial resolution, producing a final dense image labeling. Contrary to previous frameworks, our network contains only convolution and deconvolution operations. Experiments on aerial images show that our network produces more accurate classifications in lower computational time. © 2016 IEEE.","classification; convolutional neural networks; deep learning; Remote sensing images",
"Using CNN-based high-level features for remote sensing scene classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007417697&doi=10.1109%2fIGARSS.2016.7729674&partnerID=40&md5=4199b8788a02fd4da7d5cae540e0d4ae","In this paper, convolutional neural networks (CNNs) is employed for remote-sensing scene classification, which fully utilizes the semantic features extracted from the images while ignoring some traditional features. Consider the limited labeled samples, CaffeNet model as the pre-trained architecture is adopted. By fine-tuning the pre-trained models, the proposed method is expected to be robust and efficient. Its performance is evaluated with two remote-sensing scene datasets. From the experimental results, the proposed CNN-based scene classification method does provide more excellent performance and be superior to several state-of-the-art methods. © 2016 IEEE.","convolutional neural networks; deep learning; feature extraction; Scene classification",
"Object detection in pleiades images using deep features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007492933&doi=10.1109%2fIGARSS.2016.7729396&partnerID=40&md5=ebf11f2b2e3e8b2bf011120780e4d04e","Extracting and identifying objects in very high resolution imagery has been a popular research topic in remote sensing. Since the beginning of this decade, deep learning techniques have revolutionized computer vision providing significant performance gains compared to traditional 'shallow' techniques in various challenging vision problems. The training of deep neural networks usually requires very large training datasets. The advantage of using deep features is to exploit already trained Convolutional Neural Networks (CNN) in order to produce high level features without the burden of having to train a CNN from scratch. In this paper, we are investigating the use of deep features for the detection of small objects (cars and individual trees) in high resolution Pleiades imagery. Preliminary results show good detection performance and are very encouraging for future applications. © 2016 IEEE.","deep learning; Object detection; very high resolution",
"Spatiotemporal scene interpretation of space videos via deep neural network and tracklet analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007436568&doi=10.1109%2fIGARSS.2016.7729468&partnerID=40&md5=71070228d7b00559b903daad1ac490c9","Spaceborne remote sensing videos are becoming indispensable resources, opening up opportunities for new remote sensing applications. To exploit this new type of data, we need sophisticated algorithms for semantic scene interpretation. The main difficulties are: 1) Due to the relatively poor spatial resolution of the video acquired from space, moving objects, like cars, are very difficult to detect, not to mention track; 2) camera movement handicaps scene interpretation. To address these challenges, in this paper we propose a novel framework that fuses multispectral images and space videos for spatiotemporal analysis. Taking a multispectral image and a spaceborne video as input, an innovative deep neural network is proposed to fuse them in order to achieve a fine-resolution spatial scene labeling map. Moreover, a sophisticated approach is proposed to analyze activities and estimate traffic density from 150,000+ tracklets produced by a Kanade-Lucas-Tomasi keypoint tracker. The proposed framework is validated using data provided for the 2016 IEEE GRSS data fusion contest, including a video acquired from the International Space Station and a DEIMOS-2 multispectral image. Both visual and quantitative analysis of the experimental results demonstrates the effectiveness of our approach. © 2016 IEEE.","activity analysis; deep learning; scene labeling; space videos; traffic density estimation",
"CRF learning with CNN features for hyperspectral image segmentation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007496775&doi=10.1109%2fIGARSS.2016.7730798&partnerID=40&md5=5e43f4218cd44c5fc53565473e48263b","This paper proposes a method that uses both spectral and spatial information to segment remote sensing hyperspectral images. After a hyperspectral image is over-segmented into superpixels, a deep Convolutional Neural Network (CNN) is used to perform superpixel-level labelling. To further delineate objects from a hyperspectral scene, this paper attempts to combine the properties of CNN and Conditional Random Field (CRF). A mean-field approximation algorithm for CRF inference is used and formulated with Gaussian pairwise potentials as Recurrent Neural Network. This combined network is then plugged into the CNN which leads to a deep network that has robust characteristics of both CNN and CRF. Preliminary results suggest the usefulness of this framework to a promising extent. © 2016 IEEE.","Conditional Random Field; Convolutional Neural Network; Deep Learning; Image Segmentation; Superpixel",
"Hyperspectral image classification based on deep stacking network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007463494&doi=10.1109%2fIGARSS.2016.7729850&partnerID=40&md5=ee19d10880522af3a60986dac72b38ee","Hyperspectral image (HIS) classification is a hot topic in remote sensing community and most of the existing methods extract the features of original Hyperspectral data using shallow layer networks such as neural network (NN) and support vector machine (SVM). As deep learning recently achieves great success in machine learning and pattern recognition area for its ability in deep feature extraction and representations, two deep networks i.e. deep convolutional network (DCN) and deep belief network (DBN) have been used for hyperspectral image classification and better results have been achieved. Differing from those deep networks for HSI classification, in this paper, we propose a new method for hyperspectral image classification based on deep stacking network (DSN), which owns advantages to other deep models for its simplicity when processing in batch-mode learning - not requiring stochastic gradient descent that other DNNs require. The feature extraction is gradually obtained by employing nonlinear activation function on the hidden layer nodes of each module, which is different from those DSNs that usually use linear weights between the hidden layer and the output layer. Experimental results on AVIRIS hyperspectral images show that the proposed method achieves improved classification performance when compared with that via SVM and NN methods. © 2016 IEEE.","classification; deep stacking network; hyperspectral image; logistic regression",
"Region-based convolutional neural networks for object detection in very high resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997770219&doi=10.1109%2fFSKD.2016.7603232&partnerID=40&md5=9cacb6441e4b77933558b99399958dde","Recently, the automatic object detection in high-resolution remote sensing images has become the key point in the application of remote sensing technology. The traditional methods, such as bag-of-visual-words (BOVW), could perform well in simple scenes, but when it used in complex scenes, the performance drops quickly. This paper we first try to use the current hot deep learning technology: Region-based convolutional neural networks (R-CNN), to detect aircrafts under the complex environments in high-resolution remote sensing images. This method has been proved to be very efficiency when using in object detection in natural images. Here, we tried to introduce this method into the field of the remote sensing. During our experiments, we also compared the impact of different proposal generate methods on the final detection results. And we also proposed some practical tips to accelerate the detection speed. After detection, we proposed to use a novel algorithm which we called box-fusion, to eliminate the redundant and repetitive boxes that covering the same object. As experiments and results shows, the R-CNN method is much more effective and robust than the traditional BOVW method when dealing with aircrafts detection under complex scenes in high-resolution remote sensing images. © 2016 IEEE.","aircraft detection; convolutional neural network; deep learning; feature learning","Aircraft detection; Complex networks; Convolution; Fuzzy systems; Image reconstruction; Neural networks; Object detection; Object recognition; Bag-of-visual-words; Complex environments; Convolutional neural network; Deep learning; Feature learning; High resolution remote sensing images; Remote sensing technology; Very high resolution; Remote sensing"
"Snow cover recognition for qinghai-tibetan plateau using deep learning and multispectral remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994378464&doi=10.11947%2fj.AGCS.2016.20160183&partnerID=40&md5=844b7b24e98e986571581bc898134504","Snow cover in Qinghai-Tibetan plateau (QT plateau) is very important to global climate change. Because of the complex topography and high altitude, the recognition accuracies of existing snow cover products in QT plateau are significantly lower than flat areas. This paper proposed a new method of snow cover recognition for QT plateau based on deep learning. The multispectral remote sensing data from Chinese meteorological satellite FY-3A and the multiple geographic elements information are put together as the data sources, the insitu snow depth measurements and existing snow cover products are used for selecting the labeled samples. A stacked denoising auto-encoders (SDAE) network was built and trained for feature extraction and classification, this network can be used as a classifier for distinguishing the snow cover from cloud and other snow-free surface features. The recognition results are verified by snow depth data of meteorological station observations, verification results show that the recognition accuracy of this method is significantly higher than the snow product FY-3A/MULSS, which is using the same remote sensing data source FY-3A, and slightly higher than the widely used snow products MOD10A1 and MYD10A1,and the cloud coverage rate of this method is the lowest. According to the validation results, this method can effectively improve the accuracy of snow cover recognition, and reduce the interference of clouds. © 2016, Surveying and Mapping Press. All right reserved.","Deep learning; FengYun-3; Multispectral data fusion; Qinghai-Tibetan plateau; Satellite remote sensing; Snow cover","Climate change; Complex networks; Data fusion; Feature extraction; Remote sensing; Snow melting systems; Deep learning; FengYun-3; Multi-spectral data; Qinghai-Tibetan plateau; Satellite remote sensing; Snow covers; Snow; accuracy assessment; climate change; cloud cover; data processing; FengYun; learning; multispectral image; satellite data; satellite imagery; snow cover; topography; China; Qinghai-Xizang Plateau"
"Semisupervised classification for hyperspectral image based on multi-decision labeling and deep feature learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988038682&doi=10.1016%2fj.isprsjprs.2016.09.001&partnerID=40&md5=76bc0e3c7b51f992fafff7885a643e5b","Semisupervised learning is widely used in hyperspectral image classification to deal with the limited training samples, however, some more information of hyperspectral image should be further explored. In this paper, a novel semisupervised classification based on multi-decision labeling and deep feature learning is presented to exploit and utilize as much information as possible to realize the classification task. First, the proposed method takes two decisions to pre-label each unlabeled sample: local decision based on weighted neighborhood information is made by the surrounding samples, and global decision based on deep learning is performed by the most similar training samples. Then, some unlabeled ones with high confidence are selected to extent the training set. Finally, self decision, which depends on the self features exploited by deep learning, is employed on the updated training set to extract spectral-spatial features and produce classification map. Experimental results with real data indicate that it is an effective and promising semisupervised classification method for hyperspectral image. © 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Hyperspectral image; Semisupervised classification","Classification (of information); Sampling; Spectroscopy; Deep feature learning; Deep learning; Hyper-spectral images; Hyperspectral image classification; Neighborhood information; Semi- supervised learning; Semi-supervised classification; Semi-supervised classification method; Image classification; data acquisition; image classification; learning; spatial analysis; spectral analysis"
"A Self-Improving Convolution Neural Network for the Classification of Hyperspectral Data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982237011&doi=10.1109%2fLGRS.2016.2595108&partnerID=40&md5=73133f493d4b4e6557eb541317701673","In this letter, a self-improving convolutional neural network (CNN) based method is proposed for the classification of hyperspectral data. This approach solves the so-called curse of dimensionality and the lack of available training samples by iteratively selecting the most informative bands suitable for the designed network via fractional order Darwinian particle swarm optimization. The selected bands are then fed to the classification system to produce the final classification map. Experimental results have been conducted with two well-known hyperspectral data sets: Indian Pines and Pavia University. Results indicate that the proposed approach significantly improves a CNN-based classification method in terms of classification accuracy. In addition, this letter uses the concept of dither for the first time in the remote sensing community to tackle overfitting. © 2016 IEEE.","Convolutional neural network (CNN); deep learning; feature selection; fractional order Darwinian particle swarm optimization (FODPSO); hyperspectral image classification","Convolution; Deep learning; Feature extraction; Hyperspectral imaging; Iterative methods; Neural networks; Particle swarm optimization (PSO); Remote sensing; Spectroscopy; Classification accuracy; Classification methods; Classification system; Convolution neural network; Convolutional Neural Networks (CNN); Curse of dimensionality; Fractional order; Hyperspectral Data; Classification (of information)"
"Multilayer Unmixing for Hyperspectral Imagery with Fast Kernel Archetypal Analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027517666&doi=10.1109%2fLGRS.2016.2595102&partnerID=40&md5=acce1617a5c369643892647de532bb77","The multilayer network in deep learning provides a promising means for rich data representation. Inspired by this approach, we investigate multilayer unmixing for spectral decomposition with fast kernel archetypal analysis (KAA). KAA is used for endmember extraction and abundance estimation simultaneously. To refine the initial unmixing results, a multilayer process is utilized to provide final unmixing results at the end of the network. Moreover, a fast implementation of KAA is proposed via using the Nystrom method to relieve KAA's memory issue and decrease the processing time. The proposed method is tested on both synthetic and real hyperspectral image data sets. The results demonstrate that the multilayer unmixing algorithm outperforms the conventional unmixing techniques. © 2017 IEEE.","Hyperspectral imagery; kernel archetypal analysis (KAA); multilayer network; Nyström method; spectral unmixing","Multilayers; Remote sensing; Spectroscopy; Archetypal analysis; Hyper-spectral imageries; M method; Multi-layer network; Spectral unmixing; Image analysis"
"Framework of remote sensing image automatic processing with ""invariant feature point set"" as control data set","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992413296&partnerID=40&md5=63cbf448c6e35b5736507c26653aa897","Global change research consistently requires the combined use of cross-sensor images with similar spectral wavelengths. To meet the temporal resolution and coverage requirements of remote sensing applications, new requirements are proposed for remote sensing image processing technology. Such technology requirements are related to how to obtain geometric consistency between cross-sensor/multi temporal data, how to obtain radiometric normalization, and how to obtain land cover class labels that are consistent between cross-sensor/multi temporal data. They are also related to highly automated processing. For the above requirements, we propose a framework for the automatic processing of remote sensing images with ""an invariant feature point set"" (IFPs) as the control data set. Specifically, we combine the spatial and temporal alignment in geometry space, radiation space, and land cover class space into a unified framework and thereby provide an indirect means to achieve fast processing. The key technologies for building IFPs are also reviewed. © 2016, Science Press. All right reserved.","Active learning; Control point set; Deep learning; Image controt point; Invariant feature point set; Pseudo-In variant features; Remote sensing image processing","Deep learning; Geometry; Image processing; Space optics; Active Learning; Control point; Image controt point; Point set; Pseudo-In variant features; Remote sensing image processing; Remote sensing"
"Review of methods and applications of high spatiotemporal fusion of remote sensing data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992345420&doi=10.11834%2fjrs.20166218&partnerID=40&md5=e38697fca9878b1d51ce28417719de16","Remote sensing images can provide important and abundant information about the Earth at a global or local scale. Thus, many applications often require remote sensing data with high acquisition frequency and high spatial resolution. However, meeting this requirement is a considerable challenge given satellite limitations. The spatiotemporal fusion method provides a feasible way to solve these ""spatialtemporal"" contradictions. In the last 10 years, spatiotemporal fusion has elicited wide interest in various applications because it integrates the superiority of multisource satellite data in fine spatial resolution or frequent temporal coverage and it can generate fused images with high spatial and temporal resolution. In this study, we reviewed the advantages and limitations of three types of method for spatiotemporal fusion, namely, transformation- based, reconstruction-based, and learning-based methods. First, the transformation-based method consistently filters and processes transformed data and then accesses high-spatiotemporal resolution data via inverse transform. It mainly focuses on the spatial and spectral information of multi-source satellite image enhancement or fusion. The spatial resolution of the results obtained with this method remains low, and the accuracy is relatively poor because the temporal change information is not used in this method. Second, the reconstruction-based method has elicited much attention since the proposal of a semi-physical fusion model and STARFM. This method integrates the information of temporal change, spatial change, and spectral change among multi-source satellite images acquired in different times and generates high-spatiotemporal resolution data by calculating the weight of different changes. This method provides an excellent fusion approach for spatiotemporal fusion because the results show high accuracy. However, the results would be poor when the type of land cover changes or the cover area is heterogeneous. Third, the learning-based method is based on the development of compressed sensing and sparse representation technology. This method represents a recent development that relies on learning the relationship and difference of multi-source satellite images by training samples and constructing an image dictionary. Although the learning-based method could obtain good results, the processing efficiency is lower than that of other methods, and it requires the training of sample selection. Recently, the result of spatiotemporal fusion has been used in various applications, especially in the reconstruction-based method. This method is mainly used in time series data analysis as well as in retrieval and regional data set generation. For time series data analysis and retrieval, many researchers have used the results in developing the missing images of time series, detecting phenology, inversing urban environment parameters, estimating gross primary production, evaluating biomass, calculating land surface temperature, and so on. Given that the covered area of a low spatial resolution is large and the spectrum continuity of spatiotemporal fusion results is high, these results could be applied to the generation of regional data sets. Although the spatiotemporal fusion method has seen considerable development, certain problems remain. The uncertainties are attributed to the complexity of land cover change, the errors of sensor calibration, and the data pretreatment process. The five potential aspects of the spatiotemporal fusion method that require further study are the consistency of data from different sensors, introduction of nonlinear mixed models, addition of prior knowledge, introduction of deep learning theory, and expansion into other satellites. © 2016, Science Press. All right reserved.","High spatiotemporal fusion; Model; Multi-source data; Remote sensing; Spatial-temporal contradictions","Atmospheric temperature; Data handling; Deep learning; Image enhancement; Image fusion; Image resolution; Information analysis; Inverse transforms; Metadata; Models; Remote sensing; Satellite imagery; Time series; Time series analysis; Gross primary production; Land surface temperature; Multisource data; Spatial and temporal resolutions; Spatial temporals; Spatio-temporal fusions; Spatio-temporal resolution; Time series data analysis; Inverse problems"
"Continous updating and refinement of land cover data product","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992342806&doi=10.11834%2fjrs.20166250&partnerID=40&md5=b7962d10c4829ccd3dc53aaca8ce46a3","In the past years, the land cover community has strived to develop and supply more datasets at different spatial scales (e.g., regional, national, or global) with increasing spatial-temporal resolutions and improved classification accuracy. Although these data products have been widely applied in climate change studies, environmental monitoring, sustainable development, and many other societal benefit areas, the user communities constantly propose new demands, such as additional land cover classes, up-to-date time series, and consistency among different datasets. Therefore, the continuous updating and content refinement of land cover data products have become key objectives of the land cover community. The updating and refinement of land cover data products differ from their original creation. Change detection with remotely sensed imagery is a major approach for updating a large area land cover, and the rapidly increasing crowdsourcing information provides another valuable resource. However, as a technical challenge is that no existing change detection algorithm can be applied to all kinds of imageries and geographic regions because of the extremely complex spectral heterogeneity of land cover classes. Therefore, an efficient change detection approach with the consideration of the existing land cover datasets needs to be developed. One cuttingedge issue is to integrate the imagery-based change intensity measurement with prior knowledge represented by existing land cover datasets. Change detection for time series imagery is moving from the comparison of two neighboring points to global trend analysis. The coupling of SAR and infrared images with multispectral images needs to be explored from several aspects, such as relative radiometric correction, spectral matching, and temporal-spatial data fusion. Another key challenge comes from the rational utilization of crowdsourcing information for updating and refining land cover. Crowdsourcing information may differ in terms of data contents, position accuracy, spatial-temporal resolution, and uncertainty, and hence, previous studies have aimed to develop appropriate methods and techniques for evaluating volunteered data quality, discovering useful information from deep web, extracting incremental changes, and integrating multi-source datasets. The increasing amount of freely accessible remote sensing data has increased the data intensiveness of the generating future land cover data products. Specific tools and systems must be designed and developed to support the updating and refining of large area land cover. One of these tools is an online land cover updating system that allows users to execute web-based land cover change detection and processing in an open web environment. The key issues in using this tool include domain-knowledge-based change detection service modeling and dynamic service composition. Data Cube is another system that has a flexible classification concept, but this tool is still under investigation. Nevertheless, this tool is expected to facilitate the on-demand extraction of land cover classes with deep learning and other data mining algorithms. © 2016, Science Press. All right reserved.","Change detection; Crowdsourcing; Land cover; Remote sensed data product; Updating","Classification (of information); Climate change; Crowdsourcing; Data fusion; Image matching; Infrared imaging; Knowledge based systems; Refining; Remote sensing; Signal detection; Sustainable development; Time series; Time series analysis; Change detection; Change detection algorithms; Dynamic service composition; Environmental Monitoring; Land cover; Relative radiometric correction; Remote sensed data; Updating; Data mining"
"Scene classification using multi-scale deeply described visual words","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979619476&doi=10.1080%2f01431161.2016.1207266&partnerID=40&md5=8139367d7c2da5702adc2a9a7cd92eaf","ABSTRACT: This article presents a deep learning-based Multi-scale Bag-of-Visual Words (MBVW) representation for scene classification of high-resolution aerial imagery. Specifically, the convolutional neural network (CNN) is introduced to learn and characterize the complex local spatial patterns at different scales. Then, the learnt deep features are exploited in a novel way to generate visual words. Moreover, the MBVW representation is constructed using the statistics of the visual word co-occurrences at different scales, which are derived from a training data set. We apply our technique to the challenging aerial scene data set: the University of California (UC) Merced data set consisting of 21 different aerial scene categories with sub-metre resolution. The experimental results show that the statistics of deeply described visual words can characterize the scene well and improve classification accuracy. It demonstrates that the proposed method is highly effective in the scene classification of high-resolution remote-sensing imagery. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",,"Abstracting; Aerial photography; Deep learning; Neural networks; Remote sensing; Uranium compounds; Bag-of-visual words; Classification accuracy; Convolutional Neural Networks (CNN); High resolution aerial imagery; High resolution remote sensing imagery; Scene classification; Training data sets; University of California; Antennas; artificial neural network; image classification; image resolution; imagery; remote sensing; California; United States"
"Development of an automated system for building detection from high-resolution satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988005943&doi=10.1109%2fEORSA.2016.7552806&partnerID=40&md5=c395d451ed982be73d0c1e5d3b201f35","Detail information of human settlements is crucial for development issues, such as public health, energy development, and disaster risk management. While the methodologies of settlement mapping in urban areas have been developed well, there are still few studies on settlement mapping in rural areas and villages, which are important for urban development in regard to connectivity of development impacts. For the settlement mapping of rural areas and villages, we developed a system of automated building detection from high-resolution satellite images which are provided from Bing Maps. The algorithm of building detection is based on the Convolutional Neural Network, a well-known deep learning method for image recognition. Because the amount of satellite images is enormous, we implemented the algorithm with a high-performance computing with massive parallel processing on the Data Integration and Analysis System (DIAS) in the University of Tokyo. We demonstrated the building detection using the developed system for Yangon, Myanmar. © 2016 IEEE.","building detection; deep learning; high-performance computing; high-resolution satellite images","Automation; Buildings; Data handling; Data integration; Health risks; Image recognition; Mapping; Neural networks; Observatories; Remote sensing; Risk management; Satellites; Urban growth; Automated buildings; Building detection; Convolutional neural network; Deep learning; Development impacts; High performance computing; High resolution satellite images; University of Tokyo; Rural areas"
"Remote sensing image fusion based on Deep Support Value Learning Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982214191&doi=10.11897%2fSP.J.1016.2016.01583&partnerID=40&md5=96ae2dd39507dfa19df5189c18751607","A novel method based on Deep Support Value Learning Networks (DSVL Nets) is proposed for fusion of remote sensing images. The loss function based on structural risk minimization is used in the training of deep learning network. In order to avoid the loss of information, we abandon the downsampling of feature mapping layer of traditional convolution neural network. The DSVL Nets contains five hidden layers, where each layer consists of convolution layer and linear layer. And each layer provides a redundant transform which is multi-scale, multi-direction, anisotropy and non-subsampled. All convolution layers and the fifth linear layer are regarded as the outputs of DSVL Nets. The convolution layers images are fused by abs-maximum model. The linear layer images are sparsely represented on overcomplete dictionary, and then the coefficients are fused by abs-maximum model. The fused convolution layers images and the linear layer image are reconstructed, and one can obtain the fused result image. Some experiments are taken on several QuickBird and Geoeye satellite datasets. Compared with PCA, AWLP, PN-TSSC and SVT, the experimental results show that the proposed method outperforms some related pan-sharpening approaches in both visual results and numerical guidelines, and reduces the distortion in both the spectral and spatial domain. © 2016, Science Press. All right reserved.","Convolutional neural networks; Deep learning; Deep support value learning networks; Machine learning; Overcomplete dictionary; Remote sensing image fusion","Artificial intelligence; Convolution; Image reconstruction; Learning systems; Neural networks; Numerical methods; Remote sensing; Convolutional neural network; Deep learning; Learning network; Over-complete dictionaries; Remote sensing images; Image fusion"
"Deep feature learning for hyperspectral image classification and land cover estimation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988487454&partnerID=40&md5=008c83697f81cbf99ccf3decba29364b","The differences in spatial sampling between field measurements and remote-sensing imagery can hinder the exploitation of contemporary data. When the field-based sampling is higher than airborne and spaceborne imagery, each pixel is naturally associated with multiple pixels due to the multiplexing of the reflectances of different materials. To address this scale inconsistency, we propose the introduction of the multi-label classification framework where classifiers are trained to predict multiple labels per pixel. Furthermore, instead of relying on raw hyperspectral measurements for the classification process, we investigate the Stacked Sparse Autoencoders framework, an example of a deep learning network, for descriptive feature extraction. To validate the merits of the proposed scheme, we consider real data from the Hyperion instrument on-board the EO-1 and NYC land cover data from 2010.","Feature learning; Hyperspectral; Multi-label classification","Classification (of information); Feature extraction; Learning systems; Pixels; Remote sensing; Spectroscopy; Classification process; Deep feature learning; Feature learning; HyperSpectral; Hyperspectral image classification; Hyperspectral measurements; Multi label classification; Remote sensing imagery; Image classification"
"Machine learning approaches to corn yield estimation using satellite images and climate data: A case of Iowa State","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988958512&doi=10.7848%2fksgpc.2016.34.4.383&partnerID=40&md5=cdc43316d985c4aa5b2efca2b14ba12a","Remote sensing data has been widely used in the estimation of crop yields by employing statistical methods such as regression model. Machine learning, which is an efficient empirical method for classification and prediction, is another approach to crop yield estimation. This paper described the corn yield estimation in Iowa State using four machine learning approaches such as SVM (Support Vector Machine), RF (Random Forest), ERT (Extremely Randomized Trees) and DL (Deep Learning). Also, comparisons of the validation statistics among them were presented. To examine the seasonal sensitivities of the corn yields, three period groups were set up: (1) MJJAS (May to September), (2) JA (July and August) and (3) OC (optimal combination of month). In overall, the DL method showed the highest accuracies in terms of the correlation coefficient for the three period groups. The accuracies were relatively favorable in the OC group, which indicates the optimal combination of month can be significant in statistical modeling of crop yields. The differences between our predictions and USDA (United States Department of Agriculture) statistics were about 6-8 %, which shows the machine learning approaches can be a viable option for crop yield modeling. In particular, the DL showed more stable results by overcoming the overfitting problem of generic machine learning methods.","Climate data; Crop yield; Machine learning; Remote sensing","crop yield; machine learning; maize; regression analysis; remote sensing; satellite imagery; seasonal variation; Iowa; United States; Zea mays"
"A survey on object detection in optical remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961970561&doi=10.1016%2fj.isprsjprs.2016.03.014&partnerID=40&md5=559d930d637c87155d991d36b271de74","Object detection in optical remote sensing images, being a fundamental but challenging problem in the field of aerial and satellite image analysis, plays an important role for a wide range of applications and is receiving significant attention in recent years. While enormous methods exist, a deep review of the literature concerning generic object detection is still lacking. This paper aims to provide a review of the recent progress in this field. Different from several previously published surveys that focus on a specific object class such as building and road, we concentrate on more generic object categories including, but are not limited to, road, building, tree, vehicle, ship, airport, urban-area. Covering about 270 publications we survey (1) template matching-based object detection methods, (2) knowledge-based object detection methods, (3) object-based image analysis (OBIA)-based object detection methods, (4) machine learning-based object detection methods, and (5) five publicly available datasets and three standard evaluation metrics. We also discuss the challenges of current studies and propose two promising research directions, namely deep learning-based feature representation and weakly supervised learning-based geospatial object detection. It is our hope that this survey will be beneficial for the researchers to have better understanding of this research field. © 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).","Deep learning; Machine learning; Object detection; Object-based image analysis (OBIA); Optical remote sensing images; Template matching; Weakly supervised learning","Antennas; Artificial intelligence; Computer vision; Deep learning; Knowledge based systems; Learning systems; Object recognition; Remote sensing; Roads and streets; Supervised learning; Surveys; Template matching; Feature representation; Geo-spatial objects; Object based image analysis (OBIA); Object detection method; Optical remote sensing; Satellite image analysis; Standard evaluations; Weakly supervised learning; Object detection"
"Semantic Labeling of Aerial and Satellite Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978858032&doi=10.1109%2fJSTARS.2016.2582921&partnerID=40&md5=9af7d3f9c2c6cdcf16fc575781589c9f","Inspired by the recent success of deep convolutional neural networks (CNNs) and feature aggregation in the field of computer vision and machine learning, we propose an effective approach to semantic pixel labeling of aerial and satellite imagery using both CNN features and hand-crafted features. Both CNN and hand-crafted features are applied to dense image patches to produce per-pixel class probabilities. Conditional random fields (CRFs) are applied as a postprocessing step. The CRF infers a labeling that smooths regions while respecting the edges present in the imagery. The combination of these factors leads to a semantic labeling framework which outperforms all existing algorithms on the International Society of Photogrammetry and Remote Sensing (ISPRS) two-dimensional Semantic Labeling Challenge dataset. We advance state-of-the-art results by improving the overall accuracy to $88\%$ on the ISPRS Semantic Labeling Contest. In this paper, we also explore the possibility of applying the proposed framework to other types of data. Our experimental results demonstrate the generalization capability of our approach and its ability to produce accurate results. © 2008-2012 IEEE.","Aerial imagery; conditional random fields; convolutional neural networks; deep learning; satellite imagery and remote sensing; semantic labeling","Artificial intelligence; Computer vision; Image segmentation; Learning systems; Neural networks; Pixels; Random processes; Remote sensing; Semantics; Class probabilities; Conditional Random Fields(CRFs); Convolutional neural network; Effective approaches; Feature aggregation; Generalization capability; International society; Overall accuracies; Satellite imagery; aerial photography; algorithm; artificial neural network; photogrammetry; remote sensing; satellite imagery"
"Greedy deep dictionary learning for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037547056&doi=10.1109%2fWHISPERS.2016.8071740&partnerID=40&md5=fe1223c9641a1a7f8cb01cb9cf702ca7","In this work we propose a new deep learning tool - deep dictionary learning. We give an alternate neural network type interpretation to dictionary learning. Based on this, we build a deep architecture by cascading one dictionary after the other. The learning proceeds in a greedy fashion, therefore for each level we only need to learn a single layer of dictionary - time tested tools are there to solve this problem. We compare our approach to the deep belief network (DBN) and stacked autoencoder (SAE) based techniques for hyperspectral image classification. We find that in the practical scenario, when the training data is limited, our method outperforms the more established tools like SAE and DBN. © 2016 IEEE.","Classification; Deep Learning; Dictionary Learning","Classification (of information); Image classification; Remote sensing; Spectroscopy; Deep architectures; Deep belief network (DBN); Dictionary learning; Single layer; Stacked autoencoder; Training data; Deep learning"
"Deep stacking network with coarse features for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037539479&doi=10.1109%2fWHISPERS.2016.8071666&partnerID=40&md5=2303b6af489d9223aad86272d90caf58","Hyperspectral image (HSI) classification attracts increasing attentions in remote sensing community for its academic significance and potential wide applications. Although most of used existing 'intelligent' methods extracting the features of original hyperspectral data are based on shallow-layer networks such as neural network (NN) and support vector machine (SVM) for its simplicity in realization, two deep neural networks (DNN), i.e. the deep convolutional network (DCN) and the deep belief network (DBN), have been used for HSI classifications with improved performance, as deep learning recently achieves great success for its ability in deep feature extraction and representations. In this paper, another DNN, a deep stacking network (DSN) based approach with coarse features for hyperspectral image classification is proposed, as its advantages over shadow networks and other deep models in its simplicity and batch-mode learning - not requiring stochastic gradient descent that other DNNs require. In this approach, the inputs to the DSN are coarse features, i.e. coarse spectral features by band reduction, coarse spatial features extracted by PCA from original HSI, or combination of both. The fine deeper features in the hyperspectral data are gradually obtained by employing the nonlinear activation function on the hidden layer nodes of each module, which is different from the current DSN that usually uses linear weights between hidden layer to output layer. A closed limit on its node's input - output is also exerted. The theoretical analysis and experimental results with AVIRIS hyperspectral image have shown that, (1) the proposed DSN approach achieves improved classification performance compared with shadow networks, (2) the new DSN approach with both the coarse spectral feature and coarse spatial feature has higher classification accuracy compared to that with either only, (3) this study has insight of necessity to develop new deep neural network for HSI classification from direct HSI data. © 2016 IEEE.","Classification; Deep stacking network; Hyperspectral image; Logistic regression","Deep neural networks; Hyperspectral imaging; Image classification; Image enhancement; Network layers; Remote sensing; Spectroscopy; Stochastic models; Stochastic systems; Support vector machines; Classification accuracy; Classification performance; Convolutional networks; Deep belief network (DBN); Hyperspectral Data; Logistic regressions; Nonlinear activation functions; Stochastic gradient descent; Classification (of information)"
"Embedded high performance computing for on-board hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037546515&doi=10.1109%2fWHISPERS.2016.8071710&partnerID=40&md5=90046a438d8749a24452b693279633e9","Jetson TK1 is a recently launched embedded application development platform from NVIDIA, which features the Tegra K1 processor and Kepler Graphics Processing Unit (GPU). We envisage that such a system has huge potential for deploying an embedded system for on-board classification of hyperspectral images. We used a convolutional deep neural network for designing a unified model for hyperspectral image classification. Deep convolutional model hierarchically extracts spectral-spatial features from hyperspectral imagery and these features are used by the fully connected layer of neural network to perform pixel level classification of hyperspectral imagery. Our experimental results show that Jetson TK1 based hyperspectral image classification gives promising results and the possibility of having Jetson based embedded platform for on-board classification of hyperspectral images. © 2016 IEEE.","Convolutional neural networks; Deep learning; Hyperspectral image classification; Jetson TK1","Computer graphics; Computer graphics equipment; Convolution; Deep learning; Deep neural networks; Embedded systems; Graphics processing unit; Hyperspectral imaging; Independent component analysis; Network layers; Neural networks; Program processors; Remote sensing; Spectroscopy; Convolutional model; Convolutional neural network; Embedded application; Fully-connected layers; Graphics Processing Unit (GPU); High performance computing; Hyper-spectral imageries; Jetson TK1; Image classification"
"BENCHMARKING DEEP LEARNING FRAMEWORKS for the CLASSIFICATION of VERY HIGH RESOLUTION SATELLITE MULTISPECTRAL DATA","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034983308&doi=10.5194%2fisprs-annals-III-7-83-2016&partnerID=40&md5=84e054d573fb555c0904bbe7eaf3c74d","In this paper we evaluated deep-learning frameworks based on Convolutional Neural Networks for the accurate classification of multispectral remote sensing data. Certain state-of-the-art models have been tested on the publicly available SAT-4 and SAT-6 high resolution satellite multispectral datasets. In particular, the performed benchmark included the AlexNet, AlexNet-small and VGG models which had been trained and applied to both datasets exploiting all the available spectral information. Deep Belief Networks, Autoencoders and other semi-supervised frameworks have been, also, compared. The high level features that were calculated from the tested models managed to classify the different land cover classes with significantly high accuracy rates i.e., above 99.9%. The experimental results demonstrate the great potentials of advanced deep-learning frameworks for the supervised classification of high resolution multispectral remote sensing data.","Classification; Convolutional; Data Mining; Land Cover; Land Use; Machine Learning; Neural Networks",
"ESTIMATING CORN YIELD in the United States with MODIS EVI and MACHINE LEARNING METHODS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048923897&doi=10.5194%2fisprs-annals-III-8-131-2016&partnerID=40&md5=196700ac1dc256f037765d263fc7a6af","Satellite remote sensing is commonly used to monitor crop yield in wide areas. Because many parameters are necessary for crop yield estimation, modelling the relationships between parameters and crop yield is generally complicated. Several methodologies using machine learning have been proposed to solve this issue, but the accuracy of county-level estimation remains to be improved. In addition, estimating county-level crop yield across an entire country has not yet been achieved. In this study, we applied a deep neural network (DNN) to estimate corn yield. We evaluated the estimation accuracy of the DNN model by comparing it with other models trained by different machine learning algorithms. We also prepared two time-series datasets differing in duration and confirmed the feature extraction performance of models by inputting each dataset. As a result, the DNN estimated county-level corn yield for the entire area of the United States with a determination coefficient (R2) of 0.780 and a root mean square error (RMSE) of 18.2 bushels/acre. In addition, our results showed that estimation models that were trained by a neural network extracted features from the input data better than an existing machine learning algorithm.","Artificial Neural Network; Corn Yield; Deep Learning; MODIS EVI; Support Vector Machine; Wavelet Transform",
"Deep learning for remote sensing data: A technical tutorial on the state of the art","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976384382&doi=10.1109%2fMGRS.2016.2540798&partnerID=40&md5=c63a9d2c22ef91e5096cb7f906e2f7f5","Deep-learning (DL) algorithms, which learn the representative and discriminative features in a hierarchical manner from the data, have recently become a hotspot in the machine-learning area and have been introduced into the geoscience and remote sensing (RS) community for RS big data analysis. Considering the low-level features (e.g., spectral and texture) as the bottom level, the output feature representation from the top level of the network can be directly fed into a subsequent classifier for pixel-based classification. As a matter of fact, by carefully addressing the practical demands in RS applications and designing the input""output levels of the whole network, we have found that DL is actually everywhere in RS data analysis: from the traditional topics of image preprocessing, pixel-based classification, and target recognition, to the recent challenging tasks of high-level semantic feature extraction and RS scene understanding. © 2013 IEEE.",,"Big data; Data handling; Data mining; Pixels; Remote sensing; Semantics; Discriminative features; Feature representation; High-level semantic features; Image preprocessing; Low-level features; Pixel based classifications; Remote sensing data; Scene understanding; Deep learning; data assimilation; geology; hierarchical system; machine learning; pixel; remote sensing; teaching"
"Spectral-spatial joint classification method of hyperspectral remote sensing image","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979603345&partnerID=40&md5=8be7e60f2485328f3fb342abefa9e80a","In remote sensing image research area, hyperspectral data classification is a hot topic. In recent years, many study methods for this issue emerge; however, the majority of the methods adopt the shallow layer method to extract the characteristics of original data. In this paper, the deep study method is introduced in the hyperspectral image classification; a new characteristic extraction method and image classification construction based on deep belief network (DBN) is proposed, and used in hyperspectral data analysis. The spectral-spatial feature extraction and classifier are combined together to achieve high classification accuracy. Experiment was carried out using the hyperspectral data; experiment results indicate that the proposed classifier is superior to some current advanced classification methods. In addition, this paper also reveals that the deep learning system has great potential in the study of hyperspectral image classification. © 2016, Science Press. All right reserved.","Deep belief network (DBN); Deep learning; Feature extraction (FE); Hyperspectral image classification","Classification (of information); Extraction; Feature extraction; Image processing; Image reconstruction; Remote sensing; Spectroscopy; Characteristic extraction; Classification accuracy; Deep belief network (DBN); Deep learning; Hyperspectral data analysis; Hyperspectral data classification; Hyperspectral image classification; Hyperspectral Remote Sensing Image; Image classification"
"Hyperspectral remote sensing image chassification using the stacked sparse autoencoder","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970021357&doi=10.3969%2fj.issn.1001-0548.2016.02.012&partnerID=40&md5=10dbdcb1dc167f53bac058660b9350be","To extract rich features of hyperspectral image, this study explores the deep features of the raw data by using a stacked sparse autoencoder in the deep learning theory. First we create a sparse expression of raw hyperspectral image using sparse autoencoder. Then a deep neural network generating the deep features of raw data is built through learning stacked sparse autoencoder layer by layer. In addition, the deep feature-related model parameters are precisely calibrated by the statistical learning algorithm of the support vector machine (SVM). The performance of the experiment indicates that the overall accuracy of classification model based on stacked sparse autoencoder reaches 87.82%, superior to other experimental methods. From our experiments, it follows that the deep learning theory and stacked sparse autoencoder are of high potential in hyperspectral remote sensing image classification. © 2016, Editorial Board of Journal of the University of Electronic Science and Technology of China. All right reserved.","Deep neural network; Feature extraction; Hyperspectral image classification; Stacked sparse autoencoder; Support vector machine (SVM)","Algorithms; Feature extraction; Image processing; Image reconstruction; Learning algorithms; Learning systems; Remote sensing; Spectroscopy; Support vector machines; Auto encoders; Deep neural networks; Experimental methods; Hyper-spectral images; Hyperspectral image classification; Hyperspectral Remote Sensing Image; Overall accuracies; Statistical learning; Image classification"
"Corrigendum to: Road Network Extraction: a Neural-Dynamic Framework Based on Deep Learning and a Finite State Mmachine(International Journal of Remote Sensing, (2015), 36, 12(3144-3169), Doi:10.1080/01431161.2015.1054049)","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046509626&doi=10.1080%2f01431161.2016.1162454&partnerID=40&md5=55a437c4dbdbc7cacf605d9ef2b32191",[No abstract available],,
"Novel segmented stacked autoencoder for effective dimensionality reduction and feature extraction in hyperspectral imaging","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961344134&doi=10.1016%2fj.neucom.2015.11.044&partnerID=40&md5=a7520477269edef3d8b7aaa932be459f","Stacked autoencoders (SAEs), as part of the deep learning (DL) framework, have been recently proposed for feature extraction in hyperspectral remote sensing. With the help of hidden nodes in deep layers, a high-level abstraction is achieved for data reduction whilst maintaining the key information of the data. As hidden nodes in SAEs have to deal simultaneously with hundreds of features from hypercubes as inputs, this increases the complexity of the process and leads to limited abstraction and performance. As such, segmented SAE (S-SAE) is proposed by confronting the original features into smaller data segments, which are separately processed by different smaller SAEs. This has resulted in reduced complexity but improved efficacy of data abstraction and accuracy of data classification. © 2016.","Data reduction; Deep learning (DL); Hyperspectral remote sensing; Segmented stacked autoencoder (S-SAE)","Abstracting; Classification (of information); Extraction; Feature extraction; Learning systems; Remote sensing; Spectroscopy; Auto encoders; Data classification; Deep learning; Dimensionality reduction; High-level abstraction; Hyperspectral Imaging; Hyperspectral remote sensing; Reduced complexity; Data reduction; Article; classification algorithm; controlled study; feature extraction; hyperspectral remote sensing; image processing; intermethod comparison; machine learning; measurement accuracy; priority journal; remote sensing; segmented stacked autoencoder"
"High spatial resolution remote sensing image classification based on deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964306850&doi=10.3788%2fAOS201636.0428001&partnerID=40&md5=732769f9ce9f51f35f8076d87c00e92f","A classification method based on deep learning is proposed for the classification of high spatial resolution remote sensing images. The texture features of the images are calculated through nonsubsampled contourlet transform, the deep learning common model-deep belief networks (DBN) are used to classify the high spatial resolution remote sensing images based on spectral and texture features. The proposed method is compared with the DBN classification method based on single spectral information, the support vector machine (SVM) method and the traditional neural network (NN) classification method. Experimental results show that comparing with the single spectral information, the use of spectral and texture information can effectively improve the classification accuracy of high spatial resolution remote sensing images, and comparing with methods of SVM and NN, the DBN method can accurately explore the distribution law of the high spatial resolution remote sensing images and improve the accuracy of classification. © 2016, Chinese Lasers Press. All right reserved.","Deep belief networks; Deep learning; High spatial resolution; Nonsubsampled contourlet transform; Remote sensing; Remote sensing image classification; Texture","Classification (of information); Image enhancement; Image reconstruction; Image resolution; Image texture; Information use; Remote sensing; Support vector machines; Textures; Deep belief networks; Deep learning; High spatial resolution; Non-sub-sampled contourlet transforms; Remote sensing image classification; Image classification"
"Large patch convolutional neural networks for the scene classification of high spatial resolution imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967166415&doi=10.1117%2f1.JRS.10.025006&partnerID=40&md5=1a711ffa9cee5a11893fd3fb92dc0941","The increase of the spatial resolution of remote-sensing sensors helps to capture the abundant details related to the semantics of surface objects. However, it is difficult for the popular object-oriented classification approaches to acquire higher level semantics from the high spatial resolution remote-sensing (HSR-RS) images, which is often referred to as the ""semantic gap."" Instead of designing sophisticated operators, convolutional neural networks (CNNs), a typical deep learning method, can automatically discover intrinsic feature descriptors from a large number of input images to bridge the semantic gap. Due to the small data volume of the available HSR-RS scene datasets, which is far away from that of the natural scene datasets, there have been few reports of CNN approaches for HSR-RS image scene classifications. We propose a practical CNN architecture for HSR-RS scene classification, named the large patch convolutional neural network (LPCNN). The large patch sampling is used to generate hundreds of possible scene patches for the feature learning, and a global average pooling layer is used to replace the fully connected network as the classifier, which can greatly reduce the total parameters. The experiments confirm that the proposed LPCNN can learn effective local features to form an effective representation for different land-use scenes, and can achieve a performance that is comparable to the state-of-the-art on public HSR-RS scene datasets. © 2016 Society of Photo-Optical Instrumentation Engineers (SPIE).","deep convolutional neural networks; high spatial resolution image; large patch sampling; remote sensing; scene classification","Classification (of information); Convolution; Image classification; Image resolution; Land use; Neural networks; Remote sensing; Semantics; Convolutional neural network; Fully connected networks; High spatial resolution; High spatial resolution imagery; High spatial resolution images; Object oriented classification; Remote sensing sensors; Scene classification; Deep neural networks"
"Research on marine floating raft aquaculture SAR image target recognition based on deep collaborative sparse coding network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966487149&doi=10.16383%2fj.aas.2016.c150425&partnerID=40&md5=c05c6b7bb3e7b28c0b9501ba57221f56","Floating raft aquaculture is widely distributed in the offshore ocean of China. Since raft information cannot be obtained accurately in the visible remote sensing image, active imaging images acquired from synthetic aperture radar (SAR) are applied. However, oceanic SAR images are seriously contaminated by speckle noise, and effective features of SAR images are deficient, which make recognition difficult. In order to overcome these problems, a deep collaborative sparse coding network (DCSCN) is proposed to extract features and conduct recognition automatically. The proposed method extracts texture features and contour features from the pre-processed image firstly. Then, it segments the image into patches and learns features of each patch collaboratively through the DCSCN network. The optimized features are used for recognition finally. Experiments on the artificial SAR image and the images of Beidaihe demonstrate that the proposed DCSCN network can accurately obtain the area of floating raft aquaculture. Since the network can learn discriminative features and integrate the correlated neighbor pixels, the DCSCN network improves the recognition accuracy and has better performance in overcoming the contamination of speckle noise. Copyright © 2016 Acta Automatica Sinica. All rights reserved.","Deep learning; Floating raft aquaculture; Sparse auto-encoders; Synthetic aperture radar (SAR); Target recognition","Aquaculture; Codes (symbols); Deep learning; Image coding; Network coding; Radar target recognition; Remote sensing; Speckle; Synthetic aperture radar; Auto encoders; Discriminative features; Floating rafts; Processed images; Recognition accuracy; Remote sensing images; Target recognition; Texture features; Radar imaging"
"Classification of Hyperspectral Remote Sensing Image Using Hierarchical Local-Receptive-Field-Based Extreme Learning Machine","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969338344&doi=10.1109%2fLGRS.2016.2517178&partnerID=40&md5=e8232ca89a318fc9ca53683e2a970cd0","This letter proposes a novel classification approach for a hyperspectral image (HSI) using a hierarchical local-receptive-field (LRF)-based extreme learning machine (ELM). As a fast and accurate pattern classification algorithm, ELM has been applied in numerous fields, including the HSI classification. The LRF concept originates from research in neuroscience. Considering the local correlations of spectral features, it is promising to improve the performance of HSI classification by introducing the LRFs. Recent research on deep learning has shown that hierarchical architectures with more layers can potentially extract abstract representation and invariant features for better classification performance. Therefore, we further extend the LRF-based ELM method to a hierarchical model for HSI classification. Experimental results on two widely used real hyperspectral data sets confirm the effectiveness of the proposed HSI classification approach. © 2015 IEEE.","Deep learning; extreme learningmachine (ELM); hyperspectral image(HSI) classification; local receptivefield(LRF)","Deep learning; Hierarchical systems; Image classification; Knowledge acquisition; Remote sensing; Spectroscopy; Abstract representation; Classification algorithm; Classification performance; Extreme learning machine; extreme learningmachine (ELM); Hierarchical architectures; Hyperspectral Remote Sensing Image; local receptivefield(LRF); Classification (of information)"
"Unsupervised deep feature extraction for remote sensing image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940417789&doi=10.1109%2fTGRS.2015.2478379&partnerID=40&md5=be2526034a35ba64c24a09896d7892e9","This paper introduces the use of single-layer and deep convolutional networks for remote sensing data analysis. Direct application to multi- and hyperspectral imagery of supervised (shallow or deep) convolutional networks is very challenging given the high input data dimensionality and the relatively small amount of available labeled data. Therefore, we propose the use of greedy layerwise unsupervised pretraining coupled with a highly efficient algorithm for unsupervised learning of sparse features. The algorithm is rooted on sparse representations and enforces both population and lifetime sparsity of the extracted features, simultaneously. We successfully illustrate the expressive power of the extracted representations in several scenarios: classification of aerial scenes, as well as land-use classification in very high resolution or land-cover classification from multi- and hyperspectral images. The proposed algorithm clearly outperforms standard principal component analysis (PCA) and its kernel counterpart (kPCA), as well as current state-of-the-art algorithms of aerial classification, while being extremely computationally efficient at learning representations of data. Results show that single-layer convolutional networks can extract powerful discriminative features only when the receptive field accounts for neighboring pixels and are preferred when the classification requires high resolution and detailed results. However, deep architectures significantly outperform single-layer variants, capturing increasing levels of abstraction and complexity throughout the feature hierarchy. © 1980-2012 IEEE.","Aerial image classification; Classification; Deep convolutional networks; Deep learning; Feature extraction; Hyperspectral (HS) image; Multispectral (MS) images; Segmentation; Sparse features learning; Very high resolution (VHR)","Algorithms; Complex networks; Convolution; Feature extraction; Image processing; Image reconstruction; Land use; Learning algorithms; Network layers; Principal component analysis; Remote sensing; Spectroscopy; Computationally efficient; Convolutional networks; Discriminative features; Hyper-spectral imageries; Land cover classification; Landuse classifications; Remote sensing image classification; State-of-the-art algorithms; Image classification; artificial neural network; complexity; image classification; multispectral image; principal component analysis; remote sensing; segmentation"
"Learning multiscale and deep representations for classifying remotely sensed imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956620231&doi=10.1016%2fj.isprsjprs.2016.01.004&partnerID=40&md5=ff8228b509a14fc238fcdcc0d8085386","It is widely agreed that spatial features can be combined with spectral properties for improving interpretation performances on very-high-resolution (VHR) images in urban areas. However, many existing methods for extracting spatial features can only generate low-level features and consider limited scales, leading to unpleasant classification results. In this study, multiscale convolutional neural network (MCNN) algorithm was presented to learn spatial-related deep features for hyperspectral remote imagery classification. Unlike traditional methods for extracting spatial features, the MCNN first transforms the original data sets into a pyramid structure containing spatial information at multiple scales, and then automatically extracts high-level spatial features using multiscale training data sets. Specifically, the MCNN has two merits: (1) high-level spatial features can be effectively learned by using the hierarchical learning structure and (2) multiscale learning scheme can capture contextual information at different scales. To evaluate the effectiveness of the proposed approach, the MCNN was applied to classify the well-known hyperspectral data sets and compared with traditional methods. The experimental results shown a significant increase in classification accuracies especially for urban areas. © 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).","Deep learning; Feature extraction; Multiscale convolutional neural network (MCNN); Remote sensing image classification","Convolution; Data mining; Feature extraction; Image classification; Image reconstruction; Neural networks; Remote sensing; Classification accuracy; Classification results; Contextual information; Convolutional neural network; Deep learning; Remote sensing image classification; Remotely sensed imagery; Very high resolution (VHR) image; Classification (of information); algorithm; data set; experimental study; hierarchical system; image analysis; image classification; imagery; learning; remote sensing; urban area"
"Efficient Saliency-Based Object Detection in Remote Sensing Images Using Deep Belief Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954133710&doi=10.1109%2fLGRS.2015.2498644&partnerID=40&md5=929ee0f99b1ec371286653f1d8126147","Object detection has been one of the hottest issues in the field of remote sensing image analysis. In this letter, an efficient object detection framework is proposed, which combines the strength of the unsupervised feature learning of deep belief networks (DBNs) and visual saliency. In particular, we propose an efficient coarse object locating method based on a saliency mechanism. The method could avoid an exhaustive search across the image and generate a small number of bounding boxes, which can locate the object quickly and precisely. After that, the trained DBN is used for feature extraction and classification on subimages. The feature learning of the DBN is operated by pretraining each layer of restricted Boltzmann machines (RBMs) using the general layerwise training algorithm. An unsupervised blockwise pretraining strategy is introduced to train the first layer of RBMs, which combines the raw pixels with a saliency map as inputs. This makes an RBM generate local and edge filters. The precise edge position information and pixel value information are more efficient to build a good model of images. Comparative experiments are conducted on the data set acquired by QuickBird with a 60-cm resolution. The results demonstrate the accuracy and efficiency of our method. © 2016 IEEE.","Deep belief networks (DBNs); object detection; visual saliency","Classification (of information); Deep learning; Feature extraction; Object recognition; Pixels; Remote sensing; Visualization; Comparative experiments; Deep belief networks; Efficient object detections; Feature extraction and classification; Remote sensing images; Restricted boltzmann machine; Unsupervised feature learning; Visual saliency; Object detection"
"Rotation-and-scale-invariant airplane detection in high-resolution satellite images based on deep-Hough-forests","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955144341&doi=10.1016%2fj.isprsjprs.2015.04.014&partnerID=40&md5=5f8e1d9b253939cd8038f41eb7f1b551","This paper proposes a rotation-and-scale-invariant method for detecting airplanes from high-resolution satellite images. To improve feature representation capability, a multi-layer feature generation model is created to produce high-order feature representations for local image patches through deep learning techniques. To effectively estimate airplane centroids, a Hough forest model is trained to learn mappings from high-order patch features to the probabilities of an airplane being present at specific locations. To handle airplanes with varying orientations, patch orientation is defined and integrated into the Hough forest to augment Hough voting. The scale invariance is achieved by using a set of scale factors embedded in the Hough forest. Quantitative evaluations on the images collected from Google Earth service show that the proposed method achieves a completeness, correctness, quality, and F1-measure of 0.968, 0.972, 0.942, and 0.970, respectively, in detecting airplanes with arbitrary orientations and sizes. Comparative studies also demonstrate that the proposed method outperforms the other three existing methods in accurately and completely detecting airplanes in high-resolution remotely sensed images. © 2015 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).","Airplane detection; Deep learning; High-resolution satellite imagery; Hough forest; Rotation and scale invariance","Aircraft; Forestry; Satellites; Airplane detections; Deep learning; High resolution satellite imagery; Hough forests; Scale invariance; Satellite imagery; aircraft; detection method; learning; mapping method; photogrammetry; probability; quantitative analysis; remote sensing; rotation; satellite imagery; spatial resolution; variance analysis"
"Geological Disaster Recognition on Optical Remote Sensing Images Using Deep Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984939548&doi=10.1016%2fj.procs.2016.07.144&partnerID=40&md5=337959612fbe6116efad442f420450bc","Geological disaster recognition, especially, landslide recognition, is of vital importance in disaster prevention, disaster monitoring and other applications. As more and more optical remote sensing images are available in recent years, landslide recognition on optical remote sensing images is in demand. Therefore, in this paper, we propose a deep learning based landslide recognition method for optical remote sensing images. In order to capture more distinct features hidden in landslide images, a particular wavelet transformation is proposed to be used as the preprocessing method. Next, a corrupting & denoising method is proposed to enhance the robustness of the model in recognize landslide features. Then, a deep auto-encoder network with multiple hidden layers is proposed to learn the high-level features and representations of each image. A softmax classifier is used for class prediction. Experiments are conducted on the remote sensing images from Google Earth. The experimental results indicate that the proposed wavDAE method outperforms the state-of-the-art classifiers both in efficiency and accuracy. © 2016 Published by Elsevier B.V.","deep learning; remote sensing image; target recognition","Disaster prevention; Disasters; Geology; Image reconstruction; Landslides; Deep learning; Disaster monitoring; Geological disaster; Optical remote sensing; Pre-processing method; Remote sensing images; Target recognition; Wavelet transformations; Remote sensing"
"Single-image super resolution for multispectral remote sensing data using convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978081674&doi=10.5194%2fisprsarchives-XLI-B3-883-2016&partnerID=40&md5=dadf2c4296fb6012db985cae90e14967","In optical remote sensing, spatial resolution of images is crucial for numerous applications. Space-borne systems are most likely to be affected by a lack of spatial resolution, due to their natural disadvantage of a large distance between the sensor and the sensed object. Thus, methods for single-image super resolution are desirable to exceed the limits of the sensor. Apart from assisting visual inspection of datasets, post-processing operations-e.g., segmentation or feature extraction-can benefit from detailed and distinguishable structures. In this paper, we show that recently introduced state-of-The-Art approaches for single-image super resolution of conventional photographs, making use of deep learning techniques, such as convolutional neural networks (CNN), can successfully be applied to remote sensing data. With a huge amount of training data available, end-To-end learning is reasonably easy to apply and can achieve results unattainable using conventional handcrafted algorithms. We trained our CNN on a specifically designed, domain-specific dataset, in order to take into account the special characteristics of multispectral remote sensing data. This dataset consists of publicly available SENTINEL-2 images featuring 13 spectral bands, a ground resolution of up to 10m, and a high radiometric resolution and thus satisfying our requirements in terms of quality and quantity. In experiments, we obtained results superior compared to competing approaches trained on generic image sets, which failed to reasonably scale satellite images with a high radiometric resolution, as well as conventional interpolation methods.","Convolutional Neural Networks; Deep Learning; Sentinel-2; Single-Image Super Resolution","Convolution; Feature extraction; Image resolution; Neural networks; Optical resolving power; Radiometry; Space optics; Convolutional neural network; Deep learning; Multispectral remote sensing; Optical remote sensing; Radiometric resolution; Sentinel-2; Single images; State-of-the-art approach; Remote sensing"
"Deep learning earth observation classification using ImageNet pretrained networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949921276&doi=10.1109%2fLGRS.2015.2499239&partnerID=40&md5=ff2a85f7ac783ad24b22d2ae3dc8b847","Deep learning methods such as convolutional neural networks (CNNs) can deliver highly accurate classification results when provided with large enough data sets and respective labels. However, using CNNs along with limited labeled data can be problematic, as this leads to extensive overfitting. In this letter, we propose a novel method by considering a pretrained CNN designed for tackling an entirely different classification problem, namely, the ImageNet challenge, and exploit it to extract an initial set of representations. The derived representations are then transferred into a supervised CNN classifier, along with their class labels, effectively training the system. Through this two-stage framework, we successfully deal with the limited-data problem in an end-to-end processing scheme. Comparative results over the UC Merced Land Use benchmark prove that our method significantly outperforms the previously best stated results, improving the overall accuracy from 83.1% up to 92.4%. Apart from statistical improvements, our method introduces a novel feature fusion algorithm that effectively tackles the large data dimensionality by using a simple and computationally efficient approach. © 2015 IEEE.","Convolutional neural networks (CNNs); Deep learning (DL); Feature extraction; Land-use classification; Pretrained network; Remote sensing (RS)","Land use; Neural networks; Classification results; Computationally efficient; Convolutional neural network; Earth observations; Feature fusion; Highly accurate; Large data dimensionalities; Overall accuracies; Classification (of information)"
"Domain adaptation based on deep denoising auto-encoders for classification of remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011416564&doi=10.1117%2f12.2241982&partnerID=40&md5=a1946406e447ffca26c8ae57c61cb20d","This paper investigates the effectiveness of deep learning (DL) for domain adaptation (DA) problems in the classification of remote sensing images to generate land-cover maps. To this end, we introduce two different DL architectures: 1) single-stage domain adaptation (SS-DA) architecture; and 2) hierarchal domain adaptation (H-DA) architecture. Both architectures require that a reliable training set is available only for one of the images (i.e., the source domain) from a previous analysis, whereas it is not for another image to be classified (i.e., the target domain). To classify the target domain image, the proposed architectures aim to learn a shared feature representation that is invariant across the source and target domains in a completely unsupervised fashion. To this end, both architectures are defined based on the stacked denoising auto-encoders (SDAEs) due to their high capability to define high-level feature representations. The SS-DA architecture leads to a common feature space by: 1) initially unifying the samples in source and target domains; and 2) then feeding them simultaneously into the SDAE. To further increase the robustness of the shared representations, the H-DA employs: 1) two SDAEs for learning independently the high level representations of source and target domains; and 2) a consensus SDAE to learn the domain invariant high-level features. After obtaining the domain invariant features through proposed architectures, the classifier is trained by the domain invariant labeled samples of the source domain, and then the domain invariant samples of the target domain are classified to generate the related classification map. Experimental results obtained for the classification of very high resolution images confirm the effectiveness of the proposed DL architectures. © 2016 SPIE.","deep learning; domain adaptation; image classification; remote sensing; stacked denoising auto-encoders","Architecture; Deep learning; Image classification; Image processing; Image reconstruction; Learning systems; Remote sensing; Signal processing; Space optics; Auto encoders; Classification of remote sensing image; Domain adaptation; Feature representation; High-level features; Proposed architectures; Shared representations; Very high resolution (VHR) image; Image denoising"
"A study on the recognition and classification method of high resolution remote sensing image based on deep belief network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010070449&doi=10.1007%2f978-981-10-3611-8_29&partnerID=40&md5=60e45678cf25bd1b1e1662ebff412d31","High resolution remote sensing images can describe the geometric features, spatial features and texture features of objects more accurately, which are widely used in various fields. How to get more useful information from the remote sensing image, and then the recognition and classification of the image from the information has become one of the hot spots in the field of high resolution remote sensing image research. Deep learning is a learning algorithm based on the depth network structure, which can better fit the intrinsic structure of the sample, compared with the traditional shallow classifier. Depth of learning in a deep belief network model is based on single-layer Boltzmann machine learning algorithm, each layer is made up of the generation and cognition, and make the bidirectional weight updatin g come true, the net output of each layer can be reduced to the input signal, so that the model can be infinitely close to the global optimum in the pre training stage. The author propose an improved dropout strategy based on the study of deep belief network model, this strategy only chooses partial local area data to zero out the weight at each time. It not only maintains the local information of the image itself, but also enhances the generalization ability of the model. The experimental results show that the improved dropout strategy improves about 2.5% of the classification accuracy, and it has better classification performance. © Springer Nature Singapore Pte Ltd. 2016.","Classification; Deep belief network; Dropout strategy; High resolution remote sensing image","Classification (of information); Computation theory; Image classification; Image reconstruction; Learning algorithms; Learning systems; Optical character recognition; Classification accuracy; Classification methods; Classification performance; Deep belief networks; Dropout strategy; Generalization ability; High resolution remote sensing images; Remote sensing images; Remote sensing"
"Learning to semantically segment high-resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019077911&doi=10.1109%2fICPR.2016.7900187&partnerID=40&md5=4a3c054d5aae8ffc4d675ccc46547721","Land cover classification is a task that requires methods capable of learning high-level features while dealing with high volume of data. Overcoming these challenges, Convolutional Networks (ConvNets) can learn specific and adaptable features depending on the data while, at the same time, learn classifiers. In this work, we propose a novel technique to automatically perform pixel-wise land cover classification. To the best of our knowledge, there is no other work in the literature that perform pixel-wise semantic segmentation based on data-driven feature descriptors for high-resolution remote sensing images. The main idea is to exploit the power of ConvNet feature representations to learn how to semantically segment remote sensing images. First, our method learns each label in a pixel-wise manner by taking into account the spatial context of each pixel. In a predicting phase, the probability of a pixel belonging to a class is also estimated according to its spatial context and the learned patterns. We conducted a systematic evaluation of the proposed algorithm using two remote sensing datasets with very distinct properties. Our results show that the proposed algorithm provides improvements when compared to traditional and state-of-the-art methods that ranges from 5 to 15% in terms of accuracy. © 2016 IEEE.","Deep Learning; Feature Learning; High-resolution Images; Land-cover Mapping; Pixel-wise Classification; Remote Sensing; Semantic Segmentation","Deep learning; Image segmentation; Mapping; Pattern recognition; Pixels; Semantics; Feature learning; Feature representation; High resolution image; High resolution remote sensing images; Land cover classification; Land cover mapping; Semantic segmentation; State-of-the-art methods; Remote sensing"
"Deep Learning for Hyperspectral Data Classification through Exponential Momentum Deep Convolution Neural Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994659865&doi=10.1155%2f2016%2f3150632&partnerID=40&md5=307233f510412f9f1c27e4a9a9e74c62","Classification is a hot topic in hyperspectral remote sensing community. In the last decades, numerous efforts have been concentrated on the classification problem. Most of the existing studies and research efforts are following the conventional pattern recognition paradigm, which is based on complex handcrafted features. However, it is rarely known which features are important for the problem. In this paper, a new classification skeleton based on deep machine learning is proposed for hyperspectral data. The proposed classification framework, which is composed of exponential momentum deep convolution neural network and support vector machine (SVM), can hierarchically construct high-level spectral-spatial features in an automated way. Experimental results and quantitative validation on widely used datasets showcase the potential of the developed approach for accurate hyperspectral data classification. © 2016 Qi Yue and Caiwen Ma.",,"Artificial intelligence; Complex networks; Convolution; Learning systems; Pattern recognition; Remote sensing; Support vector machines; Classification framework; Convolution neural network; Hyperspectral Data; Hyperspectral data classification; Hyperspectral remote sensing; Quantitative validation; Research efforts; Spatial features; Classification (of information)"
"Hyperspectral imagery classification using sparse representations of convolutional neural network features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962611241&doi=10.3390%2frs8020099&partnerID=40&md5=3aa500e045b5070c8a731aa3ba0b7ef3","In recent years, deep learning has been widely studied for remote sensing image analysis. In this paper, we propose a method for remotely-sensed image classification by using sparse representation of deep learning features. Specifically, we use convolutional neural networks (CNN) to extract deep features from high levels of the image data. Deep features provide high level spatial information created by hierarchical structures. Although the deep features may have high dimensionality, they lie in class-dependent sub-spaces or sub-manifolds. We investigate the characteristics of deep features by using a sparse representation classification framework. The experimental results reveal that the proposed method exploits the inherent low-dimensional structure of the deep features to provide better classification results as compared to the results obtained by widely-used feature exploration algorithms, such as the extended morphological attribute profiles (EMAPs) and sparse coding (SC). © 2016 by the authors.","Deep features; Deep learning; Remote sensing image classific; Sparse representation","Classification (of information); Convolution; Data mining; Image reconstruction; Neural networks; Remote sensing; Spectroscopy; Classification framework; Convolutional neural network; Deep features; Deep learning; Hyperspectral imagery classifications; Low dimensional structure; Remote sensing images; Sparse representation; Image classification"
"A fully automated pipeline for classification tasks with an application to remote sensing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978069289&doi=10.5194%2fisprsarchives-XLI-B3-923-2016&partnerID=40&md5=4f5529ccafaead13ce38fcc405570f93","Nowadays deep learning has been intensively in spotlight owing to its great victories at major competitions, which undeservedly pushed 'shallow' machine learning methods, relatively naive/handy algorithms commonly used by industrial engineers, to the background in spite of their facilities such as small requisite amount of time/dataset for training. We, with a practical point of view, utilized shallow learning algorithms to construct a learning pipeline such that operators can utilize machine learning without any special knowledge, expensive computation environment, and a large amount of labelled data. The proposed pipeline automates a whole classification process, namely feature-selection, weighting features and the selection of the most suitable classifier with optimized hyperparameters. The configuration facilitates particle swarm optimization, one of well-known metaheuristic algorithms for the sake of generally fast and fine optimization, which enables us not only to optimize (hyper)parameters but also to determine appropriate features/classifier to the problem, which has conventionally been a priori based on domain knowledge and remained untouched or dealt with naïve algorithms such as grid search. Through experiments with the MNIST and CIFAR-10 datasets, common datasets in computer vision field for character recognition and object recognition problems respectively, our automated learning approach provides high performance considering its simple setting (i.e. non-specialized setting depending on dataset), small amount of training data, and practical learning time. Moreover, compared to deep learning the performance stays robust without almost any modification even with a remote sensing object recognition problem, which in turn indicates that there is a high possibility that our approach contributes to general classification problems.","Automated Machine Learning; Classification; Feature Generation; Meta-heuristic Hyperparameter Optimization; Particle Swarm Optimization; UC Merced Land Use data set","Algorithms; Artificial intelligence; Automation; Character recognition; Classification (of information); Computer vision; Land use; Learning systems; Object recognition; Optimization; Particle swarm optimization (PSO); Personnel training; Pipelines; Remote sensing; Automated machines; Computation environments; Data set; Feature generation; Hyper-parameter optimizations; Machine learning methods; Meta heuristic algorithm; Object recognition problem; Learning algorithms"
"Change detection of hyperspectral remote sensing images based on deep belief network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005951121&partnerID=40&md5=8f09becb22c38fefb13c85ebfe8fa7ad","To address the performance bottlenecks of existing methods for change detection of hyperspectral remote sensing (HSRS) images, a new scheme for change detection of HSRS based on deep belief network (CDHSRS-DBN) is proposed. First, the HSRS images collected at two different phases (dual-temporal HSRS images) are pre-processed and registered, and then the spectral-difference images of the dual-temporal images are computed. Next, the endmember spectrums and the abundance-difference images are extracted using the pixel unmixing method based on independent component analysis (ICA). The low-layer feature vector of deep learning in CDHSRS-DBN adopts the integrated feature vector that consists of the pixel spectral-difference vector, endmember abundance-difference vector and the pixel spectral feature angle vector. Finally, a deep belief network (DBN) model that contains multi-layer restricted Boltzmann machines (RBM) and a support vector machine (SVM) is devised, and the weights of connections between visible and hidden layers are adjusted through pre-training. The accuracy of change detection is further improved by fine-tuning all the weights via the SVM classifier. In order to evaluate the performance of the proposed CDHSRS-DBN method, four pairs of EO-1 Hyperion test images at different phases, which collected in four different experimental zones, are used as the test data and CDHSRS-DBN is compared with six other typical HSRS change detection algorithms (CDHSRS-SCD, CDHSRS-MPD, CDHSRS-ICA, IR-MAD, CD-PCA and PCCD). The experiments focus on the detection of land-use changes. The average recall and precision reach 90.39% and 87.10%, respectively. The average value of F-Score and time consumption is 0.8871 and 242.5, respectively. Experimental results demonstrate the better performance of CDHSRS-DBN to detect changes of multi-temporal HSRS images accurately and efficiently. © 2016 CAFET-INNOVA TECHNICAL SOCIETY. All rights reserved.","Change detection; Deep belief network; Hyperspectral remote sensing images; Pixel unmixing; Restricted boltzmann machines","Image reconstruction; Image retrieval; Independent component analysis; Land use; Pixels; Principal component analysis; Remote sensing; Signal detection; Support vector machines; Vectors; Change detection; Deep belief networks; Hyperspectral Remote Sensing Image; Pixel unmixing; Restricted boltzmann machine; Image processing; algorithm; image analysis; multispectral image; numerical model; pixel; remote sensing; vector"
"Systematic infrared image quality improvement using deep learning based techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010888144&doi=10.1117%2f12.2242036&partnerID=40&md5=b251dfc7626ebb42cdf87602a4f179dd","Infrared thermography (IRT, or thermal video) uses thermographic cameras to detect and record radiation in the longwavelength infrared range of the electromagnetic spectrum. It allows sensing environments beyond the visual perception limitations, and thus has been widely used in many civilian and military applications. Even though current thermal cameras are able to provide high resolution and bit-depth images, there are significant challenges to be addressed in specific applications such as poor contrast, low target signature resolution, etc. This paper addresses quality improvement in IRT images for object recognition. A systematic approach based on image bias correction and deep learning is proposed to increase target signature resolution and optimise the baseline quality of inputs for object recognition. Our main objective is to maximise the useful information on the object to be detected even when the number of pixels on target is adversely small. The experimental results show that our approach can significantly improve target resolution and thus helps making object recognition more efficient in automatic target detection/recognition systems (ATD/R). © 2016 SPIE.","ADT/R; IRT; super-resolution; Super-Resolution Convolutional Neural Network (SRCNN)","Cameras; Infrared devices; Infrared imaging; Military applications; Neural networks; Optical resolving power; Remote sensing; Urban planning; ADT/R; Automatic target detection; Convolutional neural network; Electromagnetic spectra; Image quality improvements; Long-wavelength infrared ranges; Super resolution; Thermographic cameras; Object recognition"
"A preliminary study of the suitability of deep learning to improve LiDAR-derived biomass estimation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964001508&doi=10.1007%2f978-3-319-32034-2_49&partnerID=40&md5=1d2031e022072c6c16e1c091d4aafd86","Light Detection and Ranging (LiDAR) is a remote sensor able to extract three-dimensional information about forest structure. Biophysical models have taken advantage of the use of LiDAR-derived information to improve their accuracy. Multiple Linear Regression (MLR) is the most common method in the literature regarding biomass estimation to define the relation between the set of field measurements and the statistics extracted from a LiDAR flight. Unfortunately, there exist open issues regarding the generalization of models from one area to another due to the lack of knowledge about noise distribution, relationship between statistical features and risk of overfitting. Autoencoders (a type of deep neural network) has been applied to improve the results of machine learning techniques in recent times by undoing possible data corruption process and improving feature selection. This paper presents a preliminary comparison between the use of MLR with and without preprocessing by autoencoders on real LiDAR data from two areas in the province of Lugo (Galizia, Spain). The results show that autoencoders statistically increased the quality of MLR estimations by around 15–30%. © Springer International Publishing Switzerland 2016.","Deep learning; LiDAR; Regression; Remote sensing; Soft computing","Artificial intelligence; Intelligent systems; Learning systems; Linear regression; Remote sensing; Soft computing; Deep learning; Deep neural networks; Light detection and ranging; Machine learning techniques; Multiple linear regressions; Regression; Statistical features; Three-dimensional information; Optical radar"
"Deep Learning for Remote Sensing Image Understanding","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949292765&doi=10.1155%2f2016%2f7954154&partnerID=40&md5=15302c7ff42ef283a64a9bfb9e2dfedf",[No abstract available],,
"Classification of remote sensed images using random forests and deep learning framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011339638&doi=10.1117%2f12.2243169&partnerID=40&md5=411bed8bf4413cdc3d7b3d8392aa0965","In this paper, we explore the use of two machine learning algorithms: (a) random forest for structured labels and (b) fully convolutional neural network for the land cover classification of multi-sensor remote sensed images. In random forest algorithm, individual decision trees are trained on features obtained from image patches and corresponding patch labels. Structural information present in the image patches improves the classification performance when compared to just utilizing pixel features. Random forest method was trained and evaluated on the ISPRS Vaihingen dataset that consist of true ortho photo (TOP: near IR, R, G) and Digital Surface Model (DSM) data. The method achieves an overall accuracy of 86.3% on the test dataset. We also show qualitative results on a SAR image. In addition, we employ a fully convolutional neural network framework (FCN) to do pixel-wise classification of the above multi-sensor image. TOP and DSM data have individual convolutional layers with features fused before the fully convolutional layers. The network when evaluated on the Vaihingen dataset achieves an overall classification accuracy of 88%. © 2016 SPIE.","Deep Learning; Random Forest for structured label","Classification (of information); Convolution; Decision trees; Deep learning; Image processing; Learning algorithms; Learning systems; Neural networks; Pixels; Remote sensing; Signal processing; Statistical tests; Synthetic aperture radar; Classification accuracy; Classification performance; Convolutional neural network; Digital surface models; Land cover classification; Random forest algorithm; Random forests; Structural information; Image classification"
"Land cover changes analysis based on deep machine learning technique","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981350581&doi=10.1615%2fJAutomatInfScien.v48.i5.40&partnerID=40&md5=e8206887e516df21d1b3a618ec833c6d","The methodology for solving the problem of processing of large amount of remote sensing data is proposed. The hierarchical structure of the model of deep learning method is based on neural network approach and geospatial analysis methods. This methodology was applied for high resolution land cover change mapping for Ukraine territory from 1990 to 2010. The efficiency of this approach was shown for non-arable agricultural area and changes analysis, particularly, in the eastern regions during the occupation period. © 2016 by Begell House Inc.","Deep machine learning technique; Geospatial analysis methods; Land cover changes analysis; Large volumes satellite and spatial data; Neural network approach; Ukraine territory mapping.","Agricultural machinery; Agriculture; Artificial intelligence; Learning algorithms; Learning systems; Remote sensing; Geo-spatial analysis; Land-cover change; Machine learning techniques; Spatial data; Ukraine; Mapping"
"Deep subspace mapping in hyperspectral imaging","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011076087&doi=10.1117%2f12.2241771&partnerID=40&md5=f236dad0f8d7b7c843e5c2a716ab193a","We propose a novel Deep learning approach using autoencoders to map spectral bands to a space of lower dimensionality while preserving the information that makes it possible to discriminate different materials. Deep learning is a relatively new pattern recognition approach which has given promising result in many applications. In Deep learning a hierarchical representation of increasing level of abstraction of the features is learned. Autoencoder is an important unsupervised technique frequently used in Deep learning for extracting important properties of the data. The learned latent representation is a non-linear mapping of the original data which potentially preserve the discrimination capacity. © 2016 SPIE.","autoencoder; Deep learning; hyperspectral imaging; stacked autoencoder; subspace mapping","Hyperspectral imaging; Learning systems; Mapping; Pattern recognition; Remote sensing; Space optics; Spectroscopy; Auto encoders; Hierarchical representation; Learning approach; Level of abstraction; Nonlinear mappings; Spectral band; Unsupervised techniques; Deep learning"
"Convolutional neural network features based change detection in satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983078381&doi=10.1117%2f12.2243798&partnerID=40&md5=5d8a1f55807a23685e6e4cbe81fcde0f","With the popular use of high resolution remote sensing (HRRS) satellite images, a huge research efforts have been placed on change detection (CD) problem. An effective feature selection method can significantly boost the final result. While hand-designed features have proven difficulties to design features that effectively capture high and mid-level representations, the recent developments in machine learning (Deep Learning) omit this problem by learning hierarchical representation in an unsupervised manner directly from data without human intervention. In this letter, we propose approaching the change detection problem from a feature learning perspective. A novel deep Convolutional Neural Networks (CNN) features based HR satellite images change detection method is proposed. The main guideline is to produce a change detection map directly from two images using a pretrained CNN. This method can omit the limited performance of hand-crafted features. Firstly, CNN features are extracted through different convolutional layers. Then, a concatenation step is evaluated after an normalization step, resulting in a unique higher dimensional feature map. Finally, a change map was computed using pixel-wise Euclidean distance. Our method has been validated on real bitemporal HRRS satellite images according to qualitative and quantitative analyses. The results obtained confirm the interest of the proposed method. © 2016 SPIE.","Change Detection (CD); Convolutional Neural Network (CNN); High Resolution Remote Sensing (HRRS)","Artificial intelligence; Convolution; Learning systems; Neural networks; Pattern recognition; Remote sensing; Satellites; Signal detection; Change detection; Convolutional neural network; Feature selection methods; Hierarchical representation; High resolution remote sensing; Higher dimensional features; Mid-level representation; Qualitative and quantitative analysis; Feature extraction"
"Learning a transferable change rule from a recurrent neural network for land cover change detection","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974817496&doi=10.3390%2frs8060506&partnerID=40&md5=743464f17b05bfdcd1c96469a63ae4a0","When exploited in remote sensing analysis, a reliable change rule with transfer ability can detect changes accurately and be applied widely. However, in practice, the complexity of land cover changes makes it difficult to use only one change rule or change feature learned from a given multi-temporal dataset to detect any other new target images without applying other learning processes. In this study, we consider the design of an efficient change rule having transferability to detect both binary and multi-class changes. The proposed method relies on an improved Long Short-Term Memory (LSTM) model to acquire and record the change information of long-term sequence remote sensing data. In particular, a core memory cell is utilized to learn the change rule from the information concerning binary changes or multi-class changes. Three gates are utilized to control the input, output and update of the LSTM model for optimization. In addition, the learned rule can be applied to detect changes and transfer the change rule from one learned image to another new target multi-temporal image. In this study, binary experiments, transfer experiments and multi-class change experiments are exploited to demonstrate the superiority of our method. Three contributions of this work can be summarized as follows: (1) the proposed method can learn an effective change rule to provide reliable change information for multi-temporal images; (2) the learned change rule has good transferability for detecting changes in new target images without any extra learning process, and the new target images should have a multi-spectral distribution similar to that of the training images; and (3) to the authors' best knowledge, this is the first time that deep learning in recurrent neural networks is exploited for change detection. In addition, under the framework of the proposed method, changes can be detected under both binary detection and multi-class change detection.","Change detection; LSTMmodel; Recurrent neural network; Transferability;multi-spectral image","Bins; Complex networks; Image processing; Remote sensing; Signal detection; Spectroscopy; Change detection; Information concerning; Long short term memory; LSTMmodel; Multi-temporal image; Multispectral images; Remote sensing analysis; Remote sensing data; Recurrent neural networks"
"Determining feature extractors for unsupervised learning on satellite images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015208047&doi=10.1109%2fBigData.2016.7840908&partnerID=40&md5=71aed1fabc45baf78bff53f5b993855e","Advances in satellite imagery presents unprecedented opportunities for understanding natural and social phenomena at global and regional scales. Although the field of satellite remote sensing has evaluated imperative questions to human and environmental sustainability, scaling those techniques to very high spatial resolutions at regional scales remains a challenge. Satellite imagery is now more accessible with greater spatial, spectral and temporal resolution creating a data bottleneck in identifying the content of images. Because satellite images are unlabeled, unsupervised methods allow us to organize images into coherent groups or clusters. However, the performance of unsupervised methods, like all other machine learning methods, depends on features. Recent studies using features from pre-trained networks have shown promise for learning in new datasets. This suggests that features from pre-trained networks can be used for learning in temporally and spatially dynamic data sources such as satellite imagery. It is not clear, however, which features from which layer and network architecture should be used for learning new tasks. In this paper, we present an approach to evaluate the transferability of features from pre-trained Deep Convolutional Neural Networks for satellite imagery. We explore and evaluate different features and feature combinations extracted from various deep network architectures, and systematically evaluate over 2,000 network-layer combinations. In addition, we test the transferability of our engineered features and learned features from an unlabeled dataset to a different labeled dataset. Our feature engineering and learning are done on the unlabeled Draper Satellite Chronology dataset, and we test on the labeled UC Merced Land dataset to achieve near state-of-the-art classification results. These results suggest that even without any or minimal training, these networks can generalize well to other datasets. This method could be useful in the task of clustering unlabeled images and other unsupervised machine learning tasks. © 2016 IEEE.","convolutional neural networks; deep learning; remote sensing; satellite imagery; transfer learning","Artificial intelligence; Big data; Classification (of information); Convolution; Deep learning; Deep neural networks; Learning systems; Network architecture; Network layers; Neural networks; Remote sensing; Satellites; Statistical tests; Sustainable development; Classification results; Convolutional neural network; Environmental sustainability; Machine learning methods; Satellite remote sensing; Transfer learning; Unsupervised machine learning; Very high spatial resolutions; Satellite imagery"
"Knowledge based 3d building model recognition using convolutional neural networks from lidar and aerial imageries","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978062704&doi=10.5194%2fisprsarchives-XLI-B3-833-2016&partnerID=40&md5=8849924744b2c5930cd10cd35821b67d","In recent years, with the development of the high resolution data acquisition technologies, many different approaches and algorithms have been presented to extract the accurate and timely updated 3D models of buildings as a key element of city structures for numerous applications in urban mapping. In this paper, a novel and model-based approach is proposed for automatic recognition of buildings' roof models such as flat, gable, hip, and pyramid hip roof models based on deep structures for hierarchical learning of features that are extracted from both LiDAR and aerial ortho-photos. The main steps of this approach include building segmentation, feature extraction and learning, and finally building roof labeling in a supervised pre-Trained Convolutional Neural Network (CNN) framework to have an automatic recognition system for various types of buildings over an urban area. In this framework, the height information provides invariant geometric features for convolutional neural network to localize the boundary of each individual roofs. CNN is a kind of feed-forward neural network with the multilayer perceptron concept which consists of a number of convolutional and subsampling layers in an adaptable structure and it is widely used in pattern recognition and object detection application. Since the training dataset is a small library of labeled models for different shapes of roofs, the computation time of learning can be decreased significantly using the pre-Trained models. The experimental results highlight the effectiveness of the deep learning approach to detect and extract the pattern of buildings' roofs automatically considering the complementary nature of height and RGB information.","3D Building Model; Convolutional Neural Network; Deep Learning; LiDAR; Pattern Recognition","Aerial photography; Algorithms; Buildings; Convolution; Data acquisition; Feature extraction; Knowledge based systems; Neural networks; Object detection; Optical radar; Pattern recognition; Remote sensing; Roofs; 3D building models; Adaptable structures; Automatic recognition; Automatic recognition system; Convolutional neural network; Deep learning; Hierarchical learning; Model based approach; Three dimensional computer graphics"
"Semi-supervised classification of hyperspectral imagery based on stacked autoencoders","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000450755&doi=10.1117%2f12.2245011&partnerID=40&md5=d1f5c2f7038d66b15d6aba5a0a95c19d","Hyperspectral imagery has high spectral resolution, and spectrum of it has always been non-linear. The traditional classification methods cannot get better result when the number of samples is small. Combined with the theory of deep learning, a new semi-supervised method based on stacked autoencoders (SAE) is proposed for hyperspectral imagery classification. Firstly, with stacked autoencoders, a deep network model is constructed. Then, unsupervised pre-training is carried combined SOFTMAX classifier with unlabeled samples. Finally, fine-tuning the network model with small labeled samples, the SAE-based classifier can be got to learn implicit feature of spectrum of hyperspectral imagery and achieve classification of hyperspectral imagery. According to comparative experiments, the results indicate that the proposed method is effective to improve the hyperspectral imagery classification accuracy in case of small samples. © 2016 SPIE.","Hyperspectral imagery; Image classification; Stacked autoencoders","Image processing; Learning systems; Remote sensing; Spectral resolution; Spectroscopy; Supervised learning; Autoencoders; Classification methods; Comparative experiments; High spectral resolution; Hyper-spectral imageries; Hyperspectral imagery classifications; Semi-supervised classification; Semi-supervised method; Image classification"
"Scene classification in high resolution remotely sensed images based on PCANet","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992708872&doi=10.1007%2f978-3-319-45835-9_16&partnerID=40&md5=675f9d93d379636f69d6c5976ef3dbb0","Rich information provided by high resolution remotely sensed images allow us to classify scenes by understanding their spatial and structural patterns. The key of scene classification task with remotely sensed images lies in feature learning efficiency and invariant image representations. While deep neutral network-based approaches achieved good classification accuracy for remotely sensed images, they often have to train millions of parameters and involve heavily iterative computation. In this paper, we propose a new framework for scene classification based on a simple PCA Net which is introduced into high remotely sensed image classification for the first time. First, we verify the eligibility of PCA Net on classifying large scale scenes from high resolution remotely sensed images. Then we explore the impact of PCA Net parameters including filter size, number of filters, and block overlap ratio on classification accuracy. Lastly, we do comprehensive experiments with the public UC-Merced dataset to exemplify the effectiveness of the approach. Experimental results show that the proposed framework achieved on par with the state-of-the-art deep neutral network-based classification accuracy without training a huge amount of parameters. We demonstrate that the proposed classification framework can be highly effective in developing a classification system that can be used to automatically scan large-scale high resolution satellite imagery for classifying scenes. © Springer International Publishing Switzerland 2016.","Deep learning; Feature learning; High resolution remotely sensed image; Image classification; PCANet","Classification (of information); Data mining; Filtration; Information management; Remote sensing; Satellite imagery; Space division multiple access; Classification accuracy; Classification framework; Deep learning; Feature learning; High resolution remotely sensed images; High resolution satellite imagery; PCANet; Remotely sensed images; Image classification"
"A holistic approach for inspection of civil infrastructures based on computer vision techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979294557&doi=10.5194%2fisprsarchives-XLI-B5-131-2016&partnerID=40&md5=bb1011da667139abd94f5f94fe422adc","In this work, it is examined the 2D recognition and 3D modelling of concrete tunnel cracks, through visual cues. At the time being, the structural integrity inspection of large-scale infrastructures is mainly performed through visual observations by human inspectors, who identify structural defects, rate them and, then, categorize their severity. The described approach targets at minimum human intervention, for autonomous inspection of civil infrastructures. The shortfalls of existing approaches in crack assessment are being addressed by proposing a novel detection scheme. Although efforts have been made in the field, synergies among proposed techniques are still missing. The holistic approach of this paper exploits the state of the art techniques of pattern recognition and stereo-matching, in order to build accurate 3D crack models. The innovation lies in the hybrid approach for the CNN detector initialization, and the use of the modified census transformation for stereo matching along with a binary fusion of two state-of- The- Art optimization schemes. The described approach manages to deal with images of harsh radiometry, along with severe radiometric differences in the stereo pair. The effectiveness of this workflow is evaluated on a real dataset gathered in highway and railway tunnels. What is promising is that the computer vision workflow described in this work can be transferred, with adaptations of course, to other infrastructure such as pipelines, bridges and large industrial facilities that are in the need of continuous state assessment during their operational life cycle.","Automation; Cracks; Deep learning; Inspection; Matching; Recognition; Reconstruction","Automation; Computer vision; Crack detection; Cracks; Image reconstruction; Inspection; Life cycle; Pattern recognition; Radiometry; Remote sensing; Civil infrastructures; Computer vision techniques; Deep learning; Industrial facilities; Large scale infrastructures; Matching; Recognition; State-of-the-art techniques; Stereo image processing"
"A convolutional network for semantic facade segmentation and interpretation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978127779&doi=10.5194%2fisprsarchives-XLI-B3-709-2016&partnerID=40&md5=0124e38722a3734193bc19d4d63378a7","In this paper we present an approach for semantic interpretation of facade images based on a Convolutional Network. Our network processes the input images in a fully convolutional way and generates pixel-wise predictions. We show that there is no need for large datasets to train the network when transfer learning is employed, i. e., a part of an already existing network is used and fine-Tuned, and when the available data is augmented by using deformed patches of the images for training. The network is trained end-To-end with patches of the images and each patch is augmented independently. To undo the downsampling for the classification, we add deconvolutional layers to the network. Outputs of different layers of the network are combined to achieve more precise pixel-wise predictions. We demonstrate the potential of our network based on results for the eTRIMS (Korč and Förstner, 2009) dataset reduced to facades.","Convolutional Network; Deep Learning; Facade Interpretation; Object Detection; Segmentation","Convolution; Facades; Forecasting; Image segmentation; Object detection; Pixels; Remote sensing; Semantics; Convolutional networks; Deep learning; Different layers; Large datasets; Network process; Network-based; Semantic interpretation; Transfer learning; Network layers"
"A deep neural network modeling framework to reduce bias in satellite precipitation products","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961392473&doi=10.1175%2fJHM-D-15-0075.1&partnerID=40&md5=3fe822f55a1058a64c9d4d6448cc4ad6","Despite the advantage of global coverage at high spatiotemporal resolutions, satellite remotely sensed precipitation estimates still suffer from insufficient accuracy that needs to be improved for weather, climate, and hydrologic applications. This paper presents a framework of a deep neural network (DNN) that improves the accuracy of satellite precipitation products, focusing on reducing the bias and false alarms. The state-of-the-art deep learning techniques developed in the area of machine learning specialize in extracting structural information from a massive amount of image data, which fits nicely into the task of retrieving precipitation data from satellite cloud images. Stacked denoising autoencoder (SDAE), a widely used DNN, is applied to perform bias correction of satellite precipitation products. A case study is conducted on the Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks Cloud Classification System (PERSIANN-CCS) with spatial resolution of 0.08° × 0.08° over the central United States, where SDAE is used to process satellite cloud imagery to extract information over a window of 15 × 15 pixels. In the study, the summer of 2012 (June-August) and the winter of 2012/13 (December-February) serve as the training periods, while the same seasons of the following year (summer of 2013 and winter of 2013/14) are used for validation purposes. To demonstrate the effectiveness of the methodology outside the study area, three more regions are selected for additional validation. Significant improvements are achieved in both rain/no-rain (R/NR) detection and precipitation rate quantification: the results make 33% and 43% corrections on false alarm pixels and 98% and 78% bias reductions in precipitation rates over the validation periods of the summer and winter seasons, respectively. © 2016 American Meteorological Society.","Bias; Mathematical and statistical techniques; Neural networks; Observational techniques and algorithms; Remote sensing; Satellite observations","algorithm; artificial neural network; machine learning; pixel; remote sensing; satellite imagery; statistical analysis; United States"
"Hyperspectral classification via learnt features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956603880&doi=10.1109%2fICIP.2015.7351271&partnerID=40&md5=17661254b39ecba18fcfc2c8295a352f","This paper presents a new hyperspectral image (HSI) classification method which is capable of automatic feature learning while achieving high classification accuracy. The method contains two major modules: the spectral classification module and the spatial constraint module. Spectral classification module uses a deep network named stacked denoising autoencoders (SdA) to learn feature representation of the data. Through SdA, the data are projected nonlinearly from its original hyperspectral space to some higher dimensional space where more compact distribution is obtained. An interesting aspect of this method is that it does not need a feature design/extraction process guided by human prior. The suitable feature for the classification is learned by the deep network itself. Superpixel is utilized to generate the spatial constraints to refine the spectral classification results. By exploiting the spatial consistency of neighborhood pixels, the accuracy of classification is further improved by a big margin. Experiments on the public datasets reveal the superior performance of the proposed method. © 2015 IEEE.","Deep learning; hyperspectral image classification; remote sensing; stacked denoising autoencoders (SdA); superpixel",
"Multiview Deep Learning for Land-Use Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947868906&doi=10.1109%2fLGRS.2015.2483680&partnerID=40&md5=e1100cd72d54b4585c59166e0fe470a0","A multiscale input strategy for multiview deep learning is proposed for supervised multispectral land-use classification, and it is validated on a well-known data set. The hypothesis that simultaneous multiscale views can improve composition-based inference of classes containing size-varying objects compared to single-scale multiview is investigated. The end-to-end learning system learns a hierarchical feature representation with the aid of convolutional layers to shift the burden of feature determination from hand-engineering to a deep convolutional neural network (DCNN). This allows the classifier to obtain problem-specific features that are optimal for minimizing the multinomial logistic regression objective, as opposed to user-defined features which trade optimality for generality. A heuristic approach to the optimization of the DCNN hyperparameters is used, based on empirical performance evidence. It is shown that a single DCNN can be trained simultaneously with multiscale views to improve prediction accuracy over multiple single-scale views. Competitive performance is achieved for the UC Merced data set, where the 93.48% accuracy of multiview deep learning outperforms the 85.37% accuracy of SIFT-based methods and the 90.26% accuracy of unsupervised feature learning. © 2004-2012 IEEE.","Accuracy; Machine learning; Neural networks; Neurons; Remote sensing; Storage tanks; Training","Convolution; Heuristic methods; Land use; Neural networks; Optimization; Competitive performance; Convolutional neural network; Empirical performance; Hierarchical features; Landuse classifications; Multinomial logistic regression; Unsupervised feature learning; User-defined features; Classification (of information)"
"Automatic fusion and classification using random forests and features extracted with deep learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962541943&doi=10.1109%2fIGARSS.2015.7326432&partnerID=40&md5=0fb8e48c3338d159b280bd57d16cd604","Fusion of different sensor modalities has proven very effective in numerous remote sensing applications. However, in order to benefit from fusion, advanced feature extraction mechanisms that rely on domain expertise are typically required. In this paper we present an automated feature extraction scheme based on deep learning. The feature extraction is unsupervised and hierarchical. Furthermore, computational efficiency (often a challenge for deep learning methods) is a primary goal in order to make certain that the method can be applied in large remote sensing datasets. Promising classification results show the applicability of the approach for both reducing the gap between naive feature extraction and methods relying on domain expertise, as well as further improving the performance of the latter in two challenging datasets. © 2015 IEEE.",,
"A deep learning approach for unsupervised domain adaptation in multitemporal remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962484009&doi=10.1109%2fIGARSS.2015.7326293&partnerID=40&md5=f11ea2254b0f2f9e87c15e13f17a0bb3","In this paper, we propose a novel deep convex network method for domain adaptation in multitemporal remote sensing imagery. We fuse the capabilities of the extreme learning machine (ELM) classifier and local feature descriptor techniques to boost the classification accuracy. We use the Affine Scale Invariant Feature Transform (ASIFT) to extract the key points from the image pair, i.e. source and target domain images. The neural network consist of two layers, one layer uses the keypoints extracted by ASIFT to map the training points of the source image to the target image, while layer 2 is used for the purpose of classification. Experimental results obtained on multitemporal VHR images acquired by the IKONOS2 confirm the promising capability of the proposed method. © 2015 IEEE.","Affine Scale Invariant Feature Transform (SIFT); Domain Adaptation (DA); Extreme Learning Machine (ELM); Multitemporal VHR images",
"Deep hierarchical representation and segmentation of high resolution remote sensing images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962609582&doi=10.1109%2fIGARSS.2015.7326782&partnerID=40&md5=1da2fcc305e9c47f8595cff041693a7f","This paper presents a novel deep hierarchical representation and segmentation approach for high resolution remote sensing image understanding. An information extraction approach using deep hierarchical exploitation for remote sensing image is presented. The key idea is that we adopt a fast scanning image segmentation within a deep hierarchical feature representation framework, using a deep learning technique to split and merge over-segmented regions until they form meaningful objects. The contribution is to develop an effective procedure for multi-scale image representation to address the issue of information uncertainty in practical applications. We test our method on two optical high resolution remote sensing image datasets and produce promising experimental results in the form of multiple layer outputs, which confirm the effectiveness and robustness of the proposed procedure. © 2015 IEEE.","Hierarchical representation; High resolution remote sensing images; Image segmentation",
"Detection of seals in remote sensing images using features extracted from deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962512661&doi=10.1109%2fIGARSS.2015.7326163&partnerID=40&md5=56ed89549a010fae7d29cc819c916767","In this paper, we propose an algorithm for automatic detection of seals in aerial remote sensing images using features extracted from a pre-trained deep convolutional neural network (CNN). The method consists of three stages: (i) Detection of potential objects, (ii) feature extraction and (iii) classification of potential objects. The first stage is application dependent, with the aim of detecting all seal pups in the image, with the expense of detecting a large amount of false objects. The second stage extracts generic image features from a local image corresponding to each potential seal detected in the first stage using a CNN trained on the ImageNet database. In the third stage we apply a linear support vector machine to classify the feature vectors extracted in the second stage. The proposed method was demonstrated to an aerial image that contains 84 pups and 128 adult harp seals, and the results show that we are able to detect the seals with high accuracy (2.7% for the adults and 7.3% for the pups). We conclude that deep CNNs trained on the ImageNet database are well suited as a feature extraction module, and using a simple linear SVM, we were able to separate seals from other objects with very high accuracy. We believe that this methodology may be applied to other remote sensing object recognition tasks. © 2015 IEEE.","convolutional neural networks; deep learning; detection of seals; Object detection",
"Building detection in very high resolution multispectral data with deep learning features","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962523390&doi=10.1109%2fIGARSS.2015.7326158&partnerID=40&md5=85feaec346cd7de52e47f086861708b6","The automated man-made object detection and building extraction from single satellite images is, still, one of the most challenging tasks for various urban planning and monitoring engineering applications. To this end, in this paper we propose an automated building detection framework from very high resolution remote sensing data based on deep convolutional neural networks. The core of the developed method is based on a supervised classification procedure employing a very large training dataset. An MRF model is then responsible for obtaining the optimal labels regarding the detection of scene buildings. The experimental results and the performed quantitative validation indicate the quite promising potentials of the developed approach. © 2015 IEEE.","deep convolutional networks; extraction; ImageNet; Machine learning; man made objects",
"Deep feature representation for hyperspectral image classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962522930&doi=10.1109%2fIGARSS.2015.7326943&partnerID=40&md5=ffc9177189c8a2880985f9a6d63b1ad2","Hyperspectral data classification problems have been extensively studied in the past decade. However, well designed features and a robust classifier are still open issues that impact on the performance of an automatic land-cover classification system. In this paper, we propose a deep feature represenation method that generates very good features and a classifier for pixel-wise hyperspectral data classification. The proposed method has two main steps: principle components of the hyperspectral image cube is first filtered by three dimensional Gabor wavelets; second, stacked autoencoders are trained on the outputs of the previous step through unsupervised pre-training, finally deep neural network is trained on those stacked autoencoders. Experimental results obtained on real hyperspectral image confirmed the effectiveness of the proposed approach in favors of the high classification accuracy and computation efficiency. © 2015 IEEE.","classification; deep learning; hyperspectral image; remote sensing; stacked autoencoders",
"DeepSat - A learning framework for satellite imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961207640&doi=10.1145%2f2820783.2820816&partnerID=40&md5=94d886664aa478c91ed7063a3148887a","Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled highresolution dataset with multiple class labels. The contributions of this paper are twofold - (1) first, we present two new satellite datasets called SAT-4 and SAT-6, and (2) then, we propose a classification framework that extracts features from an input image, normalizes them and feeds the normalized feature vectors to a Deep Belief Network for classification. On the SAT-4 dataset, our best network produces a classification accuracy of 97.95% and outperforms three state-of-the-art object recognition algorithms, namely - Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by ∼11%. On SAT-6, it produces a classification accuracy of 93.9% and outperforms the other algorithms by ∼15%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques. A statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation substantiates the effectiveness of our approach in learning better representations for satellite imagery. © 2015 ACM.","Deep learning; High resolution; Satellite imagery","Artificial intelligence; Bayesian networks; Classification (of information); Computer vision; Decision trees; Geographic information systems; Information systems; Learning systems; Neural networks; Object recognition; Remote sensing; Satellite imagery; Satellites; Supervised learning; Classification framework; Convolutional neural network; Deep learning; High resolution; Intrinsic dimensionalities; Object recognition algorithm; Random forest classifier; Satellite image classification; Image classification"
"Deep Learning Based Feature Selection for Remote Sensing Scene Classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947127828&doi=10.1109%2fLGRS.2015.2475299&partnerID=40&md5=6d9ff5c177da68f8c3d50926a03564fa","With the popular use of high-resolution satellite images, more and more research efforts have been placed on remote sensing scene classification/recognition. In scene classification, effective feature selection can significantly boost the final performance. In this letter, a novel deep-learning-based feature-selection method is proposed, which formulates the feature-selection problem as a feature reconstruction problem. Note that the popular deep-learning technique, i.e., the deep belief network (DBN), achieves feature abstraction by minimizing the reconstruction error over the whole feature set, and features with smaller reconstruction errors would hold more feature intrinsics for image representation. Therefore, the proposed method selects features that are more reconstructible as the discriminative features. Specifically, an iterative algorithm is developed to adapt the DBN to produce the inquired reconstruction weights. In the experiments, 2800 remote sensing scene images of seven categories are collected for performance evaluation. Experimental results demonstrate the effectiveness of the proposed method. © 2004-2012 IEEE.","Feature extraction; Image reconstruction; Machine learning; Remote sensing; Satellites; Testing; Training","Algorithms; Classification (of information); Iterative methods; Remote sensing; Deep belief network (DBN); Discriminative features; Feature reconstruction; Feature selection methods; Feature selection problem; High resolution satellite images; Image representations; Reconstruction error; Feature extraction"
"Rotation-Invariant Object Detection in High-Resolution Satellite Imagery Using Superpixel-Based Deep Hough Forests","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947128048&doi=10.1109%2fLGRS.2015.2432135&partnerID=40&md5=e06a6e90b0e91b8aa83e8d4b3b9b28c5","This letter presents a rotation-invariant method for detecting geospatial objects from high-resolution satellite images. First, a superpixel segmentation strategy is proposed to generate meaningful and nonredundant patches. Second, a multilayer deep feature generation model is developed to generate high-level feature representations of patches using deep learning techniques. Third, a set of multiscale Hough forests with embedded patch orientations is constructed to cast rotation-invariant votes for estimating object centroids. Quantitative evaluations on the images collected from Google Earth service show that an average completeness, correctness, quality, and F1- measure values of 0.958, 0.969, 0.929, and 0.963, respectively, are obtained. Comparative studies with three existing methods demonstrate the superior performance of the proposed method in accurately and correctly detecting objects that are arbitrarily oriented and of varying sizes. © 2004-2012 IEEE.","Airplanes; Computational modeling; Marine vehicles; Object detection; Remote sensing; Satellites; Training","Forestry; Pixels; Satellite imagery; Comparative studies; Feature generation; Geo-spatial objects; High resolution satellite imagery; High resolution satellite images; High-level features; Quantitative evaluation; Superpixel segmentations; Object detection"
"Improving Spatial Feature Representation from Aerial Scenes by Using Convolutional Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959360430&doi=10.1109%2fSIBGRAPI.2015.39&partnerID=40&md5=e4deccfdccb58800a14fa00115b0d30d","The performance of image classification is highly dependent on the quality of extracted features. Concerning high resolution remote image images, encoding the spatial features in an efficient and robust fashion is the key to generating discriminatory models to classify them. Even though many visual descriptors have been proposed or successfully used to encode spatial features of remote sensing images, some applications, using this sort of images, demand more specific description techniques. Deep Learning, an emergent machine learning approach based on neural networks, is capable of learning specific features and classifiers at the same time and adjust at each step, in real time, to better fit the need of each problem. For several task, such image classification, it has achieved very good results, mainly boosted by the feature learning performed which allows the method to extract specific and adaptable visual features depending on the data. In this paper, we propose a novel network capable of learning specific spatial features from remote sensing images, with any pre-processing step or descriptor evaluation, and classify them. Specifically, automatic feature learning task aims at discovering hierarchical structures from the raw data, leading to a more representative information. This task not only poses interesting challenges for existing vision and recognition algorithms, but also brings huge opportunities for urban planning, crop and forest management and climate modelling. The propose convolutional neural network has six layers: three convolutional, two fully-connected and one classifier layer. So, the five first layers are responsible to extract visual features while the last one is responsible to classify the images. We conducted a systematic evaluation of the proposed method using two datasets: (i) the popular aerial image dataset UCMerced Land-use and, (ii) a multispectral high-resolution scenes of the Brazilian Coffee Scenes. The experiments show that the proposed method outperforms state-of-the-art algorithms in terms of overall accuracy. © 2015 IEEE.","Deep Learning; Feature Learning; High-resolution Images; Image Classification; Machine Learning; Remote Sensing","Artificial intelligence; Classification (of information); Convolution; Encoding (symbols); Image coding; Image processing; Image reconstruction; Land use; Learning systems; Network layers; Neural networks; Remote sensing; Convolutional networks; Convolutional neural network; Deep learning; Feature learning; Hierarchical structures; High resolution image; Machine learning approaches; State-of-the-art algorithms; Image classification"
"Hyperspectral image classification with convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962910954&doi=10.1145%2f2733373.2806306&partnerID=40&md5=58b1e76bb7984b3bb41cacc15dd6b8f1","Hyperspectral image (HSI) classification is one of the most widely used methods for scene analysis from hyperspectral imagery. In the past, many different engineered features have been proposed for the HSI classification problem. In this paper, however, we propose a feature learning approach for hyperspectral image classification based on convolutional neural networks (CNNs). The proposed CNN model is able to learn structured features, roughly resembling different spectral band-pass filters, directly from the hyperspectral in-put data. Our experimental results, conducted on a commonly-used remote sensing hyperspectral dataset, show that the proposed method provides classification results that are among the state-of-The-Art, without using any prior knowledge or engineered features. © 2015 ACM.","Classification; Convolutional Neural Networks; Deep Learning; Hyperspectral Imaging","Bandpass filters; Classification (of information); Convolution; Neural networks; Remote sensing; Spectroscopy; Classification results; Convolutional neural network; Deep learning; Feature learning; Hyper-spectral imageries; Hyperspectral image classification; Hyperspectral images; Hyperspectral Imaging; Image classification"
"A Hierarchical Oil Tank Detector with Deep Surrounding Features for High-Resolution Optical Satellite Imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940765289&doi=10.1109%2fJSTARS.2015.2467377&partnerID=40&md5=42786669c8b91b684ae8b26498a95098","Automatic oil tank detection plays a very important role for remote sensing image processing. To accomplish the task, a hierarchical oil tank detector with deep surrounding features is proposed in this paper. The surrounding features extracted by the deep learning model aim at making the oil tanks more easily to recognize, since the appearance of oil tanks is a circle and this information is not enough to separate targets from the complex background. The proposed method is divided into three modules: 1) candidate selection; 2) feature extraction; and 3) classification. First, a modified ellipse and line segment detector (ELSD) based on gradient orientation is used to select candidates in the image. Afterward, the feature combing local and surrounding information together is extracted to represent the target. Histogram of oriented gradients (HOG) which can reliably capture the shape information is extracted to characterize the local patch. For the surrounding area, the convolutional neural network (CNN) trained in ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) contest is applied as a blackbox feature extractor to extract rich surrounding feature. Then, the linear support vector machine (SVM) is utilized as the classifier to give the final output. Experimental results indicate that the proposed method is robust under different complex backgrounds and has high detection rate with low false alarm. © 2008-2012 IEEE.","Convolutional neural network (CNN); deep learning; ellipse and line segment detector (ELSD); oil tank detection; surrounding information","Complex networks; Complex networks; Feature extraction; Feature extraction; Image processing; Image processing; Image reconstruction; Image reconstruction; Image segmentation; Image segmentation; Neural networks; Neural networks; Optical data processing; Optical data processing; Remote sensing; Remote sensing; Satellite imagery; Satellite imagery; Support vector machines; Support vector machines; Tanks (containers); Tanks (containers); Candidate selection; Candidate selection; Convolutional neural network; Convolutional neural network; Gradient orientations; Gradient orientations; Histogram of oriented gradients (HOG); Histogram of oriented gradients (HOG); Linear Support Vector Machines; Linear Support Vector Machines; Optical satellite imagery; Optical satellite imagery; Remote sensing image processing; Remote sensing image processing; Surrounding feature; Surrounding feature; Oil tanks; Oil tanks; artificial neural network; detection method; histogram; image classification; image processing; remote sensing; satellite imagery; support vector machine"
"On combining multiscale deep learning features for the classification of hyperspectral remote sensing imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937137588&doi=10.1080%2f2150704X.2015.1062157&partnerID=40&md5=ce79da31d9881c4e301ed301bfb065a9","In recent years, satellite imagery has greatly improved in both spatial and spectral resolution. One of the major unsolved problems in highly developed remote sensing imagery is the manual selection and combination of appropriate features according to spectral and spatial properties. Deep learning framework can learn global and robust features from the training data set automatically, and it has achieved state-of-the-art classification accuracies over different image classification tasks. In this study, a technique is proposed which attempts to classify hyperspectral imagery by incorporating deep learning features. Firstly, deep learning features are extracted by multiscale convolutional auto-encoder. Then, based on the learned deep learning features, a logistic regression classifier is trained for classification. Finally, parameters of deep learning framework are analysed and the potential development is introduced. Experiments are conducted on the well-known Pavia data set which is acquired by the reflective optics system imaging spectrometer sensor. It is found that the deep learning-based method provides a more accurate classification result than the traditional ones. © 2015 Taylor & Francis.",,"Classification (of information); Image classification; Image enhancement; Remote sensing; Satellite imagery; Spectroscopy; Classification accuracy; Classification results; Hyper-spectral imageries; Hyperspectral remote sensing; Imaging spectrometers; Learning-based methods; Logistic regression classifier; Remote sensing imagery; Deep learning; image analysis; image classification; imagery; learning; sensor; spatial resolution; spectral resolution"
"Multiclass feature learning for hyperspectral image classification: Sparse and hierarchical solutions","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929495655&doi=10.1016%2fj.isprsjprs.2015.01.006&partnerID=40&md5=c812a01a361e8b99ee03aceb84da0d50","In this paper, we tackle the question of discovering an effective set of spatial filters to solve hyperspectral classification problems. Instead of fixing a priori the filters and their parameters using expert knowledge, we let the model find them within random draws in the (possibly infinite) space of possible filters. We define an active set feature learner that includes in the model only features that improve the classifier. To this end, we consider a fast and linear classifier, multiclass logistic classification, and show that with a good representation (the filters discovered), such a simple classifier can reach at least state of the art performances. We apply the proposed active set learner in four hyperspectral image classification problems, including agricultural and urban classification at different resolutions, as well as multimodal data. We also propose a hierarchical setting, which allows to generate more complex banks of features that can better describe the nonlinearities present in the data. © 2015 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).","Active set; Deep learning; Feature selection; Hierarchical feature extraction; Hyperspectral imaging; Multimodal","Classification (of information); Feature extraction; Spectroscopy; Active sets; Deep learning; Hierarchical feature extraction; Hyperspectral Imaging; Multi-modal; Image classification"
"Spectral-Spatial Classification of Hyperspectral Data Based on Deep Belief Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027942618&doi=10.1109%2fJSTARS.2015.2388577&partnerID=40&md5=9152fb3e8ff23bc855b8e8e4d54d7e47","Hyperspectral data classification is a hot topic in remote sensing community. In recent years, significant effort has been focused on this issue. However, most of the methods extract the features of original data in a shallow manner. In this paper, we introduce a deep learning approach into hyperspectral image classification. A new feature extraction (FE) and image classification framework are proposed for hyperspectral data analysis based on deep belief network (DBN). First, we verify the eligibility of restricted Boltzmann machine (RBM) and DBN by the following spectral information-based classification. Then, we propose a novel deep architecture, which combines the spectral-spatial FE and classification together to get high classification accuracy. The framework is a hybrid of principal component analysis (PCA), hierarchical learning-based FE, and logistic regression (LR). Experimental results with hyperspectral data indicate that the classifier provide competitive solution with the state-of-the-art methods. In addition, this paper reveals that deep learning system has huge potential for hyperspectral data classification. © 2008-2012 IEEE.","Deep belief network (DBN); deep learning; feature extraction (FE); hyperspectral data classification; logistic regression (LR); restricted Boltzmann machine (RBM); Support vector machine (SVM)","Classification (of information); Extraction; Feature extraction; Principal component analysis; Regression analysis; Remote sensing; Spectroscopy; Support vector machines; Deep belief network (DBN); Deep learning; Hyperspectral data classification; Logistic regressions; Restricted boltzmann machine; Image classification; artificial intelligence; image classification; regression analysis; remote sensing; spatial analysis; spectral analysis"
"Learning hierarchical features for automated extraction of road markings from 3-D mobile LiDAR point clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027949298&doi=10.1109%2fJSTARS.2014.2347276&partnerID=40&md5=e49a167deda428119d2e439f3296a209","This paper presents a novel method for automated extraction of road markings directly from three dimensional (3-D) point clouds acquired by a mobile light detection and ranging (LiDAR) system. First, road surface points are segmented from a raw point cloud using a curb-based approach. Then, road markings are directly extracted from road surface points through multisegment thresholding and spatial density filtering. Finally, seven specific types of road markings are further accurately delineated through a combination of Euclidean distance clustering, voxel-based normalized cut segmentation, large-size marking classification based on trajectory and curb-lines, and small-size marking classification based on deep learning, and principal component analysis (PCA). Quantitative evaluations indicate that the proposed method achieves an average completeness, correctness, and F-measure of 0.93, 0.92, and 0.93, respectively. Comparative studies also demonstrate that the proposed method achieves better performance and accuracy than those of the two existing methods. © 2014 IEEE.","Deep learning; mobile light detection and ranging (LiDAR); point cloud; road marking; three dimensional (3-D) extraction","Curbs; Extraction; Highway markings; Optical radar; Principal component analysis; Roads and streets; Transportation; Deep learning; Light detection and ranging; Point cloud; Road marking; Threedimensional (3-d); Road and street markings; image classification; lidar; pattern recognition; remote sensing; road; segmentation"
"Deep learning for extracting water body from landsat imagery","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958755668&partnerID=40&md5=5079094f8bad08923c4373bd9ea50833","There are regional limitations in traditional methods of water body extraction. For different terrain, all the methods rely heavily on c arefully hand-engineered feature selection and large amounts of prior knowledge. Due to the difficulty and high cost in acquiring, the labeled data of remote sensing is relatively small. Thus, there exist some challenges in the classification of huge amount of high dimension remote sensing data. Deep Learning has a good capacity of hierarchical feature learning from unlabeled data. Stacked sparse autoencoder (SSAE), one deep learning method, is widely investigated for image recognition. In this paper, a new water body extraction model based on SSAE is established. At first, current useful features (NDWI, NDVI, NDBI and so forth) are collected to construct unique feature matrix for each pixel. Next, a Feature Expansion Algorithm (FEA) is designed by taking account of the influence of neighboring pixels to expand feature matrixes. Setting the expansion features as inputs, SSAE is trained to extract water body. The experimental results showed that the proposed model outperformed Support Vector Machine (SVM) and traditional neural network (NN). Meanwhile, the proposed FEA explored more distinct features of water body so that the accuracy of water body extraction was improved to a great extent. © 2015.","Deep Learning; Feature expansion algorithm; Stacked sparse autoencoders; Unsupervised feature learning; Water body extraction","Image recognition; Learning systems; Pixels; Remote sensing; Support vector machines; Autoencoders; Deep learning; Different terrains; Hierarchical features; Neural network (nn); Remote sensing data; Unsupervised feature learning; Waterbodies; Extraction"
"Deep learning for multi-label land cover classification","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961674941&doi=10.1117%2f12.2195082&partnerID=40&md5=e067c2fa5a3dfe38a316e3a2fe81a7ff","Whereas single class classification has been a highly active topic in optical remote sensing, much less effort has been given to the multi-label classification framework, where pixels are associated with more than one labels, an approach closer to the reality than single-label classification. Given the complexity of this problem, identifying representative features extracted from raw images is of paramount importance. In this work, we investigate feature learning as a feature extraction process in order to identify the underlying explanatory patterns hidden in low-level satellite data for the purpose of multi-label classification. Sparse auto-encoders composed of a single hidden layer, as well as stacked in a greedy layer-wise fashion formulate the core concept of our approach. The results suggest that learning such sparse and abstract representations of the features can aid in both remote sensing and multi-label problems. The results presented in the paper correspond to a novel real dataset of annotated spectral imagery naturally leading to the multi-label formulation. © 2015 SPIE.","autoencoders; corine; deep learning; feature learning; modis; multi-label classification; Remote sensing; representation learning; sparse autoencoders","Classification (of information); Feature extraction; Image processing; Learning systems; Signal processing; Spectroscopy; Autoencoders; corine; Deep learning; Feature learning; modis; Multi label classification; representation learning; Remote sensing"
"Coffee crop recognition using multi-scale convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983627834&doi=10.1007%2f978-3-319-25751-8_9&partnerID=40&md5=b2fa9040058172c1ebec113614caaf1d","Identifying crops from remote sensing images is a fundamental to know and monitor land-use. However, manual identification is expensive and maybe impracticable given the amount data. Automatic methods, although interesting, are highly dependent on the quality of extracted features, since encoding the spatial features in an efficient and robust fashion is the key to generating discriminatory models. Even though many visual descriptors have been proposed or successfully used to encode spatial features, in some cases, more specific description are needed. Deep learning has achieved very good results in some tasks, mainly boosted by the feature learning performed which allows the method to extract specific and adaptable visual features depending on the data In this paper, we propose two multi-scale methods, based on deep learning, to identify coffee crops. Specifically, we propose the Cascade Convolutional Neural Networks, or simply CCNN, that identifies crops considering a hierarchy of networks and, also, propose the Iterative Convolutional Neural Network, called ICNN, which feeds a same network with data several times. We conducted a systematic evaluation of the proposed algorithms using a remote sensing dataset. The experiments show that the proposed methods outperform the baseline consistent of state-of-the-art components by a factor that ranges from 3 to 6%, in terms of average accuracy. © Springer International Publishing Switzerland 2015.","Coffee crop; Deep learning; Feature learning; Remote sensing","Convolution; Crops; Encoding (symbols); Image reconstruction; Iterative methods; Land use; Neural networks; Pattern recognition; Automatic method; Convolutional neural network; Deep learning; Feature learning; Manual identification; Multiscale method; Remote sensing images; Systematic evaluation; Remote sensing"
"Aircraft detection by deep convolutional neural networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930015347&doi=10.2197%2fipsjtcva.7.10&partnerID=40&md5=651e48c01a984d6e216445e12e655cd1","Features play crucial role in the performance of classifier for object detection from high-resolution remote sensing images. In this paper, we implemented two types of deep learning methods, deep convolutional neural network (DNN) and deep belief net (DBN), comparing their performances with that of the traditional methods (handcrafted features with a shallow classifier) in the task of aircraft detection. These methods learn robust features from a large set of training samples to obtain a better performance. The depth of their layers (>6 layers) grants them the ability to extract stable and large-scale features from the image. Our experiments show both deep learning methods reduce at least 40% of the false alarm rate of the traditional methods (HOG, LBP+SVM), and DNN performs a little better than DBN. We also fed some multi-preprocessed images simultaneously to one DNN model, and found that such a practice helps to improve the performance of the model obviously with no extra-computing burden adding. © 2015 Information Processing Society of Japan.","Deep belief nets; Deep convolutional neural networks; Object detection; Remote sensing","Convolution; Feature extraction; Image reconstruction; Learning systems; Neural networks; Object detection; Object recognition; Remote sensing; Training aircraft; Convolutional neural network; Deep belief nets; Deep learning; False alarm rate; High resolution remote sensing images; Performance of classifier; Training sample; Aircraft detection"
"Urban land use and land cover classification using remotely sensed sar data through deep belief networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939208923&doi=10.1155%2f2015%2f538063&partnerID=40&md5=dc30de1363e1be1a0ac83322b6f7ba99","Land use and land cover (LULC) mapping in urban areas is one of the core applications in remote sensing, and it plays an important role in modern urban planning and management. Deep learning is springing up in the field of machine learning recently. By mimicking the hierarchical structure of the human brain, deep learning can gradually extract features from lower level to higher level. The Deep Belief Networks (DBN) model is a widely investigated and deployed deep learning architecture. It combines the advantages of unsupervised and supervised learning and can archive good classification performance. This study proposes a classification approach based on the DBN model for detailed urban mapping using polarimetric synthetic aperture radar (PolSAR) data. Through the DBN model, effective contextual mapping features can be automatically extracted from the PolSAR data to improve the classification performance. Two-date high-resolution RADARSAT-2 PolSAR data over the Great Toronto Area were used for evaluation. Comparisons with the support vector machine (SVM), conventional neural networks (NN), and stochastic Expectation-Maximization (SEM) were conducted to assess the potential of the DBN-based classification approach. Experimental results show that the DBN-based method outperforms three other approaches and produces homogenous mapping results with preserved shape details. © 2015 Qi Lv et al.",,"Artificial intelligence; Image retrieval; Land use; Learning systems; Mapping; Maximum principle; Remote sensing; Stochastic systems; Support vector machines; Synthetic aperture radar; Urban planning; Classification approach; Classification performance; Deep belief network (DBN); Deep belief networks; Hierarchical structures; Land use and land cover; Polarimetric synthetic aperture radars; Stochastic expectation maximization; Classification (of information)"
"A deep belief network for classifying remotely-sensed hyperspectral data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952700936&doi=10.1007%2f978-3-319-27857-5_61&partnerID=40&md5=f75fd23fe97b968e2640c7a55ef6dc29","Improving the classification accuracy of remotely sensed data is of paramount interest for science and defense applications. In this paper, we investigate deep learning architectures (DLAs), whose popularity has grown recently due to the discovery of efficient algorithms to train them, one of which, unsupervised pre-training, seeks to initialize the learned model in a way that greatly encourages efficient supervised learning. We propose a structure for a DLA, the deep belief network (DBN), suitable for the classification of remotely-sensed hyperspectral data. To arrive at this structure, we first study the role of the DBN’s width and the duration of pre-training in the learning of features used for the multiclass discrimination of spectral data. We then study the effect of exploiting joint spectral-spatial information. The support vector machine (SVM) is used as a baseline to determine that the proposed method is feasible, offering consistently high classification accuracies in comparison. © Springer International Publishing Switzerland 2015.",,"Algorithms; Remote sensing; Support vector machines; Classification accuracy; Deep belief network (DBN); Deep belief networks; Deep learning; Hyperspectral Data; Remotely sensed data; Spatial informations; Spectral data; Classification (of information)"
"Proceedings of 2015 International Conference on Intelligent Computing and Internet of Things, ICIT 2015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938523389&partnerID=40&md5=29ed1ec5a96047108a3e2f841319dc41","The proceedings contain 35 papers. The topics discussed include: deep belief networks and deep learning; density induced p-norm support vector machine for binary classification; towards a hybrid approach of primitive cognitive network process and self-organizing map for computer product recommendation; an improved particle swarm optimization algorithm of radial basis neural network; learning methods of radial basis function neural network; a segmentation method for remote sensing image region on Riemannian manifolds; a new cooperative spectrum sensing with radio environment map in cognitive radio networks; probability distribution function based iris recognition boosted by the mean rule; watershed image segmentation algorithm base on particle swarm and region growing; and layered perceptual representation for shadow vision: from detection to removal.",,
"Unsupervised deep feature extraction of hyperspectral images","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038570341&doi=10.1109%2fWHISPERS.2014.8077647&partnerID=40&md5=84d9d444c57df1071424dddf6ead4a8b","This paper presents an effective unsupervised sparse feature learning algorithm to train deep convolutional networks on hyperspectral images. Deep convolutional hierarchical representations are learned and then used for pixel classification. Features in lower layers present less abstract representations of data, while higher layers represent more abstract and complex characteristics. We successfully illustrate the performance of the extracted representations in a challenging AVIRIS hyperspectral image classification problem, compared to standard dimensionality reduction methods like principal component analysis (PCA) and its kernel counterpart (kPCA). The proposed method largely outperforms the previous state-of-the-art results on the same experimental setting. Results show that single layer networks can extract powerful discriminative features only when the receptive field accounts for neighboring pixels. Regarding the deep architecture, we can conclude that: (1) additional layers in a deep architecture significantly improve the performance w.r.t. single layer variants; (2) the max-pooling step in each layer is mandatory to achieve satisfactory results; and (3) the performance gain w.r.t. the number of layers is upper bounded, since the spatial resolution is reduced at each pooling, resulting in too spatially coarse output features. © 2014 IEEE.","Convolutional networks; deep learning; feature extraction; hyperspectral image classification; sparse learning","Convolution; Deep learning; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Independent component analysis; Learning algorithms; Network architecture; Network layers; Pixels; Principal component analysis; Remote sensing; Spectroscopy; Abstract representation; Complex characteristics; Convolutional networks; Dimensionality reduction method; Discriminative features; Experimental settings; Hierarchical representation; sparse learning; Classification (of information)"
"Deep learning-based classification of hyperspectral data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905925092&doi=10.1109%2fJSTARS.2014.2329330&partnerID=40&md5=dc64b85b53248d485e4fc0d401a5f95a","Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed joint spectral-spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods' huge potential for accurate hyperspectral data classification. © 2014 IEEE.","Autoencoder (AE); deep learning; feature extraction; hyperspectral data classification; logistic regression; stacked autoencoder (SAE); support vector machine (SVM)","Feature extraction; Principal component analysis; Regression analysis; Support vector machines; Auto encoders; Classification accuracy; Competitive performance; Deep learning; Hyperspectral data classification; Hyperspectral remote sensing; Logistic regressions; Principle component analysis; Network architecture; accuracy assessment; artificial intelligence; artificial neural network; logistics; principal component analysis; regression analysis; remote sensing; spectral analysis"
"Remote sensing image classification based on DBN model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908054616&doi=10.7544%2fissn1000-1239.2014.20140199&partnerID=40&md5=fca5ddbd29cc958e7167e4b3f658f48f","Remote sensing image classification is one of the key technologies in geographic information system (GIS), and it plays an important role in modern urban planning and management. In the field of machine learning, deep learning is springing up in recent years. By mimicking the hierarchical structure of human brain, deep learning can extract features from lower level to higher level gradually, and distill the spatio-temporal regularizes of input data, thus improve the classification performance. Deep belief network (DBN) is a widely investigated and deployed deep learning model. It combines the advantages of unsupervised and supervised learning, and can archive good classification performance for high-dimensional data. In this paper, a remote sensing image classification method based on DBN model is proposed. This is one of the first attempts to apply deep learning approach to urban detailed classification. Six-day high-resolution RADARSAT-2 polarimetric synthetic aperture radar (SAR) data were used for evaluation. Experimental results show that the proposed method can outperform SVM (support vector machine) and traditional neural network (NN).","Deep belief network (DBN); Deep learning; Land cover classification; Remote sensing image; Restricted Boltzmann machine (RBM); Synthetic aperture radar (SAR)","Artificial intelligence; Classification (of information); Geographic information systems; Image reconstruction; Image retrieval; Information management; Radar; Remote sensing; Support vector machines; Synthetic aperture radar; Deep belief network (DBN); Deep learning; Land cover classification; Remote sensing images; Restricted boltzmann machine; Image classification"
"Spectral-spatial classification of hyperspectral image using autoencoders","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899107237&doi=10.1109%2fICICS.2013.6782778&partnerID=40&md5=fac8b4cf0dac8cd504c9c7a86fd77e1a","Hyperspectral image (HSI) classification is a hot topic in the remote sensing community. This paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM. © 2013 IEEE.","Autoencoders; Deep learning; Hyperspectral; Image classification; Neural networks; Stacked autoencoders","Feature extraction; Image classification; Neural networks; Signal processing; Spectroscopy; Autoencoders; Classification accuracy; Deep learning; Hyper-spectral images; HyperSpectral; Spectral dimensions; Spectral information; Spectral-spatial classification; Learning systems"
"Deep learning in very high resolution remote sensing image information mining communication concept","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869762769&partnerID=40&md5=7b92e3745bf25fbc2d5f51672f5a14cf","This paper presents the image information mining based on a communication channel concept. The feature extraction algorithms encode the image, while an analysis of topic discovery will decode and send its content to the user in the shape of a semantic map. We consider this approach for a real meaning based semantic annotation of very high resolution remote sensing images. The scene content is described using a multi-level hierarchical information representation. Feature hierarchies are discovered considering that higher levels are formed by combining features from lower level. Such a level to level mapping defines our methodology as a deep learning process. The whole analysis can be divided in two major learning steps. The first one regards the Bayesian inference to extract objects and assign basic semantic to the image. The second step models the spatial interactions between the scene objects based on Latent Dirichlet Allocation, performing a high level semantic annotation. We used a WorldView2 image to exemplify the processing results. © 2012 EURASIP.","deep learning; Information theory; semantic annotation","Bayesian inference; Deep learning; Feature extraction algorithms; Hierarchical information; High level semantics; Image information mining; Latent Dirichlet allocation; Level mapping; Scene object; Semantic annotations; Semantic map; Spatial interaction; Topic Discovery; Very high resolution; Bayesian networks; Feature extraction; Image reconstruction; Indexing (of information); Inference engines; Information theory; Signal processing; Statistics; Semantics"
"Working together for better student learning: A multi-university, multi-federal partner program for asynchronous learning module development for radar-based remote sensing systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955470369&doi=10.1109%2fTE.2009.2032167&partnerID=40&md5=7315df9a40ce4a6a20127d43be326b3f","Students are not exposed to enough real-life data. This paper describes how a community of scholars seeks to remedy this deficiency and gives the pedagogical details of an ongoing project that commenced in the Fall 2004 semester. Fostering deep learning, this multiyear project offers a new active-learning, hands-on interdisciplinary laboratory program in which engineering, geoscience, and meteorology students are encouraged to participate actively. Storms, tornadoes, and hazardous weather cause damage and loss that could be minimized through enhanced radar technologies and longer warning lead times. To study these topics, the program has generated a unique, interdisciplinary research-oriented learning environment that will train future engineers and meteorologists in the full set of competencies needed to take raw radar data and transform them into meaningful interpretations of weather phenomena. © 2006 IEEE.","Active learning; educational modules; Internet; radar systems; remote sensing; weather environment","Active learning; Asynchronous learning; Deep learning; educational modules; Geosciences; Interdisciplinary research; Lead time; Learning environments; Multiyear projects; Radar data; Radar technology; Real life data; Remote sensing system; Student learning; Weather phenomena; Internet; Metadata; Meteorological problems; Meteorological radar; Radar; Remote sensing; Storms; Students; Wavelet transforms; Radar systems"
"Physics and dynamics of clouds and precipitation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922897133&doi=10.1017%2fCBO9780511794285&partnerID=40&md5=011553aaae338e8b7f76fea1d5bfffe3","This key new textbook provides a state-of-the-art view of the physics of cloud and precipitation formation, covering the most important topics in the field: The microphysics, thermodynamics and cloud-scale dynamics. Highlights include: The condensation process explained with new insights from chemical physics studies; the impact of the particle curvature (the Kelvin equation) and solute effect (the Kohler equation); homogeneous and heterogeneous nucleation from recent molecular dynamic simulations; and the hydrodynamics of falling hydrometeors and their impact on collision growth. 3D cloud-model simulations demonstrate the dynamics and microphysics of deep convective clouds and cirrus formation, and each chapter contains problems enabling students to review and implement their new learning. Packed with detailed mathematical derivations and cutting-edge stereographic illustrations, this is an ideal text for graduate and advanced undergraduate courses, and also serves as a reference for academic researchers and professionals working in atmospheric science, meteorology, climatology, remote sensing and environmental science. © Pao K. Wang 2013.",,"Atmospheric thermodynamics; Clouds; Deep learning; Dynamics; Education; Molecular dynamics; Nucleation; Thermodynamics; Atmospheric science; Condensation process; Deep convective clouds; Environmental science; Heterogeneous nucleation; Mathematical derivation; Precipitation formation; Undergraduate Courses; Precipitation (meteorology)"
"An empirical study on the correlation between project-based learning and deep approach of learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350675976&doi=10.1109%2fETTandGRS.2008.88&partnerID=40&md5=265cf8b1e32a78f11fae8d6252d8d8b3","Project-based learning, a constructivist teaching method, is gaining in popularity in language classrooms. This paper explores the influence of project-based learning on the deep approach of learning among language learners. Through an empirical study combining qualitative and quantitative research methods, it is revealed that project-based learning has a positive effect on the deep motivation and deep strategy of language learners, and thus contribute to improved deep approach of learning. © 2008 IEEE.","Deep approach; Project-based learning; SPQ","Geology; Remote sensing; Deep approach; Empirical studies; Project based learning; Quantitative research methods; Teaching methods; Deep learning"
"A progress report on a hands-on interdisciplinary program for severe weather and next-generation multi-function radar","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029117925&partnerID=40&md5=acd1b56e9f20f6723cf97d5079017a5b","Through the team's community of scholars, this paper describes the pedagogical details of an on-going NSF Department of Undergraduate Education (DUE) project that commenced in the fall 2004 semester. Fostering deep learning, this multi-year project offers a new active-learning and hands-on laboratory program that is interdisciplinary, in which engineering, geoscience, and meteorology students are encouraged to actively participate. As discussed in a report by the Bureau of Economic Analysis, about 1/3 of the nation's $10 trillion dollar economy is influenced by weather. Storm cells, tornadoes, and hazardous weather cause damage and loss that could be minimized through enhanced radar and longer warning lead times. To study these topics, the program has generated a unique, interdisciplinary research-oriented learning environment that will train future engineers and meteorologists in the full set of competencies needed to take raw radar data and transform it into meaningful interpretations of weather phenomena. The heart of the program is the development, implementation and refinement of a set of several undergraduate courses and laboratory modules that are offered by the Schools of Meteorology and Electrical & Computer Engineering, that provides hands-on experiences in the special knowledge and skills necessary for organizing real-time weather data, improving and preparing that data for display, and interpreting its meteorological and scientific significance. In addition, programs for middle school teachers have been generated for the purpose of increasing their students' interest in science and engineering prior to entering college. The principal investigators have partnered with a major statewide climatology office during the summers of 2005, 2006, 2007, and 2008 to adapt and implement project materials directly to middle school teachers via its Earthstorm outreach program. Finally, an assessment plan has been devised by a nationally known expert who specializes in learning and course development. There are several special features in this research-oriented teaching program, including: (1) it is the only program in the country with a full and equal collaboration between the School of Meteorology and the School of Electrical & Computer Engineering for the purpose of adding strength to an existing, successfully integrated curriculum on weather radar, (2) it has access to weather data from the recently constructed National Weather Radar Testbed (NWRT). Students have a unique opportunity to take advantage of the weather data derived from the new phased array radar, specifically suited for weather observations. By placing the radar's data on the Internet in parallel with a set of learning modules, a diverse population of students will be able to use this state-of-the-art facility. To broaden the richness of the students? learning experiences, data from other remote sensors, such as profilers, conventional dish antennas, mobile radars, and the like are available. In-situ sensors, such as distrometers, also play an important role in the overall suite of atmospheric instrumentation. © American Society for Engineering Education, 2008.",,"Antenna phased arrays; Computer aided instruction; Curricula; Economic analysis; Education; Engineering education; Meteorological problems; Meteorological radar; Meteorology; Parabolic antennas; Population statistics; Radar; Radar antennas; Remote sensing; Students; Testbeds; Bureau of Economic Analysis; Interdisciplinary programs; Interdisciplinary research; Principal investigators; Research-oriented teaching; Science and engineering; State-of-the-art facility; Undergraduate education; Teaching"
